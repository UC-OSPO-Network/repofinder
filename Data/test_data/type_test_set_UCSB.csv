id,node_id,name,full_name,private,owner,html_url,description,fork,created_at,updated_at,pushed_at,homepage,size,stargazers_count,watchers_count,language,has_issues,has_projects,has_downloads,has_wiki,has_pages,has_discussions,forks_count,archived,disabled,open_issues_count,license,allow_forking,is_template,web_commit_signoff_required,visibility,forks,open_issues,watchers,default_branch,score,organization,readme,project_type,,contributors,manual_label,prediction_nn,prediction,release_downloads,code_of_conduct,contributing,security_policy,issue_templates,pull_request_template,subscribers_count,ai_prediction
176812190,MDEwOlJlcG9zaXRvcnkxNzY4MTIxOTA=,ictf2019,0-bit-adders/ictf2019,0,0-bit-adders,https://github.com/0-bit-adders/ictf2019,Write-ups and scripts for USCB iCTF 2019. https://itcf.cs.ucsb.edu,0,2019-03-20 20:30:48+00:00,2019-03-20 20:30:51+00:00,2019-03-20 20:30:49+00:00,,1,0,0,,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,0,master,1,1,"# ictf2019
Write-ups and scripts for USCB iCTF 2019. https://itcf.cs.ucsb.edu
",OTHER,,['3ch01c'],,,1,0,,,,,,1,0.8
523817403,R_kgDOHzjRuw,acostauribe,acostauribe/acostauribe,0,acostauribe,https://github.com/acostauribe/acostauribe,Config files for my GitHub profile.,0,2022-08-11 17:48:16+00:00,2022-08-11 17:48:16+00:00,2024-01-09 23:31:51+00:00,https://github.com/acostauribe,7,0,0,,0,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"I‚Äôm **Juliana @acostauribe** and am a results-driven researcher with a strong background in neurogenetic research, bioinformatics, and collaborative projects. I am M.D. with a Ph.D. in Molecular, Cellular, and Developmental Biology, and I am committed to making significant contributions to the field of neurodegenerative diseases.

üîç Current Role:
As an Assistant Researcher at the Neuroscience Research Institute, UC Santa Barbara, I am immersed in cutting-edge research aimed at unraveling the complex genetics behind Alzheimer's disease and related dementias. My main interests are Medical and Population Genetics and my work focuses on analyzing genetic data from Latin American populations and collaborating with multi-disciplinary teams to drive impactful outcomes.

üí° Mentorship and Community Involvement:
Beyond my research, I am a dedicated mentor and actively contribute to academic and professional communities. My involvement in various organizations, and leading the Computational Genetics training for the Multi-Partner Consortium to Expand Dementia Research in Latin America (ReD-Lat), underscores my commitment to collaborative progress.

I am excited about connecting with fellow researchers, professionals, and individuals who share a passion for coding and unraveling the mysteries of neurodegeneration and making a meaningful difference. Let's collaborate, innovate, build capacity and drive positive change together!

#Neuroscience #Genetics #Bioinformatics #Research #Mentorship #BuildingCapacity
",OTHER,,['acostauribe'],,,1,0,,,,,,1,0.8
529983036,R_kgDOH5bmPA,adelaiderobinson,adelaiderobinson/adelaiderobinson,0,adelaiderobinson,https://github.com/adelaiderobinson/adelaiderobinson,,0,2022-08-28 21:24:14+00:00,2022-08-28 21:24:14+00:00,2023-10-24 23:30:05+00:00,,9,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"
### :sunflower: Hello I'm Adelaide! 
pronouns: (she/her)

:book:  Recent graduate of the Master of Environmental Data Science (MEDS) Program at UC Santa Barbara within the Bren School of Environmental Science & Management

üê≥ Recently worked on the [Ocean Health Index](https://www.oceanhealthindex.org/) project for the [National Center for Ecological Analysis & Synthesis](https://www.nceas.ucsb.edu/)

:fish: My background is in fisheries monitoring and conservation

:snail: My long-term goal is to use data science to protect endangered species and ecosystems 
### Fun Extras
:hiking_boot: I love going on outdoor adventures

:rabbit: Bunny mom 


<!--
**adelaiderobinson/adelaiderobinson** is a ‚ú® _special_ ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
- üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->
",OTHER,,['adelaiderobinson'],,,1,0,,,,,,1,0.6
392228282,MDEwOlJlcG9zaXRvcnkzOTIyMjgyODI=,PromotionskollegModule6800_2021,agpo-ilr-uni-bonn/PromotionskollegModule6800_2021,0,agpo-ilr-uni-bonn,https://github.com/agpo-ilr-uni-bonn/PromotionskollegModule6800_2021,,0,2021-08-03 07:23:36+00:00,2021-08-27 11:04:14+00:00,2021-08-27 10:23:10+00:00,,837,4,4,Jupyter Notebook,1,1,1,1,0,0,7,0,0,0,,1,0,0,public,7,0,4,main,1,1,"# PromotionskollegModule6800_2021
Course material and links for Promotionskolleg module ""Machine learning in applied economics""
### Instructors 
Kathy Baylis - University of California, Santa Barbara, USA

Thomas Heckelei - University of Bonn, Germany

Hugo Storm - University of Bonn, Germany

### Links to Intro material

- [Intro slides](https://docs.google.com/presentation/d/1y5iono-CsO2mPmB-yU7PqYTYPmlMHBLME5afiJw-1Ls/edit?usp=sharing)
- [Intro jupyter notebook](https://github.com/agpo-ilr-uni-bonn/PromotionskollegModule6800_2021/blob/main/labIntro.ipynb)

### Links to Day 1 material 

- [Day 1 slides](https://docs.google.com/presentation/d/1K8LWWnomUNXvcnbXxn3eFeeFMTlWYaGFrwLrW0ifjBQ/edit?usp=sharing)
- [Day 1 video part I - Intro to ML with OLS](https://youtu.be/0dJmWUDkzNY)
- [Day 1 video part II - Overfitting and train/test split](https://youtu.be/_kFbZaCVNlc)
- [Day 1 video part III - Penalized Regression](https://youtu.be/6KofZlteszw)
- [Day 1 jupyter notebook for lecture and lab](https://github.com/agpo-ilr-uni-bonn/PromotionskollegModule6800_2021/blob/main/6800_Day1.ipynb)

### Links to Day 2 material 

- [Day 2 slides](https://docs.google.com/presentation/d/1mbZbCS_0Tz3oi2NFO8aC-QDmQcy-_cjyDinirjIuBxQ/edit?usp=sharing)
- [Day 2 video part I - Intro to trees](https://youtu.be/SbZrIynqytA)
- [Day 2 video part II - Random Forest and Boosted trees](https://youtu.be/lD0Gju_MIdA)
- [Day 2 video part III - Intro to interpreting ML Models](https://youtu.be/NGGad3aM0lw)
- [Day 2 video part IV - Visualizing marginal effects](https://youtu.be/agjwrG2m1go)
- [Day 2 jupyter notebook for lecture and lab](https://github.com/agpo-ilr-uni-bonn/PromotionskollegModule6800_2021/blob/main/6800_Day2.ipynb)


### Links to Day 3 material 
- [Day 3a slides - Interpretation part II, Shapley values and other approaches](https://docs.google.com/presentation/d/1ajDVri8mjvihYwZm2jxYDSqsYmrTeqMfwfrr7LWsJlg/edit?usp=sharing)
- [Day 3b slides - Neural Networks (also include part of day 4 slides)](https://docs.google.com/presentation/d/1dxaM8_2Bz7BKPoFQQvIT-0v2SsA0NGLqrkqJxQYuGwA/edit?usp=sharing)
- [Day 3 video part I - Shap-values](https://youtu.be/D6M0V1YGQH4)
- [Day 3 video part II - Other approaches to interpreting ML Models](https://youtu.be/_CV2DjXEgfw)
- [Day 3 video part III - Intro Neural Networks and Autoencoder](https://youtu.be/3sldUdcAhX4)
- [Day 3-4 jupyter notebook for lecture and lab](https://github.com/agpo-ilr-uni-bonn/PromotionskollegModule6800_2021/blob/main/6800_Day3-4.ipynb)


### Links to Day 4 material 
- [Day 4a slides - Neural Networks (same as day 3)](https://docs.google.com/presentation/d/1dxaM8_2Bz7BKPoFQQvIT-0v2SsA0NGLqrkqJxQYuGwA/edit?usp=sharing)
- [Day 4b slides - ML and causal analysis (also include part of day 5 slides)](https://docs.google.com/presentation/d/1wNfltZo-vuEHQHKhTKABMulkXLeGju7H7JxuN8YWf-0/edit?usp=sharing)
- [Day 4 video part I - Types of NN](https://youtu.be/T-PXzdHOZZE)
- [Day 4 video part II - NN-applications](https://youtu.be/ERtXTGVoOVA)
- [Day 4 video part III - Review of causal identification issues](https://youtu.be/rraN7suQYps)
- [Day 4 video part IV - True model selection with LASSO](https://youtu.be/dNTZ-w4-CyE)
- [Day 4-5 jupyter notebook for lecture and lab](https://github.com/agpo-ilr-uni-bonn/PromotionskollegModule6800_2021/blob/main/6800_Day4-5.ipynb)


### Links to Day 5 material 
- [Day 5 slides - ML and causal analysis (same as day 4b)](https://docs.google.com/presentation/d/1wNfltZo-vuEHQHKhTKABMulkXLeGju7H7JxuN8YWf-0/edit?usp=sharing)
- [Day 5 video part I - Intro to causal identification with ML](https://youtu.be/Vam7F5NCIn4)
- [Day 5 video part II - Counterfactual Simulation, Matching, Causal Forest](https://youtu.be/b1uzoTIYHiU)
- [Day 5 video part III - Double ML, Panel, Deep IV](https://youtu.be/tWmqwULSPWw)
- [Day 4-5 jupyter notebook for lecture and lab](https://github.com/agpo-ilr-uni-bonn/PromotionskollegModule6800_2021/blob/main/6800_Day4-5.ipynb)
",EDU,,"['bsrthyle', 'heckelei', 'linmeishang', 'hstorm']",,,1,0,,,,,,2,0.9
131209324,MDEwOlJlcG9zaXRvcnkxMzEyMDkzMjQ=,UCSBstolenbike,alex7962/UCSBstolenbike,0,alex7962,https://github.com/alex7962/UCSBstolenbike,Data Cleaning & Data Visualization using R (Heat Map using plotly),0,2018-04-26 20:51:11+00:00,2019-12-06 18:09:43+00:00,2019-12-06 18:09:41+00:00,,43,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,,"# UCSBstolenbike
### Data Cleaning &amp; Data Visualization using R (Heat Map using plotly)

# Table of Contents
* [Abstract](#Abstract)
* [Contributors](#Contributors)
* [Dataset](#Dataset)
* [Packages](#Packages)
* [Steps](#Steps)
* [Methodology](#Methodology)
* [Citation](#Citation)

## Abstract
  UC Santa Barbara is considered one of the biggest biker friendly campuses in the state of California. Unfortunately every year, more than 400 bikes get reported stolen. Our group's project goal is to build an interactive heatmap based on the data of stolen bikes at UCSB from the year of 2010 to 2017. The heatmap will show users a trend, entailing where more bikes tend to get stolen. We hope to use our project to contribute to the community of UCSB students in a positive light and help them be worry-free from their bikes getting stolen.
  
## Contributors
* [Junki Kwon](#Abstract)
* [Alexander Yoon](#Contributors)

## Dataset
  The dataset is provided by the UCSB Police Department. The dataset is a csv file that includes all bike theft reports on campus from the year 2010 to 2017. Each report includes the date, day of the incident, bike brand, location, time of day, registered, lock, and the property value. Dataset is 1298 observations.
",DATA,,['alex7962'],,,1,0,,,,,,0,0.75
654717965,R_kgDOJwY0DQ,fertility-rates,aliason/fertility-rates,0,aliason,https://github.com/aliason/fertility-rates,,0,2023-06-16 19:20:38+00:00,2024-09-28 06:50:25+00:00,2024-09-28 06:50:22+00:00,,908,0,0,Jupyter Notebook,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# Research Project: Analysis of Fertility Rates 

## Project description: 
This repository contains the code for the final course project of PSTAT 100 (Data Science Concepts & Analysis) at the University of California, Santa Barbara taken in Spring 2023. Students practice the data science lifecycle on a research topic of their choosing, demonstrating key course concepts such as inference and prediction, principles of measurement, missing data, and notions of causality.

My project examines the decline in fertility rates across select developed nations in 2018 and seeks to quantify the contribution of gender, country, and world development indicators to total fertility rates. I will be conducting in-depth analysis of fertility data in Python and implementing multiple techniques to address my research questions.  

## Problem statement
Over the late 50 years, the global fertility rate has halved. Modernization and technological advancements were coupled with a stark decline in fertility rates and accelerated population growth, which was predicted to come to an end as total fertility rates ‚Äì measured as the average number of children per woman ‚Äì continue to fall. As of 2021, the global fertility rate is 1.66 and falling.  

Current perspectives attribute this decline to the social mobility of women; economic and political stability; and the increased well-being of children. But can we be certain? Which explaination is more probable, or are all of them equally valid? Through what means do upward mobility, economic prosperity, or improved well-being impact total fertility rates? Which ones are most predictive of a country's fertility rates? These questions, and more, will be explored in this project. 

## Data sources
Data are sourced from the [United Nations Development Programme](https://www.undp.org/) and the [World Bank](https://data.worldbank.org/). Preprocessed datasets were imported from [lab06](https://github.com/ucsb-ds/pstat100-content/tree/main/labs/lab6-regression/data). 


## Project outline 
First, I will provide background information, specify my data sources, and define my research questions. I will then conduct exploratory analysis and refine the dataset to isolate my factors of interest, using Python data science libraries to facilitate data cleaning and visualization. I perform correlation analysis to inspect relationships; prinicpal component analysis (PCA) to identify key drivers of variability; and multiple regression analysis t

## Methods used
* Data processing
* Data cleaning 
* Feature engineering 
* Exploratory data analysis 
    * Summary statistics 
    * Data visualization
    * Correlation analysis 
* Dimensionality reduction (PCA) 
* Multiple regression
",EDU,,['aliason'],,,1,0,,,,,,1,0.95
428904721,R_kgDOGZCREQ,group-17-ucsb-grad,alissapatterson/group-17-ucsb-grad,0,alissapatterson,https://github.com/alissapatterson/group-17-ucsb-grad,,0,2021-11-17 04:12:55+00:00,2021-11-17 23:51:31+00:00,2021-11-17 23:51:28+00:00,,16,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,# group-17-ucsb-grad,EDU,,"['alissapatterson', 'erindeleonsanchez']",,,1,0,,,,,,1,0.8
82445735,MDEwOlJlcG9zaXRvcnk4MjQ0NTczNQ==,allotemplate,AlloSphere-Research-Group/allotemplate,0,AlloSphere-Research-Group,https://github.com/AlloSphere-Research-Group/allotemplate,Template for sophisticated allolib projects,0,2017-02-19 08:33:37+00:00,2024-10-19 05:07:34+00:00,2024-10-19 05:07:31+00:00,,206,3,3,CMake,1,1,1,1,0,0,3,0,0,0,,1,1,0,public,3,0,3,main,1,1,"# allotemplate
Template repository for projects using allolib.
Contains [allolib](https://github.com/AlloSphere-Research-Group/allolib) and [al_ext](https://github.com/AlloSphere-Research-Group/al_ext) as submodules.

This template is suitable for large projects wil multiple files and dependencies where you need more control.

If you are prototyping single files or want to explore the allolib examples, use the [allolib_playground repo](https://github.com/AlloSphere-Research-Group/allolib_playground).

Developed by:
AlloSphere Research Group
University of California, Santa Barbara

# Installation
## Creating a github repository from template
Use https://github.com/AlloSphere-Research-Group/allotemplate/generate

or from https://github.com/AlloSphere-Research-Group/allotemplate,
click on 'Use this template' then 'Create a new repository'.

## Manually creating a new project based on allotemplate
On a bash shell:

    git clone https://github.com/AlloSphere-Research-Group/allotemplate.git <project folder name>
    cd <project folder name>
    ./init.sh
    git remote set-url origin <URL to new repo>

init.sh will set the folder as a brand new git repository, re-add the submodules into the index, and delete itself.

# Building your project
## How to compile / run
The src/ folder contains the initial main.cpp starter code.

On a bash shell you can run:

    ./configure.sh
    ./run.sh

This will configure and compile the project, and run the binary if compilation is successful.

Alternatively, you can open the CMakeLists.txt project in an IDE like VS Code, Visual Studio or Qt Creator and have the IDE manage the configuration and execution of cmake.

You can also generate other IDE projects through cmake.

## How to perform a distclean
If you need to delete the build,

    ./distclean.sh

should recursively clean all the build directories of the project including those of allolib and its submodules.

## Keeping your project up to date
Run

    ./update.sh

or manually run following from a bash shell:

    git pull
    git submodule update --recursive --init",DEV,,"['younkhg', 'konhyong', 'mantaraya36', 'donghaoren', 'ethwu', 'jtilbian', 'allosphere']",,,1,0,,,,,,15,1
384495762,MDEwOlJlcG9zaXRvcnkzODQ0OTU3NjI=,video_player,AlloSphere-Research-Group/video_player,0,AlloSphere-Research-Group,https://github.com/AlloSphere-Research-Group/video_player,Distributed video player,0,2021-07-09 16:35:23+00:00,2022-09-16 22:48:55+00:00,2025-01-03 20:56:03+00:00,,202,1,1,C++,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,main,1,1,"# allotemplate
Template for creating applications using allolib. This template is suitable for large projects wil multiple files and dependencies where you need more control. If you are prototyping single files or want to explore the allolib examples, use the [allolib_playground repo](https://github.com/AlloSphere-Research-Group/allolib_playground).

Developed by:

AlloSphere Research Group

University of California, Santa Barbara

# Installation

## Dependencies

terminal to run bash

git

cmake version 3.0 or higher

## How to setup
On a bash shell:

    git clone https://github.com/AlloSphere-Research-Group/allotemplate.git <project folder name>
    cd <project folder name>
    ./init.sh

### Optional: Pushing to your own git repository
You can convert the folder into a git repository of your choice with the following commands. (Example is using a github repository: replace username and repository name)

    git remote add origin git@github.com:username/new_repo

After the initial commit, set the upstream with the following command.

    git push -u origin master

## How to compile / run
src folder contains the initial test code you can replace.

Edit CMakeLists.txt and run.sh to match your code.

On a bash shell:

    ./configure.sh

This will execute cmake on the project

    ./run.sh

This will compile the project, and run the binary if compilation is successful.

## How to perform a distclean
If you need to delete the distribution,

    ./distclean.sh

should recursively clean all the build directories of the project including those of allolib and its submodules.
",DEV,,"['mantaraya36', 'konhyong']",,,1,0,,,,,,13,1
608445310,R_kgDOJEQjfg,EDS-240_data-vis-and-customization,an-bui/EDS-240_data-vis-and-customization,0,an-bui,https://github.com/an-bui/EDS-240_data-vis-and-customization,,0,2023-03-02 02:56:38+00:00,2023-03-02 21:12:19+00:00,2023-03-02 18:43:30+00:00,,1932,1,1,HTML,1,1,1,1,1,0,14,0,0,0,,1,0,0,public,14,0,1,main,1,,"# EDS 240: Data visualization and customization

This is a workshop written for UCSB MEDS Winter 2023, EDS 240 (Data Visualization and Communication) given on 2 March 2023.  

# Libraries

```
# general use
library(tidyverse) # general tidying and visualization: ggplot is loaded by default with tidyverse
library(lterdatasampler) # data we're using comes from this package
library(lubridate) # working with dates
library(here) # folder organization

# extras
library(patchwork) # arranging plots
library(magick) # putting images into ggplots
```

# Code

The code template is [here](https://github.com/an-bui/EDS-240_data-vis-and-customization/blob/main/code/ggplot-code_2023-03-02.qmd) and knitted output is [here](https://an-bui.github.io/EDS-240_data-vis-and-customization/code/ggplot-code_2023-03-02.html).

# Independent work time tasks

For independent work time, you have a few different options:

## a. deconstruct Tidy Tuesday visualizations

**Goal**: understand how `theme` options work and/or explore package add-ons to `ggplot`  

**Task**: recreate Tidy Tuesday visualizations from week 8 [(data on Bob Ross paintings)](https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-02-21/readme.md). Treat this like a puzzle: you know the end result (the image), but all the pieces you have are in disarray (the `theme` tools). Try working backwards from the output to see if you can recreate the plot.  

See the [document](https://an-bui.github.io/EDS-240_data-vis-and-customization/code/ggplot-code_2023-03-02.html) for visualizations to recreate.

## b. edit visualizations

**Goal**: improve visualizations from 1 Mar submissions to _convey a message_ (not just showing the data)  

**Task**: revisit the visualizations you submitted and consider two points:  
1) What message am I trying to convey with this figure?  
2) How can I convey that message?  
Sometimes addressing 1) and 2) together mean taking things away (because your figure is too data-dense) or adding things in (because your figure is a map without data on it).  

After you've answered 1) and 2) for yourself, **write them down** and **sketch out the plot** you would make to address both points. **Remake your figure** and **ask someone to tell you what the main message is** (without you telling them what you _think_ it is). It's important that this person _is not in your group and doesn't know the details of your group project._

### Visual vocabulary

You have already learned about different types of plots for different types of data: this is the [Visual Vocabulary](https://public.tableau.com/views/VisualVocabulary/VisualVocabulary?:showVizHome=no). You can also use this [flow chart](https://www.data-to-viz.com/) or [directory of data visualizations](https://clauswilke.com/dataviz/directory-of-visualizations.html). Additionally, you have learned about visual variables (see lecture slides from 26 Jan).

### Mapping activity

If you're stuck, you can also revisit the [mapping activity](https://docs.google.com/spreadsheets/d/1-RWVs4RqfHJHjvrhBUTuI05iqgVE6RmF_9zAP_Yi-7k/edit) we did in class during week 4. With your group mates, you wrote down the kinds of visualizations you wanted to make and the visual variables you would manipulate in each visualization.

## c. revise evaluation plan and digital mock ups

**Goal**: incorporate feedback into a revised evaluation plan  

**Task**: meet with your group members and **carefully consider** feedback you have received. As with any feedback: if you agree, incorporate it into your plan; if you disagree, add justification (in text or otherwise) for why you do not need to incorporate that feedback _and context_ for why that feedback doesn't apply to you.








",EDU,,['an-bui'],,,1,0,,,,,,1,0.95
34227854,MDEwOlJlcG9zaXRvcnkzNDIyNzg1NA==,virality,ArturoDeza/virality,0,ArturoDeza,https://github.com/ArturoDeza/virality,Virality Dataset of the CVPR'15 paper,0,2015-04-19 23:06:50+00:00,2022-11-15 14:15:26+00:00,2015-12-07 07:19:06+00:00,http://arturodeza.github.io/virality/,9659,7,7,Matlab,1,1,1,1,1,0,2,0,0,1,,1,0,0,public,2,1,7,master,1,,"
## Understanding Image Virality Documentation
Created by Arturo Deza. April,2015a<br/>
Last updated: December, 2015b.

pair_matrix.mat: Contains all the viral and non viral images randomly paired as described in Section 3.3.1 of the paper. It is a [5039x2] matrix where the left column has the viral image id, and the right column has the non viral image id.

virality_label.mat: Contains the pair order as shown on AMT for image annotation. We wanted to randomize viral and non viral images on left and right so that the annotations
were counter-balanced.
{1} represents the viral image on the left and the non viral image on the right.
{0} represents the viral image on the right and the non viral image on the left.

Attribute_matrix.mat: Contains all relative attributes annotated for all the previously mentioned pairs. 
{+1} represents that the image on the left has 'more' of an attribute than the image on the right.
{0} represents that the image on the left has 'equal' of an attribute than the image on the right.
{-1} represents that the image on the left has 'less' of an attribute than the image on the right.

Top_489_pair.mat: Contains the 500p dataset which consists of 489 unique pairs used for testing.

viral_decaf6_feats.mat: Contains all the DECAF6 features computed for the 10078 images in the dataset. The img_id maps to the row of the matrix that has the image feature. DECAF6 features are 4096 dimensional.


### Attribute And Virality Prediction 


 To predict virality you will have to create a different folder for every attribute you want to use. The SVM should output a +1,0,-1 label for every image pair after training.
 If you are using libsvm, you can use the class_hat to know the predicted output for each attribute, and you can write an additional script that will load all of these and
 combined them. You also ideally want to use the kernel that maximizes performance (grid search on C and G parameters from 10^-5 to 10^5). 
 Combinining attributes is simply a sum of the labels, where a positive value means that virality was correctly predicted, and a negative label means it was
 incorrectly predicted. Ties were removed by selecting a random positive/negative outcome and running the classifier 100 times, to later compute average performance. Notice
 however that ties need not to be removed for future applications, we had to do such process since our ground truth was a virality score that presented no equal values (one
 image was always more viral than another one in our dataset, and there weren't any ties).


### Other reminders 


You will need to know the virality_label of each pair to train your SVM with correct labels for every label.
Moreoever, we create the relative features by always subtracting the nonviral descriptor from the viral descriptor, hence saving each relative feature with the id of the viral image.


### Download DECAF6 features:
http://brie.uchicago.edu/ftp-www/Viral_total/viral_decaf6_feats.mat

### ArXiv paper pre-print:
http://arxiv.org/abs/1503.02318

### Virality Project Website
http://arturodeza.github.io/virality/

### Dataset Download
https://computing.ece.vt.edu/~arturo/viral_datasetV2.mat

### Poster Summary
Presented at CVPR and at the Scene Understanding Workshop (SUNw), 2015.

http://brie.uchicago.edu/ftp-www/Posters/virality_poster_update_final.pdf

### Raw Dataset
Originally created by the SNAP group at Stanford for the paper: ""What's in a name?"", 2013. H. Lakkaraju, J. J. McAuley & J. Leskovec.
https://snap.stanford.edu/data/web-Reddit.html

### Data and Code Walkthrough

##### Revisit Virality Computation Score:
./Create_Code_Dataset/create_viral_dataset_annotation_doublecheck.m

##### Compute Virality Classification Accuracy:
./Results_Computations/Relative_Attributes/virality_N/compute_virality_N_nodrop.m

##### Prepare data, extract features and compute classifiers
Coming Soon!

### Contact
deza@dyns.ucsb.edu

### BibTex Citation

    @inproceedings{deza2015virality, 
    Author = {Arturo Deza and Devi Parikh}, 
    Title = {Understanding Image Virality}, 
    Year = {2015}, 
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)} }

### License

Redistribution and reuse is allowed under the simplified BSD license.

    Copyright (c) 2015, Arturo Deza
    All rights reserved.
    
    Redistribution and use in source and binary forms, with or without modification,
    are permitted provided that the following conditions are met:
    
    1. Redistributions of source code must retain the above copyright notice, this
    list of conditions and the following disclaimer.
    
    2. Redistributions in binary form must reproduce the above copyright notice,
    this list of conditions and the following disclaimer in the documentation and/or other
    materials provided with the distribution.
    
    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY
    EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
    MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL
    THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
    SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
    PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
    OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER
    IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
    IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
    SUCH DAMAGE.
",DATA,,['ArturoDeza'],,,1,0,,,,,,1,0.6
76677213,MDEwOlJlcG9zaXRvcnk3NjY3NzIxMw==,BEA_Retrieve,bengriffy/BEA_Retrieve,0,bengriffy,https://github.com/bengriffy/BEA_Retrieve,,0,2016-12-16 18:45:17+00:00,2025-02-06 22:11:27+00:00,2016-12-16 19:00:52+00:00,,2,1,1,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,master,1,,"# Interactively Retrieve Data from BEA Website

Author: Ben Griffy

Institution: University of California, Santa Barbara

email: griffy@umail.ucsb.edu

website: www.bengriffy.com

Date: 12/16/2016

Topic: Data Retrieval Tools

## Description

This is a simple command-line interactive tool to search the BEA database for economic series and download series with requested options.

## Instructions:
Open terminal and run the program with API-Key and output destination (absolute path, including "".csv""). This will give you a list of series that are available through the BEA api retrieval service. Pick the series that you want. This will then give you a list of options. If you know the option that you want, enter it here. Otherwise, write ""unknown,"" and the script will query the BEA website for all available options. After selecting all options, the code will generate a .csv file and save it to the destination requested.

## Notes:
This is part of a longer-term project to make large amounts of economic data easily accessible and plottable. More to come.
",DEV,,['bengriffy'],,,1,0,,,,,,1,0.6
752048423,R_kgDOLNNZJw,pollinator-exhibit,Big-Bee-Network/pollinator-exhibit,0,Big-Bee-Network,https://github.com/Big-Bee-Network/pollinator-exhibit,"Bee Exhibit at the California Nature Art Museum Solvang, CA  2024-03-02/2024-09-02",0,2024-02-02 22:32:45+00:00,2024-02-02 22:32:46+00:00,2024-02-02 22:46:32+00:00,,12,0,0,,1,1,1,1,0,0,0,0,0,0,cc0-1.0,1,0,0,public,0,0,0,main,1,1,"- The Birds and the Bees and More: Pollinators
- March 2, 2024 ‚Äì September 2, 2024
- California Nature Art Museum

The Cheadle Center for Biodiversity and Ecological Restoration is proud to present a captivating collection of 12 images, part of the Big Bee NSF project, showcasing local, native bees from the UCSB campus and nearby coastal Santa Barbara. These images are the result of a precise photographic technique known as multi-focal stacking, executed with a Macropod 3D. The talent behind these vibrant, intricately detailed bee portraits comes from undergraduate students at UC Santa Barbara, highlighting their contribution to the project and the ongoing efforts in biodiversity and ecological studies of the Cheadle Center.


| catalogNumber | photographer | species | family | locality |
| --- | --- | --- | --- | --- |
| UCSB-IZC00030183 | Mark Kugel | Diadasia bituberculata | Apidae | Coal Oil Point Reserve |
| UCSB-IZC00040452 | Lyanne Yeh | Peponapis pruinosa | Apidae | Coal Oil Point Reserve |
| UCSB-IZC00040597 | Luz Ceja | Halictus tripartitus | Halictidae | Coal Oil Point Reserve |
| UCSB-IZC00042274 | Rina Utrop | Anthidium palliventre | Apidae | Coal Oil Point Reserve |
| UCSB-IZC00035505 | Luz Ceja | Bombus melanopygus | Apidae | Isla Vista |
| UCSB-IZC00042948 | Tianruo You | Agapostemon texanus | Halictidae | UC Santa Barbara, Lagoon |
| UCSB-IZC00005621 | Luz Ceja | Agapostemon texanus | Halictidae | UC Santa Barbara, Lagoon Restoration |
| UCSB-IZC00056005 | Sheccid Rivas Trasvina | Melissodes tepidus timberlakei | Apidae | UC Santa Barbara, Lagoon Restoration |
| UCSB-IZC00056043 | Sheccid Rivas Trasvina | Triepeolus sp. | Apidae | UC Santa Barbara, Lagoon Restoration |
| UCSB-IZC00021647 | Melanie Lee | Hylaeus mesillae | Colletidae | UC Santa Barbara, North Campus Open Space Restoration |
| UCSB-IZC00046564 | Matthew Rosen | Anthophora curta | Apidae | UC Santa Barbara, North Campus Open Space Restoration |
| UCSB-IZC00038876 | Matthew Rosen | Ashmeadiella chumashae | Megachilidae | Santa Cruz Island |

",DATA,,"['seltmann', 'jhpoelen']",,,1,0,,,,,,2,0.95
397750987,MDEwOlJlcG9zaXRvcnkzOTc3NTA5ODc=,Creative-Coding-Course-with-Processing,bluekamandy/Creative-Coding-Course-with-Processing,0,bluekamandy,https://github.com/bluekamandy/Creative-Coding-Course-with-Processing,Creative-Coding-Course-with-Processing,0,2021-08-18 22:33:34+00:00,2025-01-18 17:45:59+00:00,2022-04-13 23:59:52+00:00,,129763,4,4,,1,1,1,1,0,0,1,0,0,0,lgpl-2.1,1,0,0,public,1,0,4,main,1,,"# Creative Coding with Processing

Created by Prof. Masood Kamandy of Pasadena City College and the University of California Santa Barbara

## Introduction

Welcome to Creative Coding with Processing! This course is a free and open-source course developed by Masood Kamandy for students at Pasadena City College and the University of California Santa Barbara.

Creative Coding is a way of learning how to program by creating visual art with computer graphics. It's an umbrella term that encompasses many different approaches to using **code as a medium for art**. You'll hear words like **generative** and **procedural** a lot. One way of defining looking at it is that we will be making art by writing instructions for a computer to follow and sometimes we'll introduce randomness to make the outcomes varied and unknown.

There is a long history and many approaches to using code to create art. The goal of this course is to help you find your own approach and perspective through hands-on practice.

You can engage in creative coding through many different languages. There are creative coding frameworks written in Java, JavaScript, Python, C++, Rust, and Swift to name just a few. In this course we'll be using Processing, which is a Java-based framework.

## Table of Contents

- [Hardware Requrements](#Hardware-Requrements)
- [Software Requirements](#Software-Requirements)
- [Textbooks](#Textbooks)
- [Recommended Resources](#Recommended-Resources)
  - [Code As Art Resources](#Code-As-Art-Resources)
  - [Java Resources](#Java-Resources)
  - [Processing Resources](#Processing-Resources)
  - [Processing Family](#Processing-Family)
  - [My Other Courses](#My-Other-Courses)
- [Course Content Overview and Schedule](#Course-Content-Overview-and-Schedule)
  - [16-Week Course](#16-Week-Course)
  - [10-Week Course](#10-Week-Course)
- [Inclusion Statement](#Inclusion-Statement)
- [Code of Conduct](#Code-of-Conduct)
- [Course Grade Breakdown](#Course-Grade-Breakdown)
- [Acknowledgements](#Acknowledgements)
- [Corrections](#Corrections)
- [License](#License)

## Hardware Requrements

To develop code in this course you'll need a **macOS**, **Windows**, or **Linux**-based computer. 

[Table of Contents](#Table-of-Contents)

## Software Requirements

We'll be using the Processing Development Environment (PDE):

- [Download Processing](https://processing.org/download)

[Table of Contents](#Table-of-Contents)

## Textbooks

### Required

- [Getting Started with Processing](https://www.oreilly.com/library/view/make-getting-started/9781457187070/) (2nd Edition)
- [Form+Code](http://formandcode.com/)

### Optional

- [Processing: A Programming Handbook for Visual Designers and Artists](https://mitpress.mit.edu/books/processing-second-edition) (2nd Edition) - This book is *optional*, but strongly recomended. My students often feel it is the best reference available for Processing and helps with their understanding.

[Table of Contents](#Table-of-Contents)

## Recommended Resources

### Code As Art Resources

- [Code as Creative Medium](https://mitpress.mit.edu/books/code-creative-medium)
- [Generative Design](http://www.generative-gestaltung.de/2/)
- [When the Machine Made Art](https://www.bloomsbury.com/us/when-the-machine-made-art-9781623562724)

### Java Resources

- [The Java Tutorials](https://docs.oracle.com/javase/tutorial/java/nutsandbolts/index.html)
- [GeeksforGeeks - Java Programming Language](https://www.geeksforgeeks.org/java/)
- [W3Schools - Java Tutorial](https://www.w3schools.com/java/default.asp)

### Processing Resources

The online communities and resources available for the Processing platforms can be enormously helpful.  once you learn the differences between Processing, p5.js, SwiftProcessing, and  it can be easy to translate between the frameworks.

- [processing.org](https://processing.org/)
- [Processing Forums](https://discourse.processing.org/)
- [The Coding Train YouTube Channel](https://www.youtube.com/channel/UCvjgXvBlbQiydffZU7m1_aw)
- [The Nature of Code](https://natureofcode.com/)

### The Processing Family

- [p5.js](https://p5js.org/)
- [Processing Python](https://py.processing.org/)
- [SwiftProcessing](https://github.com/jjkaufman/SwiftProcessing)

### My Other Courses

If you'd like to learn creative coding in another language like Swift or JavaScript, this course is available in those flavors as well.

- [SwiftProcessing](https://github.com/masoodkamandy/Creative-Coding-Course-with-SwiftProcessing)
- p5.js (Coming Soon)

[Table of Contents](#Table-of-Contents)

## Course Content Overview and Schedule

### 16-Week Course

| Week  # | Module | Content Covered                                              | Exercise                                                     | Project                     |
| ------- | ------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------------- |
| 1       | Intro  | **Orientation**<br />Syllabus. Useful resources. Outcomes for the course. Strategies for success in this course. | -                                                            | -                           |
| 1, 2    | 1      | **Drawing with Code**<br />Coding without a computer. Drawing with basic shapes in Processing. | **Coding for Humans**                                        | **Drawing with Code**       |
| 3, 4    | 2      | **Color, Loops, and Animation**<br />Coordinate system. Digital color. Built-in functions. Animation. Variables. Basic arithmetic. For loops & conditional logic. Relational & logical operators. Arrays. | **For Loop & Repetition**                                    | **Animation & Interaction** |
| 5, 6    | 3      | **Motion, Collages, and Images**<br />Motion. Map and lerp. Random numbers. Sliders. | **Collage (Still)**                                          | **Mask**                    |
| 7, 8    | 4      | **Functions and Expanded Cinema**<br />Functions. Switches.  | **Collage (Animated)**<br /><br />**Typography with Functions** | **Expanded Cinema Part 1**  |
| 9, 10   | 5      | **Classes, Objects, and More Arrays**<br />Objects and classes. Arrays. | **Typography with Objects**                                  | **Expanded Cinema Part 2**  |
| 11, 12  | 6      | **Using Objects to Create a Game**<br />Breaking a game into its object oriented parts. | **Game Elements**                                            | **Game**                    |
| 13‚Äì16   | 7      | **Final Project**                                            | **Final Proposal**                                           | **Final Project**           |

[Table of Contents](#Table-of-Contents)

### 10-Week Course

| Week  # | Module | Content Covered                                              | Exercise                                                     | Project                     |
| ------- | ------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------------- |
| 1       | Intro  | **Orientation**<br />Syllabus. Useful resources. Outcomes for the course. Strategies for success in this course. | -                                                            | -                           |
| 1       | 1      | **Drawing with Code**<br />Coding without a computer. Drawing with basic shapes in Processing. | **Coding for Humans**                                        | **Drawing with Code**       |
| 2       | 2      | **Color, Loops, and Animation**<br />Coordinate system. Digital color. Built-in functions. Animation. Variables. Basic arithmetic. For loops & conditional logic. Relational & logical operators. Arrays. | **For Loop & Repetition**                                    | **Animation & Interaction** |
| 3       | 3      | **Motion, Collages, and Images**<br />Motion. Map and lerp. Random numbers. Sliders. | **Collage (Still)**                                          | **Mask**                    |
| 4       | 4      | **Functions and Expanded Cinema**<br />Functions. Switches.  | **Collage (Animated)**<br /><br />**Typography with Functions** | **Expanded Cinema Part 1**  |
| 5       | 5      | **Classes, Objects, and More Arrays**<br />Objects and classes. Arrays. | **Typography with Objects**                                  | **Expanded Cinema Part 2**  |
| 6, 7    | 6      | **Using Objects to Create a Game**<br />Breaking a game into its object oriented parts. | **Game Elements**                                            | **Game**                    |
| 8‚Äì10    | 7      | **Final Project**                                            | **Final Proposal**                                           | **Final Project**           |

[Table of Contents](#Table-of-Contents)

## Inclusion Statement

We all benefit from being exposed to and understanding different perspectives. It improves everything we make and makes the world more livable when we understand that our perspective is just one of many. To that end, my classroom is one where there will be no discrimination on the basis of race, color, religion (creed), gender, gender expression, age, national origin (ancestry), disability, marital status, sexual orientation, or military status. We will assume no previous knowledge and that everyone is a beginner and create a community of support so that all of us can thrive.

[Table of Contents](#Table-of-Contents)

## Code of Conduct

In my classrooms I like to follow in the footsteps of the [p5.js's Community Statement and Code of Conduct](https://github.com/processing/p5.js/blob/main/CODE_OF_CONDUCT.md). This code of conduct sums up the values I try to adhere to in my classrooms and I expect students to stick to them too.

> - **Be mindful of your language.** Any of the following behavior is unacceptable:
>
>   - Offensive comments related to gender identity and expression, sexual orientation, race, ethnicity, language, neuro-type, size, ability,  class, religion, culture, subculture, political opinion, age, skill  level, occupation, or background
>   - Threats of violence
>   - Deliberate intimidation
>   - Sexually explicit or violent material that is not contextualized and preceded by a considerate warning
>   - Unwelcome sexual attention
>   - Stalking or following
>   - Or any other kinds of harassment
>
>   Use your best judgement. If it will possibly make others uncomfortable, do not post it.
>
> - **Be respectful.** Disagreement is not an opportunity to attack someone else's thoughts or opinions. Although views may differ,  remember to approach every situation with patience and care.
>
> - **Be considerate.** Think about how your contribution will affect others in the community.
>
> - **Be open minded.** Embrace new people and new ideas. Our community is continually evolving and we welcome positive change.

[Table of Contents](#Table-of-Contents)

## Course Grade Breakdown

| Category            | Weight |
| ------------------- | ------ |
| Projects            | 35%    |
| Exercises           | 20%    |
| In-Class Activities | 10%    |
| Final Project       | 35%    |

[Table of Contents](#Table-of-Contents)

## Acknowledgements

> ""If I have seen further it is by standing on the shoulders of Giants."" ‚ÄîIsaac Newton, 1676

Attribution is a challenge, so I've made every effort to give credit where credit is due, and every effort has been made to ensure that this course is as unique as possible within the context of the open-source community. If you have any concerns in this regard, please [contact me](mailto:masood@masoodkamandy.com).

This course was created using knowledge I've accrued through my experience from many, many teachers, mentors, friends, and other contacts who fed my excitement about art, design, photography, and code. Contained in all of these assignments, directly or indirectly, are their teachings, their support, or both, and for them I am grateful.

The single largest influence was having Casey Reas as a professor and mentor. Many of these assignments started out in his courses and served as the foundation of my own teaching career.

Others who I offer my gratitude to are Derek Milne, Ben Fry, Adam Ferriss, Chandler McWilliams, Daniel Temkin, Jon Kaufman, The Recurse Center, UCSB's Media Arts & Technology Dept., Pasadena City College, Christopher O'Leary, April Kawaoka, Silvia Rigon, Melanie Willhide, Zaki Kamandy, Brad Larsen, Lauren Lee McCarthy, Z√∂e Wood, Jennifer Jacobs, Penelope Umbrico, James Welling, Cathy Opie, Kathy Ryan, Stephen Frailey, Golan Levin, Daniel Shiffmann, George Legrady, Tobias Hollerer, and Keiko *the* Shiba Inu.

[Table of Contents](#Table-of-Contents)

## Corrections

Please submit any correctiosn to me at masood@masoodkamandy.com. Mistakes are a natural part of a large project like this and I'm always appreciative of suggestions for corrections.

[Table of Contents](#Table-of-Contents)

## License

The course content (assignments and other written material) is available as free and open source course under a [Creative Commons BY-NC-SA 4.0 License](https://creativecommons.org/licenses/by-nc-sa/4.0/). This means you may use the content of this course for non-commercial purposes with attribution and as long as any modifications you make are also made open source under this same license.

All code provided in this course is provided under a [GNU Lesser General Public Release (LGPL) 2.1](https://opensource.org/licenses/LGPL-2.1) license.

All artwork and images contained within this course are owned by their respective owners. If you'd like an image to be taken down, please write to me to request it be taken down at masood@masoodkamandy.com.

[Table of Contents](#Table-of-Contents)

[Next Section: Frequently Asked Questions](FAQ.md)

",EDU,,['bluekamandy'],,,1,0,,,,,,3,0.8
347187054,MDEwOlJlcG9zaXRvcnkzNDcxODcwNTQ=,oblique-for-processing,bluekamandy/oblique-for-processing,0,bluekamandy,https://github.com/bluekamandy/oblique-for-processing,oblique is a processing sketch for image processing and filtration that relies upon chained fragment shaders.,0,2021-03-12 20:16:45+00:00,2025-01-26 03:37:22+00:00,2021-03-17 22:29:41+00:00,,6997,20,20,GLSL,1,1,1,1,0,0,0,0,0,0,gpl-3.0,1,0,0,public,0,0,20,main,1,,"# Oblique for Processing

Oblique for Processing (OfP) is a way of applying pixel transformations to still images. The basic adjustments will be reminiscent of traditional software packages for image editing, but Oblique filters are snippets of code that radically change an image by reorganizing pixels in different ways.

OfP allows you to:

- Import any image.
- Apply basic image adjustments, color shifts, and inversions.
- Utilize Oblique filters (which are really OpenGL ES GLSL fragment shaders) to rearrange the pixels of your image.
- Randomize all parameters.
- Export your image back to your file system.

This proejct is based on [an iOS app](http://appstore.com/obliqueanewwaytophotograph) I released in 2017 which enables real-time filtering of your camera feed. This project is a port of that project into Processing to create an environment for filtering high resolution digital images captured with any camera and/or found on the internet.

## Who uses Oblique for Processing?

OfP is meant to be used for anyone curious about manipulating the pixels of their images. This could be media artists, photographers, creative coders, or people getting started with computer graphics.

Another audience may be people who are interested in learning OpenGL ES GLSL fragment shaders. There are many resources to learn online. Among them are:

- [The Book of Shaders](https://thebookofshaders.com/)
- [Shadertoy](https://www.shadertoy.com/)

## How does Oblique work?

The basic premise of oblique is the idea that if you chain enough fragment shaders together, that you can allow users to create interesting effects.

Oblique has a chain system that looks like this:

![Chain diagram](images/chain_diagram.svg)

The order of the shaders matters, so you will notice some unique behavior that is affected by this chain. For example, a saturation change will not affect a hue change because saturation is higher up in the chain.

## Interface

![Oblique Interface](images/oblique_interface.png)

## Can I use this source code for my projects?

Because OfP is released under a GPL 3.0 license, you are free to modify the source code and use it for your projects with some caveats.

If you intend to ditribute your modified version of this code, you are required to open-source and release any modifications to the community, as well as document your changes.

Here's a great TLDR of GPL3 ([Source](https://gist.github.com/kn9ts/cbe95340d29fc1aaeaa5dd5c059d2e60)):

```
1. Anyone can copy, modify and distribute this software.
2. You have to include the license and copyright notice with each and every distribution.
3. You can use this software privately.
4. You can use this software for commercial purposes.
5. If you dare build your business solely from this code, you risk open-sourcing the whole code base.
6. If you modify it, you have to indicate changes made to the code.
7. Any modifications of this code base MUST be distributed with the same license, GPLv3.
8. This software is provided without warranty.
9. The software author or license can not be held liable for any damages inflicted by the software.
```

## Acknowledgements

OfP was developed in Prof. Jennifer Jacobs course *Computational Systems for Visual Art and Design* in UC Santa Barbara's Media Art & Technology department. I received generous feedback in this course from Prof. Jacobs, as well as my peers Ashley DelValle, Suriya Dakshina Murthy, and our TA, Stejara Iulia Dinulescu.

Thank you to Adam Ferriss for helping get the shader chain up and running.

Thank you to the overall Processing community.

And thank you to Andreas Schlegel, who developed Control P5 which Oblique uses for its UI.

OfP's default image of the false color moon was created by NASA/JPL (Public domain, via Wikimedia Commons).
",DEV,,['bluekamandy'],,,1,0,,,,,,4,0.9
235725512,MDEwOlJlcG9zaXRvcnkyMzU3MjU1MTI=,libBot,Boomshaka/libBot,0,Boomshaka,https://github.com/Boomshaka/libBot,Optimizing booking study rooms at UCSB campus library,0,2020-01-23 04:56:27+00:00,2021-01-07 01:01:52+00:00,2021-01-07 01:01:50+00:00,,23,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,,"# libBot

### Currently unavailable due to campus library shutting down because of the pandemic

A bot that uses Selenium WebDriver to automatically reserve study rooms from the UCSB campus library.
",DEV,,['Boomshaka'],,,1,0,,,,,,1,0.75
699548811,R_kgDOKbJEiw,briana-barajas,briana-barajas/briana-barajas,0,briana-barajas,https://github.com/briana-barajas/briana-barajas,,0,2023-10-02 21:14:58+00:00,2024-09-10 15:54:52+00:00,2024-09-10 15:53:00+00:00,,24,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"<h1 align=""center""> Briana Barajas üêõ </h1>
<h3 align=""center"">  Data Science | Environmental Stewardship </h3>

**üë©‚Äçüíª Pronouns**: she/her

**üìù Personal Website**: https://briana-barajas.github.io/

### üå± What am I up to?
- Current Projects:
    - Interactive spatial data visualization and modeling to inform management of Santa Barbara vernal pools
    - Assessing species distribution model performance on a small spatial scale for the Black-Bellied Slender Salamander (*Batrachoseps nigriventris*)
      
- Past Projects:
  - Master's Group Capstone, Mapping Global Tree Vulnerability Under Climate Change

- Past Courses:
    - Statistics for Environmental Data Science
    - Geospatial Analysis & Remote Sensing
    - Databases & Data Management
    - Modeling Environmental Systems


### üìö Education
- Master of Environmental Data Science - University of California, Santa Barbara
- B.S. Zoology - University of California, Santa Barbara


<!--
**bbarajas429/bbarajas429** is a ‚ú® _special_ ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
- üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->
",OTHER,,['briana-barajas'],,,1,0,,,,,,1,0.6
740631026,R_kgDOLCUh8g,eds240-course-examples,briana-barajas/eds240-course-examples,0,briana-barajas,https://github.com/briana-barajas/eds240-course-examples,"Winter 2024 - Course examples for EDS 240, Data Visualization",0,2024-01-08 18:33:10+00:00,2024-01-09 02:08:52+00:00,2024-02-14 19:54:00+00:00,,12441,0,0,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# EDS 240 Course Examples
Winter 2024 - Course examples for EDS 240, Data Visualization

*Course completed as part of Master of Environmental Data Science (MEDS) degree at the University of California, Santa Barbara.*
",EDU,,['briana-barajas'],,,1,0,,,,,,1,0.9
206418400,MDEwOlJlcG9zaXRvcnkyMDY0MTg0MDA=,brownfield-team.github.io,brownfield-team/brownfield-team.github.io,0,brownfield-team,https://github.com/brownfield-team/brownfield-team.github.io,Website for brownfield-team.github.io,0,2019-09-04 21:34:17+00:00,2020-08-23 20:16:52+00:00,2020-08-23 20:16:50+00:00,https://brownfield-team.github.io,100,0,0,,1,1,1,1,1,0,0,0,0,0,mit,1,0,0,public,0,0,0,master,1,1,"# brownfield-team.github.io

This is a placeholder website for the following NSF project:

Collaborative Research: Exploring Brownfield Programming Assignments in Undergraduate Computing Education

* <https://www.nsf.gov/awardsearch/showAward?AWD_ID=1915196&HistoricalAwards=false>

## PIs:

| Name | Institution |
|------|-------------|
| Christopher Hundhausen | Washington State University |
| Adesola Adesope        | Washington State University |
| Adam Carter            | Humboldt State University |
| Phill Conrad           | University of California, Santa Barbara |

## Abstract

This project aims to serve the national interest by helping undergraduate computer science students develop the soft skills needed for successful careers in software development. The most common type of software development project is called a Brownfield project, in which a software solution is developed within an existing software system. In a Brownfield project, developers modify, debug, refine, and expand an existing code base that they may not have been originally involved in writing. Although most modern software design projects are Brownfield projects, undergraduate computer science curricula focus almost exclusively on teaching students to write code from scratch. This emphasis on ""design from scratch"" leads to graduates who lack key soft skills required in the software profession today. These skills include the ability to plan, reflect on, and direct the programming process; the ability to locate, assess, and apply needed information; and the ability to ask and answer questions in team settings. This project aims to involve instructors, students, education experts, and professional software developers in designing Brownfield programming assignments and teaching strategies that will help undergraduates in computer science courses gain these skills. The project includes a series of rigorous research studies that will provide insights into the educational effectiveness of this educational approach. The findings from these studies will inform efforts to integrate Brownfield projects into undergraduate STEM education at other colleges or universities and in other STEM fields. 

The Brownfield pedagogy will apply social learning theory. It will require students to first complete a series of multimedia instructional modules that provide explicit instruction on the targeted soft skills (i.e. metacognition, information literacy, and communication). Students will then be placed on teams and tasked with fixing bugs and implementing features in an existing code base. A specialized dashboard will scaffold the process through strategic prompts and guidance. The dashboard will also log data about the students' software development processes, search queries, and online interactions with teammates. The project will use a participatory design process for iterative improvements that enable the pedagogy to be educationally effective and readily adaptable to a variety of instructional contexts. Through pilot studies and a quasi-experimental study involving 10 instructors and over 4,500 students at seven diverse institutions, the project will gain insight into the pedagogy's educational effectiveness. The study will explore the relationships between formation of soft skills and development of programming abilities. In addition, the project will contribute novel approaches to measuring soft skills development and new understandings of the interplay between soft and technical skills development. It also aims to develop new theories about the collective contributions of metacognitive, information literacy, and communication skills in self-regulated learning and technical skills development. The pedagogy and technologies developed through this project may be useful as a model for supporting and assessing Brownfield projects in other STEM disciplines. As a result, the project addresses a national priority by helping to produce graduates who possess 21st century STEM workplace skills. This Engaged Student Learning project is supported by the NSF Improving Undergraduate STEM Education Program: Education and Human Resources (IUSE: EHR), which supports research and development projects to improve the effectiveness of STEM education for all undergraduate students. Through the Engaged Student Learning track, the program supports the creation, exploration, and implementation of promising practices and tools.
",WEB,,['pconrad'],,,1,0,,,,,,0,0.7
371114483,MDEwOlJlcG9zaXRvcnkzNzExMTQ0ODM=,CARDAMOM_v2.2,CARDAMOM-framework/CARDAMOM_v2.2,0,CARDAMOM-framework,https://github.com/CARDAMOM-framework/CARDAMOM_v2.2,,0,2021-05-26 17:17:16+00:00,2024-10-08 10:24:19+00:00,2021-06-04 21:05:27+00:00,,41660,11,11,C,1,1,1,1,0,0,6,0,0,0,gpl-3.0,1,0,0,public,6,0,11,main,1,1,"
## CARDAMOM framework version 2.2
#### *JPL, Stanford & UCSB CARDAMOM framework*

## General description 

The Carbon data model framework (CARDAMOM) is a Bayesian inference approach for using terrestrial ecosystem observations to optimize terrestrial carbon cycle model states and processes parameters. The CARDAMOM code presented here is the culmination of a grassroots model development effort undertaken across multiple institutions, including the Jet Propulsion Laboratory (California Institute for Technology), University of Edinburgh, Stanford University and University of California Santa Barbara. The CARDAMOM framework version 2.2 code provided here (https://github.com/CARDAMOM-framework/) was used in Bloom et al. (2020), Quetin et al., (2020), Yin et al. (2020), Famiglietti et al., (2021), and remains backward compatible with Bloom et al., (2016). 

The Data Assimilation Linked Ecosystem Carbon (DALEC) model used in CARDAMOM is described in Williams et al. (2005).  Additional information and references for individual DALEC versions and module components are provided throughout the code.

Points of contact for the JPL, Stanford & UCSB CARDAMOM code:
Anthony Bloom (JPL, abloom @ jpl . nasa . gov)
Caroline Famiglietti (Stanford University, cfamigli @ stanford . edu)
Gregory Quetin (UC Santa Barbara, gquetin @ ucsb . edu)


Updates to CARDAMOM version 2.2 (and subsequent versions) will be made publicly available at https://github.com/CARDAMOM-framework/ as ""read-only"" github repositories. If you wish to collaborate with the CARDAMOM development team or contribute to the CARDAMOM code release, we encourage you to communicate with the points of contact (above).

For the University of Edinburgh/NCEO (UK) CARDAMOM code (used in Exbrayat et al., 2018, Smallman et al., 2021, Famiglietti et al., 2021,  and references therein), the code is available at https://github.com/GCEL/CARDAMOM; contact Luke Smallman (t . l . smallman @ ed . ac . uk) and Mathew Williams (Mat . Williams @ ed . ac . uk) for access.

For general information on the scientific applications of both CARDAMOM frameworks, we refer users to aforementioned papers. 

## References

Bloom, A.A., Exbrayat, J.F., Van Der Velde, I.R., Feng, L. and Williams, M., 2016. The decadal state of the terrestrial carbon cycle: Global retrievals of terrestrial carbon allocation, pools, and residence times. Proceedings of the National Academy of Sciences, 113(5), pp.1285-1290.

Bloom, A.A., Bowman, K.W., Liu, J., Konings, A.G., Worden, J.R., Parazoo, N.C., Meyer, V., Reager, J.T., Worden, H.M., Jiang, Z. and Quetin, G.R., 2020. Lagged effects regulate the inter-annual variability of the tropical carbon balance. Biogeosciences, 17(24), pp.6393-6422.

Exbrayat, J.F., Smallman, T.L., Bloom, A.A., Hutley, L.B. and Williams, M., 2018. Inverse determination of the influence of fire on vegetation carbon turnover in the pantropics. Global Biogeochemical Cycles, 32(12), pp.1776-1789.

Famiglietti, C.A., Smallman, T.L., Levine, P.A., Flack-Prain, S., Quetin, G.R., Meyer, V., Parazoo, N.C., Stettz, S.G., Yang, Y., Bonal, D. and Bloom, A.A., 2021. Optimal model complexity for terrestrial carbon cycle prediction. Biogeosciences, 18(8), pp.2727-2754.

Myrgiotis, V., Blei, E., Clement, R., Jones, S.K., Keane, B., Lee, M.A., Levy, P.E., Rees, R.M., Skiba, U.M., Smallman, T.L. and Toet, S., 2020. A model-data fusion approach to analyse carbon dynamics in managed grasslands. Agricultural Systems, 184, p.102907.

Quetin, G.R., Bloom, A.A., Bowman, K.W. and Konings, A.G., 2020. Carbon flux variability from a relatively simple ecosystem model with assimilated data is consistent with terrestrial biosphere model estimates. Journal of Advances in Modeling Earth Systems, 12(3), p.e2019MS001889.

Smallman, T.L., Exbrayat, J.F., Mencuccini, M., Bloom, A.A. and Williams, M., 2017. Assimilation of repeated woody biomass observations constrains decadal ecosystem carbon cycle uncertainty in aggrading forests. Journal of Geophysical Research: Biogeosciences, 122(3), pp.528-545.

Smallman, T. L., Milodowski, D. T., Neto, E. S., Koren, G., Ometto, J., and Williams, M.: Parameter uncertainty dominates C cycle forecast errors over most of Brazil for the 21st Century, Earth Syst. Dynam. Discuss. [preprint], https://doi.org/10.5194/esd-2021-17, in review, 2021. 

Williams, M., Schwarz, P.A., Law, B.E., Irvine, J. and Kurpius, M.R., 2005. An improved analysis of forest carbon dynamics using data assimilation. Global change biology, 11(1), pp.89-105.

Yin, Y., Bloom, A.A., Worden, J., Saatchi, S., Yang, Y., Williams, M., Liu, J., Jiang, Z., Worden, H., Bowman, K. and Frankenberg, C., 2020. Fire decline in dry tropical ecosystems enhances decadal land carbon sink. Nature communications, 11(1), pp.1-7.


## CARDAMOM copyright statement
Copyright  (c) 2020 California  Institute  of Technology (‚ÄúCaltech‚Äù) and University of Washington. U.S. Government  sponsorship acknowledged.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED  TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
",DEV,,['aantonyb'],,,1,0,,,,,,5,0.8
279933613,MDEwOlJlcG9zaXRvcnkyNzk5MzM2MTM=,domoic_acid,cfree14/domoic_acid,0,cfree14,https://github.com/cfree14/domoic_acid,Predicting domoic acid contamination in California seafood species,0,2020-07-15 17:27:05+00:00,2021-08-25 22:31:54+00:00,2021-08-25 22:31:51+00:00,,227028,0,0,R,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,,"### Predicting domoic acid contamination in California seafood species

This is the GitHub repository for a project seeking to predict domoic acid contamination in California fisheries and aquaculture species.

The project is being conducted by [emLab](http://emlab.msi.ucsb.edu/) at the University of California, Santa Barbara with funding from [TNC](https://www.nature.org/en-us/) and is a collaboration between Chris Free (UCSB), Clarissa Anderson (Scripps), Eric Bjorkstedt (NOAA), Raphael Kudela (UCSC), and Lyall Bellquist (TNC).

The repository contains the data and code for the following paper in preparation:

Free CM, Kudela RM, Bjorkstedt EP, Bellquist LF, Anderson CR. Predicting toxin contamination in harvested marine species to guide dynamic ocean management. Near submission.
",DEV,,['cfree14'],,,1,0,,,,,,0,0.95
593729851,R_kgDOI2OZOw,UCSB-ProstheticVision-ObjectRecognitionTask,Chtun/UCSB-ProstheticVision-ObjectRecognitionTask,0,Chtun,https://github.com/Chtun/UCSB-ProstheticVision-ObjectRecognitionTask,Developed during Research Internship at UCSB Bionic Vision Lab.,0,2023-01-26 17:58:18+00:00,2024-01-04 00:42:56+00:00,2024-03-20 17:29:16+00:00,,53318,0,0,C#,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"Completed during 2021 UCSB Research Mentorship Program.

This is a simulation of what a user with prosthetic vision would see with different parameters based on the device being used and the specific placement of the device behind the retinal layers. Compatible with Windows systems.

The files in this repository are source code files, not the original Unity development files.
",EDU,,['Chtun'],,,1,2,,,,,,1,0.8
125365199,MDEwOlJlcG9zaXRvcnkxMjUzNjUxOTk=,defcc-angr,cyyFh/defcc-angr,0,cyyFh,https://github.com/cyyFh/defcc-angr,,0,2018-03-15 12:38:44+00:00,2018-04-14 13:08:03+00:00,2018-04-15 10:37:23+00:00,,35993,0,0,Python,1,1,1,1,0,0,0,0,0,0,bsd-2-clause,1,0,0,public,0,0,0,master,1,,"angr
====

[![Latest Release](https://img.shields.io/pypi/v/angr.svg)](https://pypi.python.org/pypi/angr/)
[![Build Status](https://travis-ci.org/angr/angr.svg?branch=master)](https://travis-ci.org/angr/angr)
[![License](https://img.shields.io/github/license/angr/angr.svg)](https://github.com/angr/angr/blob/master/LICENSE)
[![Gitbook](https://img.shields.io/badge/docs-gitbook-green.svg)](http://docs.angr.io)
[![API Docs](https://img.shields.io/badge/docs-api-green.svg)](http://angr.io/api-doc)

angr is a platform-agnostic binary analysis framework developed by the Computer Security Lab at UC Santa Barbara and their associated CTF team, Shellphish.

# What?

angr is a suite of python libraries that let you load a binary and do a lot of cool things to it:

- Disassembly and intermediate-representation lifting
- Program instrumentation
- Symbolic execution
- Control-flow analysis
- Data-dependency analysis
- Value-set analysis (VSA)

The most common angr operation is loading a binary: `p = angr.Project('/bin/bash')` If you do this in IPython, you can use tab-autocomplete to browse the [top-level-accessible methods](http://docs.angr.io/docs/toplevel.html) and their docstrings.

The short version of ""how to install angr"" is `mkvirtualenv angr && pip install angr`.

# Example

angr does a lot of binary analysis stuff.
To get you started, here's a simple example of using symbolic execution to get a flag in a CTF challenge.

```python
import angr

project = angr.Project(""angr-doc/examples/defcamp_r100/r100"", auto_load_libs=False)

@project.hook(0x400844)
def print_flag(state):
    print ""FLAG SHOULD BE:"", state.posix.dump_fd(0)
    project.terminate_execution()

project.execute()
```

# Quick Start

- [Install Instructions](http://docs.angr.io/INSTALL.html)
- Documentation as [HTML](http://docs.angr.io/) and as a [Github repository](https://github.com/angr/angr-doc)
- Dive right in: [top-level-accessible methods](http://docs.angr.io/docs/toplevel.html)
- [Examples using angr to solve CTF challenges](http://docs.angr.io/docs/examples.html).
- [API Reference](http://angr.io/api-doc/)
",DEV,,"['ltfish', 'zardus', 'rhelmot', 'kereoz', 'salls', 'NickStephens', 'badnack', 'tyb0807', 'jmgrosen', 'ronnychevalier', 'acama', 'subwire', 'domenukk', 'Lukas-Dresel', 'schieb', 'nebirhos', 'ekilmer', 'danse-macabre', 'bannsec', 'bennofs', 'axt', 'P1kachu', 'tiffanyb', 'ercoppa', 'JinBlack', 'antoniobianchi333', 'm1ghtym0', 'Owlz', 'ekse', 'CodeMaxx', '5lipper', 'sam-b', 'iamahuman', 'farosato', 'drone29a', 'ArtemShypotilov', 'sraboy', 'stef', 'extremecoders-re', 'ocean1', 'lowks', 'tunefish', 'f-prettyland', 'JsHuang', 'moshekaplan', 'adamgrimm99', 'symflood', 'adrianherrera', 'amlweems', 'LiptonB', 'Manouchehri', 'themaks', 'qsphan', 'riyadparvez', 'clslgrnc', 'lockshaw', 'gsingh93', 'benquike', 'Mic92', 'f0rki', 'lordmoses', 'pdcsec', 'odell89', 'otibsa', 'tdube', 'ctfhacker', 'grant-h', 'fmagin', 'ttdennis', 'amatus', 'tunz', 'sch3m4', 'commial', '0xbc', 'sciencemanx', 'Spirotot']",,,1,0,,,,,,0,0.9
52640900,MDEwOlJlcG9zaXRvcnk1MjY0MDkwMA==,RC-Spanner-Implementation,darshanmaiya/RC-Spanner-Implementation,0,darshanmaiya,https://github.com/darshanmaiya/RC-Spanner-Implementation,Implementation of the Replicated Commit and Spanner protocols for Geo Replication.,0,2016-02-27 00:03:55+00:00,2016-04-16 02:28:27+00:00,2016-04-16 02:33:57+00:00,,259,0,0,Java,1,1,1,1,0,0,0,0,0,0,apache-2.0,1,0,0,public,0,0,0,master,1,,"# RC-Spanner-Implementation

Implementation of the Replicated Commit and Spanner protocols for Geo Replication as a part of the Advanced Database Systems course during Winter 2016 at the University of California, Santa Barbara.

## Team Members
Alan Buzdar<br />
Darshan Maiya<br />
Suraj Rajesh<br />
",EDU,,"['Suraj-Rajesh', 'alanbuzdar', 'darshanmaiya']",,,1,0,,,,,,3,0.9
814398515,R_kgDOMIq8Mw,figshare-import,DataONEorg/figshare-import,0,DataONEorg,https://github.com/DataONEorg/figshare-import,Import and convert Figshare data and records to a DataONE repository,0,2024-06-13 00:00:56+00:00,2024-08-16 22:18:11+00:00,2024-08-16 22:18:07+00:00,,260,0,0,Python,1,1,1,1,0,0,0,0,0,0,apache-2.0,1,0,0,public,0,0,0,main,1,1,"## figshare-import
*Import and convert Figshare data and records to a DataONE repository*

- **Authors**: Nesbitt, Ian ([http://orcid.org/0000-0001-5828-6070](http://orcid.org/0000-0001-5828-6070))
- **License**: [Apache 2](http://opensource.org/licenses/Apache-2.0)
- [Package source code on GitHub](https://github.com/DataONEorg/figshare-import)
- [**Submit Bugs and feature requests**](https://github.com/DataONEorg/figshare-import/issues)
- Contact us: support@dataone.org
- [DataONE discussions](https://github.com/DataONEorg/dataone/discussions)

This software is meant to provide transport of data and translation of metadata from Figshare's json format to DataONE. It uses a custom translation method to convert to Ecological Metadata Language (EML) and upload data, metadata and resource maps to a DataONE Metacat instance. This workflow may be run during repository setup to move a large corpus from Figshare into a new DataONE repository.

DataONE in general, and figshare-import in particular, are open source, community projects.  We [welcome contributions](./CONTRIBUTING.md) in many forms, including code, graphics, documentation, bug reports, testing, etc.  Use the [DataONE discussions](https://github.com/DataONEorg/dataone/discussions) to discuss these contributions with us.


## Documentation

Documentation is a work in progress. All functions have reStructuredText docstrings and fairly well commented. In the future, a documentation site will be built into the repository.

## Quickstart

1. Set the config values in `~/.config/figshare-import/config.json`. Be mindful to run test operations only on staging servers prior to operating in a production environment:
    ```json
    {
        ""rightsholder_orcid"": ""http://orcid.org/0000-0001-5828-6070"",
        ""write_groups"": [""CN=Test_Group,DC=dataone,DC=org""],
        ""changePermission_groups"": [""CN=Test_Group,DC=dataone,DC=org""],
        ""nodeid"": ""urn:node:mnTestKNB"",
        ""mnurl"": ""https://dev.nceas.ucsb.edu/knb/d1/mn/"",
        ""cnurl"": ""https://cn-stage.test.dataone.org/cn"",
        ""metadata_json"": ""~/figshare-import/article-details-test.json"",
        ""data_root"": ""/mnt/ceph/repos/si/figshare/FIG-12/""
    }
    ```
2. Copy your DataONE authentication token to `~/.config/figshare-import/.d1_token`.
3. Ensure the `article-details.json` file is in place and noted in the `""metadata_json""` field of the config file.
4. Run the download script `./figshare_import/run_figshare_download.py`. This may take a while depending on how much content you are downloading from Figshare.
5. Run the upload script `./figshare_import/run_data_upload.py`. This may also take a while. Operations will be significantly quicker when run within the same network as the Member Node you are uploading to.

## Trouble shooting

- Ensure all config values are correct. Triple-check them.
- Ensure your DataONE authentication token is valid and current, and that you have at least write permission on the member node. DataONE tokens expire after 24 hours. Long-lived tokens can be obtained from DataONE support in appropriate cases.
- Ensure Figshare content is public. Script will need to be modified to pass Figshare authentication credentials if you intend to download private datasets.
- [File an issue](https://github.com/DataONEorg/figshare-import/issues). Be sure to describe your problem in detail, and post the content of your configuration file. **DO NOT** post your authentication token.

## Usage Examples

In the terminal:

```bash
$ figsharedownload
$ figshareimport
```

In Python:

```py
>>> from figshare_import.run_figshare_download import run_figshare_download
>>> from figshare_import.run_data_upload import run_data_upload
>>> run_figshare_download()
>>> run_data_upload()
```

## Development and testing

This is a python package built using the [Python Poetry](https://python-poetry.org) build tool.

To install locally, create a virtual environment for python 3.9+, 
install poetry, and then install or build the package with `poetry install` or `poetry build`, respectively.

To run unit tests, navigate to the root directory and run `python -m unittest test.py`.
Tests have not yet been fully implemented for this software.

## License
```
Copyright [2024] [Regents of the University of California]

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```

## Acknowledgements
Work on this package was supported by:

- DataONE Network

Additional support was provided for collaboration by the National Center for Ecological Analysis and Synthesis, a Center funded by the University of California, Santa Barbara, and the State of California.

[![DataONE_footer](https://user-images.githubusercontent.com/6643222/162324180-b5cf0f5f-ae7a-4ca6-87c3-9733a2590634.png)](https://dataone.org)
",DEV,,"['iannesbitt', 'mbjones']",,,1,0,,"# Contributing

**üéâ First off, thanks for contributing! üéâ**

- [‚ú® Types of contributions](#types-of-contributions)
- [üì§ Pull Requests](#pull-requests)
- [üöÄ Development Workflow](#development-workflow)
- [üîÄ Release process](#release-process)
- [üî¨ Testing](#testing)
- [üé® Code style](#code-style)
- [üìÑ Contributor license agreement](#contributor-license-agreement)

## ‚ú® Types of contributions

We welcome all types of contributions, including bug fixes, feature enhancements,
bug reports, documentation, graphics, and many others.  You might consider contributing by:

- Report a bug or request a new feature in our [issue tracker](https://github.com/DataONEorg/figshare-import/issues)
- Fix a bug and contribute the code with a Pull Request
- Write or edit some documentation
- Sharing helpful tips or FAQ-type answers to users or future contributors
- Create screenshots or tutorials of features of MetacatUI
- Answer questions on DataONE Discussions
- ...

This is an open source project, and we welcome full
participation in the project.  Contributions are reviewed and suggestions are
made to increase value to the community.  We strive to
incorporate code, documentation, and other useful contributions quickly and
efficiently while maintaining a high-quality software product.

## üì§ Pull Requests
We use the pull-request model for contributions. See [GitHub's help on pull-requests](https://help.github.com/articles/about-pull-requests/).

In short:

- add an [issue](https://github.com/DataONEorg/figshare-import/issues) describing your planned changes, or add a comment to an existing issue;
- on GitHub, fork the [repository](https://github.com/DataONEorg/figshare-import)
- on your computer, clone your forked copy of the repository
- base your work on the `develop` branch and commit your changes
- push your branch to your forked repository, and submit a pull-request
- our team will be notified of your Pull Request and will review your changes
- our team may request changes before we will approve the Pull Request, or we will make them for you
- once the code is reviewed, our team will merge in your changes to `develop` for the next planned release

## üöÄ Development Workflow

Development is managed through the git repository at https://github.com/DataONEorg/figshare-import.  The repository is organized into several branches, each with a specific purpose.  

**main**. The `main` branch represents the stable branch that is constantly maintained with the current release.  It should generally be safe to install and use the `main` branch the same way as binary releases. The version number in all configuration files and the README on the `main` branch follows [semantic versioning](https://semver.org/) and should always be set to the current stable release, for example `2.8.5`.

**develop**. Development takes place on a single branch for integrated development and testing of the set of features
targeting the next release. Commits should only be pushed to this branch once they are ready to be deployed to
production immediately after being pushed. This keeps the `develop` branch in a state of readiness for the next release.
Any unreleased code changes on the `develop` branch represent changes that have been tested and staged for the next 
release. 
The tip of the `develop` branch always represents the set of features that are awaiting the next release. The develop
branch represents the opportunity to integrate changes from multiple features for integrated testing before release.

Version numbers on the `develop` branch represent either the planned next release number (e.g., `2.9.0`), or the planned next release number with a `beta` designator or release candidate `rc` designator appended as appropriate.  For example, `2.8.6-beta1` or `2.9.0-rc1`.

**feature**. To isolate development on a specific set of capabilities, especially if it may be disruptive to other 
developers working on the `develop` branch, feature branches should be created.

Feature branches are named as `feature-` + `{issue}` +  `-{short-description}`, with `{issue}` being the GitHub issue number related to that new feature. e.g. `feature-23-refactor-storage`.

All `feature-*` branches should be frequently merged with changes from `develop` to
ensure that the branch stays up to date with other features that have
been tested and are awaiting release.  Thus, each `feature-*` branch can be tested on its own before it is merged with other features on develop, and afterwards as well. Once a feature is complete and ready for full integration testing, it is generally merged into the `develop` branch after review through a pull request.

**bugfix**. A final branch type are `bugfix` branches, which work the same as feature branches, but fix bugs rather than adding new functionality. Sometimes it is hard to distinguish features from bug fixes, so some repositories may choose to use `feature` branches for both types of change. Bugfix branches are named similarly, following the pattern: `bugfix-` + `{issue}` +  `-{short-description}`, with `{issue}` being the GitHub issue number related to that bug. e.g. `bugfix-83-fix-name-display`.

### Development flow overview

```mermaid
%%{init: {  'theme': 'base', 
            'gitGraph': {
                'rotateCommitLabel': false,
                'showCommitLabel': false
            },            
            'themeVariables': {
              'commitLabelColor': '#ffffffff',
              'commitLabelBackground': '#000000'
            }
}}%%
gitGraph
    commit id: ""1"" tag: ""v1.0.0""
    branch develop
    checkout develop
    commit id: ""2""
    branch feature-A
    commit id: ""3""
    commit id: ""4""
    checkout develop
    merge feature-A id: ""5""
    commit id: ""6""
    commit id: ""7""
    branch feature-B
    commit id: ""8""
    commit id: ""9""
    checkout develop
    merge feature-B  id: ""10"" type: NORMAL
    checkout main
    merge develop id: ""11"" tag: ""v1.1.0""
```

## üîÄ Release process

1. Our release process starts with integration testing in a `develop` branch. Once all
changes that are desired in a release are merged into the `develop` branch, we run
the full set of tests on a clean checkout of the `develop` branch.
2. After testing, the `develop` branch is merged to main, and the `main` branch is tagged with
the new version number (e.g. `2.11.2`). At this point, the tip of the `main` branch will 
reflect the new release and the `develop` branch can be fast-forwarded to sync with `main` to 
start work on the next release.
3. Releases can be downloaded from the [GitHub releases page](https://github.com/DataONEorg/figshare-import/releases).

## üî¨ Testing

**Unit and integration tests**. We maintain a full suite of tests in the `tests` subdirectory.
Any new code developed should include a robust set of tests for each public
method, as well as integration tests from new feature sets.  Tests should fully
exercise the feature to ensure that it responds correctly to both good data inputs
and various classes of corrupt or bad data.  All tests should pass before submitting a PR
or merging to `develop`.

Tests are automatically run via GitHub Actions. Check the root `README.md` file
for this GitHub Actions status badge and make sure it says ""Passing"":

## üé® Code style

Code should be written to professional standards to enable clean, well-documented,
readable, and maintainable software.  While there has been significant variability
in the coding styles applied historically, new contributions should strive for
clean code formatting.  We generally follow PEP8 guidelines for Python code formatting,
typically enforced through the `black` code formatting package.

## üìÑ Contributor license agreement

In order to clarify the intellectual property license
granted with Contributions from any person or entity, you agree to
a Contributor License Agreement (""CLA"") with the Regents of the University of
California (hereafter, the ""Regents"").

1. Definitions.
   ""You"" (or ""Your"") shall mean the copyright owner or legal entity
   authorized by the copyright owner that is making this Agreement
   with the Regents. For legal entities, the entity making a
   Contribution and all other entities that control, are controlled
   by, or are under common control with that entity are considered to
   be a single Contributor. For the purposes of this definition,
   ""control"" means (i) the power, direct or indirect, to cause the
   direction or management of such entity, whether by contract or
   otherwise, or (ii) ownership of fifty percent (50%) or more of the
   outstanding shares, or (iii) beneficial ownership of such entity.
   ""Contribution"" shall mean any original work of authorship,
   including any modifications or additions to an existing work, that
   is intentionally submitted by You to the Regents for inclusion
   in, or documentation of, any of the products owned or managed by
   the Regents (the ""Work""). For the purposes of this definition,
   ""submitted"" means any form of electronic, verbal, or written
   communication sent to the Regents or its representatives,
   including but not limited to communication on electronic mailing
   lists, source code control systems, and issue tracking systems that
   are managed by, or on behalf of, the Regents for the purpose of
   discussing and improving the Work, but excluding communication that
   is conspicuously marked or otherwise designated in writing by You
   as ""Not a Contribution.""
2. Grant of Copyright License. Subject to the terms and conditions of
   this Agreement, You hereby grant to the Regents and to
   recipients of software distributed by the Regents a perpetual,
   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
   copyright license to reproduce, prepare derivative works of,
   publicly display, publicly perform, sublicense, and distribute Your
   Contributions and such derivative works.
3. Grant of Patent License. Subject to the terms and conditions of
   this Agreement, You hereby grant to the Regents and to
   recipients of software distributed by the Regents a perpetual,
   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
   (except as stated in this section) patent license to make, have
   made, use, offer to sell, sell, import, and otherwise transfer the
   Work, where such license applies only to those patent claims
   licensable by You that are necessarily infringed by Your
   Contribution(s) alone or by combination of Your Contribution(s)
   with the Work to which such Contribution(s) was submitted. If any
   entity institutes patent litigation against You or any other entity
   (including a cross-claim or counterclaim in a lawsuit) alleging
   that your Contribution, or the Work to which you have contributed,
   constitutes direct or contributory patent infringement, then any
   patent licenses granted to that entity under this Agreement for
   that Contribution or Work shall terminate as of the date such
   litigation is filed.
4. You represent that you are legally entitled to grant the above
   license. If your employer(s) has rights to intellectual property
   that you create that includes your Contributions, you represent
   that you have received permission to make Contributions on behalf
   of that employer, that your employer has waived such rights for
   your Contributions to the Regents, or that your employer has
   executed a separate Corporate CLA with the Regents.
5. You represent that each of Your Contributions is Your original
   creation (see section 7 for submissions on behalf of others).  You
   represent that Your Contribution submissions include complete
   details of any third-party license or other restriction (including,
   but not limited to, related patents and trademarks) of which you
   are personally aware and which are associated with any part of Your
   Contributions.
",,,,7,0.9
631056358,R_kgDOJZ0n5g,ecegsa-ucsb.github.io,ecegsa-ucsb/ecegsa-ucsb.github.io,0,ecegsa-ucsb,https://github.com/ecegsa-ucsb/ecegsa-ucsb.github.io,Webpage for ECE GSA at UCSB !,0,2023-04-21 20:15:39+00:00,2024-11-20 20:39:51+00:00,2024-11-20 20:39:48+00:00,https://gsa.ece.ucsb.edu,72111,0,0,CSS,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,responsive,1,1,"# devfest2015
Material Design Template for DevFest 2015 Season

Demo you can find here: http://vg.am/devfest
",WEB,,['monsij'],,,1,0,,,,,,1,0.9
753404695,R_kgDOLOgLFw,ca-transport-supply-decarb-shiny,emlab-ucsb/ca-transport-supply-decarb-shiny,0,emlab-ucsb,https://github.com/emlab-ucsb/ca-transport-supply-decarb-shiny,,0,2024-02-06 03:41:27+00:00,2024-09-26 22:31:41+00:00,2024-09-26 22:31:37+00:00,,32564,0,0,R,1,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,1,"# Impact of Oil Wells in California Interactive Dashboard

UCSB Master of Environmental Data Science Project Team Members: Mariam Garcia, Haejin Kim, and Maxwell Patterson

This interactive dashboard is submitted in partial satisfaction of the requirements for the degree of Master of Environmental Data Science for the Bren School of Environmental Science & Management.


## Contents
There are four main pages of the interactive dashboard: 'Oil well Explorer','About','Statewide Impacts of SB 1137', and 'Research Methods'.

An overview of each page follows:

*Oil Well Explorer*

The Oil Well Explorer contains a brief description of SB 1137, providing hard-hitting information in an interpretable manner. The main component of this page is reactivity. There are two pickerInputs, one for California County and one for Well Type. The 58 California Counties are listed in the first pickerInput, and the Well Types were consolidated into two types: Active and non-Active. After the user picks both inputs, a reactive Leaflet map will appear on the user's right-hand side. It will display all of California, with black borders representing the county lines. This map also includes a cornflower blue layer representing a 3,200 foot buffer around sensitive areas. The Leaflet map used was WorldStreetMap, so that users are able to easily identify areas of interest. Moreover, the Leaflet map will also include a pop-up message that includes county-specific information: County Name, % Reduction in PM 2.5 if SB 1137 is implemented, % of Disadvantaged Communtiies, and Population. 

![first-page](https://github.com/capstone-freshcair/freshcair-shiny/assets/105567684/0f4b1f56-8f12-4d94-8534-cbf12854da7b)


*About*

This page includes a brief overview about the health impacts associated with well activity. This page is meant to give the user a deeper understanding of the importance of SB 1137. This page also includes helpful context about what fossil fuel racism is, and how disadvantaged communities have often  faced the unequal distribution of the harmful effects associated with well activity.

![second-page](https://github.com/capstone-freshcair/freshcair-shiny/assets/105567684/2c68dcc0-25c2-4f0e-b1f6-a0f88c57df9a)


*Statewide Impacts of SB 1137*
This page includes a summary of findings associated with the additional setback scenario. This page also hosts three digestible visualizations showing different implications of the setback policy. It is important to note that these values are projections from the years 2020-2045. Two of the three visualizations are `geom_point()` graphs comparing the impacts of SB 1137 with other supply side policies: excise taxes and carbon taxes. The last visualization is a projection of the amount of oil produced, with two lines representing two different scenarios: SB 1137 and Business as Usual (BAU).

![third-page](https://github.com/capstone-freshcair/freshcair-shiny/assets/105567684/747abefe-ca75-4b41-b2d9-7feb78fdfcb5)

*Research Methods*
This page houses technical justifications for the approaches and selections made for the predictive models the freshCAir team decided to use for predicting the number of new wells and oil production. Visualizations on this page demonstrate the comparison between the existing Poisson model the client provided and the random forest model the freshCAir team crafted. This page also includes explanations of metrics used to assess performance and parameters used to craft the model.

![fourth-page](https://github.com/capstone-freshcair/freshcair-shiny/assets/105567684/4ba6d8b7-7bf4-4d5e-8eb5-95b4ce945542)




This is the file structure of the `shinydashboard` content:


```bash
‚îú‚îÄ‚îÄ data
‚îÇ   ‚îú‚îÄ‚îÄ buffer_3200ft.shp
‚îÇ   ‚îú‚îÄ‚îÄ county_health_results.csv
‚îÇ   ‚îú‚îÄ‚îÄ datagenerated-buffer_3200ft.dbf
‚îÇ   ‚îú‚îÄ‚îÄ datagenerated-buffer_3200ft.prj
‚îÇ   ‚îú‚îÄ‚îÄ datagenerated-buffer_3200ft.shp
‚îÇ   ‚îú‚îÄ‚îÄ datagenerated-buffer_3200ft.shx
‚îÇ   ‚îú‚îÄ‚îÄ proprietary
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AllWells_gis
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Wells_All.dbf
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Wells_All.prj
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Wells_All.sbn
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Wells_All.sbx
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Wells_All.shp
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Wells_All.shp.xml
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Wells_All.shx
‚îÇ   ‚îî‚îÄ‚îÄ subset_county_hs_results.csv
‚îú‚îÄ‚îÄ global.R
‚îú‚îÄ‚îÄ R
‚îÇ   ‚îú‚îÄ‚îÄ leaflet_map.R
‚îÇ   ‚îî‚îÄ‚îÄ well_location.R
‚îú‚îÄ‚îÄ server.R
‚îú‚îÄ‚îÄ ui.R
‚îî‚îÄ‚îÄ www
    ‚îú‚îÄ‚îÄ 2035-logo.png
    ‚îú‚îÄ‚îÄ BrenLogo-FullColor-RGB-transparent.png
    ‚îú‚îÄ‚îÄ ca-oil-prod.png
    ‚îú‚îÄ‚îÄ dac-emp.png
    ‚îú‚îÄ‚îÄ dac-mort.png
    ‚îú‚îÄ‚îÄ mortality_plot.png
    ‚îú‚îÄ‚îÄ oil-well-comp.png
    ‚îú‚îÄ‚îÄ oilwell.jpg
    ‚îú‚îÄ‚îÄ oil-well-prod.png
    ‚îú‚îÄ‚îÄ pred-wells.png
    ‚îú‚îÄ‚îÄ shinydashboard-fresh-theme.css
    ‚îú‚îÄ‚îÄ shiny-fresh-theme.css
    ‚îî‚îÄ‚îÄ styles.css
```
## Disclaimer
This capstone project was completed as a requirement for the Master of Environmental Data Science program at the Bren School of Environmental Science & Management, University of California, Santa Barbara. The data used for generating the visuals in the dashboard was made by updating the existing workflow by adding the 3,200 foot setback scenario. 


## Authors
Mariam Garcia {mkgarcia@ucsb.edu}

Haejin Kim  {haejin_kim@ucsb.edu}

Maxwell Patterson {maxwellpatterson@ucsb.edu}

## Clients
Lucas Boyd, Executive Director, The 2035 Initiative {lucasboyd@ucsb.edu}

Tracey Mangin, Senior Project Scientist, emLab, {tmangin@ucsb.edu}
",EDU,,"['mariamkg00', 'maxwellpatt', 'ranjitster']",,,1,0,,,,,,1,1
167754801,MDEwOlJlcG9zaXRvcnkxNjc3NTQ4MDE=,Universal-Hash-Function,erickim78/Universal-Hash-Function,0,erickim78,https://github.com/erickim78/Universal-Hash-Function,CS130A Programming Assignment 1,0,2019-01-27 00:42:35+00:00,2019-02-19 23:27:28+00:00,2019-02-19 23:27:27+00:00,,775,0,0,C++,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,,"CSIL Login: dongjunkim
UCSB Email: dongjunkim@ucsb.edu

Completed Advanced Portion

Implementation of a Universal Hash Function done in CS132A
",EDU,,['erickim78'],,,1,0,,,,,,0,0.8
521051509,R_kgDOHw6ddQ,EricM5,EricM5/EricM5,0,EricM5,https://github.com/EricM5/EricM5,,0,2022-08-03 22:34:37+00:00,2022-08-03 22:34:37+00:00,2024-01-17 17:26:45+00:00,,16,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"          
<!--
**EricM5/EricM5** is a ‚ú® _special_ ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.


Here are some ideas to get you started:

- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->

<h2>Hey thereüëã, I'm Eric</h2>
<p>I'm currently a 3rd Year Computer Science Student at <strong><a href=""https://www.ucsb.edu/""> UC Santa Barbara </a> </strong> with a love for developing new software, APIs, and algorithms.
</p>
<p><a href=""https://www.linkedin.com/in/eric-marzouk/""><img src=""https://img.shields.io/badge/-@ericmarzouk-0077B5?style=flat-square&amp;labelColor=0077B5&amp;logo=LinkedIn&amp;link=https://www.linkedin.com/in/eric-marzouk/"" alt=""LinkedIn Badge""></a> 
<h2>‚ö°Ô∏è A Few Quick Facts</h2>
<ul>
<li>üî≠ I‚Äôm currently working on <a href=""https://github.com/EricM5/PuckSights"">PuckSights  </a></li>
<li>:airplane: Go checkout <a href=""https://github.com/EricM5/IntelliPrice.Api""><strong>my new API! </strong> </a></li>
</ul>
<h2>üöÄ My languages/tools</h2>
<p align=""left"">

<img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/cplusplus/cplusplus-original.svg"" width=""50"" height=""50"" />
<img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/csharp/csharp-original.svg"" width=""50"" height=""50"" />
    
<img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/python/python-original.svg"" width=""50"" height=""50"" />
<img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/java/java-original.svg"" width=""50"" height=""50"" />
<img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/flask/flask-original.svg"" width=""50"" height=""50"" />
<img src=""https://user-images.githubusercontent.com/76453820/188104263-b7b1feff-97a3-4deb-9d93-70eaf0dc64dc.svg"" width=""50"" height=""50"" />
<img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/dot-net/dot-net-original.svg"" width=""50"" height=""50"" />
<img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/visualstudio/visualstudio-plain.svg"" width=""50"" height=""50"" />
<img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/heroku/heroku-plain.svg"" width=""50"" height=""50"" />
<img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/jupyter/jupyter-original.svg"" width=""50"" height=""50"" />
<img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/pandas/pandas-original.svg"" width=""50"" height=""50"" />    
</p>
",OTHER,,['EricM5'],,,1,0,,,,,,1,0.7
277892403,MDEwOlJlcG9zaXRvcnkyNzc4OTI0MDM=,rancher-charts,etoddatkins/rancher-charts,0,etoddatkins,https://github.com/etoddatkins/rancher-charts,Rancher enhanced helm charts,0,2020-07-07 18:24:59+00:00,2023-05-19 20:19:31+00:00,2023-05-22 22:13:30+00:00,,507,0,0,Smarty,1,1,1,1,1,0,0,0,0,0,bsd-3-clause,1,0,0,public,0,0,0,main,1,,"# rancher-charts
UC Santa Barbara Information Assurance and Security Rancher-ized helm charts

These charts have been augmented to be able to be deployed with [Rancher](https://ranchermanager.docs.rancher.com/) a Kubernetes management tool to deploy and run clusters anywhere and on any provider.

## Charts

* runzero-explorer: runZero [Explorers](https://www.runzero.com/docs/installing-an-explorer/) Customized for a specific [Organization](https://www.runzero.com/docs/organizations/) in [runZero](https://www.runzero.com/)
* awx: A web-based user interface, REST API, and task engine built on top of [Ansible](https://github.com/ansible/ansible). It is one of the upstream projects for [Red Hat Ansible Automation Platform](https://www.ansible.com/products/automation-platform).

## Usage

[Helm](https://helm.sh) must be installed to use the charts.  Please refer to
Helm's [documentation](https://helm.sh/docs) to get started.

Once Helm has been set up correctly, add the repo as follows:

  helm repo add ucsb-iac https://etoddatkins.github.io/helm-charts

If you had already added this repo earlier, run `helm repo update` to retrieve
the latest versions of the packages.  You can then run `helm search repo
ucsb-iac` to see the charts.

To install the <chart-name> chart:

    helm install my-<chart-name> ucsb-iac/<chart-name>

To uninstall the chart:

    helm delete my-<chart-name>",DEV,,"['etoddatkins', 'etoddatkins-awx']",,,1,215,,,,,,1,0.9
899705267,R_kgDONaBpsw,land-cover-classification,evajnewby/land-cover-classification,0,evajnewby,https://github.com/evajnewby/land-cover-classification,,0,2024-12-06 20:43:23+00:00,2024-12-06 22:57:26+00:00,2024-12-06 22:57:22+00:00,,2661,0,0,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# Land Cover Classification of Southern Santa Barbara County, California

Author: Eva Newby (https://github.com/evajnewby)

## About
The following repository was created for edicational purposes for EDS 220 - Environmental Data, for the Masters of Environmental Data Science program at the Bren School of Environmental Science and Management at the University of California, Santa Barbara. 

The purpose of this repository is to hold both data and code necessary for the completion of the one decision tree and one map detailing the landcover (green vegetation, soil/dead grass, urban, or water) for southern Santa Barbara county.

This repository contains one data folder, one R project, one Quarto document, one rendered html and associated files. 

The `land-cover-classification.qmd` contains all the code necessay to read in the data, create a decision tree, train the decision tree, and create a plot with classified land cover types. The `land-cover-classification.html` is the rendered version. 

The data folder contains the landsat data .tif files, the shapefiles (and associated files) for southern Santa Barbara county, and training data. This folder was added to the gitignore to prevent any pushing issues or delays.

## Highlights
- Load and process Landsat scene
- Crop and mask Landsat data to study area
- Extract spectral data at training sites
- Train and apply decision tree classifier
- Plot results using `tmap()`.

## Data
The `landsat-data` is from the Landsat 5 Thematic mapper, and contains 1 scene from September 25, 2007, with bands 1, 2, 3, 4, 5, 6, and 7. 

Additionally, the data folder also contains a shapefile representing southern Santa Barbara county, and training data containing polygons representing sites. 

## Repository Structure
```bash
land-cover-classification
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .gitignore
      ‚îú‚îÄ‚îÄ data
          ‚îú‚îÄ‚îÄ SB_county_south.cpg
          ‚îú‚îÄ‚îÄ SB_county_south.dbf
          ‚îú‚îÄ‚îÄ SB_county_south.prj
          ‚îú‚îÄ‚îÄ SB_county_south.sbn
          ‚îú‚îÄ‚îÄ SB_county_south.shp
          ‚îú‚îÄ‚îÄ SB_county_south.shx
          ‚îú‚îÄ‚îÄ SB_validation_points.dbf
          ‚îú‚îÄ‚îÄ SB_validation_points.prj
          ‚îú‚îÄ‚îÄ SB_validation_points.qpj
          ‚îú‚îÄ‚îÄ SB_validation_points.shp
          ‚îú‚îÄ‚îÄ SB_validation_points.shx
          ‚îú‚îÄ‚îÄ trainingdata.dbf
          ‚îú‚îÄ‚îÄ trainingdata.cpg
          ‚îú‚îÄ‚îÄ trainingdata.prj
          ‚îú‚îÄ‚îÄ trainingdata.qpj
          ‚îú‚îÄ‚îÄ trainingdata.shp
          ‚îú‚îÄ‚îÄ landsat-data.shx
                ‚îú‚îÄ‚îÄ LT05_L2SP_042036_20070925_20200829_02_T1_SR_B1.tif
                ‚îú‚îÄ‚îÄ LT05_L2SP_042036_20070925_20200829_02_T1_SR_B2.tif
                ‚îú‚îÄ‚îÄ LT05_L2SP_042036_20070925_20200829_02_T1_SR_B3.tif
                ‚îú‚îÄ‚îÄ LT05_L2SP_042036_20070925_20200829_02_T1_SR_B4.tif
                ‚îú‚îÄ‚îÄ LT05_L2SP_042036_20070925_20200829_02_T1_SR_B5.tif
                ‚îú‚îÄ‚îÄ LT05_L2SP_042036_20070925_20200829_02_T1_SR_B7.tif
‚îú‚îÄ‚îÄ land-cover-classification.Rproj
‚îú‚îÄ‚îÄ land-cover-classification_files
‚îú‚îÄ‚îÄ land-cover-classification.html
‚îî‚îÄ‚îÄ  land-cover-classification.qmd
```

## References
U.S. Geological Survey, September 25, 2007. Landsat 5. U.S. Department of the Interior. Retrieved December 6, 2024, from https://www.usgs.gov/landsat-missions/landsat-5
",EDU,,['evajnewby'],,,1,0,,,,,,1,0.95
153884166,MDEwOlJlcG9zaXRvcnkxNTM4ODQxNjY=,ECE143_Project,Fernando-LopezGarcia/ECE143_Project,0,Fernando-LopezGarcia,https://github.com/Fernando-LopezGarcia/ECE143_Project,Data Analysis and Visualization Project,0,2018-10-20 08:13:00+00:00,2018-12-21 01:54:06+00:00,2018-12-21 01:54:04+00:00,,7626,0,0,Jupyter Notebook,1,1,1,1,0,0,4,0,0,0,,1,0,0,public,4,0,0,master,1,,"# Engineering Course Analysis (Group 6)

## Team Members
- Renjie Zhu (@johnsonlumia)
- Fernando Lopez Garcia (@Fernando-LopezGarcia)
- Daoyu Li (@Nwoodle)
- Ambareesh Sreekumaran Nair Jayakumari (@ambareeshsrja16)

## Problem
Analysing course offerings in engineering departments of top universities in the United States related to industry demands.

## Summary
Technology has been evolving ever so rapidly for the past two decades.¬†
Both students and Departments¬†concerned with technical learning¬†need to keep up with the demanding changes.¬†
The Course Catalog of a department gives a brief, yet exhaustive¬†description of all subjects covered. 
Analysis on a new dataset created from course catalogs provided an interesting challenge to understand how the universities have changed over time.¬†

## Methodology

- Catalog descriptions were scrapped to analyze the occurrence of every word and pair of words (2006-2019)
- The unigrams, and bigrams along with their occurrence frequency¬†were stored in a database (managed through SQLite)
- Pairs of words (or Bigrams) were of relatively higher relevance (""Signal Processing"", ""Machine Learning"", ""Fuzzy Logic"", ""Integrated Circuits"")
- To shorten the features of interest and derive more meaning, lemmatization was used (NLTK python library)
- Words with minimal frequency and common words (""credits"", ""pre-requisites"", ""classes"") were discarded, to make the final processed dataset.
- Job descriptions from companies were collected and they underwent the above processing as well.


## Dataset

```
Universities (UCSD, UCB, UCLA, UCSB)
|
+--- ECE ---+   + Catalog of 2006 - 2007
|           |   |           .
+--- CSE ---+---|           .
|           |   |           .
+--- MAE ---+   + Catalog of 2018 - 2019
```
*Primary:*
- Course catalogs from 2002 to 2019 from 4 UC's. We will be scrapping for ECE, CSE and MAE Departments. 

*Secondary:*
-  Current industry job requirements from 10 leading companies in various domains, such as Qualcomm, Amazon, Google, Apple and so on.

## Applications
1. Identifying the most common skills that have been promoted by departments from different universities, which will serve as the basis of our project.
2. Analyzing how the course offerings of a department at UCSD have changed over time and observing the degree of change.
3. Ascertaining if changes in course curriculum of departments reflect the state of the industries that they are concerned with. We can discuss the relation between departments and the corresponding industries.
4. Analyzing how the offerings have changed for the same department across different universities.
5. Observing the extent of overlap between two departments of a university in terms of skills for domains (for e.g. Machine Learning)
6. Identifying how the present job requirements from different companies map to the course offerings.

## File Structure

```
Root
|
+----raw_data
|
+----industry_data
|
+----processed_data
|
+----scripts
|       |   create_processed_data.py
|       |   word_freq.py
|       |   SQLite.py
|       |   common_words.txt
|       |   Industry_words.txt
|
|    main.py
|    analyse_data.py
|    analyse_data_department_only.py
|    Plot_Extent_of_overlap.py
|    Plot_radar_chart.py
|    Plot_ucsd_cse.py
|    Plot_ucsd_ece.py
|    Plot_department.py
|    merge_industry.py
|    Notebook_for_overview.ipynb
```

## Instructions on running the code

* Python version: Python 3.6.6 64-bit
### Required packages

1. numpy
1. pandas
2. matplotlib
3. matplotlib_venn
3. [apsw](####apsw)
4. sqlite3
5. plotly
6. xlrd

For installing these packages, you can use either ```pip3``` to install packages. For example, 

```pip3 install numpy```

#### ```apsw```

In part of our code, ```apsw``` is used to create connections to our database. 

Please install it using:

```pip3 install apsw```

If the above failed or you don't have a C compiler, please go to [this link](https://rogerbinns.github.io/apsw/download.html) to download binaries for your specific machine. I would recommend using conda to install this package on macOS.

### Run the code
1. Run the ```main.py``` to generate all the data from raw txt files in ```industry_data``` and ```raw_data```  
2. Run the ```Plot_Extent_of_overlap.py```, ```Plot_radar_chart.py```, ```Plot_ucsd_ece.py``` etc. to get the graphs.
",EDU,,"['Nwoodle', 'renjiezhu', 'ambareeshsrja16', 'Fernando-LopezGarcia']",,,1,0,,,,,,2,0.6
211811729,MDEwOlJlcG9zaXRvcnkyMTE4MTE3Mjk=,CS292C,fredfeng/CS292C,0,fredfeng,https://github.com/fredfeng/CS292C,,0,2019-09-30 08:24:04+00:00,2024-12-08 13:17:17+00:00,2024-05-29 17:55:25+00:00,,115421,20,20,Coq,1,1,1,1,0,0,11,0,0,0,,1,0,0,public,11,0,20,master,1,,"# CS292C Computer-Aided Reasoning for Software

This course is a graduate level introduction to formal program verification techniques, and their application in tools for the design, analysis, and construction of software. In the first half of the course, we will survey the automated side of program verification -- introducing the logical foundations and algorithms behind SAT and SMT **solvers**. However, as we will see, the automated approach is inherently limited by the undecidability problem. Thus, in the second half of the course, we will turn to manual side, which aims to prove the correctness of programs with *manual proofs* machine-checked by an **interactive theorem prover**.

# Office hour

Instructor : Yu Feng

- Office hour: 9am-10am, Tuesday, HFH 2157

TA/Tutorial Instructor: Junrui Liu (junrui@ucsb.edu)

- Office hour: 2pm-3pm, Friday, Place TBD

Lecture Time: 11am-12:50pm, M/W, Phelps 3526

Slack: https://join.slack.com/t/cs292c-spring24/shared_invite/zt-2g1hpa39v-OjeYeMBPG1uMa7fCGcujqQ


| #   | Date | Lecture                                                                   | Tutorial                                                  | Out                        | Due   |
| --- | ---- | ------------------------------------------------------------------------- | --------------------------------------------------------- | -------------------------- | ----- |
| 1   | 4/1  | [Overview](./lectures/lecture1.pdf)                                       | [Dafny](./tutorials/01-dafny/demo.dfy)                    | [hw1](./homework/hw1/)     |
| 2   | 4/3  | [IMP: Syntax & Semantics](./lectures/lecture2.pdf)                        | [OCaml: Basics, Data Types](./tutorials/02-ocaml/demo.ml) |
| 3   | 4/8  | [Hoare Logic 1](./lectures/lecture3.pdf)                                  | [OCaml: Recursive Types](./tutorials/03-ocaml/)           | [hw2](./homework/hw2/)     | hw1   |
| 4   | 4/10 | [Hoare Logic 2](./lectures/lecture3.pdf)                                  | [OCaml: List, Arrays, ASTs](./homework/hw2)               |
| 5   | 4/15 | [Guarded Commands, VC Gen](./lectures/lecture3.pdf)                       | Project 1 Walkthrough                                     | [proj1](./projects/proj1/) | hw2   |
| 6   | 4/17 | [SMT Solvers: Overview](./lectures/lecture4.pdf)                          |                                                           |
| 7   | 4/22 | [SAT: Basics, DPLL](./lectures/lecture4.pdf)                              | DPLL Refutation                                           |
| 8   | 4/24 | [SAT: CDCL](./lectures/lecture5.pdf)                                      | CDCL Refutation                                           |                            |       |
| 9   | 4/29 | [SAT: Extensions & Applications](./lectures/lecture6.pdf)                 | Project 2 Walkthrough                                     | [proj2](./projects/proj2/) | proj1 |
| 10  | 5/1  | [More Theories: Uninterpreted Functions, Arrays](./lectures/lecture7.pdf) | [Coq: Basics](./tutorials/coq/Script.v)                   | [hw3](./homework/hw3/)     |
| 11  | 5/6  | More Theories: LIA, LRA                                                   | [Coq: Induction](./tutorials/coq/Script.v)                |
| 12  | 5/8  | [Nelson-Oppen](./lectures/lecture9.pdf)                                   | [Coq: Lists, Poly](./tutorials/coq/Script.v)              | [hw4](./homework/hw4/)     | hw3   |
| 13  | 5/13 | DPLL(T)                                                                   |                                                           |
| 14  | 5/15 | (TBD)                                                                     |                                                           | [hw5](./homework/hw5/)     | hw4   |
| 15  | 5/20 | No class (Out of town)                                                    |                                                           |                            | proj2 |
| 16  | 5/22 | -                                                                         | -                                                         | [hw6](./homework/hw6)      | hw5   |
| 17  | 5/27 | Memorial Day                                                              |                                                           |
| 18  | 5/29 | Program Synthesis                                                         |                                                           | [proj3](./projects/proj3/) | hw6   |
| 19  | 6/3  | Solver-Aided Programming                                                  |                                                           |
| 20  | 6/5  | (Research Topics)                                                         | -                                                         |
| -   | 6/14 | (Quarter Ends)                                                            |                                                           |                            | proj3 |


# Grading (No curving)

| Letter | Percentage |
| ------ | ---------- |
| A+     | 95‚Äì100%    |
| A      | 90‚Äì94%     |
| A-     | 85‚Äì89%     |
| B+     | 80‚Äì84%     |
| B      | 75‚Äì79%     |
| B-     | 70‚Äì74%     |
| C+     | 65‚Äì69%     |
| C      | 60‚Äì64%     |
| F      | <60%       |

- Weekly written assignments: 40%
   - Self-graded on **effort** on a scale of 0-2, but will be checked by the TA:
     - 0: problem not attempted at all
     - 1: problem attempted, but not fully solved, and all work shown
     - 2: problem fully solved
   - *A self-grade of 1 is enough to get full credit for a problem.*

- 3 programming projects: 60%



# Late Policy
You have a total of 10 late days to be used throughout the quarter. You can use them in any way you like.



# Academic Integrity
- Cheating WILL be taken seriously. It is not fair toward honest students to take cheating lightly, nor is it fair to the cheater to let him/her go on thinking that cheating is a reasonable alternative in life.
- The following is not considered cheating:
   - discussing broad ideas about programming assignments in groups, without being at a computer (with code-writing and debugging done individually, later).
- The following is considered cheating:
   - discussing programming assignments with someone who has already completed the problem, or looking at their completed solution.
   - looking at anyone else‚Äôs solution
   - Previous versions of the class.
   - leaving your code (for example in an online repository) visible to others, leading others to look at your solution.
   - receiving, providing, or soliciting assistance from unauthorized sources during a test.
- Programming assignments are not intended to be grade-makers, but to prepare you for the tests, which are the grade-makers. Cheating on the programming assignment is not only unethical, but shows a fundamental misunderstanding of the purpose of these assignments.
- Penalties: First time: a zero for the assignment; Second time: an ‚ÄúF‚Äù in the course.



# References

- Rondon, Patrick M., Ming Kawaguci, and Ranjit Jhala. ""Liquid types."" PLDI'2008.

- Ali Sinan K√∂ksal, Yewen Pu, Saurabh Srivastava, Rastislav Bod√≠k, Jasmin Fisher, Nir Piterman. Synthesis of biological models from mutation experiments. Principles of Programming Languages (POPL). 2013. ACM DL

- Srivastava, Saurabh, Sumit Gulwani, and Jeffrey S. Foster. From program verification to program synthesis. POPL 2010.

- Jha, Susmit, et al. Oracle-guided component-based program synthesis. ICSE 2010.

- Gulwani, Sumit. Automating string processing in spreadsheets using input-output examples. POPL 2011.

- Phothilimthana, Phitchaya Mangpo, et al. ""Scaling up superoptimization."" ASPLOS 2016.

- Chandra, Kartik, and Rastislav Bodik. Bonsai: synthesis-based reasoning for type systems. POPL 2017.

- Bornholt, James, et al. Optimizing synthesis with metasketches. POPL 2016.

- Yaghmazadeh, Navid, et al. SQLizer: query synthesis from natural language. OOPSLA 2017. **Distinguished Paper Award**

- Deepcoder: Learning to write programs. Matej, et al. ICLR'16.

- Helgi Sigurbjarnarson, James Bornholt, Emina Torlak, and Xi Wang. Push-Button Verification of File Systems via Crash Refinement. OSDI 2016. **Best Paper Award**

- Shaon Barman, Sarah E. Chasins, Rastislav Bodik, Sumit Gulwani. Ringer: web automation by demonstration. OOPSLA 2016.

- Luke Nelson, Jacob Van Geffen, Emina Torlak, and Xi Wang. Specification and verification in the field: Applying formal methods to BPF just-in-time compilers in the Linux kernel. OSDI 2020.

- Chenming Wu, Haisen Zhao, Chandrakana Nandi, Jeff Lipton, Zachary Tatlock, Adriana Schulz. Carpentry Compiler. SIGGRAPH ASIA 2019.

- Permenev, Anton, et al. Verx: Safety verification of smart contracts. 2020 IEEE Symposium on Security and Privacy 2020.

- Chenglong Wang, Yu Feng, Ras Bodik, Alvin Cheung, Isil Dillig. Visualization by Example. POPL'2020.

- Beckett, Ryan, et al. Network configuration synthesis with abstract topologies. PLDI'2017.

- Dai, Wang-Zhou, et al. Bridging machine learning and logical reasoning by abductive learning. NIPS'2019.



",EDU,,"['fredfeng', 'junrui-liu']",,,1,0,,,,,,5,0.95
702583751,R_kgDOKeCTxw,SDG-PROJECT,garimasaigal/SDG-PROJECT,0,garimasaigal,https://github.com/garimasaigal/SDG-PROJECT,,0,2023-10-09 15:34:32+00:00,2023-10-09 15:36:16+00:00,2023-10-09 15:36:00+00:00,,38,0,0,Python,1,1,1,1,0,0,0,0,0,0,gpl-2.0,1,0,0,public,0,0,0,main,1,,"# Trends.Earth

[![Trends.Earth](https://s3.amazonaws.com/trends.earth/sharing/trends_earth_logo_bl_600width.png)](http://trends.earth)

[![Documentation Status](https://readthedocs.org/projects/trendsearth/badge/?version=latest)](https://trendsearth.readthedocs.io/en/latest/?badge=latest)
[![Zipfile Status](https://github.com/ConservationInternational/trends.earth/actions/workflows/build_zipfile.yaml/badge.svg)](https://github.com/ConservationInternational/trends.earth/actions/workflows/build_zipfile.yaml)
[![Update translations](https://github.com/ConservationInternational/trends.earth/actions/workflows/translation_update.yaml/badge.svg)](https://github.com/ConservationInternational/trends.earth/actions/workflows/translation_update.yaml)
[![Tests](https://github.com/ConservationInternational/trends.earth/actions/workflows/test.yaml/badge.svg)](https://github.com/ConservationInternational/trends.earth/actions/workflows/test.yaml)

`Trends.Earth` is a free and open source tool to understand land change: the how and why
behind changes on the ground. Trends.Earth allows users to draw on the best available
information from across a range of sources - from globally available data to customized
local maps. A broad range of users are applying Trends.Earth for projects ranging from
planning and monitoring restoration efforts, to tracking urbanization, to developing
official national reports for submission to the United Nations Convention to Combat
Desertification (UNCCD).

`Trends.Earth` is a [QGIS](http://www.qgis.org) plugin that supports monitoring of land
change, including trends in urbanization, and changes in productivity, land cover, and
soil organic carbon. The tool can support monitoring land degradation for reporting to
the Global Environment Facility (GEF) and United Nations Convention to Combat
Desertification (UNCCD), as well as tracking progress towards achievement of Sustainable
Development Goal (SDG) target 15.3, Land Degradation Neutrality (LDN).

`Trends.Earth` was produced by a partnership of Conservation International, Lund
University, and the National Aeronautics and Space Administration (NASA), with
the support of the Global Environment Facility (GEF). It was further developed
through a partnership with Conservation International, University of Bern,
University of Colorado in partnership with USDA and USAID, University of California -
Santa Barbara in partnership with University of North Carolina - Wilmington and Brown
University with additional funding from the Global Environment Facility (GEF).

## Documentation

See the [user guide](http://trends.earth) for information on how to use
the plugin.

## Installation of stable version of plugin

The easiest way to install the plugin is from within QGIS, using the [QGIS
plugin repository](http://plugins.qgis.org/plugins/LDMP/). However, It is also
possible to install the plugin manually from a zipfile, which can be useful to
access an old version of the plugin, or to install the plugin without internet.
Instructions for both of these possibilities are below.

### Stable version from within QGIS (recommended)

The easiest way to install the plugin is from within QGIS, using the [QGIS
plugin repository](http://plugins.qgis.org/plugins/LDMP/).

### Stable version from zipfile

Download a stable version of `Trends.Earth` from
[the list of available releases on
GitHub](https://github.com/ConservationInternational/trends.earth/releases). Then follow
the instructions below on [installing the plugin from a
zipfile](#installing-plugin-from-a-zipfile).

## Installation of unstable version of plugin

If you are interested in using the unstable (actively under development) version of the
plugin, with the very latest (but not as well tested) features, or in contributing to
the development of it, you will want to install the development version. There are
two ways to install the development version:

- Using a packaged version (zipfile)

- Cloning the github repository and installing from that code

It is easier to install the plugin from a zipfile than from github, so this
option is recommended unless you are interested in contributing to development
of the plugin.

### Development version from zipfile

[Download the latest `Trends.Earth` zipfile](https://s3.amazonaws.com/trends.earth/sharing/LDMP_main.zip) (or use the
[the zipfile from the develop branch
](https://s3.amazonaws.com/trends.earth/sharing/LDMP_develop.zip) for the absolute
latest version). Then follow the instructions below on [installing the plugin
from a zipfile](#installing-plugin-from-a-zipfile).

QGIS3+ is required for the latest versions of Trends.Earth. The QGIS2 version is no
longer supported (support ended in March 2020). If you want to use a previous version of
`Trends.Earth` (e.g. versions that work with QGIS2), please refer to this
[repository](https://github.com/ConservationInternational/trends.earth/releases) where
all `Trends.Earth` releases are available.

### Development version from source

Open a terminal window and clone the latest version of the repository from
Github:

```
git clone https://github.com/ConservationInternational/trends.earth
```

Navigate to the root folder of the newly cloned repository, and install
`invoke`, a tool that assists with installing the plugin:

```
pip install invoke
```

Now run the setup task with `invoke` to pull in the external dependencies needed
for the project:

```
invoke plugin-setup
```

then you can install the plugin using invoke:

```
invoke plugin-install --profile=<profile name>
```

If you modify the code, you need to run `invoke plugin-install` to update the
installed plugin in QGIS. You only need to rerun `invoke plugin-setup` if you
change or update the plugin dependencies. After reinstalling the plugin you
will need to restart QGIS or reload the plugin. Install the ""Plugin reloader""
plugin if you plan on making a log of changes
(https://github.com/borysiasty/plugin_reloader).

## Installing plugin from a zipfile

While installing `trends.earth` directly from within QGIS is recommended, it
might be necessary to install the plugin from a zipfile if you need to install
it offline, or if you need the latest features.

To install from a zipfile, first download a zipfile of the
[stable](#stable-version-from-zipfile) or
[development](#development-version-from-zipfile) version. The zipfile might be
named `LDMP.zip`or `LDMP_QGIS3.zip` depending on what
version you are installing.

When using QGIS3.10.3 or greater versions it is possible to install `trends.earth`
directly from a zipfile. To install `trends.earth` from a zipfile, open QGIS3.10.3
(or greater) and click on ""Plugins"" then on ""Manage and install plugins"" and
choose the option ""Install from ZIP"". Browse to the folder in which the zipfile
has been saved, select the zipfile and click on 'Install Plugin'.
It is not necessary to unzip the file.

Please, note that the latest version of `trends.earth` is only supported for
QGIS3.10.3 or greater versions.

Start QGIS, and click on ""Plugins"" then ""Manage and install plugins"". In the
plugins window that appears, click on ""Installed"", and then make sure there is
a check in the box next to ""Land Degradation Monitoring Tool"". The plugin is
now installed and activated. Click ""Close"", and start using the plugin.

## Getting help

### General questions

If you have questions related to the methods used by the plugin, how to use a
particular dataset or function, etc., it is best to first check the [user
guide](http://trends.earth/docs/en) to see if your question is already
addressed there. The [frequently asked questions (FAQ)
page](http://trends.earth/docs/en/about/faq.html) is another good place to
look.

If you don't find your answer in the above pages, you can also [contact the
discussion group](https://groups.google.com/forum/#!forum/trends_earth_users).

### Reporting an issue (error, possible bug, etc.)

If you think you have found a bug in Trends.Earth, report your issue to our
[issue
tracker](https://github.com/ConservationInternational/trends.earth/issues) so
the developers can look into it.

When you report an issue, be sure to provide enough information to allow us to
be able to reproduce it. In particular, be sure to specify:

- What you were doing with the plugin when the problem or error ocurred (for
  example ""I clicked on 'Download Results' and got an error messaging saying
  `describe what the message said`"".
- The operating system you are using, version of the plugin are you using, and
  version of QGIS that you are using
- If you are emailing about an error or problem that occurred when downloading
  results, tell us your username, and the task start time that is listed in the
  download tool for the task you are referring to
- If the error occurred while processing data with the plugin, tell us the
  location you were analyzing with the tool (for example: ""I selected Argentina
  from the dropdown menu""). If you used your own shapefile, please send us the
  file you used.
- If you got a message saying ""An error has ocurred while executing Python
  code"", send us either the text of the message, or a a screenshot of the error
  message. **Also, send us the content of the Trends.Earth log messages
  panel.** To access the Trends.Earth log messages panel, select ""View"", then
  ""Panels"", then ""Log Messages"" from within QGIS. Copy and paste the text from
  that panel and include it in your issue report. It will make it easiest for
  us to track things down (as their will be fewer log messages) if you do this
  after first starting a new QGIS session and immediately reproducing the
  error.

## License

`Trends.Earth` is free and open-source. It is licensed under the GNU General
Public License, version 2.0 or later
",DEV,,['garimasaigal'],,,1,0,,,,,,1,0.7
882033514,R_kgDONJLDag,haylee360,haylee360/haylee360,0,haylee360,https://github.com/haylee360/haylee360,,0,2024-11-01 18:33:19+00:00,2024-12-20 06:18:56+00:00,2024-12-20 06:18:54+00:00,,3,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# Hi, I'm Haylee! üåø

I am an interdisciplinary environmental scientist interested in applying data science tools to study human-environment interactions. Specifically, I'm interested in environmental justice, community-based conservation, and equitable climate change transitions.

## What I'm up to
I am currently a master of environmental data science student at the Bren School of Environmental Science and Management, University of California, Santa Barbara. Learn more about what I'm up to on [my website](https://haylee360.github.io/) or via email hoyler@bren.ucsb.edu üì´

## Education 
- Master of Environmental Data Science, UCSB üåä 2025
- Environmental Science B.S., University of California, Berkeley üêª 2021

<!--
**haylee360/haylee360** is a ‚ú® _special_ ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
- üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->
",OTHER,,['haylee360'],,,1,0,,,,,,1,0.75
436921259,R_kgDOGgrjqw,CS130A_lab3,HBzhainan-wzw/CS130A_lab3,0,HBzhainan-wzw,https://github.com/HBzhainan-wzw/CS130A_lab3,,0,2021-12-10 09:26:28+00:00,2021-12-10 09:31:26+00:00,2021-12-10 09:31:23+00:00,,37,0,0,C++,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"	1. Every input file is associated with the corresponding output file(the names
are similar)
	2. When reading the edge list, you need to insert the newly discovered vertices to
the AVL tree. In case that both of the vertices of edge (x,y) do not exist in the AVL tree,
you should first insert x into the AVL tree, rebalance it if necessary and 
then insert y. (priority from left to right)
	3. Before printing the elements of a component in the graph, sort them in 
non-decreasing order. This would be also helpful for you to compare 
your results with sample output.


Feel free to contact me at jaber@ucsb.edu regarding the issues of Project3 
and input/output file. 
",EDU,,['HBzhainan-wzw'],,,1,0,,,,,,1,0.8
42129283,MDEwOlJlcG9zaXRvcnk0MjEyOTI4Mw==,TimeSync,hughesj919/TimeSync,0,hughesj919,https://github.com/hughesj919/TimeSync,A Java based Time Synchronization example,0,2015-09-08 17:53:54+00:00,2015-09-08 17:56:37+00:00,2015-09-08 18:35:02+00:00,,336,0,0,Java,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,master,1,,"#TimeSync
####Jordan Hughes
####UC Santa Barbara
####hughesj919 at gmail.com

Every computer has an internal clock, and, because of the physical nature of clocks, there is a tendency for clocks to drift or deviate over time from the actual time. This small project was for a distributed systems course in which we were to synchronize our local clock using time information obtained from a set of 5 servers around the world. The code originally contained the IP address and ports for those time servers, but this is taken out of the current code (those servers were not maintained past the length of the course).  For a little more info on clock synchronization see the following resources:

* [Clock Synchronization](http://soft.vub.ac.be/~tvcutsem/distsys/clocks.pdf) - A great overview of clocks and how they function within distributed systems 
* [Cristian's Algorithm](https://en.wikipedia.org/wiki/Cristian%27s_algorithm) - A common algorithm used for syncing a clock based on a single time server
* [Marzullo's Algorithm](https://en.wikipedia.org/wiki/Marzullo%27s_algorithm) - A method for using multiple time servers to resynchronize a local clock

Also please see the TimeSync.pdf file for the entire project write-up.
",EDU,,['hughesj919'],,,1,0,,,,,,1,0.7
576772784,R_kgDOImDasA,UCSB_Basketball,JakeJensema13/UCSB_Basketball,0,JakeJensema13,https://github.com/JakeJensema13/UCSB_Basketball,,0,2022-12-10 23:29:40+00:00,2022-12-10 23:38:21+00:00,2023-03-27 02:42:42+00:00,,1620,0,0,Jupyter Notebook,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# Link to article - https://dailynexus.com/2023-01-12/ucsb-mens-basketball-off-to-blazing-hot-start-with-high-powered-offense-although-attendance-continues-to-drop/

BigWestStats2022-2023.csv - Stats for all mens basketball teams in the Big West confrence for 2022-23 season

UCSB_Basketball (1).ipynb - Jupyter notebook

UCSB_Basketball.pdf - Fully ran jupyter notebook file

UCSB_Basketball_2022-2023.csv - Stats for UCSB mens basketball team for 2022-23 season

UCSB_Stats2020.csv -  Stats for UCSB mens basketball team for 2020-21 season

UCSB_Stats2021 - Sheet1.csv - Stats for UCSB mens basketball team for 2021-22 season

basketball_confrence2021.csv - Stats for all mens basketball teams in the Big West confrence for 2021-22 season
",DATA,,"['jakejensema', 'JakeJensema13']",,,1,0,,,,,,1,0.8
413135610,R_kgDOGJ_y-g,certificatechain,jefftellew/certificatechain,0,jefftellew,https://github.com/jefftellew/certificatechain,Repo for our project CertificateChain,0,2021-10-03 16:38:19+00:00,2021-10-03 17:47:05+00:00,2022-02-24 00:39:54+00:00,,22353,1,1,Java,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,main,1,,"# CertificateChain: Decentralized Healthcare Training Certificate Management System using Blockchain and Smart Contracts
Repo for our project CertificateChain. This project aims to create a blockchain-based database system to store certificates, as well as creating a java-based web app for users to interact with the system through.

# Authors

Jeff Tellew<sup>1</sup> and Tsung-Ting Kuo<sup>2</sup> ‚Ä†

`1` University of California Santa Barbara, Santa Barbara, CA, USA

`2` UCSD Health Department of Biomedical Informatics, University of California San Diego, La Jolla, CA, USA

`‚Ä†` Corresponding Author

# Introduction

This repo contains all code for our project, CertificateChain. Source code as well as testing scripts used to automate our experiments are both contained within this repo. There is also code for the web app that we tested with other DBMI members at the same time. Lastly, there are some example certificate PDF files that were used during experiments.

# Contact

Thank you for using our software. If you have any questions or suggestions, please kindly contact Jeff Tellew (jefftellew at gmail dot com), University of California Santa Barbara, Santa Barbara, CA, USA.

# Installation
This section will go over installing all of the necessary components to run the web app.

These steps include commands that are specifically for a `BASH` shell and directory structures that match **Unix** directories. If you are on Windows, you will need to adjust accordingly.

## Installing geth

First, you will need to install geth. This can be done with the following steps.

1. Download geth 1.8.11, available from [here](https://geth.ethereum.org/downloads/). You should download the binary file, not the installer. If you are on Ubuntu, [this is a direct download link](https://gethstore.blob.core.windows.net/builds/geth-linux-amd64-1.8.11-dea1ce05.tar.gz).
4. Unpack the archive by running the command `tar xvzf geth-linux-amd64-1.8.11-dea1ce05.tar.gz`
5. Install geth by running the command `cp geth-linux-amd64-1.8.11-dea1ce05/geth /usr/bin/geth` and `cp geth-linux-amd64-1.8.11-dea1ce05/geth /usr/local/bin`
6. Run the command `geth version` to ensure that geth is properly installed.

## Installing ethereum

Next, you will need to install some things for Ethereum. This is just a sequence of commands.

1. `apt-get install software-properties-common`
2. `add-apt-repository -y ppa:ethereum/ethereum`
3. `add-apt-repository -y ppa:openjdk-r/ppa`
4. `apt-get update`
5. `apt-get install solc openjdk-8-jdk`

## Clone the repo

Finally, you will need to clone this repo. If you haven't already, and aren't sure how, see [GitHub's help page](https://help.github.com/en/articles/cloning-a-repository). You may need to install git using `apt-get install git` and maven using `apt-get install maven`

Once you're done with that, you're all ready to go!

# Launching the Web App on a Server
The process of launching the web app on a server like an AWS Virtual Machine should be fairly straightforward. This section will explain the steps to take in order to do so.

1. Follow the steps in the [Installation](#Installation) section to set everything up.
2. Change into the root directory of the repo, which should be called `dbmi-19`
3. Start geth by running the script `startGeth.sh`. If you are running this on a Unix machine, you can use `screen` to run geth in the background. This is accomplished by creating a screen, starting geth with the script, and then detaching from the screen. The commands to do this are `screen -S geth` followed by `./scripts/startGeth.sh`. Then, hit the keys `CTRL + a + d` OR enter the command `screen -d` to detach. If you are not able to use `screen` or simply don't want to, you will need to find another way to run the script in the background or you will not be able to keep the web app running.
4. Start the web app. Again, if you have access to it, you can use `screen` to run this in the background. This can be accomplished with the command `screen -S web-app` followed by the command `mvn spring-boot:run`. Once more, use the key sequence `CTRL + a + d` OR use the command `screen -d` to detach from the screen. You will need to be in the repo root directory where `pom.xml` is located, which should be `dbmi-19` unless something has been changed.

# Launching the Web App Locally
You can launch the web app locally either from the command line as a `.jar` or from within an IDE like IntelliJ. This section will briefly cover how to launch the web app using IntelliJ.

1. Follow the steps in the [Installation](#Installation) section to set everything up.
2. Open IntelliJ and create a new project from an existing source. From the menu bar at the top, this would be `File > New > Project from Existing Sources...`.
3. Navigate to the location you downloaded the repo to and enter the repo root directory.
4. Find the file named `pom.xml` and select it.
5. Click ""OK"" to finish creating the project.
6. Start geth by running `startGeth.sh`. You can either run it through the file explorer or through the terminal.
7. Run the web app by clicking the green arrow near the top right of the screen. If you can't find it, you can also run it from the menu bar at the top by navigating to `Run > Run 'CertificatesApplication'` or by using the keyboard shortcut (Shift + F10 on Windows, not sure on Mac or Linux)

# Switching between Windows and Unix-based OS
There are a few small changes that need to be made to change between a machine running Windows and one running a Unix-based OS like Linux or a Mac.

In particular, there are modifications that need to be made to two files:
* `startGeth.sh`
* `CertificateDBSC.java`

### `startGeth.sh`
1. Open `startGeth.sh` in your text editor of choice (found in `/dbmi-19/scripts/`)
2. Locate the lines near the top of the file that are marked with `*** UNIX ***` and `*** WINDOWS ***`
3. Make sure that the lines in the section for the OS you are using are uncommented, and that the lines in the section for the other OS are commented out.

### `CertificateDBSC.java`
1. Open `CertificateDBSC.java` in your text editor of choice (found in `/dbmi/src/main/java/edu/ucsd/dbmi/certificates/contract/`)
2. Locate the line near the top of the file in the `INSTANCE VARIABLES` section that defines the variable `private static String dir`
3. Make sure that the line marked with the OS you are using is uncommented, and that the line marked with the OS you are not using is commented out
",DEV,,['jefftellew'],,,1,0,,,,,,1,0.9
399255353,MDEwOlJlcG9zaXRvcnkzOTkyNTUzNTM=,jkenchel,jkenchel/jkenchel,0,jkenchel,https://github.com/jkenchel/jkenchel,Config files for my GitHub profile.,0,2021-08-23 21:43:27+00:00,2021-08-23 21:51:45+00:00,2021-08-23 21:51:42+00:00,https://github.com/jkenchel,1,0,0,,0,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"- üëã Hi, I‚Äôm Josh Kenchel @jkenchel
- üëÄ I‚Äôm interested in ... computational biology
- üå± I‚Äôm currently learning ... Python
- üíûÔ∏è I‚Äôm looking to collaborate on ... modeling projects
- üì´ How to reach me ... jkenchel@ucsb.edu

<!---
jkenchel/jkenchel is a ‚ú® special ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
",OTHER,,['jkenchel'],,,1,0,,,,,,1,0.9
483377777,R_kgDOHM_CcQ,COVID-19-Vaccination-Report,jkhaidinh/COVID-19-Vaccination-Report,0,jkhaidinh,https://github.com/jkhaidinh/COVID-19-Vaccination-Report,"Our report looks into COVID vaccinations, vaccine equity metric quartile, and political parties by each county or zip code. Our goal in this report is to see whether higher vaccine equity metric quartile means higher COVID vaccinations as well as whether higher vaccinations is tied to a Democratic majority county. We will look into the various plots to analyze the data and ultimately come up with a conclusion if there is any correlation.",0,2022-04-19 19:12:40+00:00,2022-04-19 19:34:20+00:00,2022-04-20 04:07:35+00:00,,876,0,0,Jupyter Notebook,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# COVID-19-Vaccination-Report
This is a project for my PSTAT 100 course at UC Santa Barbara.

Our report looks into COVID vaccinations, vaccine equity metric quartile, and political parties by each county or zip code. Our goal in this report is to see whether higher vaccine equity metric quartile means higher COVID vaccinations as well as whether higher vaccinations is tied to a Democratic majority county. We will look into the various plots to analyze the data and ultimately come up with a conclusion if there is any correlation.

# Data
Refer to the covid-19-dataset folder for the datasets.

# Code
There are two separate files. 

The project-code.ipynb file is the code for:
- tidying data
- comparing data to see if there's correlation
- visualization (plots, graphs, etc.)

The project-report.ipynb files is the report. The images are missing but refer to the covid-report.pdf for everything.

",EDU,,['jkhaidinh'],,,1,0,,,,,,1,0.9
766789456,R_kgDOLbRHUA,Facial-Frenzy,Johnsonchan105/Facial-Frenzy,0,Johnsonchan105,https://github.com/Johnsonchan105/Facial-Frenzy,,0,2024-03-04 06:03:00+00:00,2024-03-04 06:04:17+00:00,2024-03-04 06:10:13+00:00,,136632,0,0,Python,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,main,1,,"# CS190B-F23-Facial-Frenzy
Facial Frenzy is a game built by the **expresso** team (Gretchen Lam, Johnson Chan, and Cappillen Lee)

## Introduction
Our vision for our Raspberry Pi expression game project is to create an engaging and interactive experience in which players' emotions shape the gameplay. We are using several devices to create a facial recognition game where the player has to make the expression specified by the computer. Utilizing OpenCV, a facial expression detection library, and a laptop and webcam, our game will enable players to communicate, connect, and express themselves in a fun way. Our design implementation will seamlessly integrate the Raspberry Pi, providing an endpoint and accessible platform for players to share their emotions through expressive play. Future features we plan to include are utilizing an Amazon Echo dot to interact with the raspberry pi and share the results of the gameplay.

## Features
- Main game that tests players ability to make facial expressions
- Stores players progress in the cloud
- Players can view their past expressions
- Leaderboard system to encourage social and friendly competition 

## System requirements
Install all python libraries in requirements.txt. Systems that are supported and software tools required for this project are listed below
- System
  - Windows 10+
  - Ubuntu 20+
  - Debian Linux Bullseye 11+
  - MacOS 12+
- Software tools
  - python 3.10+
  - tensorflow 2.0+
  - Postgres SQL
  - Firebase Storage

## Installation
Clone repo.
```bash
git clone git@github.com:ucsb/CS190B-F23-expresso-cappillen.git
```
Then run
```bash
cd CS190B-F23-expresso-cappillen
pip install -r requirements.txt
```
If you are working on a M1 Apple Silicon machine then you should uninstall tensorflow and reinstall:
```
pip install tensorflow-macos
```
### Build programs
To run the game, enter the commands:
```
cd game
python main.py
```
To run the web app alongside, enter the commands in your Raspberry Pi, Docker Container, or Host Machine:
```
cd rasp-api
python app.py
```
### Environment Variables and Secrets
Make sure to pass in the correct environment variables.
- PostgresSQL Database URI (string)
- Firebase service account credientials (in a json file)

## Contributors
- Cappillen Lee (cappillen@ucsb.edu)
- Gretchen Lam (gretchenlam@ucsb.edu)
- Johnson Chan (c_chan@ucsb.edu)

## Acknowledgments
We would like to thank UCSB, Professor Krintz, and our TAs.
",EDU,,"['calee14', 'gretchenlam', 'Johnsonchan105', 'Gopu2001']",,,1,0,,,,,,1,0.95
119452164,MDEwOlJlcG9zaXRvcnkxMTk0NTIxNjQ=,How-to-file-your-final-PhD-paperwork-at-the-University-of-California-Santa-Barbara,justinpearson/How-to-file-your-final-PhD-paperwork-at-the-University-of-California-Santa-Barbara,0,justinpearson,https://github.com/justinpearson/How-to-file-your-final-PhD-paperwork-at-the-University-of-California-Santa-Barbara,How to file your final PhD paperwork at the University of California Santa Barbara,0,2018-01-29 22:50:47+00:00,2018-01-30 00:05:49+00:00,2018-01-30 00:07:40+00:00,,450,0,0,TeX,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,,"# How to file your final PhD paperwork at the University of California Santa Barbara

How to file your final PhD paperwork at the University of California Santa Barbara


- Justin Pearson
- 2017-10-29


Terminology
-----------

- PQ: ProQuest -- publisher of thesises? Just UCSB or other institutions?
- IR: Institutional repository: ucsb's IR stores theses
- ETD: electronic thesis or dissertation
- ""awarded"" = ""conferred""
- When folks say ""your degree is dated / conferred / filed on Mar 2018"", they mean the date on transcript & diploma.


Questions
---------

- why would anyone embargo (delay) the publication of their thesis?
    - may need to keep hidden bc of a pending publication or patent
- 'open access' -- bad if you want to try to get your thesis published afterward. (is this true?)
    - I did Open Access.
    - Gail McMillan's article says only 45% publishers would consider publishing thesis if it was already open-access.
        - http://archive.gradpost.ucsb.edu/tools/2014/5/5/dont-fear-the-open-access-etd-electronic-thesis-and-disserta.html
- In STEM, worth it to copyright thesis? Why do or don't?
- Diploma: $19 charged to BARC (WON'T I BE SEPARATED ALREADY? Not sure. BARC tells you your status.)
- Diploma: when mailed? A: A couple months after Grad Div tells you you've satisfied all the criteria.
- Submit to Grad Div
    1. original signature page (8.5x11"")
        - Q: is this the page in my thesis? A: Yes, but the page in the thesis is left blank, whereas you print out a separate page 
    1. Copy of title page
        - Q: Like from thesis? A: Yes.
- Q: How notified when (1) Grad Div finished reviewing package? A: They email you.
- Q: How set up mail fwding of jppearson@ucsb.edu ? Not sure, I think it's forwarded for a year.
- Q: How do employers verify my PhD?
    - Note: not many employers verify.
    - A: They can pay for an official transcript (they need to provide personal info on you) or an official verification (has less info than a transcript)
        - This takes a few months for UCSB to send to the ""national student clearing house"" your degree info, ugh.
    - Q: What if I need to be verifiale right away? A: Most employers accept the Grad Div email.
- Q: How tell when all done? Like, BARC paid off, all business finished?
- Q: How pay filing fee? How much? A: Half of student services fee = $358/2 = $188.
    - Note: Only have to pay the filing fee if you're on a leave of absence, bc if you're registered your fees pay for it already.
    - THere's a $20 fee to petition to go on a ""filing leave of absence"".
- ProQuest publishes theses for just UCSB or lots of campuses? What do they do exactly?
- Q: Who cares about when their degree is dated?
- Note: Next qtr fees will be applied. You should cancel your next qtr in GOLD: GOLD -> left side -> ""petitions"" -> ""cancel quarter"". 
    - If LOA, wouls also have to cancel the following qtr as wel.




TO DO LIST
-------------


- Choose which of 2 deadlines: 
    - File before last day of qtr: 
        - ""within academic qtr""
        - Degree is dated for that qtr
    - File before next academic qtr: 
        - Degree is dated for that next qtr
        - No pay reg fees for that qtr
    - See [Filing Deadlines](#filing-deadlines) below
- Grades posted in [GOLD](https://my.sa.ucsb.edu/gold/login.aspx) (like, if you took any classes)
- [Committee nomination form](http://www.graddiv.ucsb.edu/docs/default-source/academic-services-documents/formi-coi-1-15.pdf)
    - [Linking page](http://www.graddiv.ucsb.edu/academic/forms-petitions)
- [Committee change form](http://www.graddiv.ucsb.edu/docs/default-source/academic-services-documents/form-ia-10-14.pdf?sfvrsn=0) (if it changed)
- If on [filing leave of absence](http://www.graddiv.ucsb.edu/academic/leave-of-absence/filing-leave-of-absence) (?) submit [Petition](http://www.graddiv.ucsb.edu/docs/default-source/academic-services-documents/request-for-a-filing-leave-of-absence.pdf) and Pay filing fee 
    - If you don't want to pay reg fees bc almost finished
    - $20 at Cashier's office 1212 SAASB
    - [Full policy](http://www.graddiv.ucsb.edu/academic/leave-of-absence/filing-leave-of-absence)
    - ""If you do not complete your degree during the Filing Leave quarter, you do not pay the Filing Fee; you will need to register and pay full fees for the next quarter in order to have your degree awarded."" [Source](http://www.graddiv.ucsb.edu/academic/leave-of-absence/filing-leave-of-absence)
- [Doctoral Form III](http://www.graddiv.ucsb.edu/docs/default-source/academic-services-documents/form-iii-10-14.pdf) signed by all committee members
    - [Linking page](http://www.graddiv.ucsb.edu/academic/forms-petitions)
- Pay filing fee 
    - $188, pay at Cashier's office (in SAASB), give receipt to Grad Div (3rd floor Cheadle)
- Do 2 Exit surveys
    - Req'd by graduate council
    - https://sed-ncses.org/GradDateRouter.aspx
    - https://ucsbirpa.az1.qualtrics.com/jfe/form/SV_6M4s0sBczlHjFyt
- Verify no [BARC](https://mybarc.ucsb.edu/SIWeb/login.jsp) blocks (ie, all paid up in BARC)
- Submit thesis electronically to ProQuest (By 11:59pm on filing deadline)
    - Use link: [ProQuest for UCSB](etdadmin.com/cgi-bin/school?siteId=67)
    - Watch [Proquest screencast](https://www.youtube.com/watch?v=yEHS4b5jRzU)
    - Public access: 2 options:
        - ""traditional"": folks gotta pay to view
        - Open Access ($95): anyone can access via PQ website
- Submit to Grad Div (By 4pm on filing deadline):
    1. original signature page (8.5x11"")
        - Q: is this the page in my thesis? A: Yes, but the page in the thesis is left blank, whereas you print out a separate page 
    1. Copy of title page
        - Q: Like from thesis?
    - When you submit, tell them you had a consultation already, so they know they have already started paperwork on you.
- Grad Div takes 2-3 months to go over your thesis package
    - can request rush, ~1 week
    - They only need a few days to verify your degree, not 2-3 months.
    - can order official verification (FROM WHO? A: In GOLD. Note: UCSB outsources it to the ""National Student Clearing House"").
    - I think I should order an official transcript and official verification.
        - My CCUT certificate doesn't show up on the unoffical transcripts or unofficial degree verifications.



Diploma
--------

- mailed to addr in GOLD (WHEN?)
- will not be fwded by USPS
- 805-893-2633 (registrar office?)
- $19 charged to BARC (WON'T I BE SEPARATED ALREADY?)



Filing leave of absence
-------------------------- 

http://www.graddiv.ucsb.edu/academic/leave-of-absence/filing-leave-of-absence


The Filing Leave of Absence (LOA) enables graduate students who have fulfilled all degree requirements except the final examination and filing of the master‚Äôs thesis, doctoral dissertation, supporting document (DMA students), or completion of master‚Äôs comprehensive examination or project to take a Filing LOA in lieu of registering. All research and a substantial portion of the thesis/dissertation/DMA supporting document must be drafted prior to the Filing leave quarter. 

Students on leave of absence are not registered students and therefore relinquish most student privileges and resources such as extensive use of faculty time.



Copyright
----------

- you own copyright
- can put copyright on 3rd page of thesis
- can register copyright w/ US copyright office
    - proquest offers reg for a fee (~$50) but you can do yourself



Filing Deadlines
-------------------

Each qtr, 2 deadlines:

1. Last day of the qtr (degree dated for that qtr)
    - ex: to have degree dated Fall 2017, should file everything by Dec 15 (or Dec 8?), 2017
2. Start of next qtr (degree dated that next qtr)
    - ex: turn in Jan 5, 2018 (right before winter qtr begins), degree is dated for winter 2018.
    - Don't have to pay reg fees for that following quarter (winter 2018)


Deadlines (source: [grad div](http://www.graddiv.ucsb.edu/academic/petition-advancement-filing-deadlines)):

*file w/in a qtr*

- Degree conferral will be the end of the academic quarter
- Summer 2017 deadline: 9/15/17 
- Fall 2017 deadline: 12/15/17 
- Winter 2018 deadline: 3/23/18
- Spring 2018 deadline: 6/15/18 
- Summer 2018 deadlin: 9/14/18    

*file btwn qtrs* 

- Degree conferral will be the end of the next academic quarter
- Summer 2017: 9/22/17 
- Fall 2017: 1/5/18  
- Winter 2018: 3/29/18 
- Spring 2018: 9/22/18 
- Summer 2018: 9/21/18 



Certificate in College and University Teaching
-----------------------------------------------

In my unofficial transcript, my CCUT is listed as ""L&S/ CCG/ INTST / CUT"" for ""College/ Objective/ Major/ Emphasis"", what does this mean?

https://my.sa.ucsb.edu/gold/UnofficialTranscript.aspx

In my Unofficial Verification, it's listed as ""Cert of Completion GRAD Curric in Interdisciplinary Studies""

https://my.sa.ucsb.edu/gold/UnofficialVerifications.aspx

Called Registrar, they told me it'll show up as ""Certificate in College and University Teaching"" on my official transcript (which costs $16) and my official verification (which costs $16 and isn't availble for 2-3 months), ugh.



Resources
-----------

- [UCSB Graduate Division: Dissertation & Thesis Filing Tutorial (Spring 2017)](https://www.youtube.com/watch?v=GXJFMDOxwTQ)
- [Filing Video: Submitting to Proquest](https://www.youtube.com/watch?v=yEHS4b5jRzU)
- Workshop: Email: ""Fwd: FOR YOUR STUDENTS: SIGN UP for the Fall Quarter Thesis/Dissertation/DMA Pre-Check Session"" [Gmail](https://mail.google.com/mail/u/0/#search/check+session/15f3a65531333372)
- [Grad Div overview](http://www.graddiv.ucsb.edu/academic/Filing-Your-Thesis-Dissertation-DMA-Document)
- [Grad Div checklist (PDF)](http://www.graddiv.ucsb.edu/docs/default-source/academic-services-documents/dissertation-thesis-filing-chklist-11-09-15.pdf)
- [Grad Div filing deadlines](http://www.graddiv.ucsb.edu/academic/petition-advancement-filing-deadlines)

Grad Div academic advising: 

- 3117 Cheadle
- M-F
- 9-12 (4 advisors)
- 1-4pm (2 advisors)
- <http://graddiv.ucsb.edu/contact>







",EDU,,['justinpearson'],,,1,0,,,,,,1,0.7
459026683,R_kgDOG1ww-w,kalebkwok,kalebkwok/kalebkwok,0,kalebkwok,https://github.com/kalebkwok/kalebkwok,,0,2022-02-14 05:22:08+00:00,2022-02-14 05:22:08+00:00,2022-07-25 19:41:46+00:00,,23,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# Hey üëã, this is Kaleb Guo.



<a href=""https://leetcode-cn.com/u/kalebkwok/"">
  <img align=""left"" alt=""Leetcode"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/leetcode.svg"" />
</a>
<a href=""https://www.linkedin.com/in/kaleb-kwok/"">
  <img align=""left"" alt=""LinkedIn"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.13.0/icons/linkedin.svg"" />
</a>
<a href=""https://ilab.cs.ucsb.edu/"">
  <img align=""left"" alt=""Research"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.13.0/icons/pytorch.svg"" />
</a>
<a href=""mailto: kalebguo@gmail.com "">
  <img align=""left"" alt=""Gmail"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/gmail.svg"" />
</a>
<a href=""https://www.zhihu.com/people/kaleb-86"">
  <img align=""left"" alt=""Zhihu"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/zhihu.svg"" />
</a>
<br />
<br />


**My short-term goal is to be a FullStack Software Development Engineer, and my long-term goal is to be a great Software Architect. I really want to design awesome products that can benefits many people. I am very happy you find me. Nice to meet you!**

* üéì  I am currently a B.S. in Computer Science student of **University of California, Santa Barbara**.
* üíº   I am a Research assiant on UCSB Four Eyes Lab.
* üßê   Interested in JavaScript based full stack. Recent focus on React.
* üå±   Currently learning React.js, Next.js, AWS, Solidity, and Polygon.
* üìö   Reading „ÄäDatabase System Concepts (7th Edition)„Äã„ÄäDesigning Data-Intensive Applications„Äã.
* ‚õµ   Encouraging people for open source collaborations.
- My email is: **kalebguo@gmail.com**.



* üëë   Some GitHub statistical reports:
<p align=""center"">
<img align=""center"" src=""https://github-readme-stats.vercel.app/api/top-langs/?username=kalebkwok&hide_langs_below=1&theme=default&line_height=27&layout=compact"" />
<img align=""center"" src=""https://github-readme-stats.vercel.app/api?username=kalebkwok&show_icons=true&count_private=true&include_all_commits=true&line_height=21"" alt=""kalebkwok's Github Stats"" />
<img align=""center"" src=""https://github-profile-trophy.vercel.app/?username=kalebkwok&column=7"" alt=""kalebkwok's Github Trophy"" />
</p>




**Languages:**
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/javascript.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/python.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/cplusplus.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/java.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/ruby.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/html5.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/css3.svg""></code>


**Development Framework:**
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/react.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/redux.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/next-dot-js.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/node-dot-js.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/numpy.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/pytorch.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/scikit-learn.svg""></code>


**Databases/Toolbox:**
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/mysql.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/mongodb.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/postgresql.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/apollographql.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/amazonaws.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/jekyll.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/tailwindcss.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/material-ui.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/git.svg""></code>


**If you like this readme file, please give me star! ‚ù§Ô∏è**
",OTHER,,['kalebkwok'],,,1,0,,,,,,1,0.8
566968961,R_kgDOIctCgQ,kathywu1201.github.io,kathywu1201/kathywu1201.github.io,0,kathywu1201,https://github.com/kathywu1201/kathywu1201.github.io,This is the Lab 7 Assignment for Geo W 12 Fall22 at UCSB.,0,2022-11-16 19:50:35+00:00,2022-11-16 19:51:24+00:00,2022-11-16 19:51:38+00:00,,174,0,0,HTML,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# kathywu1201.github.io
This is the Lab 7 Assignment for Geo W 12 Fall22 at UCSB.

Description: This lab assignment includes my favorite places around the world. Enjoy!
",EDU,,['kathywu1201'],,,1,0,,,,,,1,0.7
731686331,R_kgDOK5yluw,Time_Series_Analysis,kathywu1201/Time_Series_Analysis,0,kathywu1201,https://github.com/kathywu1201/Time_Series_Analysis,"This is a project from PSTAT 174 from University of California, Santa Barbara.",0,2023-12-14 16:30:00+00:00,2024-06-05 19:04:08+00:00,2023-12-14 16:30:51+00:00,,486,0,0,,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,0,main,1,,"# StackOverFlow-Python-Questions-Time-Series-Analysis
This is a project from PSTAT 174 from University of California, Santa Barbara.
",EDU,,['kathywu1201'],,,1,0,,,,,,1,0.8
139105246,MDEwOlJlcG9zaXRvcnkxMzkxMDUyNDY=,biobib,kcaylor/biobib,0,kcaylor,https://github.com/kcaylor/biobib,Build a UCSB Biobib from csv files.,0,2018-06-29 05:34:54+00:00,2023-06-30 02:22:08+00:00,2023-06-30 02:22:03+00:00,,2470,0,0,TeX,1,1,1,1,0,0,1,0,0,8,apache-2.0,1,0,0,public,1,8,0,master,1,,"biobib
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

This file will become your README and also the index of your
documentation.

## Install

``` sh
pip install biobib
```

## How to use

Fill me in please! Don‚Äôt forget code examples:

``` python
1+1
```

    2
",EDU,,['kcaylor'],,,1,0,,,,,,1,0.9
729626532,R_kgDOK303pA,biodiversity_index_phoenix,khj9759/biodiversity_index_phoenix,0,khj9759,https://github.com/khj9759/biodiversity_index_phoenix,,0,2023-12-09 20:30:18+00:00,2023-12-09 22:36:06+00:00,2023-12-13 21:41:06+00:00,,3290,0,0,Jupyter Notebook,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"

## Author or creator 
Haejin Kim
## Date of publication
December-12-2023

## Publisher  
University of California, Santa Barbara - Bren School
EDS220 - Working with Environmental Datasets

## Title or description 
Biodiversity Intactness Index change in Phoenix subdivision

## About
In 2021, Maricopa County, part of Phoenix's metropolitan area, saw the largest increase in developed land in the US since 2001. Urban growth pressures biodiversity. We analyze a biodiversity dataset (BII) to track BII changes near Phoenix from 2017 to 2020.

## Visualization
After analyzing geospatial and STAC datasets, we have generated a map that highlights the Phoenix subdivision and showcases the area where BII>= 0.75 in 2017. You can find this map stored in the image/ directory. 

## Highlight
- Counting pixels employing NumPy functions
- Handling geospatial raster datasets for reading and writing with rioxarray
- Manipulating geospatial data using geopandas
- Generating maps utilizing contextily
- Creating and customizing SpatioTemporal Asset Catalog (STAC) maps

## Data
1) Biodiversity Index in Phoenix City, Arizona - STAC maps
2) Arizona State Map: Census data - https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions

",OTHER,,['khj9759'],,,1,0,,,,,,1,0.95
538729434,R_kgDOIBxb2g,EEMB_508,leanderegg/EEMB_508,0,leanderegg,https://github.com/leanderegg/EEMB_508,Repository for code for UCSB EEMB 508: Introduction to Ecology,0,2022-09-19 23:12:16+00:00,2025-01-08 04:07:57+00:00,2024-10-10 21:38:20+00:00,,812,1,1,R,1,1,1,1,0,0,3,0,0,0,,1,0,0,public,3,0,1,main,1,,"# EEMB_508
Repository for code for UCSB EEMB 508: Introduction to Ecology

Each folder has the code (and sometimes data) for individual extensions. You can download the files using the 'Files' button in the upper right hand corner of the screen and selecting ""Download .zip file""

# If you've never delt with R before: 
# - https://blog.uvm.edu/tdonovan-vtcfwru/r-for-fledglings/
# - https://www.neonscience.org/resources/learning-hub/tutorials/resources-learn-r

",EDU,,['leanderegg'],,,1,0,,,,,,1,0.95
442544717,R_kgDOGmCyTQ,Wk1_Intro_RSeminar_W2022,leanderegg/Wk1_Intro_RSeminar_W2022,0,leanderegg,https://github.com/leanderegg/Wk1_Intro_RSeminar_W2022,"Shared Code Repo for UCSB EEMB R Seminar, WIN2022 (EEMB 595R)",0,2021-12-28 18:03:30+00:00,2022-01-05 20:58:22+00:00,2023-01-09 22:53:05+00:00,,11212,0,0,R,1,1,1,1,0,0,23,0,0,0,gpl-3.0,1,0,0,public,23,0,0,main,1,,"# RSeminar_W2022
Shared Code Repo for UCSB EEMB R Seminar, WIN2022 (EEMB 595R)

Created by Leander Anderegg (landeregg@ucsb.edu, https://github.com/leanderegg), based on materials created by legendary Dr. Ian Breckheimer (https://github.com/ibreckhe) for an old R Seminar at the University of Washington (https://github.com/UW-RSeminar-Fall2015)

# Schedule:
https://docs.google.com/spreadsheets/d/1qT8d15lUdDRsCYuoDvtLiX6k0yRfYSody_3x51BYPI8/edit?usp=sharing

# Listserv:
rstats@eemb.ucsb.edu (add your email to the Schedule or email landeregg@ucsb.edu to be added)

Guidelines for Presenters:

- Move Slow‚ÄîYou know your code, data and approach better than your audience.

- Make it Interactive‚ÄîInclude a simple exercise to give people a chance to play with the code.

- Upload materials at least 24hr in advance‚ÄîAllows folks to install packages, work out any platform-specific kinks.

- Motivate with Science‚ÄîWe are scientists, not programmers.

## Instructions for Presenters:
Sign into the GitHub website and create a new repository to hold the files you want to share. Name it using the convention ""WeekX_topic"". For example if you are presenting about regression on week 3, the name would be ""Week3_regression"". Optionally add a description, .gitignore, README, and license. Click ""Create repository"".

In RStudio, go to File > New Project... Select Version Control > Git

Clone the repository to your local machine. The repository URL should be https://github.com/*username*/*repository_name*.git. Choose a directory name and indicate where on your local machine you want the files to live, for example ~/leeanderegg/code/.

Click ""Create Project"". This will create a directory and that will sync to the remote repository. It will also download the files (README,.gitignore,LICENSE) that you created on GitHub to this directory, and create a new Rstudio Project file.

Get Coding! To start a new script, go to File > New File > R Script, and save it to the repository directory. Write some code and run it to make sure it works.

When you get done with a chunk of significant work, Save the script and select the ""Git"" tab from the upper-right panel in RStudio. Click ""Commit"".

Click the blue down arrow to pull changes from the remote server, then hit the check marks next to the files you want to sync to the remote repo. Add a message describing what you just did and click the ""Commit"" button.

If everything has gone smooth, you are ready to push changes to the remote server. Do that by clicking the ""Push"" button. Now you are ready to do another chunk of work. Save, Pull, Commit, Push, Repeat.

When you are done with the code, email me (landeregg@ucsb.edu) and I will fork the repository to the R Seminar Organization so it will be easy for the rest of the group to find.

Note on public vs. private repositories
The default for GitHub (and this seminar) is to have all of the files in a repository, including code and data, open for public examination. If that is not possible (for example you are using someone else's data), you can choose to keep a repository private when you create it. To do so without paying for a GitHub subscription, you will need to apply for an education discount (https://education.github.com/). After you get the discount, you can select the ""Private"" option when you create a new repository. This repository will not be visible to anyone except you by default. In order to share it with the rest of the seminar, you will need to give me access to this repository so I can create a ""fork"" of it that will be visible to the rest of the seminar participants, but not the general public. To do this, go to the repository's website, and click the ""Settings"" link on the right-hand side of the page, and then select the ""Collaborators"" tab. Add my GitHub username (@leanderegg) to the collaborators list.
",EDU,,['leanderegg'],,,1,0,,,,,,2,0.95
604873383,R_kgDOJA2ipw,name-alignment-paleo,ljwalker/name-alignment-paleo,0,ljwalker,https://github.com/ljwalker/name-alignment-paleo,,0,2023-02-22 00:54:59+00:00,2023-02-22 00:54:59+00:00,2024-03-05 23:49:28+00:00,,500,0,0,,1,1,1,1,0,0,0,0,0,0,cc0-1.0,1,0,0,public,0,0,0,main,1,,"---
# replace uri to point to the name resource you'd like to align
# a url without scheme like https:// (e.g., ```url: foodorganisms.txt```) 
# is assumed to be a local file in working directory
datasets:
    - url: NMNH-Paleo_EMu_taxonomy_chordata_genera_2024-03-02b.csv
      enabled: true
      type: text/csv
    - id: mdd
      enabled: false
      name: Mammal Diversity Database
      type: application/nomer
    - url: https://example.org/data.tsv
      enabled: false
      type: text/tab-separated-values
    - url: https://serv.biokic.asu.edu/ecdysis/content/dwca/UCSB-IZC_DwC-A.zip
      enabled: false
      type: application/dwca
    - url: https://scan-bugs.org:443/portal/webservices/dwc/rss.xml 
      enabled: false
      type: application/rss+xml
# 
taxonomies:
#
# Edit list below to select taxonomies you'd like to work with. 
#
# To enable taxonomies to align with set:
#  enabled: true 
#
# To disable taxonomies to align with set: 
#  enabled: false
#
    - id: worms
      enabled: true
      name: World Register of Marine Species
    - id: wikidata-web
      enabled: true
      name: Wikidata
    - id: itis
      enabled: true
      name: Integrated Taxonomic Information System
    - id: ncbi
      enabled: true
      name: NCBI Taxonomy
    - id: discoverlife
      enabled: true
      name: Discover Life Taxonomy
    - id: batnames
      enabled: false
      name: Bat Names 
    - id: col
      enabled: true
      name: Catalogue of Life
    - id: gbif
      enabled: true
      name: GBIF Backbone Taxonomy
    - id: globi
      enabled: false
      name: GloBI Taxon Graph
    - id: indexfungorum
      enabled: false
      name: Index Fungorum
    - id: mdd
      enabled: false
      name: Mammal Diversity Database
    - id: ott
      enabled: true
      name: Open Tree of Life Taxonomy
    - id: pbdb
      enabled: true
      name: Paleobiology Database
    - id: plazi
      enabled: true
      name: Plazi Treatments
    - id: tpt
      enabled: true
      name: Terrestrial Parasite Tracker Taxonomies
    - id: wfo
      enabled: false
      name: World of Flora Online
---

# Name Alignment

[![Name Alignment by Nomer](../../actions/workflows/align.yml/badge.svg)](../../actions/workflows/align.yml)

To find your automatically created name alignment report, click on ""Name Alignment by Nomer"" above, click on a workflow run, and  download the `alignment-report` artifact.

:bulb: Note that only logged-in GitHub users with access can download the alignment report generated by GitHub Actions.

## Background


Aligning taxonomic names is a common task in biodiversity informatics. 

This template repository offers an automated method to align scientific names in csv/tsv files and darwin core archive with common taxonomic name lists like Catalogue of Life, NCBI Taxonomy, Integrated Taxonomic Information System (ITIS), and GBIF Backbone taxonomy.

## Getting Started

Follow steps below or alternatively visit the [Big Bee](https://big-bee.net) pages with materials and recordings of the [Name Alignment Workshop
](https://big-bee-network.github.io/name-alignment-workshop) held 18 January 2023 to learn more.

1. create your own repository using this repository as a template
2. edit the README.md and add the urls / filenames to the resources you'd like to review. Note that only the following types are supported at time of writing (June 2022): ```text/csv```, ```text/tab-separated-values```, ```application/dwca```, and ```application/rss+xml```. Also, delete any taxonomy entries that you are not interested in: the fewer taxonomies to align with, the faster the review. 
3. edit taxonomies list in the README.md [front-matter](https://jekyllrb.com/docs/front-matter/) to select those you are interested to work with. Many are configured by default, and you can customize to make the configuration work best for your names.  
4. for now only names in column ""scientificName"" (tsv/csv), and ""http://rs.tdwg.org/dwc/terms/scientificName"" (DwC-A) will be aligned 
5. commit the changes to github
6. inspect results of name alignment in ""Github Actions"" (e.g., [sample results](https://github.com/globalbioticinteractions/name-alignment-template/raw/main/img/name-alignment-review-2022-11-14.log))
)
7. download name alignment report from the ""artifacts"" section 
8. to re-create/re-run results, change your name list in github or select [""re-run jobs"" in Github Actions](https://docs.github.com/en/actions/managing-workflow-runs/re-running-workflows-and-jobs).

# Origin

This repository was conceived on 2022-03-08 during the [Alien CSI Hack-a-thon](https://github.com/alien-csi/alien-csi-hackathon) in Romania by Christina, Quentin, Jorrit, Jasmijn, .... For more information see https://github.com/alien-csi/alien-csi-hackathon . 

# Contributors


name | affiliation | orcid 
--- | --- | ---
Jorrit Poelen | GloBI; Ronin Institute | https://orcid.org/0000-0003-3138-4118
Lindsay Walker | Arizona State University | https://orcid.org/0000-0002-2162-6593


# Feedback / issues

This repository uses scripts in https://github.com/globalbioticinteractions/globinizer. These script use commandline tools like [GloBI](https://globalbioticinteractions.org)'s [nomer](https://github.com/globalbioticinteractions/nomer), cut, sed, etc. 

# Misc Notes


install nomer java8 / java11 - 

https://github.com/globalbioticinteractions/nomer 

e.g., Carl Boettiger taxondb R package


Print names and add a tab in front, to prepare for nomer. 

```
cat foodorganisms.txt | sed 's/^/\\t/g' > foodorganisms.tsv
```

Nomer expects the format to be:

[id][tab][name]

e.g.,
id\\tname
NCBI:9606\\tHomo sapiens


Print names to screen and append itis taxonomic interpretation, and write/redirect to a file 'name-itis.tsv'

```
cat foodorganisms.tsv | nomer append itis > name-itis.tsv
```

open in LibreOffice Calc

Repeat with 'gbif' instead of 'itis'

## Provenance of DwC-A Names

The name context of names extracted from DwC-A are captured in a funny looking text:

```
line:zip:hash://sha256/fe63af46ed66abd253ee148e383fb51da6695ce3848d0bde39af18aa77d364fb!/occurrences.csv!/L10
```

extracted from a generated ```names-aligned.tsv```:

```
$ cat names-aligned.tsv | grep hash | grep occurrence | head -n1
line:zip:hash://sha256/fe63af46ed66abd253ee148e383fb51da6695ce3848d0bde39af18aa77d364fb!/occurrences.csv!/L10	Lasioglossum	SAME_AS	line:zip:hash://sha256/fe63af46ed66abd253ee148e383fb51da6695ce3848d0bde39af18aa77d364fb!/occurrences.csv!/L10	Lasioglossum								HAS_ACCEPTED_NAME	COL:5B4P	Lasioglossum	genus		Biota | Animalia | Arthropoda | Insecta | Hymenoptera | Apoidea | Halictidae | Halictinae | Halictini | Lasioglossum	COL:5T6MX | COL:N | COL:RT | COL:H6 | COL:HYM | COL:625GP | COL:625H4 | COL:JMV | COL:KV7 | COL:5B4P	unranked | kingdom | phylum | class | order | superfamily | family | subfamily | tribe | genus	https://www.catalogueoflife.org/data/taxon/5B4P	
```

This text identifies the row from which the name was extracted. In this case, line 10, from file ```occurrences.csv``` contained in the zip file with content id ```hash://sha256/fe63af46ed66abd253ee148e383fb51da6695ce3848d0bde39af18aa77d364fb``` . If you retain the tracked dataset (in this case UC Santa Barbara Invertebrate Zoology Collection accessed on 2022-06-30) provided in the data/ folder of the name aligment archive,  you can use [Preston](https://preston.guoda.bio) to dig up the original record using:

```
$ preston cat 'line:zip:hash://sha256/fe63af46ed66abd253ee148e383fb51da6695ce3848d0bde39af18aa77d364fb!/occurrences.csv!/L10' 
881449,UCSB,IZC,,b03a3f0c-bfa5-4e02-b5d3-56ff38626302,PreservedSpecimen,a8a4f8b1-38f1-4e10-9b75-b2e86ac196fc,UCSB-IZC00038312,,Animalia|Arthropoda|Hexapoda|Insecta|Pterygota|Neoptera|Hymenoptera|Apocrita|Aculeata|Apoidea|Halictidae|Halictinae|Halictini,Animalia,Arthropoda,Insecta,Hymenoptera,Halictidae,Lasioglossum,186125,""Curtis, 1833"",Lasioglossum,,,,,Genus,""EEMB/ENV S 96"",24-May-2022,,,,,,""Sophie Cameron"",,2022-04-26,2022,4,26,116,,,,""Newly restored salt marsh"",PAN2,,,,,,,""on flower of Eschscholzia californica"",,,Adult,Female,1,Pinned,""United States"",California,""Santa Barbara"",,""University of California Santa Barbara North Campus Open Space"",,34.42174,-119.87186,WGS84,10,,,,GPS,,,,,,,,,,,,""2022-05-31 10:52:55"",http://creativecommons.org/publicdomain/zero/1.0/,""The Regents of the University of California"",https://www.ccber.ucsb.edu/collections/databases-searching-specimen-data-and-images,urn:uuid:a8a4f8b1-38f1-4e10-9b75-b2e86ac196fc,https://serv.biokic.asu.edu/ecdysis/collections/individual/index.php?occid=881449
```

which links to a preserved specimen with occurrenceId b03a3f0c-bfa5-4e02-b5d3-56ff38626302 and landing page at https://serv.biokic.asu.edu/ecdysis/collections/individual/index.php?occid=881449 . Also see [screenshot made on 2022-06-30](./img/UCSB-IZC00038312_b03a3f0c-bfa5-4e02-b5d3-56ff38626302.png). 

With this context, you can trace the origin and context of the name in great detail. This detail can be used to troubleshoot bugs in the name alignment process, or provide granular feedback to those that maintain the dataset or taxonomy.  


",DATA,,['ljwalker'],,,1,0,,,,,,1,0.6
699948930,R_kgDOKbhfgg,leezamarie_test,lmarierodriguez/leezamarie_test,0,lmarierodriguez,https://github.com/lmarierodriguez/leezamarie_test,a test repo for git / github lesson,0,2023-10-03 16:45:44+00:00,2023-10-04 17:33:50+00:00,2023-10-04 17:28:18+00:00,,10,1,1,,1,1,1,1,0,0,0,0,0,0,apache-2.0,1,0,0,public,0,0,1,main,1,,"
# Leeza + Hailie Collab



a test repo for git / github lesson


# Purpose

- create a short repo on GitHub
- practice Git workflow

#Creator 

My name is Leeza-marie Rodriguez, I am a 2nd year Ph.D. student at the University of California, Santa Barbara. My email is [leeza-marie@ucsb.edu](mailto:leeza-marie@ucsb.edu) where you can contact me.

## How to Create a Git Repository from an existing project
# How to Create a Git Repository
# you go to your icon on Github and click your repositories
# then you click the new button in green with a book and bookmark icon
# add the repository name and decide whether its is a public or private repository
# add a README file
# add gitignore template and change it to R
# and for license, click the apache 2.0 one
# hit create repository


",EDU,,"['lmarierodriguez', 'hkittner']",,,1,0,,,,,,1,0.9
845703204,R_kgDOMmhoJA,workshop-tidyverse,lter/workshop-tidyverse,0,lter,https://github.com/lter/workshop-tidyverse,LTER Scientific Computing team workshop: Coding in R's Tidyverse,0,2024-08-21 19:03:36+00:00,2024-11-19 20:11:43+00:00,2024-11-19 20:12:09+00:00,https://lter.github.io/workshop-tidyverse/,6247,0,0,SCSS,1,1,1,1,1,0,0,0,0,0,bsd-3-clause,1,0,0,public,0,0,0,main,1,1,"# LTER Scientific Computing Workshop - Coding in R's Tidyverse

## Repository Explanation

For teams that code using the R programming language, the most familiar tools are often part of ""base R"" meaning that those functions and packages come pre-loaded when R is installed. Relatively recently the Tidyverse has emerged as a comprehensive suite of packages that can complement base R or serve as an alternative for some tasks. This includes packages like `dplyr` and `tidyr` as well as the perhaps infamous pipe operator (`%>%`) among many other tools. This workshop is aimed at helping participants use the Tidyverse equivalents of fundamental data wrangling tasks that learners may be used to performing with base R.

## Acknowledgment

The development of this training material is supported through the Long Term Ecological Research Network Office (LNO) (NSF award numbers 1545288 and 1929393) and the National Center for Ecological Analysis and Synthesis (NCEAS), UC Santa Barbara.

**Citation:** Nick J Lyon, Angel Chen, and Julien Brun. 2024. Coding in the Tidyverse. LTER Network Office Scientific Computing Team.
",EDU,,['njlyon0'],,,1,0,,"## Contributing Guidelines

When contributing to this project, please see our team's [primary contributing guidelines document](https://github.com/lter/scicomp/blob/main/CONTRIBUTING.md) and follow the guidance there.
",,,,5,0.75
380073066,MDEwOlJlcG9zaXRvcnkzODAwNzMwNjY=,nceas-arcticdatacenter,melanieleung/nceas-arcticdatacenter,0,melanieleung,https://github.com/melanieleung/nceas-arcticdatacenter,Scripts for editing metadata for the Arctic Data Center at NCEAS,0,2021-06-24 23:24:08+00:00,2021-06-24 23:32:47+00:00,2021-06-24 23:32:44+00:00,,15,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# nceas-arcticdatacenter
Scripts for editing metadata for the Arctic Data Center at NCEAS

A sample set of scripts used to create, edit, and update metadata for data packages published onto the Arctic Data Center catalog: https://arcticdata.io/catalog/ 
The NSF Arctic Data Center helps the research community reproducibly preserve and discover all products of NSF-funded science in the Arctic, including data, metadata, software, documents, and provenance that link these in a coherent knowledge model. Key to the initiative is the partnership between the National Center for Ecological Analysis and Synthesis (NCEAS) at UC Santa Barbara, DataONE, and NOAA‚Äôs National Centers for Environmental Information (NCEI), each of which bring critical capabilities to the Center. Infrastructure from the successful NSF-sponsored DataONE federation of data repositories enables data replication to NCEI, providing both offsite and institutional diversity that is critical to long term preservation.

Each script included in this repository represents the metadata processing workflow for an individual ticket, or for an individual data package eventually published onto the catalog after peer-review.
",DEV,,['melanieleung'],,,1,0,,,,,,1,0.8
754509246,R_kgDOLPjlvg,website,miguel-luis9/website,0,miguel-luis9,https://github.com/miguel-luis9/website,,0,2024-02-08 07:52:11+00:00,2024-02-08 07:52:11+00:00,2024-02-08 23:57:59+00:00,,11045,0,0,,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# Luis Miguel

## CS Student at UCSB

## Education
University of California, Santa Barbara
Computer Science B.S.
",OTHER,,['miguel-luis9'],,,1,0,,,,,,1,0.7
304941727,MDEwOlJlcG9zaXRvcnkzMDQ5NDE3Mjc=,climateR-intro,mikejohnson51/climateR-intro,0,mikejohnson51,https://github.com/mikejohnson51/climateR-intro,Introduction to gridded climate data in R,0,2020-10-17 18:19:06+00:00,2024-11-05 08:19:39+00:00,2020-10-20 17:10:51+00:00,https://mikejohnson51.github.io/climateR-intro/,11346,3,3,JavaScript,1,1,1,1,1,0,4,0,0,0,mit,1,0,0,public,4,0,3,main,1,,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# Gridded climate data with R

[These notes](https://mikejohnson51.github.io/climateR-intro/) are
prepared for the [UCSB ecodata science
club](https://eco-data-science.github.io/). The goal is to introduce the
concepts of gridded climate data, the process for remote data access,
and workflows for easy data retrieval in R. Specifically, we will cover
the following:

-----

### 1\\. Multi-diminsional raster data model:

<img src=""img/diminsions.png"" width=""937"" style=""display: block; margin: auto;"" />
<br><br><br><br>

### 2\\. Concepts & workflows for ingesting subsets of remote climate data

<img src=""img/cookie-cutter.png"" width=""759"" style=""display: block; margin: auto;"" />

<br><br><br><br>

### 3\\. Example Implementations

The simplest example would answer a question like:

> ‚ÄúWhat was the temperture and rainfall rate in Florida on Janury
> 1st-3rd, 2010?‚Äù

``` r
library(AOI)
library(climateR)
library(rasterVis)

AOI  <- aoi_get(state = ""FL"")

rain <-  getGridMET(AOI, 
                  param = c(""prcp"", ""tmax""), 
                  startDate = ""2010-01-01"", 
                  endDate = ""2010-01-03"")

levelplot(rain$prcp, par.settings = BTCTheme, main = ""Rainfall (mm)"")
levelplot(rain$tmax, par.settings = BuRdTheme, main = ""Maximum Temperture (k)"")
```

<img src=""README_files/figure-gfm/unnamed-chunk-5-1.png"" style=""display: block; margin: auto;"" /><img src=""README_files/figure-gfm/unnamed-chunk-5-2.png"" style=""display: block; margin: auto;"" />

Other examples covered will include:

  - time series extraction at points and sets of points,
  - working with ensemble data from climate projections;
  - basic zonal statistics and
  - raster animation‚Ä¶

<div class=""figure"" style=""text-align: center"">

<img src=""img/ppt.gif"" alt=""Hurricane Harvey: Pixel vs. County view"" width=""49%"" height=""20%"" /><img src=""img/ppt2.gif"" alt=""Hurricane Harvey: Pixel vs. County view"" width=""49%"" height=""20%"" />

<p class=""caption"">

Hurricane Harvey: Pixel vs.¬†County view

</p>

</div>

If successful, you will not leave this session as experts at any of
these tasks but will have a grasp on the tools need to implement them
and an understanding of the data models behind them that will allow you
to tackle your own climate-related projects.

-----

### Required libraries

You will need the following libraries to reproduce the content in these
notes:

`sf`, `raster`, `rasterVis`, `ggplot2`, `remotes`, `exactextractr`,
`RNetCDF`, `tidyr`, `dplyr`

all of which are on CRAN and can be installed using
`install.packages()`.

For example:

``` r
install.packages('sf')
```

Additionally, you will need to install `climateR` and `AOI` from github
using `remotes` as they are not on CRAN:

``` r
remotes::install_github(""mikejohnson51/AOI"")
remotes::install_github(""mikejohnson51/climateR"")
```
",EDU,,['mikejohnson51'],,,1,0,,,,,,2,0.75
554523373,R_kgDOIQ1a7Q,open_flamingo,mlfoundations/open_flamingo,0,mlfoundations,https://github.com/mlfoundations/open_flamingo,An open-source framework for training large multimodal models.,0,2022-10-20 00:32:35+00:00,2025-03-06 13:51:09+00:00,2024-08-31 23:11:03+00:00,,7719,3835,3835,Python,1,1,1,1,0,0,297,0,0,49,mit,1,0,0,public,297,49,3835,main,1,1,"# ü¶© OpenFlamingo

[![PyPI version](https://badge.fury.io/py/open_flamingo.svg)](https://badge.fury.io/py/open_flamingo)

[Paper](https://arxiv.org/abs/2308.01390) | Blog posts: [1](https://laion.ai/blog/open-flamingo/), [2](https://laion.ai/blog/open-flamingo-v2/) | [Demo](https://huggingface.co/spaces/openflamingo/OpenFlamingo)

Welcome to our open source implementation of DeepMind's [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)! 

In this repository, we provide a PyTorch implementation for training and evaluating OpenFlamingo models.
If you have any questions, please feel free to open an issue. We also welcome contributions!

# Table of Contents
- [Installation](#installation)
- [Approach](#approach)
  * [Model architecture](#model-architecture)
- [Usage](#usage)
  * [Initializing an OpenFlamingo model](#initializing-an-openflamingo-model)
  * [Generating text](#generating-text)
- [Training](#training)
  * [Dataset](#dataset)
- [Evaluation](#evaluation)
- [Future plans](#future-plans)
- [Team](#team)
- [Acknowledgments](#acknowledgments)
- [Citing](#citing)

# Installation

To install the package in an existing environment, run 
```
pip install open-flamingo
```

or to create a conda environment for running OpenFlamingo, run
```
conda env create -f environment.yml
```

To install training or eval dependencies, run one of the first two commands. To install everything, run the third command.
```
pip install open-flamingo[training]
pip install open-flamingo[eval]
pip install open-flamingo[all]
```

There are three `requirements.txt` files: 
- `requirements.txt` 
- `requirements-training.txt`
- `requirements-eval.txt`

Depending on your use case, you can install any of these with `pip install -r <requirements-file.txt>`. The base file contains only the dependencies needed for running the model.

## Development

We use pre-commit hooks to align formatting with the checks in the repository. 
1. To install pre-commit, run
    ```
    pip install pre-commit
    ```
    or use brew for MacOS
    ```
    brew install pre-commit
    ```
2. Check the version installed with
    ```
    pre-commit --version
    ```
3. Then at the root of this repository, run
    ```
    pre-commit install
    ```
Then every time we run git commit, the checks are run. If the files are reformatted by the hooks, run `git add` for your changed files and `git commit` again

# Approach
OpenFlamingo is a multimodal language model that can be used for a variety of tasks. It is trained on a large multimodal dataset (e.g. Multimodal C4) and can be used to generate text conditioned on interleaved images/text. For example, OpenFlamingo can be used to generate a caption for an image, or to generate a question given an image and a text passage. The benefit of this approach is that we are able to rapidly adapt to new tasks using in-context learning.

## Model architecture
OpenFlamingo combines a pretrained vision encoder and a language model using cross attention layers. The model architecture is shown below.

![OpenFlamingo architecture](docs/flamingo.png) 
Credit: [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)

# Usage
## Initializing an OpenFlamingo model
We support pretrained vision encoders from the [OpenCLIP](https://github.com/mlfoundations/open_clip) package, which includes OpenAI's pretrained models. 
We also support pretrained language models from the `transformers` package, such as [MPT](https://huggingface.co/models?search=mosaicml%20mpt), [RedPajama](https://huggingface.co/models?search=redpajama), [LLaMA](https://huggingface.co/models?search=llama), [OPT](https://huggingface.co/models?search=opt), [GPT-Neo](https://huggingface.co/models?search=gpt-neo), [GPT-J](https://huggingface.co/models?search=gptj), and [Pythia](https://huggingface.co/models?search=pythia) models.

``` python
from open_flamingo import create_model_and_transforms

model, image_processor, tokenizer = create_model_and_transforms(
    clip_vision_encoder_path=""ViT-L-14"",
    clip_vision_encoder_pretrained=""openai"",
    lang_encoder_path=""anas-awadalla/mpt-1b-redpajama-200b"",
    tokenizer_path=""anas-awadalla/mpt-1b-redpajama-200b"",
    cross_attn_every_n_layers=1,
    cache_dir=""PATH/TO/CACHE/DIR""  # Defaults to ~/.cache
)
```

## Released OpenFlamingo models
We have trained the following OpenFlamingo models so far.

|# params|Language model|Vision encoder|Xattn interval*|COCO 4-shot CIDEr|VQAv2 4-shot Accuracy|Weights|
|------------|--------------|--------------|----------|-----------|-------|----|
|3B| anas-awadalla/mpt-1b-redpajama-200b | openai CLIP ViT-L/14 | 1 | 77.3 | 45.8 |[Link](https://huggingface.co/openflamingo/OpenFlamingo-3B-vitl-mpt1b)|
|3B| anas-awadalla/mpt-1b-redpajama-200b-dolly | openai CLIP ViT-L/14 | 1 | 82.7 | 45.7 |[Link](https://huggingface.co/openflamingo/OpenFlamingo-3B-vitl-mpt1b-langinstruct)|
|4B| togethercomputer/RedPajama-INCITE-Base-3B-v1 | openai CLIP ViT-L/14 | 2 | 81.8 | 49.0 | [Link](https://huggingface.co/openflamingo/OpenFlamingo-4B-vitl-rpj3b)|
|4B| togethercomputer/RedPajama-INCITE-Instruct-3B-v1 | openai CLIP ViT-L/14 | 2 | 85.8 | 49.0 | [Link](https://huggingface.co/openflamingo/OpenFlamingo-4B-vitl-rpj3b-langinstruct)|
|9B| anas-awadalla/mpt-7b | openai CLIP ViT-L/14 | 4 | 89.0 | 54.8 | [Link](https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b)|

*\\* Xattn interval refers to the `--cross_attn_every_n_layers` argument.*

Note: as part of our v2 release, we have deprecated a previous LLaMA-based checkpoint. However, you can continue to use our older checkpoint using the new codebase.

## Downloading pretrained weights

To instantiate an OpenFlamingo model with one of our released weights, initialize the model as above and use the following code.

```python
# grab model checkpoint from huggingface hub
from huggingface_hub import hf_hub_download
import torch

checkpoint_path = hf_hub_download(""openflamingo/OpenFlamingo-3B-vitl-mpt1b"", ""checkpoint.pt"")
model.load_state_dict(torch.load(checkpoint_path), strict=False)
```

## Generating text
Below is an example of generating text conditioned on interleaved images/text. In particular, let's try few-shot image captioning.

``` python
from PIL import Image
import requests
import torch

""""""
Step 1: Load images
""""""
demo_image_one = Image.open(
    requests.get(
        ""http://images.cocodataset.org/val2017/000000039769.jpg"", stream=True
    ).raw
)

demo_image_two = Image.open(
    requests.get(
        ""http://images.cocodataset.org/test-stuff2017/000000028137.jpg"",
        stream=True
    ).raw
)

query_image = Image.open(
    requests.get(
        ""http://images.cocodataset.org/test-stuff2017/000000028352.jpg"", 
        stream=True
    ).raw
)


""""""
Step 2: Preprocessing images
Details: For OpenFlamingo, we expect the image to be a torch tensor of shape 
 batch_size x num_media x num_frames x channels x height x width. 
 In this case batch_size = 1, num_media = 3, num_frames = 1,
 channels = 3, height = 224, width = 224.
""""""
vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]
vision_x = torch.cat(vision_x, dim=0)
vision_x = vision_x.unsqueeze(1).unsqueeze(0)

""""""
Step 3: Preprocessing text
Details: In the text we expect an <image> special token to indicate where an image is.
 We also expect an <|endofchunk|> special token to indicate the end of the text 
 portion associated with an image.
""""""
tokenizer.padding_side = ""left"" # For generation padding tokens should be on the left
lang_x = tokenizer(
    [""<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of""],
    return_tensors=""pt"",
)


""""""
Step 4: Generate text
""""""
generated_text = model.generate(
    vision_x=vision_x,
    lang_x=lang_x[""input_ids""],
    attention_mask=lang_x[""attention_mask""],
    max_new_tokens=20,
    num_beams=3,
)

print(""Generated text: "", tokenizer.decode(generated_text[0]))
```

# Training
We provide training scripts in `open_flamingo/train`. We provide an example Slurm script in `open_flamingo/scripts/run_train.py`, as well as the following example command:
```
torchrun --nnodes=1 --nproc_per_node=4 open_flamingo/train/train.py \\
  --lm_path anas-awadalla/mpt-1b-redpajama-200b \\
  --tokenizer_path anas-awadalla/mpt-1b-redpajama-200b \\
  --cross_attn_every_n_layers 1 \\
  --dataset_resampled \\
  --batch_size_mmc4 32 \\
  --batch_size_laion 64 \\
  --train_num_samples_mmc4 125000\\
  --train_num_samples_laion 250000 \\
  --loss_multiplier_laion 0.2 \\
  --workers=4 \\
  --run_name OpenFlamingo-3B-vitl-mpt1b \\
  --num_epochs 480 \\
  --warmup_steps  1875 \\
  --mmc4_textsim_threshold 0.24 \\
  --laion_shards ""/path/to/shards/shard-{0000..0999}.tar"" \\
  --mmc4_shards ""/path/to/shards/shard-{0000..0999}.tar"" \\
  --report_to_wandb
```

*Note: The MPT-1B [base](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)  and [instruct](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b-dolly) modeling code does not accept the `labels` kwarg or compute cross-entropy loss directly within `forward()`, as expected by our codebase. We suggest using a modified version of the MPT-1B models found [here](https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b) and [here](https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b-dolly).*

For more details, see our [training README](https://github.com/mlfoundations/open_flamingo/tree/main/open_flamingo/train).


# Evaluation
An example evaluation script is at `open_flamingo/scripts/run_eval.sh`. Please see our [evaluation README](https://github.com/mlfoundations/open_flamingo/tree/main/open_flamingo/eval) for more details.


To run evaluations on OKVQA you will need to run the following command:
```
import nltk
nltk.download('wordnet')
```


# Future plans
- [ ] Add support for video input

# Team

OpenFlamingo is developed by:

[Anas Awadalla*](https://anas-awadalla.streamlit.app/), [Irena Gao*](https://i-gao.github.io/), [Joshua Gardner](https://homes.cs.washington.edu/~jpgard/), [Jack Hessel](https://jmhessel.com/), [Yusuf Hanafy](https://www.linkedin.com/in/yusufhanafy/), [Wanrong Zhu](https://wanrong-zhu.com/), [Kalyani Marathe](https://sites.google.com/uw.edu/kalyanimarathe/home?authuser=0), [Yonatan Bitton](https://yonatanbitton.github.io/), [Samir Gadre](https://sagadre.github.io/), [Shiori Sagawa](https://cs.stanford.edu/~ssagawa/), [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ&hl=en), [Simon Kornblith](https://simonster.com/), [Pang Wei Koh](https://koh.pw/), [Gabriel Ilharco](https://gabrielilharco.com/), [Mitchell Wortsman](https://mitchellnw.github.io/), [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/).

The team is primarily from the University of Washington, Stanford, AI2, UCSB, and Google.

# Acknowledgments
This code is based on Lucidrains' [flamingo implementation](https://github.com/lucidrains/flamingo-pytorch) and David Hansmair's [flamingo-mini repo](https://github.com/dhansmair/flamingo-mini). Thank you for making your code public! We also thank the [OpenCLIP](https://github.com/mlfoundations/open_clip) team as we use their data loading code and take inspiration from their library design.

We would also like to thank [Jean-Baptiste Alayrac](https://www.jbalayrac.com) and [Antoine Miech](https://antoine77340.github.io) for their advice, [Rohan Taori](https://www.rohantaori.com/), [Nicholas Schiefer](https://nicholasschiefer.com/), [Deep Ganguli](https://hai.stanford.edu/people/deep-ganguli), [Thomas Liao](https://thomasliao.com/), [Tatsunori Hashimoto](https://thashim.github.io/), and [Nicholas Carlini](https://nicholas.carlini.com/) for their help with assessing the safety risks of our release, and to [Stability AI](https://stability.ai) for providing us with compute resources to train these models.

# Citing
If you found this repository useful, please consider citing:

```
@article{awadalla2023openflamingo,
  title={OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models},
  author={Anas Awadalla and Irena Gao and Josh Gardner and Jack Hessel and Yusuf Hanafy and Wanrong Zhu and Kalyani Marathe and Yonatan Bitton and Samir Gadre and Shiori Sagawa and Jenia Jitsev and Simon Kornblith and Pang Wei Koh and Gabriel Ilharco and Mitchell Wortsman and Ludwig Schmidt},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}
```

```
@software{anas_awadalla_2023_7733589,
  author = {Awadalla, Anas and Gao, Irena and Gardner, Joshua and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Jitsev, Jenia and Kornblith, Simon and Koh, Pang Wei and Ilharco, Gabriel and Wortsman, Mitchell and Schmidt, Ludwig},
  title = {OpenFlamingo},
  month        = mar,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.1.1},
  doi          = {10.5281/zenodo.7733589},
  url          = {https://doi.org/10.5281/zenodo.7733589}
}
```

```
@article{Alayrac2022FlamingoAV,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.14198}
}
```
",DEV,,"['anas-awadalla', 'jpgard', 'i-gao', 'mitchellnw', 'riotonzuk', 'yonatanbitton', 'chris-alexiuk-1', 'yassouali', 'isaac-chung', 'kalyani7195', 'triakshunn', 'loftusa', 'chris-alexiuk', 'DavidMChan', 'gabrielilharco', 'HUGHNew', 'siddk', 'VegB', 'ElegantLin']",,,1,0,,,,"blank_issues_enabled: true
contact_links:
  - name: About the OpenFlamingo Project
    url: https://laion.ai/blog/open-flamingo-v2/
    about: General information about OpenFlamingo, privacy policy, and FAQ
  - name: Blank issue
    url: https://github.com/mlfoundations/open_flamingo/issues/new
    about: Please note that bug reports and feature requests should be used in most cases instead
",,48,0.5
196063469,MDEwOlJlcG9zaXRvcnkxOTYwNjM0Njk=,metadig-dataone-fair,NCEAS/metadig-dataone-fair,0,NCEAS,https://github.com/NCEAS/metadig-dataone-fair,MetaDIG project implementation of FAIR assessment for DataONE collections,0,2019-07-09 18:37:48+00:00,2021-02-23 01:42:49+00:00,2021-02-23 01:42:47+00:00,,137057,2,2,HTML,1,1,1,1,0,0,1,0,0,1,,1,0,0,public,1,1,2,main,1,1,"#
# Evaluating FAIR within the DataONE network of repositories

- **Contributors**: Matthew B. Jones, Peter Slaughter, Ted Habermann
- **Contact**: jones@nceas.ucsb.edu
- **License**: [Apache 2](http://opensource.org/licenses/Apache-2.0)
- [Package source code on Github](https://github.com/NCEAS/metadig-dataone-fair)

Provides code for analysis and graphing of FAIR qualities from the metadata in the 
[DataONE network of data repositories](https://www.dataone.org/current-member-nodes), including the
[KNB Data Repository](https://knb.ecoinformatics.org), [Dryad](http://datadryad.org),
and the NSF [Arctic Data Center](https://arcticdata.io).
Each DataONE repository implements metadata using dialects of their choosing,
and MetaDIG provides a cross-dialect engine for runninng comprehensive quality
checks for improvement and guidannce.  In this repository, we analze the results
of those checks and create graphs of changes in collection quality over time
overall within the DataONE network and within individual repositories.

## Getting started

The analytical code is present in the top level `plotFAIRMetrics.Rmd` file.  The data
file is not included in this repository but is accessible in the archived release on the KNB:

Matthew Jones, Peter Slaughter, and Ted Habermann. 2019. Quantifying FAIR: metadata improvement and 
guidance in the DataONE repository network. Knowledge Network for Biocomplexity. [doi:10.5063/F1KP80GX](https://doi.org/doi:10.5063/F1KP80GX).

## Acknowledgments

Work on this package was supported by:

- NSF DIBBS grant #1443062 to T. Habermann and M. B. Jones
- NSF-DATANET grants #0830944 and #1430508 to W. Michener, M. B. Jones, D. Vieglais, S. Allard and P. Cruse
- NSF-PLR grant #1546024 to M. B. Jones, S. Baker-Yeboah, J. Dozier, M. Schildhauer, and A. Budden

Additional support was provided for working group collaboration by the National Center for Ecological Analysis and Synthesis, a Center funded by the University of California, Santa Barbara, and the State of California.

![NCEAS logo](https://www.nceas.ucsb.edu/files/logos/NCEAS/NCEAS-full%20logo-4C.jpg)

",DEV,,"['mbjones', 'cwbeltz', 'gothub']",,,1,0,,,,,,6,0.9
772293059,R_kgDOLghBww,quarto-dashboards-demo-lh,NCEAS/quarto-dashboards-demo-lh,0,NCEAS,https://github.com/NCEAS/quarto-dashboards-demo-lh,Demonstration materials for Learning Hub Lesson: Quarto Dashboards,0,2024-03-14 22:52:07+00:00,2024-03-14 22:52:07+00:00,2024-03-28 18:11:58+00:00,,7,0,0,,1,1,1,1,0,0,5,0,0,0,,1,0,0,public,5,0,0,main,1,1,"# NCEAS Learning Hub Quarto Dashboard Demonstration
**Demonstration materials for Learning Hub Lesson on Quarto Dashboards**

The demonstration includes examples that showcase different dashboard features:

1. Basic Dashboard
2. Interactive and Multiple Pages Dashboard
3. Reactive Dashboard using `shiny` elements [Work in progress]
4. Themed Dashboard using Quarto Themes and advanced layout

Lesson Material:

- **March 2024**: [UCSB Seminar Series Week Three](https://learning.nceas.ucsb.edu/2024-03-ucsb-faculty/session_10.html)


",EDU,,['camilavargasp'],,,1,0,,,,,,3,0.85
604243671,R_kgDOJAQG1w,Pardee_Bees,neilcobb/Pardee_Bees,0,neilcobb,https://github.com/neilcobb/Pardee_Bees,data from a recent publication,0,2023-02-20 16:30:56+00:00,2023-02-20 16:30:56+00:00,2023-02-20 16:56:28+00:00,,131,0,0,,1,1,1,1,0,0,0,0,0,0,cc0-1.0,1,0,0,public,0,0,0,main,1,,"---
# replace uri to point to the name resource you'd like to align
# a url without scheme like https:// (e.g., ```url: foodorganisms.txt```) 
# is assumed to be a local file in working directory
datasets:
    - url: names.csv
      enabled: false
      type: text/csv
    - id: mdd
      enabled: false
      name: Mammal Diversity Database
      type: application/nomer
    - url: https://example.org/data.tsv
      enabled: false
      type: text/tab-separated-values
    - url: https://serv.biokic.asu.edu/ecdysis/content/dwca/UCSB-IZC_DwC-A.zip
      enabled: false
      type: application/dwca
    - url: https://scan-bugs.org:443/portal/webservices/dwc/rss.xml 
      enabled: false
      type: application/rss+xml
    - url: https://scan-bugs.org/portal/content/dwca/UT-UTBFL_DwC-A.zip 
      enabled: true
      type: application/dwca
# 
taxonomies:
#
# Edit list below to select taxonomies you'd like to work with. 
#
# To enable taxonomies to align with set:
#  enabled: true 
#
# To disable taxonomies to align with set: 
#  enabled: false
#
    - id: itis
      enabled: true
      name: Integrated Taxonomic Information System
    - id: ncbi
      enabled: true
      name: NCBI Taxonomy
    - id: discoverlife
      enabled: true
      name: Discover Life Taxonomy
    - id: batnames
      enabled: false
      name: Bat Names 
    - id: col
      enabled: false
      name: Catalogue of Life
    - id: gbif
      enabled: false
      name: GBIF Backbone Taxonomy
    - id: globi
      enabled: false
      name: GloBI Taxon Graph
    - id: indexfungorum
      enabled: false
      name: Index Fungorum
    - id: mdd
      enabled: false
      name: Mammal Diversity Database
    - id: ott
      enabled: false
      name: Open Tree of Life Taxonomy
    - id: pbdb
      enabled: false
      name: Paleobiology Database
    - id: plazi
      enabled: false
      name: Plazi Treatments
    - id: tpt
      enabled: false
      name: Terrestrial Parasite Tracker Taxonomies
    - id: wfo
      enabled: false
      name: World of Flora Online
---

# Name Alignment

[![Name Alignment by Nomer](../../actions/workflows/align.yml/badge.svg)](../../actions/workflows/align.yml)

To find your automatically created name alignment report, click on ""Name Alignment by Nomer"" above, click on a workflow run, and  download the `alignment-report` artifact.

:bulb: Note that only logged-in GitHub users with access can download the alignment report generated by GitHub Actions.

## Background


Aligning taxonomic names is a common task in biodiversity informatics. 

This template repository offers an automated method to align scientific names in csv/tsv files and darwin core archive with common taxonomic name lists like Catalogue of Life, NCBI Taxonomy, Integrated Taxonomic Information System (ITIS), and GBIF Backbone taxonomy.

## Getting Started

Follow steps below or alternatively visit https://big-bee-network.github.io/name-alignment-workshop to learn more.

1. create your own repository using this repository as a template
2. edit the README.md and add the urls / filenames to the resources you'd like to review. Note that only the following types are supported at time of writing (June 2022): ```text/csv```, ```text/tab-separated-values```, ```application/dwca```, and ```application/rss+xml```. Also, delete any taxonomy entries that you are not interested in: the fewer taxonomies to align with, the faster the review. 
3. edit taxonomies list in the README.md [front-matter](https://jekyllrb.com/docs/front-matter/) to select those you are interested to work with. Many are configured by default, and you can customize to make the configuration work best for your names.  
4. for now only names in column ""scientificName"" (tsv/csv), and ""http://rs.tdwg.org/dwc/terms/scientificName"" (DwC-A) will be aligned 
5. commit the changes to github
6. inspect results of name alignment in ""Github Actions"" (e.g., [sample results](https://github.com/globalbioticinteractions/name-alignment-template/raw/main/img/name-alignment-review-2022-11-14.log))
)
7. download name alignment report from the ""artifacts"" section 
8. to re-create/re-run results, change your name list in github or select [""re-run jobs"" in Github Actions](https://docs.github.com/en/actions/managing-workflow-runs/re-running-workflows-and-jobs).

# Origin

This repository was conceived on 2022-03-08 during the [Alien CSI Hack-a-thon](https://github.com/alien-csi/alien-csi-hackathon) in Romania by Christina, Quentin, Jorrit, Jasmijn, .... For more information see https://github.com/alien-csi/alien-csi-hackathon . 

# Contributors


name | affiliation | orcid 
--- | --- | ---
Jorrit Poelen | GloBI; Ronin Institute | https://orcid.org/0000-0003-3138-4118
your name | your affiliation | your orcid


# Feedback / issues

This repository uses scripts in https://github.com/globalbioticinteractions/globinizer. These script use commandline tools like [GloBI](https://globalbioticinteractions.org)'s [nomer](https://github.com/globalbioticinteractions/nomer), cut, sed, etc. 

# Misc Notes


install nomer java8 / java11 - 

https://github.com/globalbioticinteractions/nomer 

e.g., Carl Boettiger taxondb R package


Print names and add a tab in front, to prepare for nomer. 

```
cat foodorganisms.txt | sed 's/^/\\t/g' > foodorganisms.tsv
```

Nomer expects the format to be:

[id][tab][name]

e.g.,
id\\tname
NCBI:9606\\tHomo sapiens


Print names to screen and append itis taxonomic interpretation, and write/redirect to a file 'name-itis.tsv'

```
cat foodorganisms.tsv | nomer append itis > name-itis.tsv
```

open in LibreOffice Calc

Repeat with 'gbif' instead of 'itis'

## Provenance of DwC-A Names

The name context of names extracted from DwC-A are captured in a funny looking text:

```
line:zip:hash://sha256/fe63af46ed66abd253ee148e383fb51da6695ce3848d0bde39af18aa77d364fb!/occurrences.csv!/L10
```

extracted from a generated ```names-aligned.tsv```:

```
$ cat names-aligned.tsv | grep hash | grep occurrence | head -n1
line:zip:hash://sha256/fe63af46ed66abd253ee148e383fb51da6695ce3848d0bde39af18aa77d364fb!/occurrences.csv!/L10	Lasioglossum	SAME_AS	line:zip:hash://sha256/fe63af46ed66abd253ee148e383fb51da6695ce3848d0bde39af18aa77d364fb!/occurrences.csv!/L10	Lasioglossum								HAS_ACCEPTED_NAME	COL:5B4P	Lasioglossum	genus		Biota | Animalia | Arthropoda | Insecta | Hymenoptera | Apoidea | Halictidae | Halictinae | Halictini | Lasioglossum	COL:5T6MX | COL:N | COL:RT | COL:H6 | COL:HYM | COL:625GP | COL:625H4 | COL:JMV | COL:KV7 | COL:5B4P	unranked | kingdom | phylum | class | order | superfamily | family | subfamily | tribe | genus	https://www.catalogueoflife.org/data/taxon/5B4P	
```

This text identifies the row from which the name was extracted. In this case, line 10, from file ```occurrences.csv``` contained in the zip file with content id ```hash://sha256/fe63af46ed66abd253ee148e383fb51da6695ce3848d0bde39af18aa77d364fb``` . If you retain the tracked dataset (in this case UC Santa Barbara Invertebrate Zoology Collection accessed on 2022-06-30) provided in the data/ folder of the name aligment archive,  you can use [Preston](https://preston.guoda.bio) to dig up the original record using:

```
$ preston cat 'line:zip:hash://sha256/fe63af46ed66abd253ee148e383fb51da6695ce3848d0bde39af18aa77d364fb!/occurrences.csv!/L10' 
881449,UCSB,IZC,,b03a3f0c-bfa5-4e02-b5d3-56ff38626302,PreservedSpecimen,a8a4f8b1-38f1-4e10-9b75-b2e86ac196fc,UCSB-IZC00038312,,Animalia|Arthropoda|Hexapoda|Insecta|Pterygota|Neoptera|Hymenoptera|Apocrita|Aculeata|Apoidea|Halictidae|Halictinae|Halictini,Animalia,Arthropoda,Insecta,Hymenoptera,Halictidae,Lasioglossum,186125,""Curtis, 1833"",Lasioglossum,,,,,Genus,""EEMB/ENV S 96"",24-May-2022,,,,,,""Sophie Cameron"",,2022-04-26,2022,4,26,116,,,,""Newly restored salt marsh"",PAN2,,,,,,,""on flower of Eschscholzia californica"",,,Adult,Female,1,Pinned,""United States"",California,""Santa Barbara"",,""University of California Santa Barbara North Campus Open Space"",,34.42174,-119.87186,WGS84,10,,,,GPS,,,,,,,,,,,,""2022-05-31 10:52:55"",http://creativecommons.org/publicdomain/zero/1.0/,""The Regents of the University of California"",https://www.ccber.ucsb.edu/collections/databases-searching-specimen-data-and-images,urn:uuid:a8a4f8b1-38f1-4e10-9b75-b2e86ac196fc,https://serv.biokic.asu.edu/ecdysis/collections/individual/index.php?occid=881449
```

which links to a preserved specimen with occurrenceId b03a3f0c-bfa5-4e02-b5d3-56ff38626302 and landing page at https://serv.biokic.asu.edu/ecdysis/collections/individual/index.php?occid=881449 . Also see [screenshot made on 2022-06-30](./img/UCSB-IZC00038312_b03a3f0c-bfa5-4e02-b5d3-56ff38626302.png). 

With this context, you can trace the origin and context of the name in great detail. This detail can be used to troubleshoot bugs in the name alignment process, or provide granular feedback to those that maintain the dataset or taxonomy.  



",DATA,,['neilcobb'],,,1,0,,,,,,1,0.6
11600280,MDEwOlJlcG9zaXRvcnkxMTYwMDI4MA==,odmbdev_depricated,odmb/odmbdev_depricated,0,odmb,https://github.com/odmb/odmbdev_depricated,UCSB software to test the ODMB,0,2013-07-23 05:22:29+00:00,2020-01-09 22:53:35+00:00,2013-10-21 14:23:48+00:00,,209,0,0,C++,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,master,1,1,,DEV,,"['fgolf', 'tedanielson']",,,1,0,,,,,,10,0.6
119460210,MDEwOlJlcG9zaXRvcnkxMTk0NjAyMTA=,tilde-pconrad,pconrad/tilde-pconrad,0,pconrad,https://github.com/pconrad/tilde-pconrad,Code for webpage at http://www.cs.ucsb.edu/~pconrad,0,2018-01-30 00:33:22+00:00,2018-11-01 16:33:14+00:00,2022-11-22 10:07:58+00:00,,2610,0,0,HTML,1,1,1,1,0,0,0,0,0,5,mit,1,0,0,public,0,5,0,master,1,,"# [Start Bootstrap - Heroic Features](https://startbootstrap.com/template-overviews/heroic-features/)

[Heroic Features](http://startbootstrap.com/template-overviews/heroic-features/) is a multipurpose HTML template for [Bootstrap](http://getbootstrap.com/) created by [Start Bootstrap](http://startbootstrap.com/).

## Preview

[![Heroic Features Preview](https://startbootstrap.com/assets/img/templates/heroic-features.jpg)](https://blackrockdigital.github.io/startbootstrap-heroic-features/)

**[View Live Preview](https://blackrockdigital.github.io/startbootstrap-heroic-features/)**

## Status

[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/BlackrockDigital/startbootstrap-heroic-features/master/LICENSE)
[![npm version](https://img.shields.io/npm/v/startbootstrap-heroic-features.svg)](https://www.npmjs.com/package/startbootstrap-heroic-features)
[![Build Status](https://travis-ci.org/BlackrockDigital/startbootstrap-heroic-features.svg?branch=master)](https://travis-ci.org/BlackrockDigital/startbootstrap-heroic-features)
[![dependencies Status](https://david-dm.org/BlackrockDigital/startbootstrap-heroic-features/status.svg)](https://david-dm.org/BlackrockDigital/startbootstrap-heroic-features)
[![devDependencies Status](https://david-dm.org/BlackrockDigital/startbootstrap-heroic-features/dev-status.svg)](https://david-dm.org/BlackrockDigital/startbootstrap-heroic-features?type=dev)

## Download and Installation

To begin using this template, choose one of the following options to get started:
* [Download the latest release on Start Bootstrap](https://startbootstrap.com/template-overviews/heroic-features/)
* Install via npm: `npm i startbootstrap-heroic-features`
* Clone the repo: `git clone https://github.com/BlackrockDigital/startbootstrap-heroic-features.git`
* [Fork, Clone, or Download on GitHub](https://github.com/BlackrockDigital/startbootstrap-heroic-features)

## Usage

### Basic Usage

After downloading, simply edit the HTML and CSS files included with the template in your favorite text editor to make changes. These are the only files you need to worry about, you can ignore everything else! To preview the changes you make to the code, you can open the `index.html` file in your web browser.

### Advanced Usage

After installation, run `npm install` and then run `gulp dev` which will open up a preview of the template in your default browser, watch for changes to core template files, and live reload the browser when changes are saved. You can view the `gulpfile.js` to see which tasks are included with the dev environment.

## Bugs and Issues

Have a bug or an issue with this template? [Open a new issue](https://github.com/BlackrockDigital/startbootstrap-heroic-features/issues) here on GitHub or leave a comment on the [template overview page at Start Bootstrap](http://startbootstrap.com/template-overviews/heroic-features/).

## Custom Builds

You can hire Start Bootstrap to create a custom build of any template, or create something from scratch using Bootstrap. For more information, visit the **[custom design services page](https://startbootstrap.com/bootstrap-design-services/)**.

## About

Start Bootstrap is an open source library of free Bootstrap templates and themes. All of the free templates and themes on Start Bootstrap are released under the MIT license, which means you can use them for any purpose, even for commercial projects.

* https://startbootstrap.com
* https://twitter.com/SBootstrap

Start Bootstrap was created by and is maintained by **[David Miller](http://davidmiller.io/)**, Owner of [Blackrock Digital](http://blackrockdigital.io/).

* http://davidmiller.io
* https://twitter.com/davidmillerskt
* https://github.com/davidtmiller

Start Bootstrap is based on the [Bootstrap](http://getbootstrap.com/) framework created by [Mark Otto](https://twitter.com/mdo) and [Jacob Thorton](https://twitter.com/fat).

## Copyright and License

Copyright 2013-2017 Blackrock Digital LLC. Code released under the [MIT](https://github.com/BlackrockDigital/startbootstrap-heroic-features/blob/gh-pages/LICENSE) license.
",WEB,,"['davidtmiller', 'pconrad']",,,1,0,,,,,,0,0.7
97790836,MDEwOlJlcG9zaXRvcnk5Nzc5MDgzNg==,Hello-World-Summer-2017,samanthaklee/Hello-World-Summer-2017,0,samanthaklee,https://github.com/samanthaklee/Hello-World-Summer-2017,Summer project proposals for Data Science @ UCSB,0,2017-07-20 04:34:57+00:00,2017-07-20 04:34:57+00:00,2017-07-24 07:54:52+00:00,,513,0,0,,1,1,1,1,0,0,1,0,0,1,,1,0,0,public,1,1,0,master,1,,"# Hello World Data Science UCSB Summer 2017
## Repo for Markdown Practice Template

## Instuctions:
Now that you have gotten familiar with *markdown* I want everyone to create a branch on this repo. It should be called something like `sam_branch` and upload your file there. 

**DO NOT** change the master branch!

You can erase the original repo if you think it'll just be clutter on your personal *GitHub* account but everyone should create a branch on this repo from the meeting. 


Once you create the branch you should be able to upload your files into that branch I will include my branch and example template for y'all to reference. 

Any questions ask me on *Slack*!
",EDU,,"['richawadaskar', 'samanthaklee']",,,1,0,,,,,,2,0.75
327342233,MDEwOlJlcG9zaXRvcnkzMjczNDIyMzM=,Interactive-Album-Cover,sayf-t/Interactive-Album-Cover,0,sayf-t,https://github.com/sayf-t/Interactive-Album-Cover,,0,2021-01-06 14:50:25+00:00,2021-01-06 15:05:21+00:00,2021-01-06 15:05:19+00:00,,568,0,0,JavaScript,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# Interactive-Album-Cover

As an introduction to the Javascript extension p5.js, I tried out this course by Zhang weidi (ART 22 course  'Computer Programming For Arts' in University of California, Santa Barbara) that introduces students to creating visuals with geometry using only code.

#### Project Link
The project can be found here: https://sayf-ismail.github.io/Interactive-Album-Cover/

#### How to run it
Using your mouse, the visual will react to movement along the horizontal x-axis.

",EDU,,['sayf-t'],,,1,0,,,,,,1,0.7
450139759,R_kgDOGtSWbw,MultiParasite,sbsambado/MultiParasite,0,sbsambado,https://github.com/sbsambado/MultiParasite,Publication of macro- & micro-parasite project led by Jordan Salomon,0,2022-01-20 14:55:13+00:00,2022-05-24 00:47:47+00:00,2023-08-22 00:31:28+00:00,,456,1,1,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,main,1,,"# MultiParasite

<b>Title</b>: Macro-parasites and micro-parasites co-exist in rodent communities but are associated with different community-level parameters

<b>Authors</b>: Jordan Salomon, Samantha B Sambado, Arielle Crews, Suhkman Sidu, Andrea Swei

<b>Keywords</b>: Vector-borne diseases, Lyme disease, macro-parasite, micro-parasite, Ixodes spp., Borrelia spp., Trichuris spp.

<b>Location</b>: Field work taken place in the San Francisco Bay Area

<b>doi</b>:10.1016/j.ijppaw.2023.08.006

Code on this github is for 1) regression analyses and 2) figure making

<u> How to reproduce code </u>

1. download data from `data` folder
2. run script `1_data_cleaning.Rmd`
3. run script `2_data_analysis.Rmd`
4. run script `3_data_figures.Rmd`

email sbsambado@ucsb.edu with any comments or questions
",DEV,,['sbsambado'],,,1,0,,,,,,1,0.8
399532774,MDEwOlJlcG9zaXRvcnkzOTk1MzI3NzQ=,HABsDataPublish,sccoos/HABsDataPublish,0,sccoos,https://github.com/sccoos/HABsDataPublish,"Workflow for ingesting, transforming, and publishing Scripp's Harmful Algal Bloom (HAB) data from ERDDAP to remote repositories.",0,2021-08-24 16:23:05+00:00,2022-07-01 04:58:19+00:00,2022-08-08 19:17:53+00:00,,4031,3,3,R,1,1,1,1,0,0,0,0,0,1,,1,0,0,public,0,1,3,master,1,1,"# HABs data remote repository publishing

This repository pulls [HABs data from ERDDAP](https://erddap.sccoos.org/erddap/tabledap/HABs-CalPoly.html) and transforms it to Darwin Core Archive format for it to be published to remote repositories like EDI, GBIF, and OBIS. There is a scheduled task using GitHub Actions to pull the updated datasets and then publish them- currently into EDI, with plans to integrate GBIF into the process in the future.

Currently, this script supports the data from the following sites:
Cal Poly Pier, Monterey Wharf, Newport Beach Pier, Santa Cruz Wharf, Santa Monica Pier, Scripps Pier, and Stearns Wharf


## Overview

A scheduled job controlled by GitHub Actions will run the following process on the *1st of each month at midnight*.
  - Build R environment with dependencies.
  - Execute `build_DwC_package.R`, which pulls the most up-to-data HABs data from ERDDAP for the included sites, reformats the data into DwC-A event core, and [generates EML](#generating-eml) metadata to describe the dataset. The generated files (DwC-A reformat and EML files), are written to file.
  - Commit generated files from previous step back into repository.
  - Execute `publish_to_EDI.R`, which will query the EDI repository's PASTA+ API (via R package EDIutils) to [update the HABs dataset](#publishing-to-edi) currently published in EDI with the up-to-date data files and metadata.
  
### Generating EML

EML is built by reading a base template, `HABS_base_EML.xml`, which describes the more static properties like the dataTables and their attributes, and then reads in a series of .csv files for properties that are more configurable/subject to change.

The following files can be updated to change properties of the EML metadata:
   - */EML/general.csv*: Make changes to title, abstract
   - */EML/geographic_coverage.csv*: Make changes to geographic coverage (geographic description, bounding coordinates)
   - */EML/keywords.csv*: Add/change keywords
   - */EML/personnel.csv*: Add/change creators/contacts
   
### Publishing to EDI

The publishing step into EDI requires a GitHub environment to be set up called **habspublish**.

The environment should contain the following secrets:
   - `EDI_USERNAME`: the username to authenticate to EDI for publishing with
   - `EDI_PASSWORD`: the password for the EDI account for publishing
   
The csv *package_identifiers.csv* needs to contain the static package identifier granted by EDI (or elsewise) for the targeted environment.
   
### About

Organization: [Southern California Coastal Ocean Observing System](https://sccoos.org/harmful-algal-bloom/)

Author: Ian Brunjes, ianbrunjes@ucsb.edu

",DEV,RS,"['ianbrunjes', 'actions-user']",,,1,0,,,,,,0,0.6
502313141,R_kgDOHfCwtQ,ScudBt,ScudBt/ScudBt,0,ScudBt,https://github.com/ScudBt/ScudBt,,0,2022-06-11 10:08:15+00:00,2022-06-11 10:08:15+00:00,2024-02-08 04:40:50+00:00,,41,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"### Hi there üëã, this is J. Z.!

**Passionate SWE, hoping to bring impact to the tech industry and the entire humanity**

- üî≠ I‚Äôm currently a B.S. in Computer Science, B.A. in Global Studies double major at University of California, Santa Barbara.
- üå± I‚Äôm currently learning advanced visualization tools including graph databases and interactive monitoring tools (Neo4j, Grafana, Tableau...)
- ü§î Finding new grad position ...
- üòÑ Pronouns: He/him
- ‚ö° Fun fact: VR/sea sports/drone/video games/sci-fi/movie lover!
- üì´ Email: zhy1594102803@gmail.com
- ü§ù LinkedIn: https://www.linkedin.com/in/jonathan-h-z/
- üíª Leetcode: https://leetcode.com/zhy1594102803/


- **Languages:**
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/javascript.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/python.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/cplusplus.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/java.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/html5.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/css3.svg""></code>


- **Development Framework:**
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/react.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/node-dot-js.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/numpy.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/pytorch.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/scikit-learn.svg""></code>


- **Databases/Toolbox:**
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/mysql.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/mongodb.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/postgresql.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/amazonaws.svg""></code>
<code><img height=""20"" src=""https://cdn.jsdelivr.net/npm/simple-icons@3.12.2/icons/git.svg""></code>

- #### Top Repositories

<a href=""https://github.com/anuraghazra/github-readme-stats"">
  <img align=""center"" src=""https://github-readme-stats.vercel.app/api/pin/?username=scudbt&repo=scudbt"" />
</a>
<a href=""https://github.com/anuraghazra/anuraghazra.github.io"">
  <img align=""center"" src=""https://github-readme-stats.vercel.app/api/pin/?username=scudbt&repo=scudbt.github.io&theme=buefy"" />
</a>
",OTHER,,['ScudBt'],,,1,0,,,,,,1,0.6
220575242,MDEwOlJlcG9zaXRvcnkyMjA1NzUyNDI=,PA2,seanjaffe1/PA2,0,seanjaffe1,https://github.com/seanjaffe1/PA2,UCSB 130A Fall 2019 PA2 starter code,0,2019-11-09 01:38:51+00:00,2019-12-03 07:28:32+00:00,2019-12-03 07:28:30+00:00,,149,0,0,C++,1,1,1,1,0,0,3,0,0,0,,1,0,0,public,3,0,0,master,1,,"# PA2
UCSB 130A Fall 2019 PA2 starter code

Notes: 
*Be sure to read the ""Policy on Academic Integrity"" in the course syllabus.

*If you are confused, ask on Piazza, during office hours, or during discussion sections.

*Remember no late assignments will be accepted. at all.

*Submit your answers on Gradescope (keep your eye out for further instructions in the next few days)

*Read the entire README before starting

*Start Early

## Overview
In this project, you will implement a number of Graph operations. 

You are allowed and highly encouraged to work in pairs on this project. You may partner with any one in the class and may use Piazza to find a partner. If you do not have a partner by Friday, November 16th, please email one of the TAs.

There will be a small deadline on November 23rd with 10% of your grade. The rest of the assignment will be due Decemember 7th at 11:59 pm.

This project will be graded using Gradescope's autograder. More instuctions on that will come in the next few days. 

## The Tasks
You will write code which maintains infomration about a weighted, undirected, graph where each node has an integer id and a skill vector.

You must implement the following methods:

**M1**: Return the diameter of the graph.

**M2**: Return the ratio of open triangles to closed triangles.

**M3**: Return the top k open triangles. A triangle is given higher prioirty if it has a larger total edge weight. Store the triangles in a heap (see M7). This method should return the top-k trianlges in a string where each triangle is given by its nodes in ascending order of node-id, separated by commas, and its triangles in descending order of score, separated by semi-colons. Ex: '1,4,5;3,4,7'

**M4**: (Early Deadline) Given a seed node with id i, and weight vector w, find the top k-skilled individuals around i.
If the weight vector w = \\[w1, w2, w3 ... wd] and the skill fector of a neighboring node f = \\[f1, f2, ....\\], then the prioirity of a node is given by dot(w, f) = w1f1 + w2f2 + .... + wdfd

**M5**: Given a seed node i and weight vector w, give the highest scoring node that does not have an edge connecting to i

**M6**: Given two seed nodes i, j and weight vector w, compute the similarites of their top-k neighborhoods unsing the Jacard Index

**M7**: Insert a node into the graph. Insert an edge into the graph. Incrementally compute M2 and M3 such that the cost of the insert plus the cost of the next call to M2 or M3 will be exponentiall faster than if the results were calculated from scratch.

Bonus:

**M8**: Remove a node  and edge from the graph. Incrementally compute M2 and M3 such that the cost of the insert plus the cost of the next call to M2 or M3 will be exponentiall faster than if the results were calculated from scratch. will only be evaluated if M1-M7 are completed. Will only receive points if incremental computation of M2 and M3 are implemented.

## The Code
Included in this repo are 3 header files: `GraphHelper.h`, `FeatureGraph.h` and `GraphAnalyzer.h`, and three .cpp files: `FeatureGraph.cpp`, `GraphAnalyzer.cpp` and `GraphTest.cpp`. 

`GraphHelper.h` contains an edge struct and a Triangle class to help you build your graph class. You may only edit the Triangle class.

The `FeatureGraph` files are meant to matinain the graph information.  You will have to determine the appropriate fields and implement the basic graph operations including the constructor and insert.

The `GraphAnalyzer` files are meant to be a wrapper around the `FeatureGraph` so the code to build the graph and analyze the graph are separated. This class contains all of the methods M1-M8 outlined above. 

Note that both the `GraphAnalyzer` and the `FeatureGraph` have an insert function. you will implement the actual insertion into the graph in the `FeatureGraph` class stored in the `GraphAnalyzer` because that is where the graph is stored. The `GraphAnalyzer` has its own insert function so it can follow the cost constriants on M2 and M3 at each insert as outlined in M7. In order to meet these constraints, you will need to have a heap as a private field in `GraphAnalyzer`.

Our testing code will always follow the following the pattern:

1. Build a graph using the `FeatureGraph` constructor

2. Make a `GraphAnalyzer` by passing in the above graph into the `GraphAnalyzer` constructor

3. Call all functions, including insert, on `GraphAnalyzer`

`GraphAnalyzer` will insert the node or edge into its graph and then make updates for fast computation of M2 and M3. We have already placed the call to `FeatureGraph` insert in the `GraphAnalyzer` inserts.

`GraphTest.cpp` gives an example for how we call each of your GraphAnalyzer methods. 

To run `GraphTest`, run the following commands:
```
make
./GraphTester
```
It is highly recommended that you do not edit the make file, and instead add your own tests to GraphTester.

## Deadlines
**Novemebr 23rd**: In order to encourage you to start early 10% of your grade will be given for passing autograded testing for M4 by November 23rd at 11:59pm

**December 7th**: The entire assignment will be due December 7th at 11:59pm
",EDU,,['seanjaffe1'],,,1,0,,,,,,1,0.95
719234270,R_kgDOKt6k3g,plant-pop-pred,shmuir/plant-pop-pred,0,shmuir,https://github.com/shmuir/plant-pop-pred,Final project for EDS222-Statistics for the MEDS program at UCSB,0,2023-11-15 18:38:13+00:00,2023-12-06 20:04:17+00:00,2023-12-14 21:57:18+00:00,,2560,0,0,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# plant-pop-pred
Final project for EDS222-Statistics for the MEDS program at UCSB
",EDU,,['shmuir'],,,1,0,,,,,,1,0.9
669348726,R_kgDOJ-Vzdg,shmuir,shmuir/shmuir,0,shmuir,https://github.com/shmuir/shmuir,,0,2023-07-22 02:19:58+00:00,2024-08-08 19:06:29+00:00,2024-08-08 19:06:25+00:00,,10,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"### Hello!

My name is Sam and I am an Environmental Data Scientist and Interdisciplinary Ecologist. My experience encompasses a range of projects focused on land cover change, spatial modeling, and the impacts of climate change on environmental health, which have strengthened my skills in coding, ecological field sampling, and laboratory analysis. I‚Äôm eager to continue developing my environmental data science skills, leverage my experience to help protect our vital ecosystems, and contribute to environmental justice efforts through accessible science communication.

About me:
- üè´ Master of Environmental Data Science: University of California, Santa Barbara, June 2024
- üè´ Bachelor of Science in Biology and Bachelor of Arts in Environmental Studies: St. Mary's College of Mayland, May 2023
- üå± Research experience in landscape ecology, plant-microbe interactions, plant pathogens, oysters, submerged aquatic vegetation, and physical oceanography (all with a climate focus).
- üí¨ Ask me about my favorite plants!

",OTHER,,['shmuir'],,,1,0,,,,,,1,0.8
375589796,MDEwOlJlcG9zaXRvcnkzNzU1ODk3OTY=,Sp_RRBS_ATAC,snbogan/Sp_RRBS_ATAC,0,snbogan,https://github.com/snbogan/Sp_RRBS_ATAC,"Analyses combining ATAC-seq, RRBS, and RNA-seq data for purple urchins",0,2021-06-10 06:11:18+00:00,2023-05-01 16:55:55+00:00,2023-03-13 16:55:15+00:00,,405944,1,1,R,1,1,1,1,0,0,2,0,0,0,,1,0,0,public,2,0,1,main,1,,"# Sp_RRBS_ATAC

## Authors
* Samuel Bogan, University of California, Santa Barbara, Dept. of Ecology, Evolution, and Marine Biology (UCSB)
* Marie Strader, Auburn University, Dept. of Biological Sciences

## Description
This repository contains all code and data used by Sam Bogan, Marie Strader, and Gretchen Hofmann for analyzing the effect of differential methylation on expression and splicing in the purple urchin during transgenerational plasticity. Analyses also include multifactorial models of transcription as a function of methylation and chromatin state (i.e., ATAC-seq data).

All R scripts were run using R version 3.6.1. Input files for Section A (see table of contents) were produced using scripts available at: https://github.com/mariestrader/S.purp_RRBS_RNAseq_2019. Raw RRBS and RNA-seq read are available through the NCBI Short Read Archive under the accession PRJNA548926 and ATAC-seq .bed files are available under GEO experssion omnibus Bioproject PRJNA377768. ATAC-seq .bed files are large and must be imported to Sp_RRBS_ATAC/B_Integr_ATAC_RRBS_RNA/Input_data/ if Section B is to be run in full.

This research was funded by a United States National Science Foundation award (IOS-1656262) to Gretchen Hofmann, UCSB. This research was also supported by the Santa Barbara Channel LTER (OCE-1831937).

## Table of Contents:

    A. A_Differential_Exp_Splicing_Meth (estimations of differential expression, differential splicing, and differential methylation)
        
        1. A1_edgeR_DE_Sp_RRBS.Rmd/.md (differential expression)
                i. Input data: matrix of RNA-seq read counts
                ii. Code: R markdown documenting edgeR differential expression analysis (A1_edgeR_DE_Sp_RRBS.Rmd)
                iii. Markdown: knit of A1_edgeR_DE_Sp_RRBS.Rmd
                iv. Outut data: CSVs of diff expression from contrasts between maternal and developmental treatment groups
                
        2. A2_edgeR_DS_Sp_RRBS.Rmd/.md (differential exon use)
                i. Input data: matrix of RNA-seq exon read counts
                ii. Code: R markdown documenting edgeR differential exon use analysis (A2_edgeR_DS_Sp_RRBS.Rmd)
                iii. Markdown: knit of A2_edgeR_DS_Sp_RRBS.Rmd
                iv. Outut data: (a) CSVs of diff exon use from contrasts between maternal and developmental treatment groups, and (b)
                    dataframes containing DEU ~ exon number linear model coefficients for genes that do (non_p_coef_filt.csv) and   
                    don't (non_spur_models) exhibit spurious transciprtion and two dfs of -log p-vals with geneids for GO-term 
                    Mann Whitney U tests
                    
        3. A3_edgeR_DM_Sp_RRBS.Rmd/.md (differential methylation)
                i. Input data: Bismark .cov files from each library and df's of CpG % methylation data grouped by feature type
                ii. Code: R markdown documenting edgeR differential methylation analyses of CpGs and genomic features and 
                binomial general linearized models testing effect of feature position on differential methylation (A3_edgeR_DM_Sp_RRBS.Rmd)
                iii. Markdown: knit of A3_edgeR_DM_Sp_RRBS.Rmd
                iv. Output data: CSVs of differential methylation coefficients across CpGs and genomic feature type in response
                to maternal and developmental treatments
                
    B. B_Integr_ATAC_RRBS_RNA (integration of ATAC-seq, RRBS, and RNA-seq data)
        
        1. B1_ATAC_Summary_Sp_RRBS.Rmd (counting/summarizing ATAC-seq data from 39 hpf larvae)
                i. Input data: IMPORTED BY USER - 3 replicate .bed files of ATAC-seq transposase inserts from 39 hpf *S. purpuratus*
                ii. Code: RUN ON HIGH PERFORMANCE SYSTEM OR CLUSTER - B1_ATAC_summary_Sp_RRBS.Rmd R and bash scripts for summarizing 
                ATAC-seq data
                iii. Output data: 
                        a. .Rdata file containing intermediate files that is perdiodically saved while running .R script
                        b. ATAC_summary_peakSummaryALL.Rdata - summarized ATAC-seq insert densities per gene by genomic feature
                        c. ATAC_summary_annots_prism_cons.Rdata - summarized ATAC-seq data for consensus inserts only
                        d. .Rdata file containing annotations of ATAC-seq inserts
                        e. EXPORTED BY USER DURING EXECUTION - combined .bed file of all ATAC-seq replicates
                        
        2. B2_Integr_Sp_RRBS.Rmd (combinatorial integrations of ATAC-seq, RRBS, and RNA-seq data)
              i. Input data:
                        a. ATAC_summary_peakSummaryALL.Rdata
                        b. Strongylocentrotus_purpuratus.Spur_3.1.42.gff3.gz
                        c. sp3_1_GCF.gff3.gz
                        d. GenePageGeneralInfo_AllGenes.csv
                        e. (From section A output) A_Differential_Exp_Splicing_Meth/Output_data/mat_edgeR_GE_table_filt.csv
                        f. (From section A output) A_Differential_Exp_Splicing_Meth/Output_data/mat_edgeR_GE_table_filt.csv
                        g. (From section A output) A_Differential_Exp_Splicing_Meth/Output_data/mat_edgeR_1kb_promoter_DM_df.csv
                        h. (From section A output) A_Differential_Exp_Splicing_Meth/Output_data/mat_edgeR_intron_gene_DM_df.csv
                        i. (From section A output) A_Differential_Exp_Splicing_Meth/Output_data/mat_edgeR_exon_gene_DM_df.csv
              iv. Output data: dataframes containing nearly all combinations of baseline and differential expression, splicing + baseline and differential measures of   
                               methylation across genic features, chromatin accessibility across genic features, and metrics for genic architecture.
                               
    C. C_Models_and_Figures (modeling and plotting gene regulation as a function of DNA methylation)
    
        1. C1_Models_Figures_Sp_RRBS.Rmd (Model fitting and selection with raw data and fitted data plots)
              i. Input data: this folder remains empty because input data are read in from B_Integr_ATAC_RRBS_RNA/Output_data
              ii. Code: MUST BE RUN WITH AT LEAST 3 CORES - C1_Models_Figures_Sp_RRBS.Rmd
              iii. GO_MWU-master: master directory for GO enrichment script by Wright, Aglyamova, Meyer, & Matz, 2015 (https://github.com/z0on/GO_MWU)
              iii. Output_data: Results of Fisher's exact and Mann Whitney U tests of GO-term enrichment among gene groups
              iv. Output_figures: Main-text and supplemental figures

    D. D_ATACseq_QC (measurement of interexprimental variation in ATACseq peaks to QC integration of ATAC data)

        1. D_ATACseq_QC.Rmd (linear regressions of ATACseq peak counts per genomic feature between experiments; calculation of     
           positional overlap in peaks between experiments)
              i. Input data ('20_hpf'): this folder contains bed files of ATACseq peak locations from two experiments studying 
                 blastula-stage *S. purpuratus* (SRA accession: GSE96927 and GSE160461)
              ii. Code: D_ATACseq_QC.Rmd
              iii. gene_info_table_header.txt: a datasheet used to resolve SPU_ID gene names and common gene names
              iv. Strongylocentrotus_purpuratus.Spur_3.1.42.gff3: an annotation file for the Spur_3.1 genome

",DATA,,['snbogan'],,,1,0,,,,,,1,0.9
87230065,MDEwOlJlcG9zaXRvcnk4NzIzMDA2NQ==,FLIR-Temp-Retrieval-Analysis,susanmeerdink/FLIR-Temp-Retrieval-Analysis,0,susanmeerdink,https://github.com/susanmeerdink/FLIR-Temp-Retrieval-Analysis,Project that reads in imagery from a FLIR camera and retrieves accurate pixel temperature,0,2017-04-04 20:07:34+00:00,2023-09-13 11:40:07+00:00,2018-04-08 19:45:45+00:00,,50063,7,7,Matlab,1,1,1,1,0,0,2,0,0,1,,1,0,0,public,2,1,7,master,1,,"# FLIR-Temp-Retrieval-Analysis

Project that reads in imagery from a FLIR camera and retrieves accurate pixel temperature based on various correction factors including camera height, relative humidity, upwelling longwave radation, and air temperature.
The concept and original code was created by Donald M. Aubrecht, Saleem Ullah, and GUI was originally created by Samuel W. Fall.
This project's PI is Dar A. Roberts.

### Background:
This code was written for the IDEAS (Innovative Datasets for Environmental Analysis by Students) project run by Dr. Dar Roberts in the University of California Santa Barbara Geography Department.
See http://geog.ucsb.edu/ideas/ for more information.
As part of this project, students lay transects at our various field sites to take measurements that relate to environmental variables. One of these measurements is taking a photo with an FLIR camera and using the resulting image to determine land surface temperature. This code corrects the imagery so that it retrieves accurate pixel temperatures based on appropriate emissivities and other correction factors.

### Dependencies/ Requirements:
This code was designed for a FLIR model T450sc (T62101). It may work on other FLIR imagery, but needs to be tested. Developed on MATLAB 2015, but code has been updated to support MATLAB 2016.

### Steps:
#### STARTING PROGRAM:
1. Open MATLAB and naviagate to folder with the FLIR-Temp-Retrieval-Analysis source code. 
2. Open gui_FLIR_analysis.m into the MATLAB editor.
3. In the editor tab, hit Run which will result in a GUI appearing.

#### STEP 1: LOADING IMAGES:
1. Click the ""Load Images"" button in the Step 1: Load Images. 
2. A dialog box will open where you can navigate to the location that contains the thermal fusion images you want to process. 
3. Select the photo or photos that you want to load into the program. Once selected hit Open. The program will then load in the photos and display the names on the left. It can take a while to load the images.

#### STEP 2: LOAD CORRECTION FACTORS:
1. Create a .csv file that contains correction factors. The format of the csv file follows (see example correction_factor_example.csv):
	* Header: The first row should contains the column names: Filename, Downwelling Longwave.
	* First Column: Filename of images must match the images that you load. Optional: You do not have to add .jpg, but you can add it to the filename. 
	* Fourth Column: Downwelling Longwave Radiation (Can be obtained from the IDEAS meterological station (geog.ucsb.edu/ideas) which updates every 15 minutes.)
2. Click the ""Step2: Load Correction Factors"" button. A dialog box will open where you can navigate to the location that contains the .csv file that contains the correction factors.  
3. The values will populate the table in the gui. You can edit or type in correction factors into the gui. 

#### STEP 3: EDIT EMISSIVITY VALUES:
1. The program will automatically load emissivity values for non-photosynthetic vegetatation (NPV), green vegetation (GV), and shade. If the user has measured their own emissivity values or would like to alter the emissivity, type in the values into the appropriate boxes. 

#### STEP 4: DESIGNATE OUTPUTS:
1. The program will automatically output an image file and .csv file of the corrected temperature (which can be corrected for longwave upwelling radiance, pixel class emissivities, relative humidity, and camera height). If you want the intermediate products, such as blackbody exitance image, click the text box next to that name. 
2. The products will be output with the following names:
	* Blackbody Exitance: originalfilename_BBExit.jpg and originalfilename_BBExit.csv
	* 0.95 Emissivity Exitance: originalfilename_95Exit.jpg and originalfilename_95Exit.csv
	* Blackbody Temperature: originalfilename_BBTemp.jpg and originalfilename_BBTemp.csv
	* 0.95 Emissivity Temperature: originalfilename_95Temp.jpg and originalfilename_95Temp.csv

#### STEP 5: RUN ANALYSIS:
1. Click ""Step 5: Run Analysis"" button to start going through your images 
2. The first part of the analysis is to classify the visible image using a pre-defined decision tree.This decision tree was built for the Coal Oil Point Reserve site. Use caution if using in a different area.
	1. A figure will pop up with the original visible image, classified image, and a colorbar. The colorbar includes the percentages for each of the classes present in the image. 
	2. The program will automatically save two classified images. An additional folder will be added to the directory containing the original folders titled ""Classification"" and the images will be saved in this new folder. The first image is the classified image and will be saved as originalname_class.jpg. The second image is from gui figure that includes the cropped image, classified image, and fractional results. This image will be saved as originalname_class_fractions.jpg.
3. The second part of the analysis is to calculate the corrected temperature product. See Analysis Steps and Equations section for details on calculations.
	1. A figure will pop up with the original thermal image and the corrected temperature image. 
	2. The program will automatically save the corrected temperature and exitance images. An additional folder will be added to the directory containing the original folders titled ""Temp_Correction"". The image will be saved into this new Classification folder with the 4 files: originalfilename_Temp.jpg, originalfilename_Temp.csv, originalfilename_Exitance.jpg, and originalfilename_Exitance.csv
	3. Additional file will be saved to this folder if any of the additional outputs were selected in Step 4: Designate Outputs.
4. A dialog window will appear saying ""Classify next image?"". To move on to the next image, hit yes. If you are done, hit no. If you select no, and start the program over, it will reclassify and reanalyze the images listed.
5. After the program has looped through all the images, the last part of the analysis will output the classification results and the temperature correction results into two separate csv files.
	1. A dialog box will appear for the user to select the output and name for the classification results.  Click ""Output to *.csv"" button to output this current sessions classification results. The program will not output results from images that were classified outside of the matlab session. 
	2. A second dialog box will for the user to select the output and name for the temperature correction results.  Click ""Output to *.csv"" button to output this current sessions temperature correction results. The program will not output results from images that were run outside of the matlab session. 
4. When the saving is complete a dialog box will appear saying ""Completed Processing.""

#### RESET & START OVER
1. When done running the images, the 'Reset & Start Over' button will clear the workspace and gui so that the user can load new images and correction factors.",EDU,,['susanmeerdink'],,,1,0,,,,,,2,0.95
803534825,R_kgDOL-T36Q,angr__angr,swe-train/angr__angr,0,swe-train,https://github.com/swe-train/angr__angr,,0,2024-05-20 23:01:35+00:00,2024-05-20 23:03:53+00:00,2024-07-03 04:43:51+00:00,,59759,0,0,Python,1,1,1,1,0,0,0,0,0,0,bsd-2-clause,1,0,0,public,0,0,0,amp_nov_2023,1,1,"# angr

[![Latest Release](https://img.shields.io/pypi/v/angr.svg)](https://pypi.python.org/pypi/angr/)
[![Python Version](https://img.shields.io/pypi/pyversions/angr)](https://pypi.python.org/pypi/angr/)
[![PyPI Statistics](https://img.shields.io/pypi/dm/angr.svg)](https://pypistats.org/packages/angr)
[![License](https://img.shields.io/github/license/angr/angr.svg)](https://github.com/angr/angr/blob/master/LICENSE)

angr is a platform-agnostic binary analysis framework.
It is brought to you by [the Computer Security Lab at UC Santa Barbara](https://seclab.cs.ucsb.edu), [SEFCOM at Arizona State University](https://sefcom.asu.edu), their associated CTF team, [Shellphish](https://shellphish.net), the open source community, and **[@rhelmot](https://github.com/rhelmot)**.

## Project Links
Homepage: https://angr.io

Project repository: https://github.com/angr/angr

Documentation: https://docs.angr.io

API Documentation: https://api.angr.io/en/latest/

## What is angr?

angr is a suite of Python 3 libraries that let you load a binary and do a lot of cool things to it:

- Disassembly and intermediate-representation lifting
- Program instrumentation
- Symbolic execution
- Control-flow analysis
- Data-dependency analysis
- Value-set analysis (VSA)
- Decompilation

The most common angr operation is loading a binary: `p = angr.Project('/bin/bash')` If you do this in an enhanced REPL like IPython, you can use tab-autocomplete to browse the [top-level-accessible methods](https://docs.angr.io/docs/toplevel) and their docstrings.

The short version of ""how to install angr"" is `mkvirtualenv --python=$(which python3) angr && python -m pip install angr`.

## Example

angr does a lot of binary analysis stuff.
To get you started, here's a simple example of using symbolic execution to get a flag in a CTF challenge.

```python
import angr

project = angr.Project(""angr-doc/examples/defcamp_r100/r100"", auto_load_libs=False)

@project.hook(0x400844)
def print_flag(state):
    print(""FLAG SHOULD BE:"", state.posix.dumps(0))
    project.terminate_execution()

project.execute()
```

# Quick Start

- [Install Instructions](https://docs.angr.io/introductory-errata/install)
- Documentation as [HTML](https://docs.angr.io/) and sources in the angr [Github repository](https://github.com/angr/angr/tree/master/docs)
- Dive right in: [top-level-accessible methods](https://docs.angr.io/core-concepts/toplevel)
- [Examples using angr to solve CTF challenges](https://docs.angr.io/examples).
- [API Reference](https://angr.io/api-doc/)
- [awesome-angr repo](https://github.com/degrigis/awesome-angr)
",DEV,RS,"['ltfish', 'zardus', 'rhelmot', 'kereoz', 'salls', 'NickStephens', 'badnack', 'tyb0807', 'mborgerson', 'thrsten', 'jmgrosen', 'twizmwazin', 'dnivra', 'angr-bot', 'subwire', 'ronnychevalier', 'Pamplemousse', 'mahaloz', 'Lukas-Dresel', 'fmagin', 'odell89', 'pre-commit-ci[bot]', 'acama', 'Phat3', 'schieb', 'domenukk', 'Kyle-Kyle', 'bannsec', 'axt', 'antoniobianchi333', 'nebirhos', 'danse-macabre', 'ekilmer', 'zwimer', 'conand', 'degrigis', 'bennofs', 'tiffanyb', 'P1kachu', 'AOS002', 'ercoppa', 'm1ghtym0', 'JinBlack', 'mohitrpatil', 'Cl4sm', 'xxr0ss', 'bluesadi', 'ArtemShypotilov', 'Owlz', 'saullocarvalho', 'yuzeming', 'ekse', '5lipper', 'capysix', 'sam-b', 'edmcman', 'CodeMaxx', 'adamdoupe', 'DennyDai', 'mephi42', 'NicolaasWeideman', 'sraboy', 'Alexeyan', 'hwu71', 'drone29a', 'iamahuman', 'lockshaw', 'intranautic', 'farosato', 'Markakd', 'HexRoman', 'Kingloko', 'sei-eschwartz', 'Adersh97', '3vilWind', 'extremecoders-re', '0mp', 'r00tus3r', 'syheliel', 'stef', 'novafacing', 'nescio007', 'mor619dx', 'ulugbekna', 'MatthewShao', 'matthewpruett', 'lks9', 'lowks', 'FantasqueX', 'ocean1', 'Atipriya', 'tunefish', 'f-prettyland', 'symflood', 'JsHuang', 'adamgrimm99', 'Mic92', 'NyaMisty', 'moshekaplan', 'woadey', 'sschritt', 'loverics', 'Luluno01', 'agnosticlines', 'bkosciarz', 'clslgrnc', 'ducorduck', 'ljjgdfs', 'mauricesvp', 'simplevuln', 'Icegrave0391', 'jasperla', 'KevOrr', 'mbrattain', 'adrianherrera', 'akibnizam', 'amlweems', 'LiptonB', 'cq674350529', 'CharlyBVo', 'ConnorNelson', 'cdisselkoen', 'saagarjha', 'riyadparvez', 'qsphan', 'themaks', 'amatus', 'Manouchehri', 'zx2c4', 'EvelynVusky', 'MarSoft', 'aheinric', 'aflorea-2k', 'zhouxuan009', 'xingyaoww', 'qwestduck', 'tgduckworth', 'trentn', 'toshipiazza', 'TodAmon', 'zeroSteiner', 'SolalPirelli', 'CFSworks', 'wuruoyu', 'za233', 'pdcsec', 'Phasip', 'nigel', 'ruaronicola', 'covanam', 'lordmoses', 'mikenawrocki', 'idiomaticrefactoring', 'zd99921', 'yaroslav-o', 'wwwzbwcom', 'trz19991228', 'tracquangthinh', 'thebluegreeny', 'ctfhacker', 'tdube', 'swthorn', 'ri-char', 'otibsa', 'mcfx', 'koko-ng', 'honghai0924', 'minroco', 'hrx1', 'f4c31e55', 'eggplants', 'damienmaier', 'CCDV2', 'canpadawan', 'candymate', 'analyst1001', 'embg', 'digitalisx', 'disconnect3d', 'ttdennis', 'chorankates', 'niftic', 'cwgreene', 'tunz', 'sch3m4', 'commial', 'LimitOrderBook', 'bdemick', '0xbc', 'capuanob', 'aeflores', 'afaerber', 'AndyXan', 'andreafioraldi', 'gkso', 'AlLongley', 'sciencemanx', 'Spirotot', '3553x', 'f0rki', 'mdxn', 'mksavic', 'kordood', 'Airtnp', 'Kylir', 'afflux', 'stevenskevin', 'kbittick3', 'kbittick', 'ArionMiles', 'jomanchu-twosix', 'jkrshnmenon', 'pickerj', 'Invincibl-e', 'benquike', 'henrikhorluck', 'crhf', 'h3lpful', 'gsingh93', 'grant-h', 'FelixMartel', 'SidekickLx']",,,1,0,,,"Security
========

angr is meant to be able to function as fully secure environment for analyzing code of any kind in its default configuration.
As a result, we take sandbox escapes - opportunities for guest code to manipulate the host environment without the analysis author explicitly allowing it - very seriously.
If you read all the documentation, you should be able to deploy angr to analyze untrusted code without worrying about it.

If you find a sandbox escape bug of any sort by this definition, please let us know through a private channel.
You can contact the core developers through their emails at audrey@rhelmot.io and fishw@asu.edu.
","blank_issues_enabled: false
contact_links:
  - name: Join our Slack community
    url: https://angr.io/invite/
    about: For questions and help with angr, you are invited to join the angr Slack community
",,0,0.95
595819121,R_kgDOI4N6cQ,energy-explorer-data-pipeline,Tbwheeler94/energy-explorer-data-pipeline,0,Tbwheeler94,https://github.com/Tbwheeler94/energy-explorer-data-pipeline,,0,2023-01-31 21:48:09+00:00,2023-01-31 21:48:17+00:00,2023-04-06 00:59:06+00:00,,33695,0,0,Jupyter Notebook,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# **Welcome to the energyexplorer.io data pipeline!**

### **What is this repository?**

This repository enables users of [energyexplorer.io](https://energy-explorer.herokuapp.com/site-finder/) to explore the backend data imports and calculations used by this web application. To do this:

1. Download this repository to your local
2. Open *data-pipeline.ipynb*
3. Follow directions provided in notebook (you will need to register for an API key and likely download a couple packages to your virtual environment)

### **Project overview**

energyexplorer.io was developed to automate and visualize a homework assignment from the [Electric Power Systems & Markets](https://bren.ucsb.edu/courses/esm-293-1s) course taught by [Professor Ranjit Deshmukh](https://bren.ucsb.edu/people/ranjit-deshmukh) at the Bren School of Environmental Science & Management, University of California, Santa Barbara. I took this course while pursuing a Master's of Environmental Science & Management degree.

### **Key files**

*data-pipeline.ipynb:* This [jupyter notebook](https://jupyter.org/) provides a step by step journey through the data imports, calculations, and outputs behind the energyexplorer.io dashboard.


*/data/LMP_Historic_Price.csv:* This is a dataset of historical hourly locational marginal pricing (LMP) at the HOLLISTR_1_N101 wholesale node for the year 2020 near Hollister, California. In the current version of energyexplorer.io, this is the dataset used for all future price calculations and was manually downloaded by me using [California Independent System Operator's (CAISO)](https://www.caiso.com/Pages/default.aspx) [OASIS](http://oasis.caiso.com/mrioasis/logon.do) tool (more about the tool below). In a future version of energyexplorer.io, LMP data will be pulled in programmatically to enable more accurate pricing forecasting.


*/data/LMPLocations.csv:* This is a dataset of all load and generation nodes operating under CAISO's jurisdiction. This dataset is used to find the nearest wholesale node to the requested solar plant location. This dataset was generated using a tool named [CAISO Scraper](https://github.com/emunsing/CAISO-Scrapers/blob/master/LMP%20Location%20Scraper/LMPLocations_vs_FullList.xls) developed by [Eric Munsing](https://www.linkedin.com/in/emunsing/) in partnership with the [Cleanweb](http://cleanweb.berkeley.edu/) group at UC Berkeley.

### **Organizations and data sources used in completion of this project**

[National Solar Radiation Database (NSRDB)](https://nsrdb.nrel.gov/)
- Database of hourly and half-hourly solar radiation (global horizontal, direct normal, and diffuse horizontal irradiance) across the United States and in some international locations. This dataset is accessed using NREL's [Python API](https://developer.nrel.gov/docs/solar/nsrdb/python-examples/).

[NREL‚Äôs System Advisor Model (SAM)](https://sam.nrel.gov/)
- SAM is a model used to calculate energy generation and financial projections for a variety of renewable energy systems. The current version of energyexplorer.io uses SAM's energy generation model to convert solar radiation data from NSRDB to energy generation by a solar plant with user defined parameters. This tool is accessed using the [NREL-PySAM package](https://pypi.org/project/NREL-PySAM/).

[CAISO OAIS](http://oasis.caiso.com/mrioasis/logon.do)
- Database of real-time and historical data related to the ISO transmission system and its Market, such as system demand forecasts, transmission outage and capacity status, market prices and market result data. Click [here](http://www.caiso.com/TodaysOutlook/Pages/prices.html) to view a realtime map of wholesale energy prices across California. As previously discussed, the current version of energyexplorer.io uses a static, one year hourly dataset from the HOLLISTR_1_N101 for all future price predictions. This dataset was manually extracted using the CAISO OASIS online downloader.

[U.S. Solar Photovoltaic System and Energy Storage Cost Benchmarks: Q1 2021](https://www.nrel.gov/docs/fy22osti/80694.pdf)
- This NREL report was used to define capex and fixed o&m parameters used to calculated the levelized cost of energy (LCOE) in energyexplorer.io. More about cost and performance parameters provided by NREL for electricity generating technologies can be found [here](https://atb.nrel.gov/electricity/2022/index).

[NREL‚Äôs Cambium Viewer](https://www.nrel.gov/analysis/cambium.html)
- Database and viewing tool developed by NREL to project future wholesale energy prices and marginal emission rates across the United States. Projections provided by Cambium were are not currently used in energyexplorer.io.",EDU,RS,['Tbwheeler94'],,,1,0,,,,,,1,0.9
35346561,MDEwOlJlcG9zaXRvcnkzNTM0NjU2MQ==,CS-186,thomasyi/CS-186,0,thomasyi,https://github.com/thomasyi/CS-186,Database UCLA,0,2015-05-09 21:41:59+00:00,2015-05-10 00:55:05+00:00,2015-05-10 00:55:04+00:00,,6977,0,0,JavaScript,1,1,1,1,0,0,0,0,0,0,other,1,0,0,public,0,0,0,master,1,,"
   //////////  //  //  //  //////
  //  //  //  //  //  //  //
 //  //  //  //  //  //  //////
//  //  //  //////////  //

THE MOBILE WEB FRAMEWORK

==============================================================================

Copyright:

    Copyright (c) 2010-12 UC Regents

License:

    http://mwf.ucla.edu/license

==============================================================================

TABLE OF CONTENTS

1. FRAMEWORK
2. INITIATIVE
3. COLLABORATION
4. LICENSE
5. CREDITS

==============================================================================

1. FRAMEWORK

The Mobile Web Framework is a cross-platform web framework that focuses
on mobile web standards, semantic markup, device agnosticism and graceful
degradation, providing a robust presentation layer that allows applications
to define a single set of markup optimized for HTML 5 capable devices that
degrades gracefully to any HTML 4.01 or XHTML MP compliant device
including Blackberry, Windows Mobile and even T9 phones.

==============================================================================

2. INITIATIVE

The framework project began in early 2010 as a joint venture between the UCLA
Office of Information Technology and UCLA Communications as a means to reach
all campus mobile users via a single platform in a reasonable and cost-
effective manner.

The framework first went into production at the beginning of Fall 2010 with
the launch of UCLA Mobile. Over ten campus units are now participating at UCLA
and four other campuses in the UC system have launched production applications
using the framework.

A number of other institutions both in the UC and beyond are currently
involved in the initiative.

More information is online at the collaboration site: http://mwf.ucla.edu

==============================================================================

3. COLLABORATION

To get involved with the Mobile Web Framework project, please see the
following links:

    Home                http://mwf.ucla.edu

    Code Repository     https://github.com/ucla/mwf

    Documentation       https://github.com/ucla/mwf/wiki

    Issue Tracker       https://jira.ats.ucla.edu:8443/

    Contact             mwf@ucla.edu

==============================================================================

4. LICENSE

The Mobile Web Framework is open-source software licensed under the BSD
3-Clause License.

For the full text of the license, please see the online version:

    http://mwf.ucla.edu/license

A copy of this license is also available in the /LICENSE file.

==============================================================================

5. CREDITS

5.1 PROJECT

    Project Lead        Rose Rocchio 
                        rrocchio@oit.ucla.edu

    Development Lead    Eric Bollens 
                        ebollens@oit.ucla.edu

    Systems Lead        Ed Sakabu
                        sakabu@ats.ucla.edu

5.2 CONTRIBUTORS

In addition to their own mobile applications, a number of participants have
contributed code directly to the Mobile Web Framework.

    UC Los Angeles      Eric Bollens
                        Ed Sakabu
                        Mike Takahashi
                        Joseph Maddela
                        Nate Emerson
                        Zorayr Khalapyan

    UC Berkeley         Sara Leavitt

    UC San Diego        Mojgan Amini
                        Ike Lin

    UC San Francisco    Richard Trott

    UC Santa Barbara    Logan Franken

    UC Riverside        Chris Webber

Beyond direct contributions, the input and suggestions of numerous others
have made the Mobile Web Framework possible.
",WEB,,"['Trott', 'ebollens', 'nateemerson', 'loganfranken', 'stopfstedt', 'zkhalapyan', 'jodytate-uw']",,,1,0,,,,,,1,0.95
387697593,MDEwOlJlcG9zaXRvcnkzODc2OTc1OTM=,tyler-pruitt,tyler-pruitt/tyler-pruitt,0,tyler-pruitt,https://github.com/tyler-pruitt/tyler-pruitt,,0,2021-07-20 06:40:32+00:00,2025-01-18 01:33:04+00:00,2025-01-18 01:33:03+00:00,,108,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"
<h2>Hello World, I'm Tyler!</h2>

<!--
```python
# Python
class SoftwareEngineer():
    def __init__(self):
        self.name = ""Tyler Pruitt""
        self.role = [""Backend"", ""Fullstack""]
        self.degree = ""BS, Physics""
        self.tech_stack = [
            ""Python"",
            ""C++"", 
            ""C#"",
            ""JavaScript/TypeScript"",
            ""MATLAB"",
            ""SQL/MySQL"",
            ""HTML"",
            ""CSS/SCSS""
        ]
        self.contact = ""tylerpruitt@ucsb.edu""
        self.languages = [""English"", ""Mandarin""]
```
-->

```cpp
// C++
class SoftwareEngineer {
    public:
        string name, degree, contact;
        vector<string> role, tech_stack, languages;
        
        SoftwareEngineer() {
            name = ""Tyler Pruitt"";
            role = {""Backend"", ""Fullstack""};
            degree = ""BS, Physics"";
            tech_stack = {
                ""Python"",
                ""C++"",
                ""C#"",
                ""JavaScript/TypeScript"",
                ""MATLAB"",
                ""SQL/MySQL"",
                ""HTML"",
                ""CSS/SCSS""
            };
            contact = ""tylerpruitt@ucsb.edu"";
            languages = {""English"", ""Mandarin""};
        }
};
```

<h3> üë®üèª‚Äçüíª &nbsp;About Me</h3>

- üéì I am have a bachelor's degree in physics and a minor in Chinese
- üíª I‚Äôm currently working as a software engineer at Santa Barbara Imaging Systems
- üå± I‚Äôm currently learning about quantitative trading and alpha generation
- üí¨ Ask me about investing, Studio Ghibli, and my favorite things to eat
- üòÑ Pronouns: He/Him/His
- ‚ö° Fun fact: I love learning foreign languages, exploring different cultures, and traveling
- üì´ How to reach me: (email: tylerpruitt@ucsb.edu)

<h3> ü§ùüèª &nbsp;Connect with Me <img src=""https://github.com/TheDudeThatCode/TheDudeThatCode/blob/master/Assets/Handshake.gif"" height=""32px""> </h3>

<p align=""left"">
<a href=""mailto:tylerpruitt@ucsb.edu""><img alt=""Email"" src=""https://img.shields.io/badge/tylerpruitt@ucsb.edu-blue?style=flat-square&logo=gmail""></a>
<a href=""https://www.linkedin.com/in/tylerpruitt01/""><img alt=""LinkedIn"" src=""https://img.shields.io/badge/LinkedIn-Tyler_Pruitt-blue?style=flat-square&logo=linkedin""></a>

<img align=""right"" width=""350"" alt=""Code"" src=""https://github.com/Wandrys-dev/Wandrys-dev/blob/main/code.gif""/>

## My Techstack

### Languages
- **Python**
- **C++**
- **C#**
- **JavaScript/TypeScript**
- **MATLAB**
- **SQL**
- **MySQL**
- **PostgreSQL**
- **HTML**
- **CSS/SCSS**

### Frameworks
- **.NET**
- **Flask**
- **Pytest**
- **NumPy**
- **Pandas**
- **SciPy**
- **Matplotlib**
- **Tensorflow**
- **PyTorch**
- **AngularJS**
- **React**
- **Node.js**
- **Qt**
- **Boost**

### Version Control and Hosting
- **Git**
- **GitHub**
- **Bitbucket**
- **GitKraken**

### Tools
- **Spyder**
- **Jupyter NoteBook**
- **Markdown**
- **Visual Studio Code**
- **Visual Studio**
- **Azure Data Studio**
- **MS SQL Server**
- **AWS**
- **Postico**
- **Docker**


## üìà My GitHub Stats

<table width=""100%""> 
  <tr>
    <td width=""40%"">
      <img src=""https://github-readme-stats.vercel.app/api?username=tyler-pruitt&show_icons=true&theme=algolia"">
    </td>
    <td width=""30%"">
      <img src=""https://github-readme-stats-eight-theta.vercel.app/api/top-langs/?username=tyler-pruitt&layout=compact&langs_count=8&theme=algolia"">
    </td>
  </tr>
</table>

<br/>

<p align=""center""> 
  Visitors :<br>
  <img src=""https://profile-counter.glitch.me/tyler-pruitt/count.svg"" />
</p>
",OTHER,,['tyler-pruitt'],,,1,0,,,,,,2,0.6
56883027,MDEwOlJlcG9zaXRvcnk1Njg4MzAyNw==,CS20-S16-lab05,UCSB-CMPTGCS20-S16/CS20-S16-lab05,0,UCSB-CMPTGCS20-S16,https://github.com/UCSB-CMPTGCS20-S16/CS20-S16-lab05,,0,2016-04-22 20:22:10+00:00,2016-04-23 02:39:32+00:00,2016-04-26 08:50:00+00:00,,44,0,0,Python,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,0,master,1,1,"# CS20-S16-lab05

Introduction
============

More practice with writing functions and tests
----------------------------------------------

In this lab, we continue practicing writing functions and tests, in
the same style as some of our previous labs.

The difference is that the functions we are writing will be a bit more
challenging.

You may work individually, OR in a pair, as you see fit.

Step by Step
============

Step 0: Create a folder/directory for lab05
--------------------------------------------------------------

Make a directory or folder for lab05.  We suggest `~/cs20/lab05`,
which as you may recall, means:

* a `cs20` folder inside your home folder
* then, a `lab05` folder inside that.

We are now going to get two files from the web.

Step 1: Copy some code into `lab05Funcs.py`
-----------------------------------------

We are going to copy some code from `lab05Funcs.py`
into a file with the same name under your `~/cs20/lab05` folder

In IDLE, select ""File=&gt;New Window"" to open a new ""untitled"" window for Python code.

When it comes up, click and drag the window by its title bar over to the right of your Python Shell window.

Now, open one of these links and copy-paste into your new window. (You may want to ""right click"", or on Mac, ""control-click"", to open it in a new window or tab.)

* As a plain file: [lab05Funcs.py](https://raw.githubusercontent.com/UCSB-CMPTGCS20-S16/CS20-S16-lab05/master/lab05Funcs.py)
* On github: [lab05Funcs.py on github](https://github.com/UCSB-CMPTGCS20-S16/CS20-S16-lab05/blob/master/lab05Funcs.py)

Use the ""Save As"" option to save that file in the`~/cs20/lab05` folder with the name `lab05Funcs.py`

Step 2: Copy some more code from `lab05Tests.py`
-----------------------------------------------

Now we are going to do the same thing again, with a second file of Python code.

We are going to copy some code from `lab05Tests.py`
into a file with the same name under your `~/cs20/lab05` folder

In IDLE, select ""File=&gt;New Window"" *again* to open *yet another* new ""untitled"" window for Python code, and open one of these links to copy the code into this new file.

* As a plain file: [lab05Tests.py](https://raw.githubusercontent.com/UCSB-CMPTGCS20-S16/CS20-S16-lab05/master/lab05Tests.py)
* On github: [lab05Tests.py on github](https://github.com/UCSB-CMPTGCS20-S16/CS20-S16-lab05/blob/master/lab05Tests.py)

Use the ""Save As"" option to save that file in the`~/cs20/lab05` folder with the name `lab05Tests.py`

Remember that upper vs. lower case matter. Save it again if you didn't get it exactly right the first time, and use the `rm` command to remove (delete) any files that are in the wrong place.

Once you see that you have `lab05Funcs.py` and `lab05Tests.py` under your `~/cs20/lab05` directory, you are good to move on to the next step.

Step 3: Importing functions from our lab02 file
-----------------------------------------------

Take a look inside your `lab05Funcs.py` file, and you'll see this line near the top:

    from lab02Funcs import isList

What we are doing here is pulling in a function from our lab02 work.

We are going to NEED an isList function to do the work for lab05. But, we don't want to have to write those functions from scratch.

So we can REUSE the code. All we have to do is copy over the `lab02Funcs.py` file from our lab02 directory into our lab05 directory.

So, make a copy of your `lab02Funcs.py` from the earlier lab, and put it in the same folder/directory with your `lab05Funcs.py` and `lab05Tests.py` files.

When you've copied the `lab02Funcs.py` file over, and you see that you have all three files that we are going to need for this week's lab, you can get started on the programming part.

Step 4: Fixing function stubs, adding functions, adding tests
-------------------------------------------------------------

As we discussed in lecture, when writing functions along with test cases, we often start with ""stub"" versions.

These allow us to ""test the test"" to make sure that when the function is bogus, that the tests work correctly.

If you look through the `lab05Funcs.py` file, you will see several function definitions that are ""stub"" versions.

Run the `lab05Tests.py` file, and you'll see that many of the tests are failing, because the function in question returns the string ""stub"" instead of the answer that it should.

Go through the file, and replace the stubs with correct values.

In addition, you'll also find:

-   several places in `lab05Funcs.py` where there are comments with ""@@@"" signs that tell you to add new function definitions
-   several places in `lab05Tests.py` where there are comments with ""@@@"" signs that tell you to add new test cases

Follow all of the instructions.

-   As you follow the instructions with the @@@ in them, REMOVE THE COMMENTS THAT HAVE THE @@@ IN THEM.

You can always look back at the versions of the files on the web if you want to see what the instructions originally said.

When you have:

-   fixed all the stubs, and the tests cases pass
-   added all the functions you are supposed to add
-   added all the test cases you are supposed to add

Then you are ready to submit!

### A few helpful hints

At the bottom of the file, you'll see that you can select either to run ALL tests, or ONLY the test for a certain function.

Follow the instructions there if you want to focus on just one function at a time.

Step 5: Getting ready to submit
-------------------------------

We are just about ready to submit your work for grading.

But first, some important preparation steps:

### Step 5a: Add your name(s) and email to the top of the file

This step is worth 10 points (10% of your grade) so don't forget it.

<b>(Again, ""grade"", ha ha. If you are in the CCS version of this course, just go with it, even if the point values don't necessarily matter. The ""grade"" stuff is still a good indication of how programming work is evaluated and assessed in traditional-style courses, so it's probably good for you to know in case you choose to study CS further.)</b>

At the top of BOTH the `lab05Funcs.py` file, and the `lab05Tests.py` file, add the following lines of code. They should be at the VERY top, the VERY first lines.

But, change the name here to YOUR name, and the email to YOUR email (use your umail address, not a gmail, yahoo or hotmail address.)

Add the word SOLO or PAIR, and if working in a pair, put both names (one per line)

    # lab05Funcs.py  SOLO, Gina Gaucho, ggaucho@umail.ucsb.edu 

    # lab05Tests.py  SOLO, Gina Gaucho, ggaucho@umail.ucsb.edu 

OR:

    # lab05Funcs.py  PAIR, Gordon Gaucho, ggaucho@umail.ucsb.edu 
    # lab05Funcs.py  PAIR, Martin Perez, mperez07@umail.ucsb.edu 

    # lab05Tests.py  PAIR, Gordon Gaucho, ggaucho@umail.ucsb.edu 
    # lab05Tests.py  PAIR, Martin Perez, mperez07@umail.ucsb.edu 

### Step 5b: Check your lab against the grading rubric

Open these lab instructions in a second browser window. Scroll down to the bottom of the lab, to the grading section.

Find the list of grading criteria. Check your own work against each of those.

If there are any parts that don't make sense to you, be sure to ask about those.

If you see that you've done everything correctly, then you are ready to submit.

Step 6: Submit your assignment via submit.cs
-------------------------------------------------------

Here's the link: https://submit.cs.ucsb.edu/form/project/473/submission

If you happen to be working on CSIL, you can also submit by typing

```
~submit/submit -p 473 lab02Funcs.py lab05Funcs.py lab05Tests.py
```

Evaluation and Grading
======================

-   lab05 directory submitted (15 pts) and contains (with correctly spelled names)
    -   lab02Funcs.py (5 pts)
    -   lab05Funcs.py (5 pts)
    -   lab05Tests.py (5 pts)

-   In lab05Funcs.py:
    -   Correct implementation of smallestInt() (40 pts)
    -   Correct implementation of indexOfSmallestInt() (40 pts)
    -   Correct implementation of longestString() (40 pts)
    -   Correct implementation of indexOfShortestString() (40 pts)
    
-   In lab05Tests.py:
    -   Six tests (ten points each) for smallestInt (60 pts)

Note: if tests are modified to make them pass (instead of modifying the code to make it pass the tests) then additional points may be deducted. Don't modify the tests. Modify the code so that it passes the tests.

-   General (50 pts)
    -   all (@@@) comments removed in both files
    -   submitting work on time
    -   registering your pair or solo on Gauchospace
    -   adding names to top of files
    -   anything else specified in the instructions

<hr>
Copyright 2014,2015, Phillip T. Conrad, CS Dept, UC Santa Barbara. Permission to copy for non-commercial, non-profit, educational purposes granted, provided appropriate credit is given; all other rights reserved.
",EDU,,"['sarahmzhong', 'pconrad']",,,1,0,,,,,,2,0.95
115650240,MDEwOlJlcG9zaXRvcnkxMTU2NTAyNDA=,ucsb-csxx-qyy-OLD-DO-NOT-USE,ucsb-cs-course-repos/ucsb-csxx-qyy-OLD-DO-NOT-USE,0,ucsb-cs-course-repos,https://github.com/ucsb-cs-course-repos/ucsb-csxx-qyy-OLD-DO-NOT-USE,Model repo for ucsb-csxx-qyy.github.io,0,2017-12-28 18:40:47+00:00,2023-01-28 18:58:50+00:00,2019-02-20 18:03:06+00:00,,1412,0,0,JavaScript,1,1,1,1,0,0,0,1,0,0,,1,0,0,public,0,0,0,master,1,1,"This repo is part of the old legacy pre-W19 format and should no longer be used.

Instead, refer to 

* https://ucsb-cs-course-repos.github.io/topics/setup_course_from_scratch/
* https://ucsb-cs-course-repos.github.io/topics/setup_offering_from_scratch/
",EDU,,['pconrad'],,,1,0,,,,,,0,0.9
354128453,MDEwOlJlcG9zaXRvcnkzNTQxMjg0NTM=,1pm-t6-ucsb-poll,ucsb-cs148-s21/1pm-t6-ucsb-poll,0,ucsb-cs148-s21,https://github.com/ucsb-cs148-s21/1pm-t6-ucsb-poll,Allows anyone with a UCSB email to vote on virtually anything.,0,2021-04-02 20:40:58+00:00,2021-08-23 08:46:37+00:00,2021-06-09 21:25:26+00:00,,66462,2,2,JavaScript,1,1,1,1,0,0,4,0,0,2,mit,1,0,0,public,4,2,2,main,1,1,"# 1pm-t6-ucsb-poll
Allows anyone with a UCSB email to vote on virtually anything. 
Techstack: React / Node.js / (Express) / Firebase


UCSB Polls will give information on what the student body's favorite beer is, what their favorite class was, what their favorite restaurant in IV is, etc. Users should be able to log in with their @ucsb.edu account and then vote and share different polls. 

This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).

Users:
- Users that look up, create, and vote on polls
- Admins can remove polls to the system (Not added yet) 

# Members
- Jasun Chen @jasunchen
- Daniel Lohn @DLohn
- TT Luo @Randp
- Zhengying Li @zhengyingl
- Daniel Shamtob @dshamtob

# Repository Structure

The backend is in the root directory under server.js.

Our meeting logs are in the 'team' folder and our documents in the 'docs' folder.

The frontend is in the myapp folder:
- Contains all components and pages under the 'src' folder.
- The entire website is hosted on index.js which calls App.js.
- App.js calls NavigationBar.js and Main.js, which is used as a router for the pages on the website.

The src folder contains three main folders:
- A 'components' folder which includes page components making up the webpages.
- An 'auth' folder for user authentication.
- A 'pages' folder for the pages on the website.
- A 'tests' folder for tests but we decided not to pursue TDD for this project. 

# Installation

Prerequisites: Javascript, Node.js, express 

# Dependencies
- @auth0/auth0-react for auth0 setup
- bootstrap/react-bootstrap for bootstrapping the webapp
- swr for linking to server
- react/react-scripts for front end
- firebase for database
   
# Installation Steps

[Deployment Instructions](./docs/DEPLOY.md)


# Functionality

[User Manual](./docs/MANUAL.md)
- Click the login button in the top right corner of the webpage to login through Auth0.
- Navigate to ""Create a Poll"" under ""Polls"" in the navigation bar. Create your own poll!
- Go back to the home page (Or browse polls by recent). You can see your own poll (click on it, vote on it, etc)

# Design Documentation

[Design Document](./docs/DESIGN.md)

# Known Problems

- Code is messy (still some bugs left over) 
- Authentication not ideally secure (api is not secure)
- No anonymity yet 
- Should switch to Firebase's bulit in authentication service ideally (but auth0 works for now)
- No moderation implemented yet


# Contributing

Fork it!
Create your feature branch: git checkout -b my-new-feature  
Commit your changes: git commit -am 'Add some feature'  
Push to the branch: git push origin my-new-feature  
Submit a pull request :D

# Deployment

View our [Heroku Deployment](https://cs148-1pm-t6-ucsb-poll.herokuapp.com/).

# Final Presentation 

https://youtu.be/H-vttr9SO-A

",EDU,,"['jasunchen', 'zhengyingl', 'dshamtob', 'Randp', 'DLohn', 'PaulKuang']",,,1,0,,,,,,2,0.95
915055878,R_kgDONoqlBg,pj06-studyconnect,ucsb-cs148-w25/pj06-studyconnect,0,ucsb-cs148-w25,https://github.com/ucsb-cs148-w25/pj06-studyconnect,,0,2025-01-10 21:43:25+00:00,2025-03-07 02:31:04+00:00,2025-03-07 04:49:59+00:00,https://studyconnect-deploy.vercel.app/,992,1,1,TypeScript,1,1,1,0,0,0,2,0,0,12,mit,1,0,0,public,2,12,1,main,1,1,"# pj06-studyconnect

Project Description: A webapp that helps connect college students in the same classes together for studying.

Full Name: Github ID
----------------------
Zhenyu Yu: ZhenyuYu1  
Wesley Chiba: jeffsmithepic  
Maria Saucedo-Flores: Maria-Saucedo  
Shelly Zhu: zhushelly  
Allen Hu: AllenHsm  
Anthony Jin: jinanthony  
Hannah Su: hannuhsu

# Tech Stack

Main Framework: Next.js

Backend:
- Database: Firebase
- Authentication: Firebase / Google OAuth
- Fetching UCSB class info: UCSB API

Deployment: Vercel

# Idea

In the front page of the website, each user is prompted with a Firebase or Google OAuth login for authentication. If login is successful, the webapp will display a list of classes that each user can choose from. The user can filter for a specific class by name or major, and can join each class. Students that join a class can send discussion messages in a chat room where they can connect with other students and share study resources. We are planning to use Firebase for storing user data, Firebase / Google OAuth for authentication, and the UCSB API to fetch class info.

# Roles

- Student:
  - Goal: To connect with other students and help study for classes
  - Permissions: View & browse classes, post messages and discussions
- Instructor:
  - Goal: To facilitate discussion about classes and help students study
  - Permissions: Pin discussion posts to channel, remove irrelevant content from chats
 - Administrator:
   - Goal: To ensure class chats are not being misused (remove spam bots, etc)
   - Permissions: Remove users from channel, remove irrelevant content
  
# Installation

### Prerequisites
- Git version 2.43 or above (check using git -v)  
- npm version 10.2 or above (check using npm -v)  

### Dependencies
Frontend: 
- React.js and Next.js
- Tailwind for styling

Backend:
- Database: Firebase
- Authentication: Firebase / Google OAuth

### Installation Steps
Clone the project: `git clone https://github.com/ucsb-cs148-w25/pj06-studyconnect.git`  
Install dependencies: `npm install`  
Run locally: `npm run dev`  
Create a .env.local file in the /study-connect folder with the following environmental variables:
- NEXT_PUBLIC_FIREBASE_API_KEY=""<your_key>""
- NEXT_PUBLIC_FIREBASE_AUTH_DOMAIN=""<your_key>""
- NEXT_PUBLIC_FIREBASE_PROJECT_ID=""<your_key>""
- NEXT_PUBLIC_FIREBASE_STORAGE_BUCKET=""<your_key>""
- NEXT_PUBLIC_FIREBASE_MESSAGING_SENDER_ID=""<your_key>""
- NEXT_PUBLIC_FIREBASE_APP_ID=""<your_key>""
- UCSB_API_KEY=""<your_key>""

You can get the Firebase keys by making a Firebase account and creating a new project here: [Firebase](https://firebase.google.com/)  
You can get a UCSB API key by filling out a request form here: [UCSB API request form](https://developer.ucsb.edu/docs/applications/application-approval-request)  
Visit site at [localhost:3000](http://localhost:3000/)

# Functionality
1. Login using your @ucsb.edu email and fill out your profile page.
2. View courses in the courses page found on the right side of the header
3. Search courses by subject code or course name

# Known Problems
1. Private DMs not Implemented
2. UI/UX Improvements

# Contributing
1. Fork it!
2. Create your feature branch: `git checkout -b my-new-feature`
3. Commit your changes: `git commit -am 'Add some feature'`
4. Push to the branch: `git push origin my-new-feature`
5. Submit a pull request :D

# License
MIT License
",EDU,,"['ZhenyuYu1', 'Maria-Saucedo', 'jeffsmithepic', 'zhushelly', 'AllenHsm', 'jinanthony', 'hannuhsu']",,,1,0,,,,,,1,0.95
249104417,MDEwOlJlcG9zaXRvcnkyNDkxMDQ0MTc=,code-from-class,ucsb-cs16-s20-nichols/code-from-class,0,ucsb-cs16-s20-nichols,https://github.com/ucsb-cs16-s20-nichols/code-from-class,,0,2020-03-22 03:17:16+00:00,2020-06-11 19:15:29+00:00,2020-06-11 19:15:26+00:00,,227,0,0,Assembly,1,1,1,1,0,0,4,0,0,0,,1,0,0,public,4,0,0,master,1,1,,EDU,,['lawtonnichols'],,,1,0,,,,,,0,0.75
179100779,MDEwOlJlcG9zaXRvcnkxNzkxMDA3Nzk=,s19-ykk,ucsb-cs16/s19-ykk,0,ucsb-cs16,https://github.com/ucsb-cs16/s19-ykk,"https://ucsb-cs16.github.io/s19-ykk/ for UCSB CS16, S19, Taught by Prof. K (Yekaterina Kharitonova)",0,2019-04-02 14:54:45+00:00,2019-09-13 21:51:02+00:00,2019-09-13 21:51:00+00:00,,4116,5,5,HTML,1,1,1,1,1,0,15,0,0,5,mit,1,0,0,public,15,5,5,master,1,1,"# s19-ykk
https://ucsb-cs16.github.io/s19-ykk/ for UCSB CS16, S19, Taught by Prof. K (Yekaterina Kharitonova)
",EDU,,"['watercannons', 'ykharitonova', 'benye11', 'btk5h', 'pmaster', 'QinghangHong1', 'chris-alsheikh', 'kheff16', 'maggie-schmit']",,,1,0,,,,,,6,0.95
867771082,R_kgDOM7kiyg,team04-GOLDTracker,ucsb-cs184-f24/team04-GOLDTracker,0,ucsb-cs184-f24,https://github.com/ucsb-cs184-f24/team04-GOLDTracker,,0,2024-10-04 17:28:48+00:00,2025-01-27 22:49:11+00:00,2024-12-11 02:34:08+00:00,,144311,3,3,JavaScript,1,1,1,1,0,0,3,0,0,14,,1,0,0,public,3,14,3,main,1,1,"# team04-GOLDTracker
<br />

### Description: 
This is a class availability notification app. <br />

### Project members:  
| Name            | GithubID       |
|-----------------|----------------|
| Simranjit Mann  | Simonmann17    |
| Daniel Jesen    | Division7      |
| Xinyao Song     | xinyao-song    |
| Karsten Lansing | KarstenLansing |
| June Bi         | zhenbi93       |
| Allen Hu        | AllenHsm       |

### Which tech stack(s) your group plans to evaluate/use? 
We are planning to use React Native with Expo Go

### Used Libraries
- react-native-navigation
- expo framework and related libraries
- firebase
- @react-native-google-signin
- react-native-community/blur
- react-native-community/datetimepicker
- react-native-async-storage
- dotenv

### User Roles
- UCSB students
  1. They could check information of courses they are interested in.
  2. They could login with their UCSB account to save their information.
  3. They could search courses in different subjects and could use filters to see the rank based on professors' scores.
  4. They could save courses in their cart, and when the course has spare space, they will receive notifications.
  5. They could use Join button to copy the course number and quickly added course in the GOLD.

# Deployment Instructions

## Android
1. Download the APK file.
2. Install it on an Android device.

## iOS/Android (Development Mode)
1. Clone the repository:
   ```bash
   git clone https://github.com/ucsb-cs184-f24/team04-GOLDTracker.git
   ```

2. In the terminal, navigate into the project directory:
   ```bash
   cd GOLDTracker
   ```

3. Install the necessary dependencies:
   ```bash
   npm install
   ```
4. Add the `.env` to the directory
   - Add the `.env` file pinned in the team04 channel on slack into the GOLDTracker directory
   
6. Linke assests
   ```
   npx react-native-asset
   ```


7. For iOS only:
   - Navigate to the ios folder:
     ```bash
     cd ios
     ```
   - Install CocoaPods dependencies:
     ```bash
     pod install
     ```
   - Return to the root project directory:
     ```bash
     cd ..
     ```

8. Run the app:
   - iOS:
     ```bash
     npx expo run:ios
     ```
   - Android:
     ```bash
     npx expo run:android
     ```
",EDU,,"['xinyao-song', 'Division7', 'zhenbi93', 'AllenHsm', 'KarstenLansing', 'Simonmann17']",,,1,10,,,,,,1,0.95
87135385,MDEwOlJlcG9zaXRvcnk4NzEzNTM4NQ==,lectures,ucsb-cs24-sp17/lectures,0,ucsb-cs24-sp17,https://github.com/ucsb-cs24-sp17/lectures,,0,2017-04-04 00:59:52+00:00,2017-04-04 01:09:46+00:00,2017-06-07 18:50:35+00:00,,270,0,0,C++,1,1,1,1,0,0,7,0,0,0,,1,0,0,public,7,0,0,master,1,1,,EDU,,['dibamirza'],,,1,0,,,,,,0,0.95
9265037,MDEwOlJlcG9zaXRvcnk5MjY1MDM3,cs56-games-pong,ucsb-cs56-projects/cs56-games-pong,0,ucsb-cs56-projects,https://github.com/ucsb-cs56-projects/cs56-games-pong,-,0,2013-04-06 19:03:18+00:00,2018-02-16 01:21:53+00:00,2018-03-21 22:24:05+00:00,,928,2,2,Java,1,1,1,1,0,0,12,0,0,17,,1,0,0,public,12,17,2,master,1,1,"Pong
==============

This is an implementation of the classic game Pong, with cooperative multiplayer support.

To compile and run: ""ant run"" in the main directory

There are seven game mode options that correspond to the size of the window when playing.

How To Play:
Player 1 (on the left)
  ""W"" -- Move up
  ""S"" -- Move down
  ""a"" -- grab ball (hold to grab)

Player 2 (on the right)
  up arrow -- Move up
  down arrow -- Move down
  left arrow -- grab ball (hold to grab)

Instructions:
  When the ball is stopped, Press the spacebar to activate motion.
  Try to not let the ball hit your side of the screen or you will lose a life.
  The player who wins the round, receives the total number of hits added to their score.
  Each Player has 3 lives.
  The Winner is whoever has the most lives at the end, their name will be prompted.
  If their score is in the top 5 of the High Score, then it will be saved to the High Score.


  ![](http://i.imgur.com/NAKKNhR.jpg)
  ![](http://i.imgur.com/jdCMrej.jpg)

TODO: maybe add a few more tickets, most of the existing ones wouldn't take much time to implement.  Other than that it builds and runs fine (David Coffill)

project history(Newest remarks to oldest)
===============

### Fall 2017 final remarks

`F17 Andrew Polk, Victoria Sneddon, 2PM lab`

* What the code does:
  * allows users to choose which mode of the game ""pong"" they want to play and the colors of the ball and paddles
* Features to be added:
  * AI opponent
  * Change y ball velocity when ball hits paddle
* Bugs:
  * issues with screen size for some of the modes
  * ball can get stuck behind paddle and wall collisions
  * pause text doesn't automatically show up always
* Opportunities for Refactoring:
  * code can still be better organized, neater
* Advice:
  * program for extensible
  * think about the poor future students that will have to fix your code 

### Winter 2016 final remarks

`W16: Angel Ortega, Ben Patient, 4PM lab`

* What the code does:
  * The code is a simple game of Pong for two players, with scores.
* Features that could be added:
  * AI for the paddle
  * Multiple ball modes
  * Unit-tests
* What bugs exist:
  * Pausing does not pause the paddles
* Opportunities for refactoring:
  * Make variables private, prefer to use nonstatic variables
  * Remove coupling
  * Consider builder pattern for GUI creation

### Fall 2016 final remarks
TODO:
* Add instructions for the ball grabbing feature, we implemented it so that holding ""a"" when the ball is within close range of the left paddle grabs the balls, and ""left arrow"" grabs the balls when they are in range of the right paddle.

* Fix the ball colliding with the top/bottom wall when grabbing the ball with paddle (play around with this and you will clearly see the issue)
* Fix how the game recognizes pausing when the ball is attached
* Integrate two balls with all the difficulty modes and extra balls

How To Play:
Player 1 (on the left)
  ""W"" -- Move up
  ""S"" -- Move down
  ""a"" -- grab ball (hold to grab)

Player 2 (on the right)
  up arrow -- Move up
  down arrow -- Move down
  left arrow -- grab ball (hold to grab)
  SUGGESTIONS
  Integrate grab paddle with thread so that it when the user presses either ""a"" or ""left arrow"", the game processes it right as the ball and paddle collide. (right now we are using distance which allows you to grab the ball while it isn't touching the paddle)
  Refactor how the two balls is implemented so that multiple balls will be easier to work with (in the code)

### Winter 2014 remarks
```
 W14 | bronhuston 4pm | sarahdarwiche,benjaminhartl | An implementation of the classic game Pong, with cooperative multiplayer support.
```

An implementation of the classic game Pong, with cooperative multiplayer support.

To Compile and Run:
""ant run"" in the main directory

The three difficulty options correspond to the size of the window when playing.

How To Play:
Player 1 (on the left)
  ""W"" -- Move up
  ""S"" -- Move down

Player 2 (on the right)
  up arrow -- Move up
  down arrow -- Move down

When the ball is stopped, Press the spacebar to activate motion.
Try to not let the ball hit your side of the screen or you will lose a life.
The player who wins the round, receives the total number of hits added to their score.
Each Player has 3 lives.
The Winner is whoever has the most lives at the end, their name will be prompted.
If their score is in the top 5 of the High Score, then it will be saved to the High Score.
",EDU,,"['joyoyoyoyoyo', 'benjaminhartl', 'bkhanijau', 'vsneddon', 'jdum66', 'joelbagan', 'zhanchengqian', 'sanchitg94', 'gonfunko', 'Lingampalli56', 'xingxinggeng', 'millanbatra1234', 'iamSamuelFu', 'kjorg50', 'dcoffill', 'pconrad', 'brianslee', 'bronhuston', 'hannavigil', 'mastergberry']",1,,1,0,,,,,,2,0.95
16473445,MDEwOlJlcG9zaXRvcnkxNjQ3MzQ0NQ==,cs56-utilities-GEAR-scraper,ucsb-cs56-projects/cs56-utilities-GEAR-scraper,0,ucsb-cs56-projects,https://github.com/ucsb-cs56-projects/cs56-utilities-GEAR-scraper,-,0,2014-02-03 08:08:51+00:00,2016-10-30 22:36:54+00:00,2016-10-30 22:36:53+00:00,,22999,0,0,Java,1,1,1,1,0,0,4,0,0,3,,1,0,0,public,4,3,0,master,1,1,"cs-utilities-GEAR-scraper
=========================

Scrape the UCSB Gear Catalog to form lists of structured information (e.g. course numbers that meet certain GE requirements.)

https://engineering.ucsb.edu/sites/engineering.ucsb.edu/files/docs/GEAR-16-17.pdf

project history
===============
```
 W14 | andrewberls 5pm | alanbuzdar | Scrape the UCSB Gear Catalog to form lists of structured information
```

```
W15  | hunterbuckhorn
```

The GEAR catalog is here, as a PDF:

http://engineering.ucsb.edu/current_undergraduates/pdf/GEAR-15-16.pdf

The library http://pdfbox.apache.org/ provides tools in Java to get information out of PDF files.



OVERVIEW
========





CLASS DOCS
==========

CoEgeCourse.java -------------------------------------------------------------------------------------------------------
    Used to contain information about each GE in GEAR

public interface CoEgeCourse {
    public String getDeptInGear();     // department offering the course, exactly as formatted in GEAR
                                       // e.g. Anthropology
    public String getDeptCode();       // department code (e.g. ANTH) as in GOLD (you'll have to translate to get that)
    public String getCourseNum();      // e.g. 118B
    public boolean isD();              // its on the area D list
    public boolean isE();              // its on the area E list
    public boolean isF();              // its on the area F list
    public boolean isG();              // its on the area G list
    public boolean isWriting();        // This course applies toward the writing requirement.
    public boolean isAmHistInst();     // This course applies toward the American History & Institutions requirement.
    public boolean isEthnicity();      //  This course applies toward the ethnicity requirement.
    public boolean isEuroTrad();       // This course applies toward the European Traditions requirement.
}


GECourse.java ----------------------------------------------------------------------------------------------------------
    Implements the CoEgeCourse.java interface
    Contains all of the definitions for the CoEgeCourse.java interface


PDFTextParser.java -----------------------------------------------------------------------------------------------------
    Processes the PDFs into text with the Apache PDF box API

    API DOCS: http://pdfbox.apache.org/docs/1.8.10/javadocs/

    class adopted from http://thottingal.in/blog/2009/06/24/pdfbox-extract-text-from-pdf/

public class PDFTextParser {

    public static String pdftoText(String fileName) {
        uses a PDFParser object from the API to handle parsing the pdf document
        uses a pdDoc, cosDoc and File object to to manipulate the parsed data.
        checks to see if the File is valid
        creates a new PDFParser by passing the file as a FileInputStream
        parses the PDF document
            creates a PDFTextStripper object and creates a cosDoc and then generates a pdDoc from the cosDoc
            the cosDoc is then passed to the PDFTextStripper object and creates a String, parsedText
        closes cosDoc and pdDoc and returns the String parsedText
    }


    public static String pdftoText(BufferedInputStream is, int page) {
        creates a PDFParser from the BufferedInputStream is
        creates a PDFTextStripperByArea object
        parses the PDF document
        creates a pdDoc with the PDFParser object
        calls method setSortByPosition(true) to sort the pdf based on a specific area
        creates two Rectangle objects to use for parsing by area
        creates a List of PDPages and creates a PDFPage object, firstPage
        adds two Rectangles to PDFTextStripper object using addRegion method
        the PDFTextStripper object calls getTextForRegion with each Rectangle region
        the result is a String, parsedText
        closes pdDoc and returns the String parsedText
    }
}



GEAR_scraper.java ------------------------------------------------------------------------------------------------------
    Uses the PDFTextParser class to scrape data from GEAR and create an ArrayList of GECourse objects
    the default url for the GEAR pdf to be scraped is stored as an instance variable in this class, along with the pages
    of the GE courses in the pdf to be parsed (we only want to parse the pages of the pdf with GE class listings)



public GEAR_scraper():
        for each page in the default range of the pdf on the default GEAR pdf url
        creates a PDFTextParser object and converts the page of the pdf into a String
        adds the String of parsed text to the ArrayList<String> textToParse


public GEAR_scraper(URL url, int startPage, int endPage):
        this constructor is essential the same as the one above, except with the ability to pass in the
        startPage value and the endPage values used in the for loop


private boolean isCourse(String s):
        helper function for checking if a line of text is a course name
        contains a very very long boolean logic statement that checks for substrings that determine if the line is a
        course name or not


public ArrayList<GECourse> createArrayList():
        creates the ArrayList<GECourses> by iterating through the textToParse ArrayList<String> and building a GECourse
        for every line of each page of the pdf text and adding it.



GEAR_scraper_GUI.java --------------------------------------------------------------------------------------------------
    A graphical interface to interact with the GEAR scraper and search for courses based on name and GE course area




APACHE PDFBOX API DOCS
======================
Can be found at http://pdfbox.apache.org/docs/1.8.10/javadocs/

API packages used:
------------------
org.apache.pdfbox.cos.COSDocument;
org.apache.pdfbox.pdfparser.PDFParser;
org.apache.pdfbox.pdmodel.PDDocument;
org.apache.pdfbox.util.PDFTextStripper;
org.apache.pdfbox.util.PDFTextStripperByArea;
org.apache.pdfbox.pdmodel.PDPage;
org.apache.pdfbox.pdmodel.common.PDStream;


The objects and methods used in this project along with a brief description are as follows:


// Handles the parsing of the PDF Document

PDFParser
    .parse(): parses the stream and populates the COSDocument
    .getPDDocument(): returns the PDDocument that was parsed



// This is the in-memory representation of the PDF document.
// You need to call close() on this object when you are done using it!!
// This class implements the Pageable interface, but since PDFBox version 1.3.0 you should be using
// the PDPageable adapter instead (see PDFBOX-788).

PDDocument
    .close(): Closes the document
    .getDocumentCatalog(): returns a PDDocumentCatalog, which represents tha acroform of a PDF document
        .getAllPages(): returns a list of PDPageNode nad PDPages, PDFs contain a hierarchical structure of PDPages and
                        PDPageNodes to store this information



// This is the in-memory representation of the PDF document.
// You need to call close() on this object when you are done using it!!

COSDocument
    .close(): Closes the document

// This class will take a pdf document and strip out all of the text and ignore the formatting and such.
// Please note; it is up to clients of this class to verify that a specific user has the correct permissions to
// extract text from the PDF document. The basic flow of this process is that we get a document and use a series
// of processXXX() functions that work on smaller and smaller chunks of the page. Eventually, we fully process
// each page and then print it.

PDFTextStripper
    .getText(PDDocument): returns the string of the parsed text file



// This will extract text from a specified region in the PDF.

PDFTextStripperByArea
    .setSortByPosition(boolean newSortByPosition): The order of the text tokens in a PDF file may not be in the same as they appear
                          visually on the screen. For example, a PDF writer may write out all text by font,
                          so all bold or larger text, then make a second pass and write out the normal text.
    .addRegion(String regionName, Rectangle2D rect): Adds a new region to group the text by
    .extractRegions(PDPage page): Process the page to extract the region text.
    .getTextForRegion(String regionName): Get the text for the region, this should be called after extractRegions()





PDF LAYOUT
==========
Helpful links:
Basic intro which is what most of the information below is based on:
    http://resources.infosecinstitute.com/pdf-file-format-basic-structure/

More in depth intro that goes slightly deeper into the structure of pdfs, objects used in pdfs, structures and layout
    https://web.archive.org/web/20141010035745/http://gnupdf.org/Introduction_to_PDF

The actual adobe PDF reference documentation:
    http://www.adobe.com/devnet/pdf/pdf_reference.html



Basic PDF layout

 -------------
|   HEADER    |
 -------------
|             |
|    BODY     |
|             |
 -------------
|'xref' Table |
 -------------
|   Trailer   |
 -------------

General information:
A simple pdf contains 4 parts:
* the header with the PDF version (and an option line to specify if the PDF contains binary data)
* the body, containing a series of objects that are used to hold all of the document's data
* xref Table: a cross-reference table, that specifies the position of the objects
* a trailer, with information about where the document starts
------------------------------------------------------------------------------------------------------------------------
In the body (the object list), there are several types of definitions:

Indirect Object (1 0 obj ... endojb): define a numbered top-level object.
    The first number (1) is the object number,
    the second number (0) is the revision number, which we don't use in this example.

There are 9 types of objects:

* Number: e.g. 3
* Indirect reference (n r R): references an object, e.g. 5 0 R. If the objects doesn't exist this is equivalent
    to the Null object (see below).
* Name (/Name): names are identifiers. If you know Lisp or Scheme, this is similar to the quote special form (e.g. 'ok).
    The initial / introduces the name but isn't part of the name; this is similar to $ in Bash, Perl or PHP.
* Dictionary (<< ... >>): this is a unordered list of (Name,Object) pairs. They are essentially hash tables.
    The Object part can be another Name (e.g. /Type /Font).
* Array ([ x y z ... ]): an ordered list of objects, e.g. [ 0 0 200 200 ].
* String Object ((text)): text. The complete syntax is complex, but for now suffice to say it's text between
    parenthesis, e.g. (Hello, world!).
* Stream (<< /Length ... >> stream ... endstream): embedded data, can be compressed. It starts with a dictionary that
    describes the stream such as its length or the encoding (/Filter) is uses.
* Boolean: true or false.
* Null Object: null.

Representing and manipulating these objects forms the Object layer of the GNUpdf library.


PDFs contain a Cross-reference table
This is a sequential list of objects (#1, #2, etc) offsets, which is where the object is stored in memory relative to
the beginning of the file. The cross reference table allows any given object to be referenced quickly and efficiently.
Each line contains the offset of the object definition, a revision number, and an on/off marker f (free) or n (in use).
If you modify this test document, remember to update all of the offsets, as well as the startxref line, which describes
the offset of the xref section.
ex)
xref
0 6
0000000000 65535 f
0000000010 00000 n
0000000079 00000 n


How to read the PDF file:
PDF files aren't read top to bottom, they are read by accessing data that is stored across the entire file
1) Reads the first line to get the PDF version
2) Goes to the end of the document to get check the %%EOF marker to get the offset of the Cross-Reference table.
3) Jumps to the Cross-Reference table and builds a list of object offsets
4) After the Cross-Reference table, it can read the trailer dictionary which contains the Catalog, which is the start
    of the document. It's specified by an indirect reference to object 1: 1 0 R

5) Now the reader checks the Catalog object. The Catalog object will contain a Pages object which is a tree like data
    structure. It can reference either leaves (pages) of other nodes (which can do the same).
    The Pages object also contains the size of the objects that it points to

6) The Pages object reference its parent object which contains a set of resources that are needed
    to render the page and its content

7)


",DEV,,"['alanbuzdar', 'drdipepperjr', 'kjorg50', 'andrewberls', 'pconrad', 'jaeaster', 'erdinckorpeoglu', 'mastergberry']",,,1,0,,,,,,3,0.95
9941739,MDEwOlJlcG9zaXRvcnk5OTQxNzM5,cs56-utilities-GEscraper,ucsb-cs56-projects/cs56-utilities-GEscraper,0,ucsb-cs56-projects,https://github.com/ucsb-cs56-projects/cs56-utilities-GEscraper,-,0,2013-05-08 17:13:26+00:00,2019-01-23 18:09:42+00:00,2017-12-07 00:32:19+00:00,,945,1,1,Java,1,1,1,1,0,0,4,0,0,11,,1,0,0,public,4,11,1,master,1,1,"cs56-utilities-GEscraper
========================

Project Information
---------------
```
 YES | mastergberry | Scrape UCSB course pages for appripriate classes that fulfill certain GE requirements
```
####Purpose
------------------
1. Scrape and show General Education Area Course list. 
2. Scrape and show courses offered by a specific department in College of Engineering

The user has the option to choose to display courses from general subject or engineering.

The user can choose to show the course description of each course.


####What's being scraped?
-------------------------------
The example websites that are being scraped are:
http://my.sa.ucsb.edu/catalog/current/UndergraduateEducation/AreaE.aspx  (general education)
https://my.sa.ucsb.edu/catalog/Current/CollegesDepartments/coe/compengr.aspx?DeptTab=Courses  (engineering)

####Who made what changed?
----------------------------------
*Changes by Dylan Lynch and Brent Kirkland*
- **DONE**: [Test for getting a specific department from an area](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/11) - *50 pts*
- **DONE**: [Help Feature](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/10) - *100 pts*
- **DONE**: [Better Formatting - Usability](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/8) - *100 pts*
- **DONE**: [Better user documentation](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/5) - *100 pts*
- **DONE**: [Allow for more sorting options](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/6) - *200 pts*
- **DONE**: [Allow reuse](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/4) - *50 pts*
- **DONE**: [Beautification - updated comments,  future refractoring, and readme (beautiful, right?)](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/14) - *25 pts*

*Recent Changes of LAST LAST GROUP (W15)*

- Improved Javadoc
- Added JUnit Tests
- Refactored files for Git
- Fixed missing class case
- Added User interaction to input an Area (eg ""B"", ""C"")

*Recent Changes of LAST GROUP (W16)*

- Added a basic GUI
- Converted the code to use Jsoup (which in turn beautified the code and made it simpler)

*Notes from W16*

- Any changes to the codebase from this point forward will probably not involve the existing code, because there isn't much left to do with it
- Code changes will likely be new features or expanding the scraper to do other things

*Changes by Xinjie You and Xingyuan Lin (F16)*
- **DONE**: [Scrape other services(scrape courses offered by a specific department in CoE)](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/21) - *400 pts*
- **DONE**: [Refactor main class](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/27) - *200 pts*
- **DONE**: [Give course description](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/26) - *400 pts(500 pts?)*


####*Notes from F16*
----------------------------------
**A major mistake we made and a valuable lesson we learned**

We put a lot of efforts working on [issue#21](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/21), which says ""*Expand the existing scraper to also scrape other UCSB services. Like the College of Engineering's GEAR book.*"" However, unfortunately, we misunderstood the issue and did all the work to scrape courses that are offered by College of Engineering, no matther whether or not they belong to GE. Because we think it is part of ""*other UCSB services*"".

Actually, we should have noticed this because the name of the project is **GEscraper**!! But we are so focused on the code and development that we ignored this problem.

So, the lesson we learned is that we write code to meet customers' needs. There should be a good system to pass information about customers' needs down to software development. The communications and cooperations among different roles in a software team are very important. 

**Advice for future groups**
- Since the work we did on [issue#21](https://github.com/UCSB-CS56-Projects/cs56-utilities-GEscraper/issues/21) does not belong to the project, it should be removed. However, the code we wrote are not meaningless in respect to code itself. Maybe groups working on project [cs56-scrapers-ucsb-curriculum](https://github.com/UCSB-CS56-Projects/cs56-scrapers-ucsb-curriculum) can refer to the code to make future developments.
- In order to get course description according to the course title, we have to map the title to correct url. Unluckily, there is no general rule, so we have to create the mapping table by hand. It is done in the file *AreaUrlMappingTable.java*. When reviewing our code, a kind lady brought up a question about whether it is more appropriate to put the mapping table into a configure file, rather than writing it in the code. We think the answer is yes, so that we do not to recompile the code if the mapping table is changed. Maybe you can work on this matter to improve the quality of the code.
- There is some reused code. Future groups can further refactor the code by creating more general classes that others inherit from.
- The GUI uses FlowLayout now, future groups can change the code to make the GUI looks same in different computer.
- The fisrt GUI window can't be closed now. Future groups can fix this bug.

F17 Final Remarks

This code is mainly set up in the EngGUI, ScraperGUI, and GeGUI. ScraperGUI contains the main method. The other two GUI files rely on the scraper files (GetEngInfo, GetGeInfo) to pull course information from external websites and display it to the user. We also implemented a mashup with UCSB Curriculum scraper. Currently, the UCSB Curriculum scraper files are in our repo. In the future, those files should be removed and instead the project should be used as a library. In the future, more details on enrollment history for past quarters could be added as well. Error handling could be implemented as well for when scraping fails (a course isn't found based on the filters entered). In general, make sure you understand what the purpose of each file is before trying to modify or use it. Make comments to document any changes you make.
",EDU,,"['eshahani7', 'aditya-nadkarni', 'brentkirkland', 'youxinjie', 'natashalee', 'marcosimone', 'bwan2', 'jaeaster', 'Simon-Huynh', 'mastergberry', 'leifdreizler', 'PazZaitGivon', 'pconrad']",,,1,0,,,,,,2,0.95
10373214,MDEwOlJlcG9zaXRvcnkxMDM3MzIxNA==,cs56-utilities-grapher,ucsb-cs56-projects/cs56-utilities-grapher,0,ucsb-cs56-projects,https://github.com/ucsb-cs56-projects/cs56-utilities-grapher,-,0,2013-05-30 01:51:53+00:00,2017-11-13 05:18:00+00:00,2017-09-30 08:51:28+00:00,,391,0,0,Java,1,1,1,1,0,0,3,0,0,19,,1,0,0,public,3,19,0,master,1,1,"cs56-utilities-grapher
======================
F16 Hi future cS56 students,<br/>
this project is a software that let's you graph functions, for example,<br/>
`f(x)=3x+4;`<br/>

Here is a brief description of the useful classes:<br/>
The GrapherApplication class is the main class and contains the JFrame of the GUI.<br/>

The FunctionsPanel class is the JPanel on the left side,<br/>
The Graph2DPanel is the center JPanel, and the Grapher2DBoundsPanel is the right JPanel.<br/>
The Bounds2DFloat class contains the bounds of the graph: the minimum and maximum x and y.<br/>
FunctionR1R1DisplayData class uses GeneralPath to display the actual data.<br/>

The ...Dialog classes contains a dialog box, and they are used in GrapherApplication.<br/>

All functions, as far as we know, use the CustomFunction class, which implements the FunctionR1R1 interface.<br/>
The CustomFunction class basically holds and represents a function inside it and can evaluate the value of the function at any point.<br/>
Other classes that represents a function, such as QuadraticFunction, are not used. These classes can probably be removed.<br/>
The Term class is not really used either and might not be used at all.<br/>

As it stands, the grapher accepts basically any function with one of the following variables: x, y, or t.<br/>
You can use the following operations: +,-,*,/, and ^.<br/>
You can use the folowing trig functions: cos, sin, tan, log, ln.<br/>
You can use the following constants: PI, e.<br/>
You can use parenthesis.<br/>
The grapher can detect syntax errors.<br/>
The parser works like this: The Tokenizer class turns input (String) into a bunch of tokens, or meaningful units of information such as a Plus symbol or a number.<br/>
The Parser class turns these tokens into a syntax tree: A tree that allows the Evaluator class to evaluate the input.<br/>
The Evaluator class evaluates the syntax tree (These classes can be found in the tokenizer folder, parser folder, and the evaluator folder, respectively).<br/>

Example functions you can input:<br/>
`coscoscos55x`<br/>
`(4+x(5+x)+5)`<br/>
`PIePIePIePIe //which will be interpreted as PI*e*PI*e*PI*e*PI*e` <br/>
`x^3loglnPI`<br/>

**Build Instructions**<br/>
Build: `ant compile`<br/>
Build & Run: `ant run`<br/>
Javadoc: `ant javadoc`<br/>
Test: `ant test`<br/>
Clean: `ant clean`<br/>

**Features to add and bugs**<br/>
Correctly working y axis.<br/>
More functions: floor, ceil, abs, cosh, sinh, derivatives, integrals<br/>
More function formats: polar, parametric, multivariable<br/>
Saving graphs<br/>
Prettier GUI<br/>
Log/ln functions sometimes don't graph right<br/>

**Existing Issues**<br/>
https://github.com/UCSB-CS56-Projects/cs56-utilities-grapher/issues/5<br/>
https://github.com/UCSB-CS56-Projects/cs56-utilities-grapher/issues/8<br/>
https://github.com/UCSB-CS56-Projects/cs56-utilities-grapher/issues/12<br/>
https://github.com/UCSB-CS56-Projects/cs56-utilities-grapher/issues/16<br/>
https://github.com/UCSB-CS56-Projects/cs56-utilities-grapher/issues/18<br/>
https://github.com/UCSB-CS56-Projects/cs56-utilities-grapher/issues/19<br/>
https://github.com/UCSB-CS56-Projects/cs56-utilities-grapher/issues/24<br/>
https://github.com/UCSB-CS56-Projects/cs56-utilities-grapher/issues/25<br/>
https://github.com/UCSB-CS56-Projects/cs56-utilities-grapher/issues/26<br/>
https://github.com/UCSB-CS56-Projects/cs56-utilities-grapher/issues/27<br/>
https://github.com/UCSB-CS56-Projects/cs56-utilities-grapher/issues/28<br/>
<br/>
There currently exists above 2000 points worth of issues some simple, and some harder.<br/>
<br/>

**Opportunities to refactor:**<br/>
Right now, the parser could use some refactoring:<br/>
To add a basic function or operator right now, you have to make a token class, add one line of code to tokenizer, and change evaluator slightly.<br/>
Ideally, we shouldn't have to change the evaluator at all.<br/>
Parser could use a lot of refactoring...but try not to touch the Parser.parse method unless you really know what you're doing.<br/>
Some classes, such as QuadraticFunction are obsolete and could probably be deleted.<br/>

Good luck,<br/>
Henry Wang and Alexander Bauer

(W15) TBD: Good project but it needs more issues for students to work on. But most of the issues I can think of (such as adding parenthesis evaluation for arbitrary expressions) are difficult beyond the scope of a CS 56 project. (David Coffill)


project history
===============
```
 W14 | bronhuston 5pm | j-so | (bkiefer13) A 2D graphing GUI
```
",EDU,,"['qwertyuiop5040', 'ryanhalbrook', 'AlexanderJBauer', 'CommanderHamster', 'Tektonbuilds', 'YunSuk', 'erdinckorpeoglu', 'dcoffill', 'jaeaster', 'bkiefer13', 'mastergberry']",,,1,0,,,,,,3,0.95
156791279,MDEwOlJlcG9zaXRvcnkxNTY3OTEyNzk=,ucsb-cs56-events-calendar,ucsb-cs56-webapps/ucsb-cs56-events-calendar,0,ucsb-cs56-webapps,https://github.com/ucsb-cs56-webapps/ucsb-cs56-events-calendar,,0,2018-11-09 01:17:13+00:00,2018-12-07 07:18:19+00:00,2018-12-07 07:18:18+00:00,,143,0,0,Java,1,1,1,1,0,0,3,0,0,0,mit,1,0,0,public,3,0,0,master,1,1,"# UCSB Events Calendar

A web app that scrapes data of events from multiple UCSB websites to put them all into one easy to view calendar. You will be 
able to separate them by categories, and dates & times that work for the visitor. There is also a submit event page where you can 
submit your own event to be added to the calendar. To see more information for this webapp or news on recent updates, visit the FAQ page. 

## Getting Started

This project uses Maven 4.0.0, Java 1.8.0_181, Springboot 2.0.5, Heroku 2.0.3, and GitHub. Deploy the website via Heroku and you can find the

Our deployed website here: https://ucsb-events-calendar.herokuapp.com/

You can find the github main branch here: https://github.com/ucsb-cs56-f18/ucsb-cs56-events-calendar

Heroku dashboard: https://dashboard.heroku.com/apps/ucsb-events-calendar

### Prerequisites

Download Maven here: https://maven.apache.org/download.cgi

Download Java here: https://www.java.com/en/download/

Springboot info: http://spring.io/

Heroku: https://www.heroku.com/


### Getting a test website

First make sure you cd into your root directory where you have your branch of the project

In your command prompt make sure you're logged into your affiliated heroku account and follow login instructions
```
$ heroku login
```
Then to deploy your code use the command
```
$ mvn spring-boot:run
```

You can find your test website at: http://localhost:8080

## Deployment

1. First make sure you're in your root directory and you're logged into your heroku account by using the command

    $ heroku login

2. Then on your command prompt use the command

    $ heroku create -insert-your-app-name-here-

3. Update your pom.xml at the top to have your app name you just created underneath the `<properties>` tag
```
  <properties>

    <my.app.name>your-app-name</my.app.name>

  </properties>
```

update you-app-name to the name of your app when you used the heroku create command

4. Then deploy your code with the command
```
    $ mvn package heroku:deploy
```
5. Then access your deployed site at:
https://your-app-name.herokuapp.com
It may take a few minutes!

## Built With

* [Spring Boot] (https://spring.io/projects/spring-boot) - The web framework used
* [Maven] (https://maven.apache.org/) - Dependency Management
* [Java] (https://www.java.com/en/download/) - The coding language used
* [Git] (https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) - The version control manager
* [Heroku] (https://devcenter.heroku.com/articles/heroku-cli) - The cloud platform used to host our application

## Contributing

When contributing to this repository please make sure you speak to your assigned cs56 instructor via the method your group
agrees on. Make sure to work on your own branch for your own issue and keep your code documented and easy to read. Please
add only useful features that add value and makes it easier for the user to reach their end goal. Every feature/addition
should be related to something on the kanban board in the projects tab of the github repository, if it is not,
Please add the appropiate user story and issue to the board.

## Versioning

**Currently unavaiable**

## Contributors

Mentor:
Scott Chow


Students:
Ronald Hoang, Ryan Gormley, Jeff Tellew, Frank Lee, Dean Narlock


## Acknowledgments

Shoutout to professor Conrad for being an enthusiastic professor and teaching us about Advanced Applications Programming
",DEV,,"['pconrad', 'ryangormley81', 'ronaldhoang', 'jtellew', 'franklee26', 'scottpchow23', 'jefftellew', 'shao158']",,,1,0,,,,,,1,1
100393073,MDEwOlJlcG9zaXRvcnkxMDAzOTMwNzM=,Lecture4_0815,ucsb-cs8-m17/Lecture4_0815,0,ucsb-cs8-m17,https://github.com/ucsb-cs8-m17/Lecture4_0815,,0,2017-08-15 15:43:28+00:00,2017-08-15 17:41:46+00:00,2017-08-16 18:30:53+00:00,,5,0,0,Python,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,0,master,1,1,"# Lecture4_0815

See also:  https://ucsb-cs8-m17.github.io/lectures/lect04/



```
ssh -X pconrad@csil-21.cs.ucsb.edu
```

The first time you'll get a message such as:

```
ECDSA key fingerprint is SHA256:ANJ7ced3JXpxKnu8OhnS3f8Z0rE9aIKA+eHHgVlEzVc.
Are you sure you want to continue connecting (yes/no)? yes
```

Just say yes.

Machines are 01-48

CTRL/D can be used to exit.


For Windows, the program is MobaXTerm

To use it, look for ""New Session"" in upper left,

Then ""ssh"" upper left.

Then enter `csil-15.cs.ucsb.edu` (or whatever number)
where it asks for host.

Then, you'll be prompted for username and password.

# If you get this message when running `idle3`

or any other program that needs graphics...

```
_tkinter.TclError: no display name and no $DISPLAY environment variable
```

On Windows: Use MobaXTerm instead of a program such as PuTTY.

On Mac: You need to install XQuartz ... go to XQuartz.org, download, follow instructions, and try again.

Also: on Mac or Linux, maybe you forgot the -X part.

# Testing with pytest

```
python3 -m pytest areaCalc.py
```

Using `import pytest` and `pytest.approx()`
",EDU,,['pconrad'],,,1,0,,,,,,2,0.95
781706351,R_kgDOLpfkbw,EDS-296-DS-portfolios,UCSB-MEDS/EDS-296-DS-portfolios,0,UCSB-MEDS,https://github.com/UCSB-MEDS/EDS-296-DS-portfolios,Course website for EDS 296 - Data science tools for building professional online portfolios (MEDS @ Bren UCSB),0,2024-04-03 22:10:58+00:00,2024-11-24 15:19:27+00:00,2024-11-08 17:06:15+00:00,https://ucsb-meds.github.io/EDS-296-DS-portfolios/,53318,3,3,SCSS,1,1,1,1,1,0,0,0,0,2,,1,0,0,public,0,2,3,main,1,1,"# EDS 296: Data science tools for building professional online portfolios

![Screenshot 2024-09-03 at 12 30 58‚ÄØPM](https://github.com/user-attachments/assets/80aa7341-b982-49a5-9cb8-6b4e04f45068)

This repository houses the code for the [EDS 296-1F course website](https://ucsb-meds.github.io/EDS-296-DS-portfolios/).

## Course Description

Having a polished online presence is essential for showcasing your skills, projects, expertise, and (importantly) your personality. Learners, collaborators, and future employers alike will look to your public online profiles to glean information about you and your work. As data scientists, this often means that they‚Äôll head to one of two places: GitHub and/or your personal website. This course is designed to help you both lay the foundation for creating and maintaining these profiles in an organized, informative, and visually-appealing way. Over the next several weeks, you will:

- begin developing your personal brand
- build and deploy your personal website using popular data science tools and frameworks
- create your first (of many!) data science blog posts
- learn about how best to communicate your work to different audiences
- understand where and how to document your projects and code on GitHub
- and more!

## Report a bug / issue

Found something that doesn't look quite right? Feel free to file an [issue](https://github.com/UCSB-MEDS/EDS-296-DS-portfolios/issues) and include a concise, clear description, along with a link to the location on the website. Screenshots are always appreciated as well!

## License

This work is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/)
",WEB,,['samanthacsik'],,,1,0,,,,,,1,0.95
728538673,R_kgDOK2yeMQ,llm_uncertainty,UCSB-NLP-Chang/llm_uncertainty,0,UCSB-NLP-Chang,https://github.com/UCSB-NLP-Chang/llm_uncertainty,,0,2023-12-07 06:43:33+00:00,2025-03-05 11:16:04+00:00,2024-02-02 18:41:20+00:00,,58,27,27,Python,1,1,1,1,0,0,3,0,0,0,,1,0,0,public,3,0,27,master,1,1,"## Uncertainty Quantification for LLMs

This is the official implementation for the paper, ""Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling"".

### Requirements

The dependency packages can be found in `requirements.txt` file. One can use `pip install -r requirements.txt` to configure the environment. We use python 3.8 to run the experiments.


### Prepare the Data

Run the following script to prepare the data. 
```sh
python tools/prepare_data.py 
```

### Running the experiments
The overall pipeline is generate clarification $\\rightarrow$ collect model answers $\\rightarrow$ quantify uncertainty $\\rightarrow$ evaluate the performance (either mistake detection or ambiguity detection). Before running experiments, you need to configure your OpenAI API key in `src/common.py` file.

1. Use the following script to generate clarifications

```sh
python tools/generate_clarification.py --dataset_name ambigqa  --output_path logs/clarification/ambigqa.json --sample --sample_n 2
```

`dataset_name`: choices include `nq_open, gsm8km, ambigqa, ambig_inst`

2. Then collect the model outputs based on the generated clarifications:

```sh
python forward.py --dataset_name ambigqa --clarification_path logs/clarification/ambigqa.json --output_path logs/forward/ambigqa_forward.json
```

3. (For Natural Question and AmbigQA) The uncertainty quantification requires the model output distribution. We sample multiple answers in Step 2 above and then count the answer frequencies to estimate the output distribution. For the GSM8K and synthetic AmbigInst dataset, we ignore the intermediate reasoning steps and take the final answer (*e.g.*, a real number from GSM8K) for frequency computation.

   However, for tasks such as Natural Question and AmbigQA, the ChatGPT model will often output a complete sentence rather than just a word or phrase as the answer. For example, given the question `When did the world‚Äôs population reach 7 billion?`, ChatGPT may generate several different answers such as `December 2017` and `The world‚Äôs  population reached 7 billion in December 2017`. Regarding them as two different answers can lead to an overestimation of the entropy of output distribution. Therefore, we follow previous work \\[[1](https://github.com/lorenzkuhn/semantic_uncertainty/tree/main)\\] and \\[[2](https://github.com/zlin7/UQ-NLG/tree/main)\\] to map these answers into different groups and each group contains semantically equaivalent answers. The output distribution is then computed on top of these groups.

   We empirically find relying the LLM itself to cluster the generated answers brings better performance compared to using an NLI model in  \\[[1](https://github.com/lorenzkuhn/semantic_uncertainty/tree/main)\\] and \\[[2](https://github.com/zlin7/UQ-NLG/tree/main)\\]. Use the following script to run our method:

 ```sh
 python tools/answer_extraction.py --log_path logs/forward/ambigqa_forward.json --prompt_path lib_prompt/common/answer_extraction.txt --answer_key clarified_all_ans --output_path logs/forward/ambigqa_forward_ext.json
 ```

   The above script will read the original answers stored in the logs under the key `clarified_all_ans`. The extracted answers will be then stored with the key `ext_clarified_all_ans` for next-step evaluation

4. Uncertainty quantification. Use the following script to quantify the uncertainty of model prediction:

```sh
python evaluate_uq_qa.py --log_path logs/forward/ambigqa_forward_ext.json --output_path logs/uq_eval/ambigqa.json --answer_key ext_clarified_all_ans
```
(As we have executed step 3, here we need to change the `answer_key` to `ext_clarified_all_ans`)

5. Performance evaluation. You can use the evaluation scripts under the `tools/` directory, such as
```sh
python tools/compute_metrics_ambigqa.py
```

Note: for the experiment on Natual Question dataset where we use the uncertainty to predict whether the model's answer is correct, we follow the experiment setting in \\[[2](https://github.com/zlin7/UQ-NLG/tree/main)\\] and use ChatGPT to judge the correctness of model's answer. Therefore, to evaluate the performance on Natural Question dataset, you need to run the following script before `python compute_metrics_nq.py`:

```sh
python evaluate_correctness.py --log_path logs/uq_eval/nq.json --prompt_path lib_prompt/evaluation/nq_eval.txt --output_path logs/uq_eval/gpt_eval_nq.json
```

You can find the (more detailed) running scripts for each dataset under the `scripts` directory.

Note: Currently, we have not integrated the running scripts for different datasets into a single one. We will update the code later on.



Some of our implementation of the experiments on GSM8K dataset comes from [chain-of-thought-hub](https://github.com/FranxYao/chain-of-thought-hub).

```
@article{fu2023chain,
  title={Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance},
  author={Fu, Yao and Ou, Litu and Chen, Mingyu and Wan, Yuhao and Peng, Hao and Khot, Tushar},
  journal={arXiv preprint arXiv:2305.17306},
  year={2023}
}
```

The experiments on Natural Question follows the evaluation setting from [Semantic Uncertainty](https://github.com/lorenzkuhn/semantic_uncertainty/tree/main) and [UQ-NLG](https://github.com/zlin7/UQ-NLG/tree/main). Some of our implementation also comes from their repositories.

```
@inproceedings{kuhn2022semantic,
  title={Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation},
  author={Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
```

```
@article{lin2023generating,
  title={Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models},
  author={Lin, Zhen and Trivedi, Shubhendu and Sun, Jimeng},
  journal={arXiv preprint arXiv:2305.19187},
  year={2023}
}
```

### Citation

```
@article{hou2023decomposing,
  title={Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling},
  author={Hou, Bairu and Liu, Yujian and Qian, Kaizhi and Andreas, Jacob and Chang, Shiyu and Zhang, Yang},
  journal={arXiv preprint arXiv:2311.08718},
  year={2023}
}
```",DEV,,['hbr690188270'],,,1,0,,,,,,1,0.95
909298021,R_kgDONjLJZQ,Winter2025,UCSB-PSTAT-234/Winter2025,0,UCSB-PSTAT-234,https://github.com/UCSB-PSTAT-234/Winter2025,,0,2024-12-28 09:29:33+00:00,2025-03-06 23:48:57+00:00,2025-03-06 23:48:53+00:00,,482,5,5,Jupyter Notebook,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,5,public,1,1,# PSTAT 234: Statistical Data Science (Winter 2025),EDU,,['syoh'],,,1,0,,,,,,1,0.95
457124012,R_kgDOGz8orA,goldphish,ucsb-seclab/goldphish,0,ucsb-seclab,https://github.com/ucsb-seclab/goldphish,Arbitrage bot for the Ethereum blockchain,0,2022-02-08 22:24:40+00:00,2025-02-19 07:38:56+00:00,2023-10-04 22:22:22+00:00,,14810,66,66,Python,1,1,1,1,0,0,12,0,0,1,,1,0,0,public,12,1,66,main,1,1,"# Goldphish - historical ethereum arbitrage analysis

Goldphish is an arbitrage analyzer for the ethereum blockchain.


# Overview

The entrypoints into this project are largely broken into two parts:

1. Historical scrape of performed arbitrages -- found in `backtest/gather_samples`
2. Arbitrage seeking through history -- found in `backtest/top_of_block`

Models for the exchanges are in `pricers/`.

The transaction relayer smart contract is in `contracts/`.

The optimization procedure is in `find_circuit/find.py`.

# Building

This system is dockerized.
To build, run `docker build -t goldphish .`

# Setup

This system requires access to a postgresql database and a go-ethereum (geth) archive node with a websocket JSON-rpc server.
At the time of writing, an archive node consumes around 12 terabytes of space -- be warned!


## Postgresql

We run postgres dockerized. For convenience, we also run it on a separate docker network (so we can get dns-resolution).

To create the network:

```
docker network create ethereum-measurement-net
```

Pull the docker image,

```
docker pull postgres:14
```

Then spawn postgres. **NOTE: we are running with a weak password and open port (BAD!!!)**

Replace `YOUR_DATA_DIR_HERE` with the path to a directory that has at least 2T storage space, this is where your postgresql database files will live.

```
docker run \\
    -d \\
    --name ethereum-measurement-pg \\
    -e POSTGRES_PASSWORD=password \\
    -e POSTGRES_USER=measure \\
    -e POSTGRES_DB=eth_measure_db \\
    -p 0.0.0.0:5410:5432 \\
    -v YOUR_DATA_DIR_HERE:/var/lib/postgresql/data \\
    --network ethereum-measurement-net \\
    postgres:14
```

Let's configure it to allow more connections. Edit `YOUR_DATA_DIR_HERE/postgresql.conf` and change:

```diff
- max_connections = 100
+ max_connections = 1000
...
- shared_buffers = 128MB
+ shared_buffers = 512MB
```

And then restart the container:

```
docker container restart ethereum-measurement-pg
```



# Getting Started

## Storage dir setup

We need a directory to persist some information (and logs), which we will call `STORAGE_DIR`. It should have this structure:

```
.
logs/
tmp/
```

## Setup block samples

We parallelize work by chunks of blocks about 1 day long. Generate this table:

```bash
docker run \\
    --rm \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.top_of_block \\
    generate-sample
```

You should see logged `generated sample`.

## Scrape exchanges

You will need to scrape the list of Uniswap, Sushiswap, Shibaswap, and Balancer exchanges.

Get started by setting up the database:

```bash
docker run \\
    --rm \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    goldphish \\
    python3 -m backtest.gather_samples \\
    --setup-db
```

And then also here:

```bash
docker run \\
    --rm \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    goldphish \\
    python3 -m backtest.gather_samples.fill_known_exchanges \\
    --setup-db
```

And finally, run the scrape:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.gather_samples.fill_known_exchanges
```

You should see an ETA printed.

## Scrape ethereum price

This scrapes the USD price of ETH using either the the Chainlink oracle, or if an early block, the MakerDAO price oracle.

First, setup the db:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.gather_samples.fill_eth_price \\
    --setup-db
```

Then, run the scrape. Here we use N_WORKERS to represent the number of worker-processes you would like to use. We set it to 50.

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_many_fill_eth_price.sh \\
    $N_WORKERS
```

Finalize the work (abt 1min):

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.gather_samples.fill_eth_price \\
    --finalize
```

## Scrape arbitrages.

This takes a while! We chose 50 workers. Be sure that you increased your postgresql connections.

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_many_gather_samples.sh \\
    $N_WORKERS
```

You can watch the ETA here, in another bash session. This takes about 1-2 days.
If you would like to perform this on multiple machines, that is okay!
Get the docker container on the second (third, fourth...) machine and launch it similar to the command below.
Except, you'll want to set the environment variables PSQL_HOST and PSQL_PORT appropriately
(ie, your database machine's ip and port). Set these with `-e PSQL_HOST=xxx.xxx.xxx.xxx` etc.

Scrape work is processed in random order, so the ETA should be somewhat reliable.

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    goldphish \\
    python3 tmp_eta_gather.py
```

## Load flashbots transactions

This scrapes flashbots transaction from their server.

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.gather_samples.load_flashbots
```

We scrape linearly in time. Since activity is greater as time goes on, expect the ETA to grow as blocks per second slows. This is a common issue with linear scans.


It should take about 5 hours. This could probably be made faster by varying the http request batch sizes a bit smarter.

## Attribute exchanges to 0x

We need to figure out which exchanges were, in fact, 0x exchanges. This needs to be run twice, first for v3, then for v4.

First, v3 (should take about 30min - 1 hour):

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.gather_samples.fill_zerox --v3
```

Then, v4 (NOTE: This is parallelized, so a bit faster. we picked 50 workers.):

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_many_fill_zerox.sh \\
    $N_WORKERS
```

## Scrape direct-to-miner coinbase transfers

NOTE: This will take a LONG time!

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_fill_coinbase_xfer.sh \\
    $N_WORKERS
```

## Fill back-runners

Do transaction re-ordering to determine who was backrunning. First, setup db and fill the work queue:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.gather_samples.fill_backrunners --setup-db
```

Then, do the reordering. This is parallelized -- will take a while! You might want to spawn more workers on more machines. If you do that, be sure to specify the postgresql host/port as an environment variable (see section ""Scrape Arbitrages"").

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_many_backrunner_detect.sh \\
    $N_WORKERS
```

You can watch the ETA here:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    goldphish \\
    python3 tmp_eta_fill_backrunner.py
```

## Compute the table of false-positives

Setup some tables:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.gather_samples.fill_odd_token_xfers --setup-db
```

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.gather_samples.fill_odd_token_xfers --setup-db
```

Fill odd token transfer table (mistaken NFTs)

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_many_fill_odd_tokens.sh \\
    $N_WORKERS
```

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.gather_samples.fill_false_positive
```

## Compute naive gas-price oracle

First, set-up the database

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.gather_samples.fill_naive_gas_price --setup-db
```

Then, run the job (NOTE: this is parallelized).

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_many_gas_price_fill.sh \\
    $N_WORKERS
```

## Find arbitrages used in sandwich-attacks

First, set-up the database

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.gather_samples.fill_arb_sandwich --setup-db
```

Run the scrape:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_fill_arb_sandwich.sh \\
    $N_WORKERS
```

You can view the ETA here:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    goldphish \\
    python3 tmp_eta_fill_arb_sandwich.py
```

## Scrape for candidate arbitrages

THIS TAKES A LONG TIME!

Run the historical arbitrage opportunity search algorithm. You will likely need to have several hundred workers across several machines. We find that each worker will peg a CPU core -- ie, this is CPU-bound, so adding many more workers than CPU cores is not recommended.

About 500 workers should finish the job in about under 1 month.

Set up the job:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.top_of_block seek-candidates --setup-db
```

And run the job. Here we show explicitly how to set the postgresql host IP and port number. This is not necessary if you are on the same docker network as the database (then defaults work fine).
One can request cancellation of work by sending SIGHUP to the worker python processes, each worker should cleanly stop its current work unit and break off any remaining work into a new unit.

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    -e PSQL_PORT=5041 \\
    -e PSQL_HOST=XXX.XXX.XXX.XXX \\
    goldphish \\
    ./spawn_many_searchers.sh \\
    $N_WORKERS
```

You can view the ETA here. Note that some warm-up time is required before the ETA computation works.
During the start, the ETA will slowly rise, as the easier (early-history) blocks are processed.
ETA will also under-estimate time remaining toward the end of the run, as the parallelism decreases because no work-units remain.

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    goldphish \\
    python3 tmp_eta.py
```

## Fill table with top candidate arbitrages

Set up database:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.top_of_block fill-top-arbs --setup-db
```

Run the fill:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_many_fill_top_arbitrages.sh \\
    $N_WORKERS
```

## Execute arbitrages

This executes the candidate arbitrages, to check profitability. Takes quite a while!

This has two modes: 'all' and 'top arbitrages'. The 'all' mode runs arbitrages in order of decreasing priority. Priority was determined when generating block samples (a few steps back), and is a shuffle of the blockchain broken into day-long segments. This facilitates random sampling, presuming (and I do) that you do not have time to execute the entire thing. 'Top arbitrages' mode will relay all of the _large_ arbitrages.


Setup DB

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.top_of_block do-relay --setup-db
```

Run 'all'

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_many_relayers.sh \\
    $N_WORKERS
```

Watch the ETA at:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    goldphish \\
    python3 tmp_eta_relay.py
```

Run 'top arbitrages'. First, we need to fill this record of exchange modification history.

To do that, first set-up the db:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    python3 -m backtest.top_of_block do-arb-duration --setup-db
```

Then do the run:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_many_fill_modifications.sh \\
    $N_WORKERS
```

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    -e WEB3_HOST=ws://$GETH_NODE \\
    goldphish \\
    ./spawn_many_relay_top_arbs.sh \\
    $N_WORKERS
```

Watch the ETA at:

```bash
docker run \\
    --rm -t \\
    --network ethereum-measurement-net \\
    -v $STORAGE_DIR:/mnt/goldphish \\
    goldphish \\
    python3 tmp_eta_relay_top_arbs.py
```
",DEV,,['robmcl4'],,,1,0,,,,,,8,0.95
755265924,R_kgDOLQRxhA,AIUQ-MATLAB,UncertaintyQuantification/AIUQ-MATLAB,0,UncertaintyQuantification,https://github.com/UncertaintyQuantification/AIUQ-MATLAB,,0,2024-02-09 19:15:10+00:00,2024-08-27 12:26:12+00:00,2024-08-27 12:26:09+00:00,,24124,1,1,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,main,1,,"# AIUQ
This package is for scattering analysis of microscopy. The code has been tested in MATLAB version 2022b.

version 0.5.1.

Description:

The package allows users to simulate 2D movements of some particles from different anisotropic or isotropic stochastic processes and model the mean squared displacement (MSD) given a dataset using Gaussian processes. Parameter estimation is performed in a fast approach, making use of the Toeplitz structure of covariance matrices.  

References:
  1. M. Gu, Y. He, X. Liu, and Y. Luo, Ab initio uncertainty quantification in scattering analysis of microscopy, arXiv preprint arXiv:2309.02468
  2. Y. Ling, Superfast Inference for Stationary Gaussian Processes in Particle Tracking Microrheology, Ph.D. thesis, University of Waterloo (2019).
  3. Cerbino, R. and Trappe, V., 2008. Differential dynamic microscopy: probing wave vector dependent dynamics with a microscope. Physical review letters, 100(18), p.188102.

Installation:

To use this package, please install the optimization toolbox in MATLAB and install the FFTW3 library. For Windows users and Mac users whose system is based on  X86_64 architecture (usually before 2021), FFTW3 library can be linked easily, and for Mac users with ARM architecture (M chips) the procedure may be more complicated.

To install and link FFTW3 to your system, please refer to the FFTW_install_tutorial_2024.pdf


Contents:

example_anisotropic.m - Show anisotropic examples of some simulations and implement aniso_SAM() to real intensity data
aniso_simulation.m - Simulation code for different anisotropic stochastic processes
anisotropic_neg_log_lik - Compute the negative log-likelihood of the processed intensity data. 
anisotropic_log_lik_grad - Compute the derivative of negative log-likelihood w.r.t parameters. 
aniso_SAM - Parameter estimation 
example.m - Show isotropic examples of some simulations and implement aniso_SAM() to real intensity data
simulation.m - Simulation code for different isotropic stochastic processes
neg_log_lik - Compute the negative log-likelihood of the processed intensity data. 
log_lik_grad - Compute the derivative of negative log-likelihood w.r.t parameters. 
SAM - Parameter estimation 
cpp files: log_density, phi_log_det, toeplitz_prod, toeplitz_solve, toeplitz_trace_gradient, toeplitz_trace_hess

The main functions are aniso_simulation(), simulation() aniso_SAM() and SAM().

Authors:
Xubo Liu, Yue He, Mengyang Gu

Department of Statistics and Applied Probability, University of California, Santa Barbara

Email: mengyang@pstat.ucsb.edu, xubo@ucsb.edu, yuehe@ucsb.edu
",DEV,,"['UncertaintyQuantification', 'Xuboliu', 'yuehe95']",,,1,0,,,,,,1,0.95
384807694,MDEwOlJlcG9zaXRvcnkzODQ4MDc2OTQ=,Time-Sensitive-QA,wenhuchen/Time-Sensitive-QA,0,wenhuchen,https://github.com/wenhuchen/Time-Sensitive-QA,"Code and Data for NeurIPS2021 Paper ""A Dataset for Answering Time-Sensitive Questions""",0,2021-07-10 22:28:56+00:00,2025-01-09 23:05:53+00:00,2022-03-03 02:49:20+00:00,,705326,66,66,Jupyter Notebook,1,1,1,1,0,0,6,0,0,2,bsd-3-clause,1,0,0,public,6,2,66,main,1,,"# Time-Sensitive-QA
The repo contains the dataset and code for NeurIPS2021 (dataset track) paper [Time-Sensitive Question Answering dataset](https://arxiv.org/abs/2108.06314). The dataset is collected by UCSB NLP group and issued under BSD 3-Clause ""New"" or ""Revised"" License.

This dataset is aimed to study the existing reading comprehension models' capability to perform temporal reasoning, and see whether they are sensitive to the temporal description in the given question. An example of annotated question-answer pairs are listed as follows:
![overview](./intro.png)

## Repo Structure
- dataset/: this folder contains all the dataset
- dataset/annotated*: these files are the annotated (passage, time-evolving facts) by crowd-workers.
- dataset/train-dev-test: these files are synthesized using templates, including both easy and hard versions.
- BigBird/: all the running code for BigBird models
- FiD/: all the running code for fusion-in-decoder models

## Requirements
- [hydra 1.0.6](https://hydra.cc/docs/intro/)
- [omegaconf 2.1.0](https://github.com/omry/omegaconf)
1. BigBird-Specific Requirements
- [Transformers 4.8.2](https://github.com/huggingface/transformers)
- [Pytorch 1.8.1+cu102](https://pytorch.org/)
2. FiD-Specific Requirements
- [Transformers 3.0.2](https://github.com/huggingface/transformers)
- [Pytorch 1.6.0](https://pytorch.org/)

## BigBird
Extractive QA baseline model, first switch to the BigBird Conda environment:
### Initialize from NQ checkpoint
Running Training (Hard)
```
    python -m BigBird.main model_id=nq dataset=hard cuda=[DEVICE] mode=train per_gpu_train_batch_size=8
```

Running Evaluation (Hard)
```
    python -m BigBird.main model_id=nq dataset=hard cuda=[DEVICE] mode=eval model_path=[YOUR_MODEL]
```

### Initialize from TriviaQA checkpoint
Running Training (Hard)
```
    python -m BigBird.main model_id=triviaqa dataset=hard cuda=[DEVICE] mode=train per_gpu_train_batch_size=2
```

Running Evaluation (Hard)
```
    python -m BigBird.main model_id=triviaqa dataset=hard mode=eval cuda=[DEVICE] model_path=[YOUR_MODEL]
```

## Fusion-in Decoder
Generative QA baseline model, first switch to the FiD Conda environment and downaload the checkpoints from [Google Drive](https://drive.google.com/file/d/19DnItecTwqUqhw09zH3eR61iz_22dX-u/view?usp=sharing):
### Initialize from NQ checkpoint
Running Training (Hard)
```
    python -m FiD.main mode=train dataset=hard model_path=/data2/wenhu/Time-Sensitive-QA/FiD/pretrained_models/nq_reader_base/
```

Running Evaluation (Hard)
```
    python -m FiD.main mode=eval cuda=3 dataset=hard model_path=[YOUR_MODEL] 
```

Running Evalution on Human-Test (Hard)
```
    python -m FiD.main mode=eval cuda=3 dataset=human_hard model_path=[YOUR_MODEL] 
```

### Initialize from TriviaQA checkpoint
Running Training (Hard)
```
    python -m FiD.main mode=train dataset=hard model_path=/data2/wenhu/Time-Sensitive-QA/FiD/pretrained_models/tqa_reader_base/
```

Running Evaluation (Hard)
```
    python -m FiD.main mode=eval cuda=3 dataset=hard model_path=[YOUR_MODEL] 
```

Running Evalution on Human-Test (Hard)
```
    python -m FiD.main mode=eval cuda=3 dataset=human_hard model_path=[YOUR_MODEL] 
```

### Open-Domain Experiments
For build the retriever, I would refer you to https://github.com/wenhuchen/OTT-QA/tree/master/retriever, which is based on DrQA's TF-IDF/BM25 retriever implementation.

## License
The data and code are released under BSD 3-Clause ""New"" or ""Revised"" License.

## Report
Please create an issue or send an email to hustchenwenhu@gmail.com for any questions/bugs/etc.
",DATA,,"['wenhuchen', 'WANGXinyiLinda']",,,1,0,,,,,,3,0.95
709056550,R_kgDOKkNYJg,2324_TeamSingh_ERSP,wrcorcoran/2324_TeamSingh_ERSP,0,wrcorcoran,https://github.com/wrcorcoran/2324_TeamSingh_ERSP,"Research logs, notes, and links for Professor Singh's 2023-2024 ERSP project.",0,2023-10-23 23:27:56+00:00,2024-05-13 22:24:20+00:00,2024-05-13 22:24:16+00:00,,31171,1,1,Jupyter Notebook,1,1,1,1,0,0,1,0,0,0,mit,1,0,0,public,1,0,1,main,1,,"# Querying graphs and their representations

<!-- TABLE OF CONTENTS -->
<details open=""open"">
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href=""#about-the-project"">About The Project</a>
    </li>
    <li><a href=""#fall-summary"">Fall Summary</a></li>
    <li><a href=""#winter-summary"">Winter Summary</a></li>
    <li><a href=""#spring-summary"">Spring Summary</a></li>
    <li><a href=""#technology"">Technology</a></li>
    <li><a href=""#license"">License</a></li>
    <li><a href=""#contact"">Contact</a></li>
  </ol>
</details>

<!-- ABOUT THE PROJECT -->

## About The Project

- Graph neural networks transform the nodes of a graph into a high dimensional latent space.

- This project will contrast the distances between nodes of a graph in the input space (graph structure) to their embedding in the latent space.

- Queries of interest will be finding node/subgraph outliers, and comparing representations produced by different deep learning methods.

- Project can be extended to consider different ways of reducing distortions in embeddings and measuring the local dimensionality of the embedding space.

<!-- FALL SUMMARY -->
## Fall Summary
### [Research Proposal](/Fall2023/frameworks-and-solutions/assets/SinghProjectProposal-Final-Final.pdf)

### [Proposal Slides](/Fall2023/frameworks-and-solutions/assets/!FINALS$$LIDES_TEAMSINGH.pdf)

<!-- WINTER SUMMARY -->
## Winter Summary
### [Research Log](/Winter2024/ResearchLog.md)

<!-- SPRING SUMMARY -->
## Spring Summary
### [Research Log](/Spring2024/ResearchLog.md)

<!-- REPRODUCTION -->
## Technology
As we are investigating two different database softwares to build on top of, we have two different forks of those repositories. 

Deprecated:
- ArangoDB fork: [**arangodb_ersp**](https://github.com/wrcorcoran/arangodb_ersp)
- OpenCypher fork: [**openCypher_ersp**](https://github.com/wrcorcoran/openCypher_ersp)

Active:
- Minimum Edge Set Perturbation: [**minimum-edge-set-perturbation**](https://github.com/wrcorcoran/minimum-edge-set-perturbation)
  - Homophily: [**homophily**](https://github.com/wrcorcoran/minimum-edge-set-perturbation/tree/main/homophily)
  - Degree: [**degree**](https://github.com/wrcorcoran/minimum-edge-set-perturbation/tree/main/degree)
  - Common Neighbors: [**common-neighbors**](https://github.com/wrcorcoran/minimum-edge-set-perturbation/tree/main/common-neighbors)

## License

Distributed under the MIT License. See `LICENSE` for more information.

<!-- CONTACT -->

## Contact

Will Corcoran - wcorcoran@ucsb.edu

Wyatt Hamabe - whamabe@ucsb.edu

Niyati Mummidivarapu - niyati@ucsb.edu

Danish Ebadulla - danish_ebadulla (at) umail (dot) ucsb (dot) edu
",EDU,,"['wrcorcoran', 'Greathambino']",,,1,0,,,,,,1,0.95
192248904,MDEwOlJlcG9zaXRvcnkxOTIyNDg5MDQ=,Knowledge-Aware-Reader,xwhan/Knowledge-Aware-Reader,0,xwhan,https://github.com/xwhan/Knowledge-Aware-Reader,"PyTorch implementation of the ACL 2019 paper ""Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader""",0,2019-06-17 00:14:17+00:00,2025-02-07 15:59:48+00:00,2020-02-04 21:25:31+00:00,,619,139,139,Python,1,1,1,1,0,0,24,0,0,4,,1,0,0,public,24,4,139,master,1,,"Code for the ACL 2019 paper:

## Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader

Paper link: [https://arxiv.org/abs/1905.07098](https://arxiv.org/abs/1905.07098)

Model Overview:
<p align=""center""><img width=""90%"" src=""assets/model.png"" /></p>

### Requirements
* ``PyTorch 1.0.1``
* ``tensorboardX``
* ``tqdm``
* ``gluonnlp``

### Prepare data
```
mkdir datasets && cd datasets && wget https://sites.cs.ucsb.edu/~xwhan/datasets/webqsp.tar.gz && tar -xzvf webqsp.tar.gz && cd ..
```

### Full KB setting
```
CUDA_VISIBLE_DEVICES=0 python train.py --model_id KAReader_full_kb --max_num_neighbors 50 --label_smooth 0.1 --data_folder datasets/webqsp/full/ 
```

### Incomplete KB setting
Note: The Hits@1 should match or be slightly better than the number reported in the paper. More tuning on threshold should give you better F1 score. 
#### 30% KB
```
CUDA_VISIBLE_DEVICES=0 python train.py --model_id KAReader_kb_03 --max_num_neighbors 50 --use_doc --data_folder datasets/webqsp/kb_03/ --eps 0.05
```

#### 10% KB
```
CUDA_VISIBLE_DEVICES=0 python train.py --model_id KAReader_kb_01 --max_num_neighbors 50 --use_doc --data_folder datasets/webqsp/kb_01/ --eps 0.05
```
#### 50% KB
```
CUDA_VISIBLE_DEVICES=0 python train.py --model_id KAReader_kb_05 --num_layer 1 --max_num_neighbors 100 --use_doc --data_folder datasets/webqsp/kb_05/ --eps 0.05 --seed 3 --hidden_drop 0.05
```

### Citation
```
@inproceedings{xiong-etal-2019-improving,
    title = ""Improving Question Answering over Incomplete {KB}s with Knowledge-Aware Reader"",
    author = ""Xiong, Wenhan  and
      Yu, Mo  and
      Chang, Shiyu  and
      Guo, Xiaoxiao  and
      Wang, William Yang"",
    booktitle = ""Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"",
    month = jul,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/P19-1417"",
    doi = ""10.18653/v1/P19-1417"",
    pages = ""4258--4264"",
}
```
",DEV,RS,['xwhan'],,,1,0,,,,,,6,0.7
424733756,R_kgDOGVDsPA,UCSB_3D,yujnkm/UCSB_3D,0,yujnkm,https://github.com/yujnkm/UCSB_3D,"The project includes 3D models of three locations at the University of California, Santa Barbara.  ",0,2021-11-04 20:38:48+00:00,2022-10-05 21:30:06+00:00,2022-05-19 23:44:01+00:00,,459036,4,4,C#,1,1,1,1,0,0,1,0,0,0,mit,1,0,0,public,1,0,4,main,1,,"# UCSB_3D
<img
  src=""ucsb_logo.png?raw=true""
  alt=""Screenshot of Unity 2018 showing the inspector with Readme content.""
  width=""880""
  height=""70""
/>


The project includes 3D models of three locations at the University of California, Santa Barbara. Each physical environment was scanned using LiDAR and post-processed to run in Unity game engine software. 

1) Kirby Crossing [Google Map (34.41471, -119.84060)](https://www.google.com/maps/place/34%C2%B024'53.0%22N+119%C2%B050'26.2%22W)

2) Student Affairs [Google Map (34.41630,-119.84730)](https://www.google.com/maps/place/34%C2%B024'58.7%22N+119%C2%B050'50.3%22W)

3) Life Science [Google Map (34.41207,-119.84425)](https://www.google.com/maps/place/34%C2%B024'43.5%22N+119%C2%B050'37.9%22W)

All 3D models are optimized as Unity environments and run well on any target device.



# Getting Started
Follow these instructions to install the necessary software. These instructions have been tested on the Unity 2020.3.10f1 LTS but the project scenes should have no problem opening from different versions of Unity as the environment is stripdown to the model itself. 

* [Unity 2020.3.10f1 LTS](https://unity3d.com/unity/whats-new/2020.3.10)


Once installed, clone this repo and open the project in Unity or Unity Hub.

The main scene can be found under UCSB_3D > Assets > Scenes.


## 1. Kirby Crossing

* 34¬∞24'53.0""N 119¬∞50'26.2""W 
* [Google Map (34.41471, -119.84060)](https://www.google.com/maps/place/34%C2%B024'53.0%22N+119%C2%B050'26.2%22W) Same link lised above.
* Unity package file size: 258.962 KB

UCSB buildings near by: Elings Hall, California NanoSystems Institute (CNSI), Kavli Institute for Theoretical Physics (KITP), Materials Department


This model can be found under UCSB_3D > Assets > Scenes > KirbyCrossing.


<img
  src=""1_1.png?raw=true""
  alt=""3D model from Unity Scene.""
  width=""880""
  height=""450""
/>

<img
  src=""1_2.png?raw=true""
  alt=""FOV from the Unity Scene.""
  width=""880""
  height=""450""
/>

<img
  src=""1_3.png?raw=true""
  alt=""Map location shown in Google map.""
  width=""880""
  height=""450""
/>


## 2. Student Affairs

* 34¬∞24'58.7""N 119¬∞50'50.3""W
* [Google Map (34.41630,-119.84730)](https://www.google.com/maps/place/34%C2%B024'58.7%22N+119%C2%B050'50.3%22W) Same link lised above.
* Unity package file size: 37,895 KB

UCSB buildings near by: Administrative Services, Office of the Registrar, Office of Financial Aid and Scholarships, UCSB Student Information Systems & Technology


This model can be found under UCSB_3D > Assets > Scenes > StudentAffairs





<img
  src=""2_1.png?raw=true""
  alt=""How to export to Unity package.""
  width=""880""
  height=""450""
/>

<img
  src=""2_2.png?raw=true""
  alt=""FOV from the Unity Scene.""
  width=""880""
  height=""450""
/>

<img
  src=""2_3.png?raw=true""
  alt=""Map location shown in Google map.""
  width=""880""
  height=""450""
/>

## 3. Life Science

* 34¬∞24'43.5""N 119¬∞50'37.9""W
* [Google Map (34.41207,-119.84425)](https://www.google.com/maps/place/34%C2%B024'43.5%22N+119%C2%B050'37.9%22W) Same link lised above.
* Unity package file size: 97,896 KB

UCSB buildings near by: Nobel Hall, Department of Molecular, Cellular & Developmental Biology, Biological Sciences Instructional Facility


This model can be found under UCSB_3D > Assets > Scenes > NobelHall



<img
  src=""3_1.png?raw=true""
  alt=""How to export to Unity package.""
  width=""880""
  height=""450""
/>

<img
  src=""3_2.png?raw=true""
  alt=""FOV from the Unity Scene.""
  width=""880""
  height=""450""
/>

<img
  src=""3_3.png?raw=true""
  alt=""Map location shown in Google map.""
  width=""880""
  height=""450""
/>

## Exporting Packages
All three models can be found under (UCSB_3D > Assets > Scenes) the Scenes folder. Each Unity scene contains each model. You can proceed to gererate standalone Unity package by  right clicking the scene from the Unity editor‚Äôs menus and select ***Export Package...***

<img
  src=""4_1.png?raw=true""
  alt=""How to export to Unity package.""
  width=""880""
  height=""450""
/>



## License & Citation
This repository contains models we constructed with a lot of care and effort. If you use the 3D model in your research, please cite us using ***Cite this repository*** found in the repository landing page.

<img
  src=""5_1.png?raw=true""
  alt=""How to cite our work.""
  width=""880""
  height=""400""
/>

Kim, Y., & Kumaran, R. (2021). UCSB_3D Campus (Version 1.0.1) [Computer software]. available at: https://github.com/yujnkm/UCSB_3D (accessed 19 May 2022).
https://doi.org/10.5281/zenodo.6565136

Read the [license](LICENSE.md).
",DATA,,['yujnkm'],,,1,0,,,,,,1,0.95
540871092,R_kgDOID0JtA,Computer-Graphics-Course,zkcr0000/Computer-Graphics-Course,0,zkcr0000,https://github.com/zkcr0000/Computer-Graphics-Course,Computer graphics course assignment https://sites.cs.ucsb.edu/~lingqi/teaching/games101.html,0,2022-09-24 15:14:53+00:00,2022-09-24 15:21:50+00:00,2022-09-24 18:59:53+00:00,,4342,0,0,C++,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,,"# Computer-Graphics-Course
This repository is based on a Computer graphics course assignments https://sites.cs.ucsb.edu/~lingqi/teaching/games101.html

It contains C++ implmentations of rasterizer, raytracing, etc. Results are shown below.

# Setup

Required libraries

- CMake
- Eigen
- OpenCV
- stb_image
- OBJ Loader

# Results

## MP1 Viewing Transformation

- Model/View transformation, 
- Perspective/Orthogonal projection, 
- Viewport transformation, 
- Bresenham's line drawing algorithm

<img src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/HW1.gif"" width=""250"" height=""250""/>

## MP2 Rasterizer

Implemented a rasterizer with the following features.

- Depth buffer
- Anti-aliasing(supersampling anti-aliasing(SSAA), multisample anti-aliasing(MSAA))

Left is the result without Anti-aliasing. Middle is the result with MSAA. Right is the result with SSAA.
After zooming in, we can see anti-aliasing has reduced the effect of zigzag at the boundary.

<img alt=""Without Anti-alising"" src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/Vanilla.png"" width=""250"" height=""250""/><img alt=""MSAA"" src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/MSAA.png"" width=""250"" height=""250""/><img alt=""SSAA"" src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/SSAA.png"" width=""250"" height=""250""/>

<img alt=""Without Anti-alising"" src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/Vanilla_zoomin.png"" width=""250"" height=""250""/><img alt=""MSAA"" src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/MSAA_zoomin.png"" width=""250"" height=""250""/><img alt=""SSAA"" src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/SSAA_zoomin.png"" width=""250"" height=""250""/>

## MP3 Shading model

Implemented the following shading methods.

- Blinn‚ÄìPhong reflection model
- Bump mapping
- Displacement mapping
- Texture

From left to right: Blinn-Phong reflection model, Bump mapping, Displacement mapping, Texture

<img src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/phong.png"" width=""250"" height=""250""/><img src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/bump.png"" width=""250"" height=""250""/><img src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/displacement.png"" width=""250"" height=""250""/><img src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/texture.png"" width=""250"" height=""250""/>


## MP4 B√©zier curve

Implemented de Casteljau's algorithm.

<img src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/HW4.gif"" width=""250"" height=""250""/>

## MP5 Whitted-Style Ray tracing

Implemented Whitted-Style Ray tracing with reflection and diffraction.

<img src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/HW5.jpg"" width=""240"" height=""180""/>

## MP6 Ray tracing with Bounded Volume Hierarchy(BVH)

- Ray tracing for Stanford Bunny
- Implemented BVH to accelerate ray tracing
- Implemented ray intersecting with Axis Aligned Bounding Box(AABB)

<img src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/HW6.jpg"" width=""240"" height=""180""/>

## MP7 Path tracing

Implemented path tracing.

- Path tracing for Cornell Box
- Rendering equation
- bidirectional reflectance distribution function (BRDF)
- Monte Carlo Integration

<img src=""https://github.com/zkcr0000/Computer-Graphics-Course/blob/main/Supplementary/HW7.jpg"" width=""250"" height=""250""/>

# Compilation

Inside each MP folder

```
mkdir build
cmake ..
make
./Rasterization # Or ./Raytracing
```

",EDU,,['zkcr0000'],,,1,0,,,,,,1,0.8
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,48,,,,,,,,,,,,