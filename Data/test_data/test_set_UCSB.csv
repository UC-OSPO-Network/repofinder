id,node_id,name,full_name,private,owner,html_url,description,fork,created_at,updated_at,pushed_at,homepage,size,stargazers_count,watchers_count,language,has_issues,has_projects,has_downloads,has_wiki,has_pages,has_discussions,forks_count,archived,disabled,open_issues_count,license,allow_forking,is_template,web_commit_signoff_required,visibility,forks,open_issues,watchers,default_branch,score,organization,readme,contributors,manual_label,prediction_nn,prediction,release_downloads,code_of_conduct,contributing,security_policy,issue_templates,pull_request_template,subscribers_count,,,
71583602,MDEwOlJlcG9zaXRvcnk3MTU4MzYwMg==,cs-video-courses,Developer-Y/cs-video-courses,0,Developer-Y,https://github.com/Developer-Y/cs-video-courses,List of Computer Science courses with video lectures.,0,2016-10-21 17:02:11+00:00,2025-03-07 05:33:54+00:00,2025-03-07 05:33:37+00:00,,840,68252,68252,,1,1,1,1,0,0,9233,0,0,2,,1,0,0,public,9233,2,68252,master,1,,,"['Developer-Y', 'butter1125', 'DateBro', 'PeskyPotato', 'Alaharon123', 'Hyle33ies', 'inf3cti0n95', 'maxprogrammer007', 'mundher', 'bivashpandey', 'bhuthesh', 'MasoudKaviani', 'anishathalye', 'spekulatius', 'solomonbstoner', 'Suraj7879', 'm229abd', 'qkenn', 'tentena', 'unfode', 'P7h', 'R-Mahmoudi', 'richwill28', 'RohitSgh', 'sashedher', 'JZZQuant', 'ShashankP19', 'Ghost93', 'shreyasdeotare', '0xsomnus', 'trisolaris233', 'ProgrammingPete', 'ppisa', 'paulosalvatore', 'pvcraven', 'guybrush', 'Omar-Yasser', 'osyvokon', 'nishanths', 'Nishank-25', 'emperor-jimmu', 'rishabhb-git', 'phscloq', 'love25nov', 'krnets', 'eternalfool', 'elarabyelaidy19', 'sootysec', 'ayoubazaouyat', 'yugborana', 'tan-i-ham', 'awxiaoxian2020', 'wiiskii', 'wcrasta', 'Utayaki', 'hxt365', 'taylorty', 'tejasurya', 'Sushants-Git', 'dripdropdr', 'soum-c', 'norswap', 'hridaydutta123', 'gusnaughton', 'GianAndreaSechi', 'GabrielRogd', 'floe', 'FabienTregan', 'danielrbradley', 'Chirag-Bansal', 'alebcay', 'subarudad', 'bbhart', 'lamberta', 'BikramHalder', 'BemwaMalak', 'ayushpandey830', 'aleichtm', 'Atcold', 'cage433', 'wp-lai', 'ei-blue', 'naeemshaikh90', 'htarsoo', 'm4salah', 'mperreux', 'mterwill', 'MaaniGhaffari', 'miangraham', 'laithshadeed', '0x42697262', 'kairat-beep', 'joel-porquet', 'JVKdouk', 'jeffin07', 'jeffheaton', 'jeevan0920', 'xtangle', 'tientaidev', 'IureRosa', 'eltociear', 'hrithik254']",0,50,0.63,0,,"# Contribution Guidelines

- Recently the quality of MOOCs has diminished, therefore only MOOCs with comprehensive lecture material which cover a subject/topic in ample detail will be added. For example, MOOC on Computer Networks or Machine Learning with 4-5 hours may not be able to cover all topics in sufficient detail and thus should be avoided.
- One philosophy used in this list while integrating MOOCs is that links should directly point to videos for viewing/downloading than registration and waiting for the next session. If videos are directly accessible through the platform/youtube or any other source, please use the direct source. This is a list of video courses, not a list of MOOCs.
- Courses within a section are roughly sorted in terms of level i.e. undergraduate courses followed by upper-level undergraduate, followed by graduate courses. As courses are from multiple Universities, sorting is not perfect and only an approximation. For example, while adding a new undergraduate course on Algorithms, please feel free to add it along with other Algorithms courses than after graduate courses.
",,,,1864,,UCSBarchlab,/ucsb/
26689598,MDEwOlJlcG9zaXRvcnkyNjY4OTU5OA==,awesome-courses,prakhar1989/awesome-courses,0,prakhar1989,https://github.com/prakhar1989/awesome-courses,:books: List of awesome university courses for learning Computer Science!,0,2014-11-15 18:36:49+00:00,2025-03-07 05:49:31+00:00,2023-05-04 12:49:34+00:00,,1401,59278,59278,,1,1,1,0,1,0,8187,0,0,56,,1,0,0,public,8187,56,59278,master,1,,,"['prakhar1989', 'awelch83', 'robbyoconnor', 'ivmarkp', 'spekulatius', 'themattchan', 'arne-cl', 'awesome-bot', 'chaorrupted', 'BhavdeepSethi', 'royels', 'samarjeet', 'Ghost93', 'ee08b397', 'trentbradley', 'anancds', 'szdmr', 'lingawakad', 'cdipaolo', 'mrdrozdov', 'yangzgnay', 'swong15', 'harryge00', 'liquidmetal', 'tokeryberg', 'seanli310', 'praveenvvstgy', 'philipmjohnson', 'smihir', 'Techfolio', 'jasonsbarr', 'DanGe42', 'colegleason', 'agungsantoso', 'abhy-kumar', 'sumitbhanushali', 'tgaeta', 'congchan', 'tuvtran', 'VictorVation', 'vkoul', 'vshan', 'Sarin1204', 'vivekpatle', 'Voluong', 'macwu1992', 'yogeshpaul', 'shawnLeeZX', 'shinsaku417', 'samyak-268', 'roger-kang-mo', 'rockytang', 'lambainsaan', 'Rethabile02', 'mrahul17', 'tanwanirahul', 'MicBrain', 'jwworth', 'yohabe', 'woodstok', 'wangbinyq', 'soupi', 'souptikji', 'sureyeaah', 'sharihunt', 'sairampola', 'diddierh', 'rjmacaranas', 'pathway27', 'ohlay', 'mrmiywj', 'miwojc', 'matt-auld', 'jeremieSimon', 'Giraj', 'fordaz', 'erichoco', 'crongray', 'cedarz', 'goalong', 'tech-cow', 'JMatharu', 'Hardmath123', 'haochuan', 'GordyD', 'Frederick-S', 'frankjwu', 'zuik', 'Developer-Y', 'daviddao', 'DateBro', 'danoneata', 'schmich', 'sup', 'bcbcb', 'ArthurMG', 'tartakynov', 'yurovant', 'aniruddh-a', 'koksal', 'aparij', 'akashks1998', 'sch', 'pyliaorachel', 'Pratyush', 'reikdas', 'PiyushDeshmukh', 'ilhan-mstf', 'mortonfox', 'mx', 'kmollee', '0rps', 'mikeliturbe', 'mquinson', 'rabas', 'marksweiss', 'materechm', 'maggiewhite', 'schneiderl', 'koallen', 'liam-middlebrook', 'bawejakunal', 'quobit', 'jonchu', 'joelowj', 'imjching']",0,50,0.62,0,,"### Contributing

Have a few courses in mind that you think are awesome and would fit in this list? Feel free to send a [pull request](https://github.com/prakhar1989/awesome-courses/pulls). However, do note that I'm not keen on adding popular courses (such as MOOCs / MIT OCW) as there are services like [ClassCentral](https://www.class-central.com/) doing a great job of aggregation. This list is ideally for courses which are relatively unknown and make their material (assignments, lectures, exams etc.) available online for free.

### A couple of things to keep in mind:

- When adding new courses to the various categories, please ensure that alphabetization (ex. CSCE 48 should be listed after CS 240) is maintained. This makes it easier for viewers to find what they are looking for.
- Next to each course title, you'll notice a few icons. These icons serve to let readers know, at a quick glance, what materials they can expect to find at the courses' websites. The meanings of these icons are explained at the top of [README.md](https://github.com/prakhar1989/awesome-courses/blob/master/README.md), and also appear as a tooltip whenever you hover over them with your mouse arrow. The code needed to add these tags is easily found in [README.md](https://github.com/prakhar1989/awesome-courses/blob/master/README.md), so you can just copy and paste as needed.",,,,2294,,ucsb-seclab,library-ucsb
29357796,MDEwOlJlcG9zaXRvcnkyOTM1Nzc5Ng==,awesome-computer-vision,jbhuang0604/awesome-computer-vision,0,jbhuang0604,https://github.com/jbhuang0604/awesome-computer-vision,A curated list of awesome computer vision resources,0,2015-01-16 16:48:47+00:00,2025-03-07 03:47:58+00:00,2024-05-17 14:24:27+00:00,,547,21397,21397,,1,1,1,1,0,0,4275,0,0,85,,1,0,0,public,4275,85,21397,master,1,,,"['jbhuang0604', 'davidstutz', 'lvzhaoyang', 'alasin', 'y12uc231', 'hsiaoyi0504', 'pmoulon', 'huangzehao', 'anujonthemove', 'phoenix104104', 'jf-parent', 'jingliu9', 'joelgallant', 'matt77hias', 'onurtemizkan', 'pkraison', 'ReadmeCritic', 'shubham-shahh', 'subeeshvasu', 'thaoshibe', 'tigelbri', 'liquidmetal', 'vinjn', 'grapeot', 'yotam', 'yukimasano', 'dk-liang', 'stoksc', 'tstanislawek', 'vincentzhang']",0,,0.71,0,,,,,,1177,,LuaAV,
35313712,MDEwOlJlcG9zaXRvcnkzNTMxMzcxMg==,awesome-malware-analysis,rshipp/awesome-malware-analysis,0,rshipp,https://github.com/rshipp/awesome-malware-analysis,Defund the Police.,0,2015-05-09 03:39:28+00:00,2025-03-07 03:29:15+00:00,2024-06-07 05:09:47+00:00,https://gazafunds.com/,659,12317,12317,,1,0,1,0,0,0,2591,0,0,24,other,1,0,0,public,2591,24,12317,main,1,,,"['rshipp', 'hslatman', 'PolluxAvenger', 'elhoim', 'knowmalware', 'keithjjones', 'ReadmeCritic', 'zbetcheckin', 'ThomasThelen', 'uppusaikiran', 'alexcpsec', 'mikesxrs', 'petruisfan', 'PatrikHudak', 'LiamRandall', 'Chan9390', 'cugu', 'magicansk', 'pe3zx', 'cccs-kevin', 'miqueet', 'RamadhanAmizudin', 'Pr0teus', 'osospeed', '1ultimat3', 'ThatLing', 'flautossec', 'jandersoncampelo', 'Granet', 'phretor', 'diogo-fernan', 'ktsaou', 'ch3k1', 'andrewgarcia808', 'mcm', 'vxsh4d0w', 'yfdyh000', 'zachad', 'baafuor', 'blacktop', 'bwhitn', 'sust4in', 'yunginnanet', 'techvoltage', 'Masrepus', 'PolarBearGod', 's3rvac', 'spekulatius', '2E0PGS', 'pedramamini', 'Dovgalyuk', 'nbeede', 'swwwolf', 'astonge', 'sigmaapex', 'scherma', 'rahulsangwn', 'ocean1', 'nop0x0f', 'netbroom', 'mnakamura1337', 'KernelPan1k', 'lubiedo', 'K2', 'jingleyang', 'foospidy', 'filinpavel', 'edsalmin', 'eschultze', 'xor3r', 'HynekPetrak', 'horsicq', 'grleblanc', 'ghoeffner', 'FR3DHK', 'Drewsif', 'davidonzo', 'dcdelia', 'danigoland', 'compsecmonkey', 'CapacitorSet', 'BlazerYoo', 'bartblaze', 'Karneades', 'adepasquale', 'sh4hin', 'alimalkhalifa', 'dhondta', 'fabacab', 'meirwah', 'mlaferrera', 'gothicx', 'progressionnetwork', '18z', 'jbremer', 'jossef', 'Jabhatt', 'jatrost', 'miller-itsec', 'janesmae', 'JusticeRage', 'ivg', 'ItaiTevet', 'ikoniaris', 'IgorSasovets', 'NoThrowForwardIt']",0,,0.67,0,,"# Contribution Guidelines

When making a pull request, please follow these guidelines:

- One commit per suggestion is preferred
- Commit message should follow this format: `Add some tool name` (For
  example, `Add cuckoo-sandbox`) [Why?](http://chris.beams.io/posts/git-commit/)
- Multiple commits per pull request is OK
- Lists within each section are alphabetized, please keep them that way
- Add sections if necessary, use existing sections if possible
- Clear, concise descriptions for each link, starting with a capital, ending
  with a period
- Use the following format: `[Item Name](homepage link) - Description.`
- No duplication of tools, put them where they make the most sense
- Wrap lines at ~80 chars, no trailing whitespace or unnecessary newlines
- Prefer quality over quantity, only submit awesome stuff
- By submitting a pull request, you agree to release your submission under
  the [LICENSE](LICENSE)
- Indent wrapped lines even with the start of the line before

```
- That means lines wrap like
  this
- Not
like this
```


The rules above take precedence, but in case I missed something, check [the
awesome guidelines](https://github.com/sindresorhus/awesome/blob/master/contributing.md)
too.

Properly formatted pull requests will almost always be approved faster than
issues or poorly formatted pull requests, because they mean less work for me!

Thanks!
",,,,700,,SEAL-UCSB,
35812923,MDEwOlJlcG9zaXRvcnkzNTgxMjkyMw==,awesome-deep-vision,kjw0612/awesome-deep-vision,0,kjw0612,https://github.com/kjw0612/awesome-deep-vision,A curated list of deep learning resources for computer vision,0,2015-05-18 11:02:08+00:00,2025-03-07 03:33:40+00:00,2023-08-15 10:52:15+00:00,,326,10899,10899,,1,1,1,1,1,0,2779,0,0,45,,1,0,0,public,2779,45,10899,master,1,,"# Awesome Deep Vision [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of deep learning resources for computer vision, inspired by [awesome-php](https://github.com/ziadoz/awesome-php) and [awesome-computer-vision](https://github.com/jbhuang0604/awesome-computer-vision).

Maintainers - [Jiwon Kim](https://github.com/kjw0612), [Heesoo Myeong](https://github.com/hmyeong), [Myungsub Choi](https://github.com/myungsub), [Jung Kwon Lee](https://github.com/deruci), [Taeksoo Kim](https://github.com/jazzsaxmafia)

The project is not actively maintained. 

## Contributing
Please feel free to [pull requests](https://github.com/kjw0612/awesome-deep-vision/pulls) to add papers.

[![Join the chat at https://gitter.im/kjw0612/awesome-deep-vision](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/kjw0612/awesome-deep-vision?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

## Sharing
+ [Share on Twitter](http://twitter.com/home?status=http://jiwonkim.org/awesome-deep-vision%0ADeep Learning Resources for Computer Vision)
+ [Share on Facebook](http://www.facebook.com/sharer/sharer.php?u=https://jiwonkim.org/awesome-deep-vision)
+ [Share on Google Plus](http://plus.google.com/share?url=https://jiwonkim.org/awesome-deep-vision)
+ [Share on LinkedIn](http://www.linkedin.com/shareArticle?mini=true&url=https://jiwonkim.org/awesome-deep-vision&title=Awesome%20Deep%20Vision&summary=&source=)

## Table of Contents
- [Papers](#papers)
  - [ImageNet Classification](#imagenet-classification)
  - [Object Detection](#object-detection)
  - [Object Tracking](#object-tracking)
  - [Low-Level Vision](#low-level-vision)
    - [Super-Resolution](#super-resolution)
    - [Other Applications](#other-applications)
  - [Edge Detection](#edge-detection)
  - [Semantic Segmentation](#semantic-segmentation)
  - [Visual Attention and Saliency](#visual-attention-and-saliency)
  - [Object Recognition](#object-recognition)
  - [Human Pose Estimation](#human-pose-estimation)
  - [Understanding CNN](#understanding-cnn)
  - [Image and Language](#image-and-language)
    - [Image Captioning](#image-captioning)
    - [Video Captioning](#video-captioning)
    - [Question Answering](#question-answering)
  - [Image Generation](#image-generation)
  - [Other Topics](#other-topics)
- [Courses](#courses)
- [Books](#books)
- [Videos](#videos)
- [Software](#software)
  - [Framework](#framework)
  - [Applications](#applications)
- [Tutorials](#tutorials)
- [Blogs](#blogs)

## Papers

### ImageNet Classification
![classification](https://cloud.githubusercontent.com/assets/5226447/8451949/327b9566-2022-11e5-8b34-53b4a64c13ad.PNG)
(from Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.)
* Microsoft (Deep Residual Learning) [[Paper](http://arxiv.org/pdf/1512.03385v1.pdf)][[Slide](http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf)]
  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition, arXiv:1512.03385.
* Microsoft (PReLu/Weight Initialization) [[Paper]](http://arxiv.org/pdf/1502.01852)
  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, arXiv:1502.01852.
* Batch Normalization [[Paper]](http://arxiv.org/pdf/1502.03167)
  * Sergey Ioffe, Christian Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, arXiv:1502.03167.
* GoogLeNet [[Paper]](http://arxiv.org/pdf/1409.4842)
  * Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, CVPR, 2015.
* VGG-Net [[Web]](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) [[Paper]](http://arxiv.org/pdf/1409.1556)
  * Karen Simonyan and Andrew Zisserman, Very Deep Convolutional Networks for Large-Scale Visual Recognition, ICLR, 2015.
* AlexNet [[Paper]](http://papers.nips.cc/book/advances-in-neural-information-processing-systems-25-2012)
  * Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.

### Object Detection
![object_detection](https://cloud.githubusercontent.com/assets/5226447/8452063/f76ba500-2022-11e5-8db1-2cd5d490e3b3.PNG)
(from Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv:1506.01497.)

* PVANET [[Paper]](https://arxiv.org/pdf/1608.08021) [[Code]](https://github.com/sanghoon/pva-faster-rcnn)
  * Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje Park, PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection, arXiv:1608.08021
* OverFeat, NYU [[Paper]](http://arxiv.org/pdf/1312.6229.pdf)
  * OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks, ICLR, 2014.
* R-CNN, UC Berkeley [[Paper-CVPR14]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) [[Paper-arXiv14]](http://arxiv.org/pdf/1311.2524)
  * Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Rich feature hierarchies for accurate object detection and semantic segmentation, CVPR, 2014.
* SPP, Microsoft Research [[Paper]](http://arxiv.org/pdf/1406.4729)
  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition, ECCV, 2014.
* Fast R-CNN, Microsoft Research [[Paper]](http://arxiv.org/pdf/1504.08083)
  * Ross Girshick, Fast R-CNN, arXiv:1504.08083.
* Faster R-CNN, Microsoft Research [[Paper]](http://arxiv.org/pdf/1506.01497)
  * Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv:1506.01497.
* R-CNN minus R, Oxford [[Paper]](http://arxiv.org/pdf/1506.06981)
  * Karel Lenc, Andrea Vedaldi, R-CNN minus R, arXiv:1506.06981.
* End-to-end people detection in crowded scenes [[Paper]](http://arxiv.org/abs/1506.04878)
  * Russell Stewart, Mykhaylo Andriluka, End-to-end people detection in crowded scenes, arXiv:1506.04878.
* You Only Look Once: Unified, Real-Time Object Detection [[Paper]](http://arxiv.org/abs/1506.02640), [[Paper Version 2]](https://arxiv.org/abs/1612.08242), [[C Code]](https://github.com/pjreddie/darknet), [[Tensorflow Code]](https://github.com/thtrieu/darkflow)
  * Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, You Only Look Once: Unified, Real-Time Object Detection, arXiv:1506.02640
  * Joseph Redmon, Ali Farhadi (Version 2)
* Inside-Outside Net [[Paper]](http://arxiv.org/abs/1512.04143)
  * Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick, Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks
* Deep Residual Network (Current State-of-the-Art) [[Paper]](http://arxiv.org/abs/1512.03385)
  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition
* Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning [[Paper](http://arxiv.org/pdf/1503.00949.pdf)]
* R-FCN [[Paper]](https://arxiv.org/abs/1605.06409) [[Code]](https://github.com/daijifeng001/R-FCN)
  * Jifeng Dai, Yi Li, Kaiming He, Jian Sun, R-FCN: Object Detection via Region-based Fully Convolutional Networks
* SSD [[Paper]](https://arxiv.org/pdf/1512.02325v2.pdf) [[Code]](https://github.com/weiliu89/caffe/tree/ssd)
  * Wei Liu1, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, SSD: Single Shot MultiBox Detector, arXiv:1512.02325
* Speed/accuracy trade-offs for modern convolutional object detectors [[Paper]](https://arxiv.org/pdf/1611.10012v1.pdf)
  * Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy, Google Research, arXiv:1611.10012

### Video Classification
* Nicolas Ballas, Li Yao, Pal Chris, Aaron Courville, ""Delving Deeper into Convolutional Networks for Learning Video Representations"", ICLR 2016. [[Paper](http://arxiv.org/pdf/1511.06432v4.pdf)]
* Michael Mathieu, camille couprie, Yann Lecun, ""Deep Multi Scale Video Prediction Beyond Mean Square Error"", ICLR 2016. [[Paper](http://arxiv.org/pdf/1511.05440v6.pdf)]

### Object Tracking
* Seunghoon Hong, Tackgeun You, Suha Kwak, Bohyung Han, Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network, arXiv:1502.06796. [[Paper]](http://arxiv.org/pdf/1502.06796)
* Hanxi Li, Yi Li and Fatih Porikli, DeepTrack: Learning Discriminative Feature Representations by Convolutional Neural Networks for Visual Tracking, BMVC, 2014. [[Paper]](http://www.bmva.org/bmvc/2014/files/paper028.pdf)
* N Wang, DY Yeung, Learning a Deep Compact Image Representation for Visual Tracking, NIPS, 2013. [[Paper]](http://winsty.net/papers/dlt.pdf)
* Chao Ma, Jia-Bin Huang, Xiaokang Yang and Ming-Hsuan Yang, Hierarchical Convolutional Features for Visual Tracking, ICCV 2015 [[Paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Ma_Hierarchical_Convolutional_Features_ICCV_2015_paper.pdf)] [[Code](https://github.com/jbhuang0604/CF2)]
* Lijun Wang, Wanli Ouyang, Xiaogang Wang, and Huchuan Lu, Visual Tracking with fully Convolutional Networks, ICCV 2015  [[Paper](http://202.118.75.4/lu/Paper/ICCV2015/iccv15_lijun.pdf)] [[Code](https://github.com/scott89/FCNT)]
* Hyeonseob Namand Bohyung Han, Learning Multi-Domain Convolutional Neural Networks for Visual Tracking, [[Paper](http://arxiv.org/pdf/1510.07945.pdf)] [[Code](https://github.com/HyeonseobNam/MDNet)] [[Project Page](http://cvlab.postech.ac.kr/research/mdnet/)]

### Low-Level Vision

#### Super-Resolution
* Iterative Image Reconstruction
  * Sven Behnke: Learning Iterative Image Reconstruction. IJCAI, 2001. [[Paper]](http://www.ais.uni-bonn.de/behnke/papers/ijcai01.pdf)
  * Sven Behnke: Learning Iterative Image Reconstruction in the Neural Abstraction Pyramid. International Journal of Computational Intelligence and Applications, vol. 1, no. 4, pp. 427-438, 2001. [[Paper]](http://www.ais.uni-bonn.de/behnke/papers/ijcia01.pdf)
* Super-Resolution (SRCNN) [[Web]](http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html) [[Paper-ECCV14]](http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepresolution.pdf) [[Paper-arXiv15]](http://arxiv.org/pdf/1501.00092.pdf)
  * Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Learning a Deep Convolutional Network for Image Super-Resolution, ECCV, 2014.
  * Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Image Super-Resolution Using Deep Convolutional Networks, arXiv:1501.00092.
* Very Deep Super-Resolution
  * Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Accurate Image Super-Resolution Using Very Deep Convolutional Networks, arXiv:1511.04587, 2015. [[Paper]](http://arxiv.org/abs/1511.04587)
* Deeply-Recursive Convolutional Network
  * Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Deeply-Recursive Convolutional Network for Image Super-Resolution, arXiv:1511.04491, 2015. [[Paper]](http://arxiv.org/abs/1511.04491)
* Casade-Sparse-Coding-Network
  * Zhaowen Wang, Ding Liu, Wei Han, Jianchao Yang and Thomas S. Huang, Deep Networks for Image Super-Resolution with Sparse Prior. ICCV, 2015. [[Paper]](http://www.ifp.illinois.edu/~dingliu2/iccv15/iccv15.pdf) [[Code]](http://www.ifp.illinois.edu/~dingliu2/iccv15/)
* Perceptual Losses for Super-Resolution
  * Justin Johnson, Alexandre Alahi, Li Fei-Fei, Perceptual Losses for Real-Time Style Transfer and Super-Resolution, arXiv:1603.08155, 2016. [[Paper]](http://arxiv.org/abs/1603.08155) [[Supplementary]](http://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf)
* SRGAN
  * Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi, Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, arXiv:1609.04802v3, 2016. [[Paper]](https://arxiv.org/pdf/1609.04802v3.pdf)
* Others
  * Osendorfer, Christian, Hubert Soyer, and Patrick van der Smagt, Image Super-Resolution with Fast Approximate Convolutional Sparse Coding, ICONIP, 2014. [[Paper ICONIP-2014]](http://brml.org/uploads/tx_sibibtex/281.pdf)

#### Other Applications
* Optical Flow (FlowNet) [[Paper]](http://arxiv.org/pdf/1504.06852)
  * Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip Häusser, Caner Hazırbaş, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox, FlowNet: Learning Optical Flow with Convolutional Networks, arXiv:1504.06852.
* Compression Artifacts Reduction [[Paper-arXiv15]](http://arxiv.org/pdf/1504.06993)
  * Chao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang, Compression Artifacts Reduction by a Deep Convolutional Network, arXiv:1504.06993.
* Blur Removal
  * Christian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard Schölkopf, Learning to Deblur, arXiv:1406.7444 [[Paper]](http://arxiv.org/pdf/1406.7444.pdf)
  * Jian Sun, Wenfei Cao, Zongben Xu, Jean Ponce, Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal, CVPR, 2015 [[Paper]](http://arxiv.org/pdf/1503.00593)
* Image Deconvolution [[Web]](http://lxu.me/projects/dcnn/) [[Paper]](http://lxu.me/mypapers/dcnn_nips14.pdf)
  * Li Xu, Jimmy SJ. Ren, Ce Liu, Jiaya Jia, Deep Convolutional Neural Network for Image Deconvolution, NIPS, 2014.
* Deep Edge-Aware Filter [[Paper]](http://jmlr.org/proceedings/papers/v37/xub15.pdf)
  * Li Xu, Jimmy SJ. Ren, Qiong Yan, Renjie Liao, Jiaya Jia, Deep Edge-Aware Filters, ICML, 2015.
* Computing the Stereo Matching Cost with a Convolutional Neural Network [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zbontar_Computing_the_Stereo_2015_CVPR_paper.pdf)
  * Jure Žbontar, Yann LeCun, Computing the Stereo Matching Cost with a Convolutional Neural Network, CVPR, 2015.
* Colorful Image Colorization Richard Zhang, Phillip Isola, Alexei A. Efros, ECCV, 2016 [[Paper]](http://arxiv.org/pdf/1603.08511.pdf), [[Code]](https://github.com/richzhang/colorization)
* Ryan Dahl, [[Blog]](http://tinyclouds.org/colorize/)
* Feature Learning by Inpainting[[Paper]](https://arxiv.org/pdf/1604.07379v1.pdf)[[Code]](https://github.com/pathak22/context-encoder)
  * Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros, Context Encoders: Feature Learning by Inpainting, CVPR, 2016

### Edge Detection
![edge_detection](https://cloud.githubusercontent.com/assets/5226447/8452371/93ca6f7e-2025-11e5-90f2-d428fd5ff7ac.PNG)
(from Gedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.)

* Holistically-Nested Edge Detection [[Paper]](http://arxiv.org/pdf/1504.06375) [[Code]](https://github.com/s9xie/hed)
  * Saining Xie, Zhuowen Tu, Holistically-Nested Edge Detection, arXiv:1504.06375.
* DeepEdge [[Paper]](http://arxiv.org/pdf/1412.1123)
  * Gedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.
* DeepContour [[Paper]](http://mc.eistar.net/UpLoadFiles/Papers/DeepContour_cvpr15.pdf)
  * Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, Zhijiang Zhang, DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection, CVPR, 2015.

### Semantic Segmentation
![semantic_segmantation](https://cloud.githubusercontent.com/assets/5226447/8452076/0ba8340c-2023-11e5-88bc-bebf4509b6bb.PNG)
(from Jifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640.)
* PASCAL VOC2012 Challenge Leaderboard (01 Sep. 2016)
  ![VOC2012_top_rankings](https://cloud.githubusercontent.com/assets/3803777/18164608/c3678488-7038-11e6-9ec1-74a1542dce13.png)
  (from PASCAL VOC2012 [leaderboards](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6))
* SEC: Seed, Expand and Constrain
  *  Alexander Kolesnikov, Christoph Lampert, Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation, ECCV, 2016. [[Paper]](http://pub.ist.ac.at/~akolesnikov/files/ECCV2016/main.pdf) [[Code]](https://github.com/kolesman/SEC)
* Adelaide
  * Guosheng Lin, Chunhua Shen, Ian Reid, Anton van dan Hengel, Efficient piecewise training of deep structured models for semantic segmentation, arXiv:1504.01013. [[Paper]](http://arxiv.org/pdf/1504.01013) (1st ranked in VOC2012)
  * Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel, Deeply Learning the Messages in Message Passing Inference, arXiv:1508.02108. [[Paper]](http://arxiv.org/pdf/1506.02108) (4th ranked in VOC2012)
* Deep Parsing Network (DPN)
  * Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang, Semantic Image Segmentation via Deep Parsing Network, arXiv:1509.02634 / ICCV 2015 [[Paper]](http://arxiv.org/pdf/1509.02634.pdf) (2nd ranked in VOC 2012)
* CentraleSuperBoundaries, INRIA [[Paper]](http://arxiv.org/pdf/1511.07386)
  * Iasonas Kokkinos, Surpassing Humans in Boundary Detection using Deep Learning, arXiv:1411.07386 (4th ranked in VOC 2012)
* BoxSup [[Paper]](http://arxiv.org/pdf/1503.01640)
  * Jifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640. (6th ranked in VOC2012)
* POSTECH
  * Hyeonwoo Noh, Seunghoon Hong, Bohyung Han, Learning Deconvolution Network for Semantic Segmentation, arXiv:1505.04366. [[Paper]](http://arxiv.org/pdf/1505.04366) (7th ranked in VOC2012)
  * Seunghoon Hong, Hyeonwoo Noh, Bohyung Han, Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation, arXiv:1506.04924. [[Paper]](http://arxiv.org/pdf/1506.04924)
  * Seunghoon Hong,Junhyuk Oh,        Bohyung Han, and        Honglak Lee, Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network, arXiv:1512.07928 [[Paper](http://arxiv.org/pdf/1512.07928.pdf)] [[Project Page](http://cvlab.postech.ac.kr/research/transfernet/)]
* Conditional Random Fields as Recurrent Neural Networks [[Paper]](http://arxiv.org/pdf/1502.03240)
  * Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr, Conditional Random Fields as Recurrent Neural Networks, arXiv:1502.03240. (8th ranked in VOC2012)
* DeepLab
  * Liang-Chieh Chen, George Papandreou, Kevin Murphy, Alan L. Yuille, Weakly-and semi-supervised learning of a DCNN for semantic image segmentation, arXiv:1502.02734. [[Paper]](http://arxiv.org/pdf/1502.02734) (9th ranked in VOC2012)
* Zoom-out [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf)
  * Mohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich, Feedforward Semantic Segmentation With Zoom-Out Features, CVPR, 2015
* Joint Calibration [[Paper]](http://arxiv.org/pdf/1507.01581)
  * Holger Caesar, Jasper Uijlings, Vittorio Ferrari, Joint Calibration for Semantic Segmentation, arXiv:1507.01581.
* Fully Convolutional Networks for Semantic Segmentation [[Paper-CVPR15]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf) [[Paper-arXiv15]](http://arxiv.org/pdf/1411.4038)
  * Jonathan Long, Evan Shelhamer, Trevor Darrell, Fully Convolutional Networks for Semantic Segmentation, CVPR, 2015.
* Hypercolumn [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Hariharan_Hypercolumns_for_Object_2015_CVPR_paper.pdf)
  * Bharath Hariharan, Pablo Arbelaez, Ross Girshick, Jitendra Malik, Hypercolumns for Object Segmentation and Fine-Grained Localization, CVPR, 2015.
* Deep Hierarchical Parsing
  * Abhishek Sharma, Oncel Tuzel, David W. Jacobs, Deep Hierarchical Parsing for Semantic Segmentation, CVPR, 2015. [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Sharma_Deep_Hierarchical_Parsing_2015_CVPR_paper.pdf)
* Learning Hierarchical Features for Scene Labeling [[Paper-ICML12]](http://yann.lecun.com/exdb/publis/pdf/farabet-icml-12.pdf) [[Paper-PAMI13]](http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf)
  * Clement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers, ICML, 2012.
  * Clement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Learning Hierarchical Features for Scene Labeling, PAMI, 2013.
* University of Cambridge [[Web]](http://mi.eng.cam.ac.uk/projects/segnet/)
  * Vijay Badrinarayanan, Alex Kendall and Roberto Cipolla ""SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation."" arXiv preprint arXiv:1511.00561, 2015. [[Paper]](http://arxiv.org/abs/1511.00561)
* Alex Kendall, Vijay Badrinarayanan and Roberto Cipolla ""Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding."" arXiv preprint arXiv:1511.02680, 2015. [[Paper]](http://arxiv.org/abs/1511.00561)
* Princeton
  * Fisher Yu, Vladlen Koltun, ""Multi-Scale Context Aggregation by Dilated Convolutions"", ICLR 2016, [[Paper](http://arxiv.org/pdf/1511.07122v2.pdf)]
* Univ. of Washington, Allen AI
  * Hamid Izadinia, Fereshteh Sadeghi, Santosh Kumar Divvala, Yejin Choi, Ali Farhadi, ""Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing"", ICCV, 2015, [[Paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Izadinia_Segment-Phrase_Table_for_ICCV_2015_paper.pdf)]
* INRIA
  * Iasonas Kokkinos, ""Pusing the Boundaries of Boundary Detection Using deep Learning"", ICLR 2016, [[Paper](http://arxiv.org/pdf/1511.07386v2.pdf)]
* UCSB
  * Niloufar Pourian, S. Karthikeyan, and B.S. Manjunath, ""Weakly supervised graph based semantic segmentation by learning communities of image-parts"", ICCV, 2015, [[Paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Pourian_Weakly_Supervised_Graph_ICCV_2015_paper.pdf)]

### Visual Attention and Saliency
![saliency](https://cloud.githubusercontent.com/assets/5226447/8492362/7ec65b88-2183-11e5-978f-017e45ddba32.png)
(from Nian Liu, Junwei Han, Dingwen Zhang, Shifeng Wen, Tianming Liu, Predicting Eye Fixations using Convolutional Neural Networks, CVPR, 2015.)

* Mr-CNN [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Predicting_Eye_Fixations_2015_CVPR_paper.pdf)
  * Nian Liu, Junwei Han, Dingwen Zhang, Shifeng Wen, Tianming Liu, Predicting Eye Fixations using Convolutional Neural Networks, CVPR, 2015.
* Learning a Sequential Search for Landmarks [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Singh_Learning_a_Sequential_2015_CVPR_paper.pdf)
  * Saurabh Singh, Derek Hoiem, David Forsyth, Learning a Sequential Search for Landmarks, CVPR, 2015.
* Multiple Object Recognition with Visual Attention [[Paper]](http://arxiv.org/pdf/1412.7755.pdf)
  * Jimmy Lei Ba, Volodymyr Mnih, Koray Kavukcuoglu, Multiple Object Recognition with Visual Attention, ICLR, 2015.
* Recurrent Models of Visual Attention [[Paper]](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf)
  * Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu, Recurrent Models of Visual Attention, NIPS, 2014.

### Object Recognition
* Weakly-supervised learning with convolutional neural networks [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Oquab_Is_Object_Localization_2015_CVPR_paper.pdf)
  * Maxime Oquab, Leon Bottou, Ivan Laptev, Josef Sivic, Is object localization for free? – Weakly-supervised learning with convolutional neural networks, CVPR, 2015.
* FV-CNN [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Cimpoi_Deep_Filter_Banks_2015_CVPR_paper.pdf)
  * Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi, Deep Filter Banks for Texture Recognition and Segmentation, CVPR, 2015.

### Human Pose Estimation
* Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh, Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields, CVPR, 2017.
* Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler, and Bernt Schiele, Deepcut: Joint subset partition and labeling for multi person pose estimation, CVPR, 2016.
* Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh, Convolutional pose machines, CVPR, 2016.
* Alejandro Newell, Kaiyu Yang, and Jia Deng, Stacked hourglass networks for human pose estimation, ECCV, 2016.
* Tomas Pfister, James Charles, and Andrew Zisserman, Flowing convnets for human pose estimation in videos, ICCV, 2015.
* Jonathan J. Tompson, Arjun Jain, Yann LeCun, Christoph Bregler, Joint training of a convolutional network and a graphical model for human pose estimation, NIPS, 2014.

### Understanding CNN
![understanding](https://cloud.githubusercontent.com/assets/5226447/8452083/1aaa0066-2023-11e5-800b-2248ead51584.PNG)
(from Aravindh Mahendran, Andrea Vedaldi, Understanding Deep Image Representations by Inverting Them, CVPR, 2015.)

* Karel Lenc, Andrea Vedaldi, Understanding image representations by measuring their equivariance and equivalence, CVPR, 2015. [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lenc_Understanding_Image_Representations_2015_CVPR_paper.pdf)
* Anh Nguyen, Jason Yosinski, Jeff Clune, Deep Neural Networks are Easily Fooled:High Confidence Predictions for Unrecognizable Images, CVPR, 2015. [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.pdf)
* Aravindh Mahendran, Andrea Vedaldi, Understanding Deep Image Representations by Inverting Them, CVPR, 2015. [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf)
* Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, Object Detectors Emerge in Deep Scene CNNs, ICLR, 2015. [[arXiv Paper]](http://arxiv.org/abs/1412.6856)
* Alexey Dosovitskiy, Thomas Brox, Inverting Visual Representations with Convolutional Networks, arXiv, 2015. [[Paper]](http://arxiv.org/abs/1506.02753)
* Matthrew Zeiler, Rob Fergus, Visualizing and Understanding Convolutional Networks, ECCV, 2014. [[Paper]](https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)


### Image and Language

#### Image Captioning
![image_captioning](https://cloud.githubusercontent.com/assets/5226447/8452051/e8f81030-2022-11e5-85db-c68e7d8251ce.PNG)
(from Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Description, CVPR, 2015.)

* UCLA / Baidu [[Paper]](http://arxiv.org/pdf/1410.1090)
  * Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille, Explain Images with Multimodal Recurrent Neural Networks, arXiv:1410.1090.
* Toronto [[Paper]](http://arxiv.org/pdf/1411.2539)
  * Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel, Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models, arXiv:1411.2539.
* Berkeley [[Paper]](http://arxiv.org/pdf/1411.4389)
  * Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, arXiv:1411.4389.
* Google [[Paper]](http://arxiv.org/pdf/1411.4555)
  * Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, arXiv:1411.4555.
* Stanford [[Web]](http://cs.stanford.edu/people/karpathy/deepimagesent/) [[Paper]](http://cs.stanford.edu/people/karpathy/cvpr2015.pdf)
  * Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Description, CVPR, 2015.
* UML / UT [[Paper]](http://arxiv.org/pdf/1412.4729)
  * Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Translating Videos to Natural Language Using Deep Recurrent Neural Networks, NAACL-HLT, 2015.
* CMU / Microsoft [[Paper-arXiv]](http://arxiv.org/pdf/1411.5654) [[Paper-CVPR]](http://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf)
  * Xinlei Chen, C. Lawrence Zitnick, Learning a Recurrent Visual Representation for Image Caption Generation, arXiv:1411.5654.
  * Xinlei Chen, C. Lawrence Zitnick, Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation, CVPR 2015
* Microsoft [[Paper]](http://arxiv.org/pdf/1411.4952)
  * Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig, From Captions to Visual Concepts and Back, CVPR, 2015.
* Univ. Montreal / Univ. Toronto [[Web](http://kelvinxu.github.io/projects/capgen.html)] [[Paper](http://www.cs.toronto.edu/~zemel/documents/captionAttn.pdf)]
  * Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, Yoshua Bengio, Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention, arXiv:1502.03044 / ICML 2015
* Idiap / EPFL / Facebook [[Paper](http://arxiv.org/pdf/1502.03671)]
  * Remi Lebret, Pedro O. Pinheiro, Ronan Collobert, Phrase-based Image Captioning, arXiv:1502.03671 / ICML 2015
* UCLA / Baidu [[Paper](http://arxiv.org/pdf/1504.06692)]
  * Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan L. Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images, arXiv:1504.06692
* MS + Berkeley
  * Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv:1505.04467 [[Paper](http://arxiv.org/pdf/1505.04467.pdf)]
  * Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv:1505.01809 [[Paper](http://arxiv.org/pdf/1505.01809.pdf)]
* Adelaide [[Paper](http://arxiv.org/pdf/1506.01144.pdf)]
  * Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, Image Captioning with an Intermediate Attributes Layer, arXiv:1506.01144
* Tilburg [[Paper](http://arxiv.org/pdf/1506.03694.pdf)]
  * Grzegorz Chrupala, Akos Kadar, Afra Alishahi, Learning language through pictures, arXiv:1506.03694
* Univ. Montreal [[Paper](http://arxiv.org/pdf/1507.01053.pdf)]
  * Kyunghyun Cho, Aaron Courville, Yoshua Bengio, Describing Multimedia Content using Attention-based Encoder-Decoder Networks, arXiv:1507.01053
* Cornell [[Paper](http://arxiv.org/pdf/1508.02091.pdf)]
  * Jack Hessel, Nicolas Savva, Michael J. Wilber, Image Representations and New Domains in Neural Image Captioning, arXiv:1508.02091
* MS + City Univ. of HongKong [[Paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Learning_Query_and_ICCV_2015_paper.pdf)]
  * Ting Yao, Tao Mei, and Chong-Wah Ngo, ""Learning Query and Image Similarities
    with Ranking Canonical Correlation Analysis"", ICCV, 2015

#### Video Captioning
* Berkeley [[Web]](http://jeffdonahue.com/lrcn/) [[Paper]](http://arxiv.org/pdf/1411.4389.pdf)
  * Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR, 2015.
* UT / UML / Berkeley [[Paper]](http://arxiv.org/pdf/1412.4729)
  * Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Translating Videos to Natural Language Using Deep Recurrent Neural Networks, arXiv:1412.4729.
* Microsoft [[Paper]](http://arxiv.org/pdf/1505.01861)
  * Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui, Joint Modeling Embedding and Translation to Bridge Video and Language, arXiv:1505.01861.
* UT / Berkeley / UML [[Paper]](http://arxiv.org/pdf/1505.00487)
  * Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko, Sequence to Sequence--Video to Text, arXiv:1505.00487.
* Univ. Montreal / Univ. Sherbrooke [[Paper](http://arxiv.org/pdf/1502.08029.pdf)]
  * Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville, Describing Videos by Exploiting Temporal Structure, arXiv:1502.08029
* MPI / Berkeley [[Paper](http://arxiv.org/pdf/1506.01698.pdf)]
  * Anna Rohrbach, Marcus Rohrbach, Bernt Schiele, The Long-Short Story of Movie Description, arXiv:1506.01698
* Univ. Toronto / MIT [[Paper](http://arxiv.org/pdf/1506.06724.pdf)]
  * Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books, arXiv:1506.06724
* Univ. Montreal [[Paper](http://arxiv.org/pdf/1507.01053.pdf)]
  * Kyunghyun Cho, Aaron Courville, Yoshua Bengio, Describing Multimedia Content using Attention-based Encoder-Decoder Networks, arXiv:1507.01053
* TAU / USC [[paper](https://arxiv.org/pdf/1612.06950.pdf)]
  * Dotan Kaufman, Gil Levi, Tal Hassner, Lior Wolf, Temporal Tessellation for Video Annotation and Summarization, arXiv:1612.06950.

#### Question Answering
![question_answering](https://cloud.githubusercontent.com/assets/5226447/8452068/ffe7b1f6-2022-11e5-87ab-4f6d4696c220.PNG)
(from Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, VQA: Visual Question Answering, CVPR, 2015 SUNw:Scene Understanding workshop)

* Virginia Tech / MSR [[Web]](http://www.visualqa.org/) [[Paper]](http://arxiv.org/pdf/1505.00468)
  * Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, VQA: Visual Question Answering, CVPR, 2015 SUNw:Scene Understanding workshop.
* MPI / Berkeley [[Web]](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/) [[Paper]](http://arxiv.org/pdf/1505.01121)
  * Mateusz Malinowski, Marcus Rohrbach, Mario Fritz, Ask Your Neurons: A Neural-based Approach to Answering Questions about Images, arXiv:1505.01121.
* Toronto [[Paper]](http://arxiv.org/pdf/1505.02074) [[Dataset]](http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/)
  * Mengye Ren, Ryan Kiros, Richard Zemel, Image Question Answering: A Visual Semantic Embedding Model and a New Dataset, arXiv:1505.02074 / ICML 2015 deep learning workshop.
* Baidu / UCLA [[Paper]](http://arxiv.org/pdf/1505.05612) [[Dataset]]()
  * Hauyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu, Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering, arXiv:1505.05612.
* POSTECH [[Paper](http://arxiv.org/pdf/1511.05756.pdf)] [[Project Page](http://cvlab.postech.ac.kr/research/dppnet/)]
  * Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han, Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction, arXiv:1511.05765
* CMU / Microsoft Research [[Paper](http://arxiv.org/pdf/1511.02274v2.pdf)]
  * Yang, Z., He, X., Gao, J., Deng, L., & Smola, A. (2015). Stacked Attention Networks for Image Question Answering. arXiv:1511.02274.
* MetaMind [[Paper](http://arxiv.org/pdf/1603.01417v1.pdf)]
  * Xiong, Caiming, Stephen Merity, and Richard Socher. ""Dynamic Memory Networks for Visual and Textual Question Answering."" arXiv:1603.01417 (2016).
* SNU + NAVER [[Paper](http://arxiv.org/abs/1606.01455)]
  * Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, *Multimodal Residual Learning for Visual QA*, arXiv:1606:01455
* UC Berkeley + Sony [[Paper](https://arxiv.org/pdf/1606.01847)]
  * Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach, *Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding*, arXiv:1606.01847
* Postech [[Paper](http://arxiv.org/pdf/1606.03647.pdf)]
  * Hyeonwoo Noh and Bohyung Han, *Training Recurrent Answering Units with Joint Loss Minimization for VQA*, arXiv:1606.03647
* SNU + NAVER [[Paper](http://arxiv.org/abs/1610.04325)]
  * Jin-Hwa Kim, Kyoung Woon On, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, *Hadamard Product for Low-rank Bilinear Pooling*, arXiv:1610.04325.

### Image Generation
* Convolutional / Recurrent Networks
  * Aäron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu. ""Conditional Image Generation with PixelCNN Decoders""[[Paper]](https://arxiv.org/pdf/1606.05328v2.pdf)[[Code]](https://github.com/kundan2510/pixelCNN)
  * Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox, ""Learning to Generate Chairs with Convolutional Neural Networks"", CVPR, 2015. [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf)
  * Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra, ""DRAW: A Recurrent Neural Network For Image Generation"", ICML, 2015. [[Paper](https://arxiv.org/pdf/1502.04623v2.pdf)] 
* Adversarial Networks
  * Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative Adversarial Networks, NIPS, 2014. [[Paper]](http://arxiv.org/abs/1406.2661)
  * Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus, Deep Generative Image Models using a ￼Laplacian Pyramid of Adversarial Networks, NIPS, 2015. [[Paper]](http://arxiv.org/abs/1506.05751)
  * Lucas Theis, Aäron van den Oord, Matthias Bethge, ""A note on the evaluation of generative models"", ICLR 2016. [[Paper](http://arxiv.org/abs/1511.01844)]
  * Zhenwen Dai, Andreas Damianou, Javier Gonzalez, Neil Lawrence, ""Variationally Auto-Encoded Deep Gaussian Processes"", ICLR 2016. [[Paper](http://arxiv.org/pdf/1511.06455v2.pdf)]
  * Elman Mansimov, Emilio Parisotto, Jimmy Ba, Ruslan Salakhutdinov, ""Generating Images from Captions with Attention"", ICLR 2016, [[Paper](http://arxiv.org/pdf/1511.02793v2.pdf)]
  * Jost Tobias Springenberg, ""Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks"", ICLR 2016, [[Paper](http://arxiv.org/pdf/1511.06390v1.pdf)]
  * Harrison Edwards, Amos Storkey, ""Censoring Representations with an Adversary"", ICLR 2016, [[Paper](http://arxiv.org/pdf/1511.05897v3.pdf)]
  * Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, Shin Ishii, ""Distributional Smoothing with Virtual Adversarial Training"", ICLR 2016, [[Paper](http://arxiv.org/pdf/1507.00677v8.pdf)]
  * Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei A. Efros, ""Generative Visual Manipulation on the Natural Image Manifold"", ECCV 2016. [[Paper](https://arxiv.org/pdf/1609.03552v2.pdf)] [[Code](https://github.com/junyanz/iGAN)] [[Video](https://youtu.be/9c4z6YsBGQ0)]
* Mixing Convolutional and Adversarial Networks
  * Alec Radford, Luke Metz, Soumith Chintala, ""Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"", ICLR 2016. [[Paper](http://arxiv.org/pdf/1511.06434.pdf)]

### Other Topics
* Visual Analogy [[Paper](https://web.eecs.umich.edu/~honglak/nips2015-analogy.pdf)]
  * Scott Reed, Yi Zhang, Yuting Zhang, Honglak Lee, Deep Visual Analogy Making, NIPS, 2015
* Surface Normal Estimation [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Designing_Deep_Networks_2015_CVPR_paper.pdf)
  * Xiaolong Wang, David F. Fouhey, Abhinav Gupta, Designing Deep Networks for Surface Normal Estimation, CVPR, 2015.
* Action Detection [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gkioxari_Finding_Action_Tubes_2015_CVPR_paper.pdf)
  * Georgia Gkioxari, Jitendra Malik, Finding Action Tubes, CVPR, 2015.
* Crowd Counting [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf)
  * Cong Zhang, Hongsheng Li, Xiaogang Wang, Xiaokang Yang, Cross-scene Crowd Counting via Deep Convolutional Neural Networks, CVPR, 2015.
* 3D Shape Retrieval [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Sketch-Based_3D_Shape_2015_CVPR_paper.pdf)
  * Fang Wang, Le Kang, Yi Li, Sketch-based 3D Shape Retrieval using Convolutional Neural Networks, CVPR, 2015.
* Weakly-supervised Classification
  * Samaneh Azadi, Jiashi Feng, Stefanie Jegelka, Trevor Darrell, ""Auxiliary Image Regularization for Deep CNNs with Noisy Labels"", ICLR 2016, [[Paper](http://arxiv.org/pdf/1511.07069v2.pdf)]
* Artistic Style [[Paper]](http://arxiv.org/abs/1508.06576) [[Code]](https://github.com/jcjohnson/neural-style)
  * Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, A Neural Algorithm of Artistic Style.
* Human Gaze Estimation
  * Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling, Appearance-Based Gaze Estimation in the Wild, CVPR, 2015. [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhang_Appearance-Based_Gaze_Estimation_2015_CVPR_paper.pdf) [[Website]](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild-mpiigaze/)
* Face Recognition
  * Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf, DeepFace: Closing the Gap to Human-Level Performance in Face Verification, CVPR, 2014. [[Paper]](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf)
  * Yi Sun, Ding Liang, Xiaogang Wang, Xiaoou Tang, DeepID3: Face Recognition with Very Deep Neural Networks, 2015. [[Paper]](http://arxiv.org/abs/1502.00873)
  * Florian Schroff, Dmitry Kalenichenko, James Philbin, FaceNet: A Unified Embedding for Face Recognition and Clustering, CVPR, 2015. [[Paper]](http://arxiv.org/abs/1503.03832)
* Facial Landmark Detection
  * Yue Wu, Tal Hassner, KangGeon Kim, Gerard Medioni, Prem Natarajan, Facial Landmark Detection with Tweaked Convolutional Neural Networks, 2015. [[Paper]](http://arxiv.org/abs/1511.04031) [[Project]](http://www.openu.ac.il/home/hassner/projects/tcnn_landmarks/)

## Courses
* Deep Vision
  * [Stanford] [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
  * [CUHK] [ELEG 5040: Advanced Topics in Signal Processing(Introduction to Deep Learning)](https://piazza.com/cuhk.edu.hk/spring2015/eleg5040/home)
* More Deep Learning
  * [Stanford] [CS224d: Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/)
  * [Oxford] [Deep Learning by Prof. Nando de Freitas](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)
  * [NYU] [Deep Learning by Prof. Yann LeCun](http://cilvr.cs.nyu.edu/doku.php?id=courses:deeplearning2014:start)

## Books
* Free Online Books
  * [Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville](http://www.iro.umontreal.ca/~bengioy/dlbook/)
  * [Neural Networks and Deep Learning by Michael Nielsen](http://neuralnetworksanddeeplearning.com/)
  * [Deep Learning Tutorial by LISA lab, University of Montreal](http://deeplearning.net/tutorial/deeplearning.pdf)

## Videos
* Talks
  * [Deep Learning, Self-Taught Learning and Unsupervised Feature Learning By Andrew Ng](https://www.youtube.com/watch?v=n1ViNeWhC24)
  * [Recent Developments in Deep Learning By Geoff Hinton](https://www.youtube.com/watch?v=vShMxxqtDDs)
  * [The Unreasonable Effectiveness of Deep Learning by Yann LeCun](https://www.youtube.com/watch?v=sc-KbuZqGkI)
  * [Deep Learning of Representations by Yoshua bengio](https://www.youtube.com/watch?v=4xsVFLnHC_0)


## Software
### Framework
* Tensorflow: An open source software library for numerical computation using data flow graph by Google [[Web](https://www.tensorflow.org/)]
* Torch7: Deep learning library in Lua, used by Facebook and Google Deepmind [[Web](http://torch.ch/)]
  * Torch-based deep learning libraries: [[torchnet](https://github.com/torchnet/torchnet)],
* Caffe: Deep learning framework by the BVLC [[Web](http://caffe.berkeleyvision.org/)]
* Theano: Mathematical library in Python, maintained by LISA lab [[Web](http://deeplearning.net/software/theano/)]
  * Theano-based deep learning libraries: [[Pylearn2](http://deeplearning.net/software/pylearn2/)], [[Blocks](https://github.com/mila-udem/blocks)], [[Keras](http://keras.io/)], [[Lasagne](https://github.com/Lasagne/Lasagne)]
* MatConvNet: CNNs for MATLAB [[Web](http://www.vlfeat.org/matconvnet/)]
* MXNet: A flexible and efficient deep learning library for heterogeneous distributed systems with multi-language support [[Web](http://mxnet.io/)]
* Deepgaze: A computer vision library for human-computer interaction based on CNNs [[Web](https://github.com/mpatacchiola/deepgaze)]

### Applications
* Adversarial Training
  * Code and hyperparameters for the paper ""Generative Adversarial Networks"" [[Web]](https://github.com/goodfeli/adversarial)
* Understanding and Visualizing
  * Source code for ""Understanding Deep Image Representations by Inverting Them,"" CVPR, 2015. [[Web]](https://github.com/aravindhm/deep-goggle)
* Semantic Segmentation
  * Source code for the paper ""Rich feature hierarchies for accurate object detection and semantic segmentation,"" CVPR, 2014. [[Web]](https://github.com/rbgirshick/rcnn)
  * Source code for the paper ""Fully Convolutional Networks for Semantic Segmentation,"" CVPR, 2015. [[Web]](https://github.com/longjon/caffe/tree/future)
* Super-Resolution
  * Image Super-Resolution for Anime-Style-Art [[Web]](https://github.com/nagadomi/waifu2x)
* Edge Detection
  * Source code for the paper ""DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection,"" CVPR, 2015. [[Web]](https://github.com/shenwei1231/DeepContour)
  * Source code for the paper ""Holistically-Nested Edge Detection"", ICCV 2015. [[Web]](https://github.com/s9xie/hed)

## Tutorials
* [CVPR 2014] [Tutorial on Deep Learning in Computer Vision](https://sites.google.com/site/deeplearningcvpr2014/)
* [CVPR 2015] [Applied Deep Learning for Computer Vision with Torch](https://github.com/soumith/cvpr2015)

## Blogs
* [Deep down the rabbit hole: CVPR 2015 and beyond@Tombone's Computer Vision Blog](http://www.computervisionblog.com/2015/06/deep-down-rabbit-hole-cvpr-2015-and.html)
* [CVPR recap and where we're going@Zoya Bylinskii (MIT PhD Student)'s Blog](http://zoyathinks.blogspot.kr/2015/06/cvpr-recap-and-where-were-going.html)
* [Facebook's AI Painting@Wired](http://www.wired.com/2015/06/facebook-googles-fake-brains-spawn-new-visual-reality/)
* [Inceptionism: Going Deeper into Neural Networks@Google Research](http://googleresearch.blogspot.kr/2015/06/inceptionism-going-deeper-into-neural.html)
* [Implementing Neural networks](http://peterroelants.github.io/) 
","['kjw0612', 'myungsub', 'mihaibujanca', 'jazzsaxmafia', 'deruci', 'hmyeong', 'ssakhavi', 'Vgandhi9', 'amineHorseman', 'huangzehao', 'sven-behnke', 'abuccts', 'thtrieu', 'shicai', 'rex-yue-wu', 'mpatacchiola', 'wwjd228', 'Dubrzr', 'GilLevi', 'cNikolaou', 'tbds', 'chaosmail', 'ksaluja15', 'adambielski', 'gitter-badger', 'royshil', 'ReadmeCritic', 'Lyken17', 'bayerj', 'jnhwkim', 'HaozhiQi', 'davisonio', 'kolesman', 'iamaaditya']",0,,0.63,0,,,,,,1099,,UCSB-NLP,
182034748,MDEwOlJlcG9zaXRvcnkxODIwMzQ3NDg=,support.996.ICU,MSWorkers/support.996.ICU,0,MSWorkers,https://github.com/MSWorkers/support.996.ICU,Microsoft and GitHub Workers Support 996.ICU,0,2019-04-18 07:05:37+00:00,2025-03-06 03:34:26+00:00,2024-06-15 08:29:46+00:00,https://github.com/996icu/996.ICU,1206,10077,10077,,1,0,1,0,0,0,647,0,0,68,other,1,0,0,public,647,68,10077,master,1,,"![Microsoft and GitHub Workers Support 996.ICU](https://repository-images.githubusercontent.com/182034748/6436e100-61af-11e9-9dc6-4cccaad40092)
# Microsoft and GitHub Workers Support 996.ICU
# 微软员工和 GitHub 员工宣布支持 996.ICU 运动


[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE)
<a href=""https://996.icu""><img src=""https://img.shields.io/badge/link-996.icu-red.svg"" alt=""996.icu""></a>


Tech workers in China started a GitHub repository titled [996.ICU](https://github.com/996icu/996.ICU), a reference to the grueling and illegal working hours of many tech companies in China - from 9am to 9pm, 6 days a week. ""By following the '996' work schedule, you are risking yourself getting into the ICU (Intensive Care Unit),"" says the 996.ICU GitHub project description. The project calls for Chinese tech companies to obey the labor laws in China and the international labor convention.

中国的技术从业者正在利用 GitHub 吸引社会各界对中国科技行业中许多公司非法用工行为的关注。该 GitHub 项目被命名为 996.ICU，其中 996 代表了中国许多科技公司的工作时间——从上午 9 点到晚上 9 点，每周工作 6 天。996.ICU GitHub 项目描述称，“如果按照 996 的模式工作，那(你)以后就得进 ICU 了”。该项目旨在呼吁中国的科技公司遵守中国劳动法和国际劳工公约。

This initiative has garnered massive support within China. GitHub users have been starring the repository as a way of showing their support. In the span of a few weeks, the project has been starred over 200,000 times, making it one of the fastest growing GitHub repositories in the service's history.


这一倡议在中国获得了大规模的支持。GitHub 用户一直在以 star GitHub 存储库的方式来表示他们对该项目的支持。在几周的时间内，该项目已经收获了超过 20 万次 star，使其成为 GitHub 服务历史上增长最快的项目之一。



The code-sharing platform GitHub, owned by Microsoft, is a place for developers to save, share, and collaborate on software projects. Most important for the 996.ICU movement is that GitHub is accessible in China. It is the dominant platform for developers to collaborate and is a crucial part of Chinese tech companies' daily operations. Since going viral, Chinese domestic browsers, such as those by Tencent and Alibaba, have restricted access to the 996.ICU repository on their web browsers, warning users that the repository contains illegal or malicious content. We must entertain the possibility that Microsoft and GitHub will be pressured to remove the repository as well.

GitHub 是 996.ICU 运动的理想之地。该平台归微软所有，是开发人员保存、共享软件项目以及进行团队协作的港湾。它提供了类似于社交网络的功能。开发人员可以保存或标记其他人的项目，并通过 GitHub 问题参与讨论。对于 996.ICU 运动来说，最重要的是 GitHub 在中国是可以访问的。它是开发人员进行协作的主要平台，是中国科技企业日常运营的重要组成部分。自从这一话题爆发以来，腾讯和阿里巴巴等中国国内的浏览器已经开始限制用户访问 Github 上的 996.ICU 存储库，并警告用户说该存储库中包含非法或恶意内容。我们必须考虑到微软和 GitHub 也将面临删除该存储库的压力。

In response to these events, we, the workers of Microsoft and GitHub, support the 996.ICU movement and stand in solidarity with tech workers in China. We know this is a problem that crosses national borders. These same issues permeate across full time and contingent jobs at Microsoft and the industry as a whole. Another reason we must take a stand in solidarity with Chinese workers is that history tells us that multinational companies will pit workers against each other in a race to the bottom as they outsource jobs and take advantage of weak labor standards in the pursuit of profit. We have to come together across national boundaries to ensure just working conditions for everyone around the globe.

为了应对可能的风险，我们，作为微软和 GitHub 的员工，决定与中国的技术从业者站在统一战线，支持 996.ICU 运动。我们知道，这是一个跨越国界的问题。同样的问题也渗透到微软和整个行业的全职和临时工作中。我们必须与中国的技术从业者站在一个立场上的另一个原因是，历史告诉我们，跨国公司会把工作外包出去，利用不健全的劳动标准谋求利润最大化，迫使工人在竞次的过程中互相竞争。我们必须跨越国界，万众一心，以确保世界上的每一个人都能够获得公正的工作条件。

We encourage Microsoft and GitHub to keep the [996.ICU GitHub repository](https://github.com/996icu/996.ICU) **uncensored and available to everyone.**


我们鼓励微软，一个坚定地相信工作和生活平衡的公司，在任何情况下都不要删改 996.ICU 的存储库，并保持其对所有人都可见。我们将尽我们最大努力继续支持 996.ICU 运动，并有效利用 GitHub 平台为中国科技工作者发声，传播他们的故事。


Signed,

__486 tech workers*__

签名，

__486 名技术工作者*__

To other tech workers and industry supporters, we urge you to join us in our support of the 996.ICU movement.

对于其他技术从业者和行业内外的支持者，我们恳请您加入我们对 996.ICU 运动的支持。

To add your name to the list of supporters, make a pull request with your name and affiliation, as you would like it to appear on the list of signatories. Make sure your signature is entered alphabetically into the list. You can also email <msworkers4good@protonmail.com> with this information and we can add it for you. Read more about adding yourself as a supporter [here](CONTRIBUTING.md).

如果您希望将您的姓名添加到支持者名单中，请使用 Pull Request 功能将您的姓名和所属单位提供给我们。请按字母顺序输入您的姓名。您也可以直接发送电子邮件至<msworkers4good@protonmail.com>，我们可以为您添加。请点击[此处](CONTRIBUTING_ZH.md)，了解如何将自己添加到支持者名单中。


> \* We launched this petition publicly at the same time that we announced it within Microsoft. We will be updating the number of signed employees periodically as signatures are added.
>
> \* 我们在微软内部发起该请愿的同时对外公开了请愿书。随着支持者人数不断增多，我们会定期更新已签名员工的数量。

Supported by / 支持者:
---

* Adam Yi, Software Engineer
* Adina Shanholtz, Microsoft
* Aladdin(唯然), Software Engineer, Alibaba Inc. 
* Albert Xu (Hanzong Xu), Software Engineer, Freelancer, Student
* Aldrich Xing (Jiaxing Xing), Software Develop Engineer, Student
* Alec Tian (Zhongtian Tian), Software Enginner, Beijing China
* Alejandro Piad, College Prof., AI Researcher, University of Havana, Cuba
* Aleksandra Culver, Senior Site Reliability Engineer, Google
* Alex Goldschmidt, Software Engineer, Microsoft
* Alex Moores, Scala Developer, Northern Virginia
* Alexander D Huang, Game Developer, Acingame
* Alexander Mancevice, Software Developer, CargoMetrics
* Ali Atasever, CTO, Vivoo
* Alice Lai, Applied Scientist, Microsoft
* Allen Yang(杨淇), Computer Engineer, Student
* Alpha Chen (陈翀), Guangzhou taimei Internet technology co. LTD
* Amber Erickson, Software Engineer, Microsoft
* Amr Gaber, Software Engineer, Google
* Andrew Schwartzmeyer, Software Engineer, Microsoft
* Angad Sharma, Software Engineering Student
* Armaan Handa, Computer Engineer, Student
* Ashe Connor, Senior Software Engineer, GitHub
* Ashly Hamilton, Program Manager, Microsoft
* Bam Wang (Wang Zhu), Development Manager, VELTRA Inc.
* Beck Qin, Software Engineer, eRealm Soft & Tech
* Ben Tarnoff, Logic Editor
* Benson Laur, Software Engineer, Guangzhou Wangshi Software Technology Co. Ltd.
* Benwei Zhu, Senior Software Engineer/Tech Lead, ThoughtWorks
* Benjamin Li, Auditer
* Bertram Liu, Auditer
* Better Zhao, Software Engineer
* Bin Xie, University of Georgia.
* Binghua Zhou, Amazon
* Bing Liu, Developer.
* Binary Wang (wangbin), Software Engineer
* Bingyao Ma (马秉尧), Software Engineer, Adtalos
* Bo Feng, Student, Southwest University of Science and Technology
* Bofei Wang (王博飞), University of New South Wales, Sydney
* Borja Canseco, Software Engineer, IBM
* Boy Liu, Software Engineer, JiNan
* Bryan Hughes, Senior Cloud Advocate, Microsoft
* Camille Malonzo, Software Engineer Microsoft
* Carlo Federico Vescovo - Backend Developer @ VeeBor / Student
* Carlos Fernando Kamiya, Support Escalation Engineer, Microsoft
* Casey Hong, Microsoft
* Cassie Mullins, Student/Software Engineer, Georgia Institute of Technology
* Catherine Bui, Web designer, GitHub
* Cathy Lee, Brand designer, Oscar Health
* Chandler Harden, SDE Intern, IBM
* Chang Yan, Front End Engineer, Facebook
* Chang Yan, Undergraduate, University of Washington 
* Changfeng Chen, Software Engineer, LongShine
* Changjun Shuai, Software Engineer, Chengdu
* Chao Tang(唐超), Software Engineer
* Charles Davis, Data Analyst, UC Berkeley
* Charles Zhang(张慕杨), Teacher, BDQN KGC
* Chuan Du, Software Engineer,HUAWEI, Xian.China
* Chelsea Wang, Product Designer, Facebook
* Chen Hailin, Student, Nanyang Technological University
* Chen Jian, Network Engineer, Suzhou
* Chen Kenan, ZhouShan Technician College 
* Chen Nan, Developer, NanJing
* Chen Rui (陈锐), 香港KC集团
* Chen Xu, Senior Software Engineer, Microsoft
* Chen Zizhuo, Customer Manager, China Construction Bank
* ChenBing Wang, Undergraduate, Peking University
* Chenglong Lu, QDaily
* Chen Hailin(陈海林), Software Engineer, Mojin Tech China
* Cherrie Yu Cheng, Product Manager
* ChiaKii Ryuu, Software Engineer, Group Of Otaku (Shenzhen) Inc.
* Chong Liu, Software Engineer, Corpy&Co., Inc.
* Christina Dunbar-Hester, Ph.D. Author of Low Power to the People, MIT Press, University of Southern California
* Christopher Senderling, Microsoft
* Chuck Zhang, Amazon
* Chunli Lin, Undergraduate, Ningbo University of Technology
* Chun Liu, Senior Consultant, Microsoft
* Conan Zhou, Architect, NimbleDroid
* Daniel Greene, Assistant Professor of Information Studies, University of Maryland
* Danni She, Software Engineer, Amazon
* Danyang Zhang, Student, Tsinghua University
* Danyow Ed(朱俊鸿), iOS/Unity Engineer, Thumbparty.
* Daidai Yang, Developer, BeiJing.China
* Daofa Li, Ex-Microsoft Software Engineer, 2006-2018
* Ding Yuxiao, Student
* Doan Pham Phu, Full-stack Web developer
* David Ding(丁鼎), Software Engineer, Student
* David Meyerson, Software Engineer II, Microsoft
* Dean Tambling, Software Engineer, GitHub
* DeepGrace, Senior C++ Expert
* Deming Feng(冯德明), Software Engineer, ShenZhen.China
* Deng Mao, Software Engineer, BONC
* Dixing (Dex) Xu, Research Assistant, Nanyang Technological University
* Dom Wang, Software Engineer, IBM
* Dong Jin Zhao, Software Engineer
* Duke (吴涛), Sunning
* Duo Zhang (张铎), Data Scientist, Broentech Solutions AS, Norway
* Duzy Chan, ExtBit Limited
* Edmund Feng(冯良贤), Software Engineer, Wuhan.China
* EINDEX Li, Software Engineer, Wuhan.China
* Enchong Wang, Software Engineer, Microsoft
* Emily Cunningham, UX Designer at Amazon
* Ercin Dedeoglu, Software Architect, Bilge Adam
* Eric Teo(张再朝), Student, Chongqing Jiaotong University
* Eric Zhang, Front End Engineer, Shanghai
* Erik Knechtel, Microsoft Contractor
* Esteban Roncancio, Software Engineer, TripAdvisor
* Fang Sixie, Software Engineer, CCI
* FenghuiXu, Software Engineer, Microsoft
* Fenghuan, Backend Engineer, Guangzhou.China
* FangYang (方阳), Software Engineer
* Flitrue, Front End Engineer, Beijing
* Francis Tseng, Public Science
* François Conil, Support, GitHub
* Frey Tang(唐飞), Project Manager,UTILITY EMBLEM
* Fruit Lee, Computer Science Master, ZJU
* Gabriel Laroche, Front End Developer, Nurun inc
* Galvin Gao, Web Developer & AI Researcher, Student
* Gangping Guo, Software Engineer, Hangzhou
* Gao Xinyu, student of foreign language department from Yangtze University
* Gregory Ellison, Data Scientist, Microsoft
* Guanqun Mu (穆冠群), Undergraduate Student, Wuhan University, China
* Guo Jian, Software Engineer, Mianyang, China
* Guojun Wang(王国军), Software Engineer
* Hal Daumé III, Principal Researcher, Microsoft Research
* Han Ting, IT manager, Shanghai.China
* Han Yin, Front-end Developer
* Hancel Lin, Software Engineer, GIGABYTE.
* Hang Yang, eCreditpal
* Hao Li (李皓), Applied Scientist, Amazon.
* Hao Shi, Software Engineer, Independent Researcher, Student, University of Massachusetts at Amherst, Microsoft
* Hao Tao, Web Developer, Bingo Information Co., Ltd.
* Hao Xu, Azure Developer PMM, Microsoft
* Haoshen Zhong(钟昊燊), Computer Science Student, Foshan.China
* Harry Yang, Microsoft
* Harshita Gupta, Student, Harvard University and former Software Engineering Intern (x2), Microsoft
* Harrison Feng, Bigdata & AI Architecture, Chengdu, China
* Henghui Liu, Software Engineer
* Hengyu Liu, Computer Science Student
* Henry Hu, Software Engineer, Fabric Group
* Herb (Chengbo He), Software Engineer
* Hewie Mei, Software Engineer, Liepin
* HongBo Du, Software Engineer, Freelancer
* Hongyu Chen(陈泓予), Software Engineer
* Huajie Zhou, Student,Guangxi Normal University
* Ikka(Dongxu Chen), Student, Shandong China
* Irene Knapp, Senior Software Engineer, Google
* İsmail Gökhan Bayram, Tech Writer, Daily Evrensel
* Ivan Wen, CUNY student
* J. Li, Student of EE, Chongqing University of Posts and Telecommunications
* Jacob Ritchie, Research Assistant, University of Toronto
* JackEggie(Jack Tang), Software Engineer, Infor
* Jackie Koon, student, South China University of Technology
* James Lord - Software Developer
* James Turnbull, CTO-in-residence, Microsoft
* Jamie Brandon, CallMiner
* Jamie Zhang, Software Engineer, Google
* Janhavi Mahajan, Software Engineer, Microsoft
* Jared McFarland, Senior Infrastructure Engineer, GitHub
* Jared Li(李公拯), M.S. in Computer Science, UC Davis
* Jasmine Xie, Front-end Engineer
* Jason Jin (金凯)Computer Engineer,shanghai
* Jason Prado, Ex-Microsoft Software Engineer 2008-2009
* Jason Wang (wangshaokun) Software Automation Test Engineer,SHANGHAI
* Jay He, Software Engineer, Smart Testing Ltd, New Zealand
* Jeremy Gross, Sales Engineer, InterSystems
* Jessica Paoli, Web designer, GitHub
* Jeff Liang, Software Development Engineer, Amazon Web Services
* Jerry Chen, Senior Software Developer, DFSI, Australia
* Jeff Chen(陈长风), Software engineer , LongShine
* Jet Wong, Software Engineer, owasp
* JiaChen Yuan, Front-End Engineer, Shanghai.China
* Jiacheng Liu, Computer Science Student, Harbin Institute of Technology (Weihai)
* Jiacong Wei, Hardware Engineer Intern, NVIDIA & Student, Fudan University
* Jiadong Guo Software Engineer, ShenZhen.China
* Jiang Siyuan, Hunan University
* Jiandong Shi, Developer, Nanjing, China
* Jianzhao Liu, Front-End Engineer, Shanghai.China
* Jiaqi Hu,Computer Science Student,Henan University of Science And Technology
* Jiaxun Pu(蒲家训), Frontend Engineer, Shanghai.China
* JiaYanwei(hltj), Technical Manager, NIO
* Jie Wang, Maintenance, Hangzhou.China
* Jing Jing Cao, UX Designer, Anki
* Jisheng Liang(梁基圣)，Student/Software Engineer
* John Kordich, Software Engineer, Microsoft
* John William Davis, Full Stack Developer
* Jonathan Ehrich, Microsoft
* Jonathan Tomer, Software Engineer, Google
* Jonathan Reilly, Software Engineer, Sense
* Jose Escudero, Software Engineer, Insights Network
* Jose Martinez, Software Engineer, Qualcomm
* Josh McDuffie, Software Engineer, Constant Contact
* Joshua Shao, Student, Rhode Island School of Design
* J.S.Patrick,Front-end developer
* JS Tan, Software Engineer, Microsoft
* Julie Mayer, Senior Privacy Manager, Microsoft
* Junhui Guan, Front-end Developer
* Junjie Lin, Student
* JunSheng Wang, Software Developer, Giant
* Kai Liu, Software Engineer, Beijing.China
* Kangour, Software Engineer, Kunming
* Kanstantsin Bucha, Software Engeneer, Viber
* Kanzakiken, Software Engineer
* Kangqi Wang(王康琦), Software Engineer, Wales
* Kathy Wu, Product Designer, MIT Media Lab
* Ke Shen, Software Engineer, Google
* Kelly Wagman, Microsoft Research Assistant
* Keyao Zhang, Senior Software Engineer, AppEx Networks
* Knightyui, Software Engineer, HUAWEI, Nanjing China
* Knove, Software Engineer, Xiaomi
* Kris Sun, Software Engineer, HangZhou，China
* Kristen Sheets, Brandeis University Computer Science
* Kui Wang(王揆), Software Engineer, Knowles
* Kuilin Xiang, Software Engineer, Juejin
* Kunxiong Ling, Development Engineer, BMW Group
* Kevin Li, System Support Analyst, TSG, Auckland, New Zealand
* Lang Qin, IC Verification Engineer, Globalfoundries
* Lantao Liu (Lax), Programmer, Full-Time Freelancer
* Lao Qixin, Shenzhen, Student
* Le Zhang, Data Scientist, Microsoft
* Lei Yao, Game Programmer, DarkSun Studio
* Lei Zhao, Computer Science Master Student, USC
* Leo Liu, Student, Department of Information and Communication Sciences, Sophia University
* Leo Wang, Cybersecurity, Hefei No.8 Senior High School International Department
* Leon Leung, Web Front-end Senior Engineer, EKing Technolgy
* Leon Sun, System Engineer, Amazon
* Leoython Chou, Software Engineer, CloudMall
* Le Qiang(强乐) ,Software Engineer
* Leroy Wang, Student, University of Washington
* Li Dian, Front-end Developer, Software Engineer Student
* Li Qida, Student
* Li Xiaoyang, Software Engineer, Didi Chuxing
* Li Yang(李洋), Software Enginner, Yantai China
* Liam(Liang Ding), Ph.D. Candidate and former NLP Algorithm Engineering
* Lang Qin, IC Verification Engineer, Globalfoundries
* Liang Ding, Full-Time Open-Sourcerer, B3log
* Liang He(何良), Software Engineer, Chengdu.China
* Liang Wang, Architecture, Shenzhen, China
* Liang Zhang,Software Engineer, HangZhou
* Lilangyan He, Software Engineer, Infosys Ltd
* Lin Zhang, Software Engineer II, Microsoft
* Ling Gaom, Microsoft Community Independent Advisor
* Linkey Leo, Software Engineer, Resignation/Freelancer/Prospective Student
* Lion Huang, Senior Software Engineer, Microsoft
* Lion Zhao(zhaolion), Software Engineer
* Lion Chen(chenliang),iOS Development Engineer,NewLand
* Lip Young(杨 征), Fontend Software Coder, Freelancer
* LiPing, Software Engineer, Chongqing plutusdog technology Co,. Ltd.
* Liu Maoxu, Software Engineer, Amazon
* Liu Jun, Software Engineer, Guangbao-uni
* Liu TianYu, Software Engineer, Beijing.China
* Liu Song, Senior Software Engineer, Alibaba Inc, Beijing, China
* Liu Yuyang Software Engineer, Guangzhou.China
* Liz O’Sullivan, former head of annotations at Clarifai
* Li Youyou, Ph.D. Candidate, China.
* Li YuJiang (李裕江), Software Developer, ChuanQingRen of GuiZhou (贵州穿青人), China
* Li Yunpeng(李云鹏|李良逸), Android Engineer, NetEase
* Logic, Software Engineer, RDS
* Long Minghui, Golang Senior Programmer
* Longfei Xia (夏龙飞)，Student, Nanjing University of Posts and Telecommunications
* Lowell Bander, Software Engineer, Facebook
* Lu QiHao(陆 启豪), Software Engineer, Hangzhou Normal University
* Luke Lin, Software Engineer
* Luo Qian, Software Engineer, Guangzhou
* Luxroid(MingWang Hao), Software Engineer, Freelancer
* Lyon Chen, Full Stack Developer
* Mahmoud Habiballah, Software Engineer, Nazareth Israel
* Mai Zhanrun, Software Engineer, Guangzhou.China
* Marc Zhao, Software Engineer
* Margox(Wang Gang), Software Engineer
* Marie Collins, Business Analyst, Google
* Mark Dudley, Software Engineer, Google
* Mark Seaborn, Software Engineer Google
* Martin Larralde, Bioinformatician, ENS Paris-Saclay
* Martin Yin, Software Engineer
* Mary L. Gray Senior Researcher Microsoft Research New England (Associate Professor School of Informatics, Computing, and Engineering Indiana University, Bloomington | Fellow Harvard University Berkman Klein Center for Internet and Society)
* Massimiliano Del Maestro, Software-Engineer, Mantu
* Mateus Gabi Moreira, Software Engineer, Eskolare
* Matthew Wiegert, Economics from SUNY at Binghamton
* Maverick Huang, student, hzwyjxy.com
* Meng Xiang, Full Stack Engineer
* Menglong Huang, Developer, PingCAP
* Mengxi Yang, User Experience Designer, ThoughtWorks
* Meng Xiang(项萌), Full Stack engineer
* Meredith Whittaker, AI Now Institute Co-founder, NYU Distinguished Research Scientist, Google Open Research Founder
* Michael An, Software Engineer, Seafile
* Michael Lebo Zhang, Computer Science PhD Candidate, UCSB
* Miguel González-Fierro, Data Scientists, Microsoft
* Mikayla Hutchinson, Software Engineer, Microsoft
* Mike He, Undergraduate Researcher, University of Washington
* Minghang Su, FPGA Engineer, Shenzhen StateMicro Electronics Co.,Ltd.
* Minsoo Thigpen, Microsoft Program Manager
* Monine (Xiang Zhou), Front-end Engineer, TanZhou EDU
* Naomi Harrington, Program Manager, Microsoft
* Nekocode(Fan Yueng), Software Engineer, Freelancer
* NightEagle, Software Engineer & Software Architect
* Ni Liang, Software Engineer
* Nisha Pillai, Software Engineer
* Nnsinex, Front-end Developer
* Noah Berman, Backend Engineer, WeTransfer
* Norman Sun, Mobile Developer, Freelancer
* nufeng74(solarnumen), Software Engineer
* Panda Blessing(王福全), Software Engineer, Project Manager, Wu'han Kuai'you, China
* Paul Leung, Network security Engineer, Labrusca Studio
* Paulo Estraich, Backend Developer
* Pavel Dubrova, Software Engineer, Freelancer
* Peijie Zhao (赵培杰), Student
* Pengli Liu, Software Engineer, Xi'an, China
* Peoly, Software Engineer, EDU
* Phodal(Fengda Huang), Senior Software Engineer, ThoughtWorks
* Qi Liu, Software Engineer
* Qiang Liu, Symbio
* Qiang Niu, Senior Software Engineer, Yet Inc.
* Qiang Zhang, levelDesigner
* Qiang Zhang, Front End Engineer, YouShu
* Qiheng Lin, Student, Hangzhou
* Qing Mu, Software Tester, NIO
* Rachel Miller, Writer
* Ranjan Pradeep, Software Engineer, Microsoft
* Rick Zhu, Software Engineer
* Rico Liu, Front_End Developer, Qing Dao
* Robert Pupel, Software Engineer, Devs Group
* Rock Yang, Software Engineer,Shen Zhen Dian Dian Block
* Rocken Li(李嘉诚), Student
* Rocky Martin, Data Analyst, Criteo 
* Roth Peng (Yu-Jhau, Peng), Frontend Developer, Trend Micro
* Rui Chen(陈锐),Software Engineer, KC Group, Hong Kong
* Ruibo Li, Software Developer, 51talk
* Ryan(Zhenkun) Ou, Software Engineer
* Rao Wei(饶维),Backend Developer,Shen Zhen,China
* Sam Zhou, Software Engineer Intern, Facebook, Student at Cornell University
* Sangyu Li, Former MSDN Editor,Editor at Hubei Commitee of CCYL,Staff of Wuhan Railway Bureau
* Satoshi Jek (Guangnan Jin), Student
* Santosh Subbarao, Chief Architect, TipoTapp Inc.
* Scott Chen, Software Engineer, Yidatec.com
* Sean Gao, Senior Software Engineer
* Sen Huang, Graduated Student, University of Wollongong
* Sergey Alekhnovich, Software Engineer, iFood Decision Sciences
* Seth Lin(XiuFeng Lin), Software Engineer, Fabric Group
* Shan Zhou, Software Engineer, Microsoft
* Shangsharon(姜尚伟), Software Engineer
* ShanWu, Frontend Engineer, ThoughtWorks
* Shaohua Huang, Software Engineer,Alibaba
* ShaoPeng, Software Engineer, ChengDu China
* Shaoqing Li, Front-End Engineer, China
* Shen Lin, Front-End Developer, DHI China
* Shengtao Li, Software Engineer, 66ifuel
* Shengkai Guo, Software Engineer, China
* SheQiao Zheng, Front-End Engineer, Jiawan Interactive
* Shevis Johnson, ML/AI Engineer, Digit
* Shuai Chang, Software Engineer, Business-intelligence Of Oriental Nations Corporation Ltd.
* Shuirong Lin, Front-end Developer, Freelancer, Student
* ShunLi Ren(任顺利), Development Manager, 君晟合众 Beijing Network Technology Co
* Shuo Hua, R&D Engineer, Agrointelli
* ShuQiao Zhang, Undergraduate, Peking University
* Sian Lerk Lau, Software Engineer, Jewel Paymentech
* Sidak Singh Aulakh, Student and Open-Source Developer, Chandigarh College of Engineering and Technology.
* Simon Zhao, Data & Applied Scientist, Microsoft
* Soeng Gam Lu(卢相锦),Android App Developer,Yulin.China
* Song Xiaofei(宋晓飞), Pythoner
* Songwen Ding(丁松文), Software Engineer, Bytedance
* Stephanie Parker, Policy Specialist, YouTube
* Stephen Fong, Software Engineer, Student
* Stephen Shen, Software Engineer, MoonLegend.GuiYang
* Steven Hitchcock, Localization Tester, Microsoft
* Sun Bojian, Software Engineer, Altran
* Sun-ha Hong, assistant professor, Simon Fraser University
* Swen Wang,Software Engineer, Shanghai.China
* Taylor Huang (Tairan Huang), Software Engineer, Shanghai, China
* Thomas Corwin, Software Engineer, Freelancer
* Thomas Liu, Software Engineer, SAP
* Tian Chen, Software Engineer,  ShenZhen, China.
* Tianqi Zhu, Student
* Tiange Luo, Student
* Tiger Tang, Software Engineer, Student
* Tim Johnson, Software Engineer, Target
* Tim(tim-hub), Software Developer, PyPureCMS & BaaS
* Tinyu Chiu(赵天雨), Student
* Topher Weiss, Software Developer, Rational Interaction @ Microsoft
* Trista, Chenzhen Chaoyue Technology Development Co.,Ltd.
* Tsing Tian(田庆), Software Engineer, YanTai.China
* TsingfunLee, Front-end Engineer, MIGU
* Tung Honwhee, Freelancer Tech Translator
* Txdy, Software Engineer, Guangzhou Wangshi Software Technology Co. Ltd.
* Varoon Mathur, Technology Fellow, The AI Now Institute
* Vicki Tardif, Ontologist, Google
* Victor Qi, iOS Engineer, NetEase
* Vincent Lee, Software Engineer, Hikvision
* Vivian Lim, Software Engineer, Microsoft
* Wang Fei, Software Developer and Tech writer
* Wang Qun Feng, Senior Software Engineer, XTDE
* WangYun, Senior Software Engineer, ThoughtWorks
* warmcheng(钱程), Android Engineer, Beijing.China
* Wayne Wang, Senior Premier Field Eigineer, Microsoft
* Wei Li, Supervisor BI&EPM, Visea consulting
* Weicheng Zheng, Fuzhou University, Student
* Weijie Hu, Software Engineer, ByteDance
* Weilin Shi, Software Engineer, Vungle
* Weiyou Zhu, Software Engineer, Shanghai.China
* Wenbo Cheng, Software Engineer, Shanghai Jizhi Information Technology Co.,Ltd.
* Wendy Liu, Tech Writer
* Wenlong Liu(刘文龙), Software Engineer, LanZhou.China
* Will Aucoin, Data Specialist, Slyce
* Winifred R. Poster, Washington University, St. Louis
* Wuhao Chen, Student, University College London
* Wenzhi Xu, Software Engineer, Program for FREEDOM.
* Xeodou Li, Software Engineer, Udacity
* Xiake Hu, Software Engineer, Student
* Xiangyu (Shawn) Xiao, Tech Support Engineer, Microsoft
* Xiang Zheng, Software Engineer, ShenZhen
* Xianli Si, Software Engineer, Belle International
* XiaoKang Ji(冀小康), Software Engineer, hangzhou
* Xiaomeng Huang, Front-End Engineer, BeiJing, China
* Xiaowei Wang, Software engineer, Designer, UC Berkeley + Logic Magazine
* Xihao Zhang, student from technical school
* Xin Huang, Student
* Xin Zhang, Software Engineer, China Unicom
* Xing Qi, Web Developer, ChongQing.China
* Xinchen Pan (Nicky), Bioinformatics Engineer, Shanghai We-health
* Xinyi Liu, Software Engineer, ETU (Beijing) Network Technology Co., Ltd
* Xiongyu Wu, Student, University of Warwick
* Xiong Chaowei(熊超伟), Software Engineer, Guangzhou
* Xu Gao, Undergraduate, Changchun University
* Xu Kai, postgraduate, China University of Geosciences
* Xudong Cai, Software Engineer, VeryStar
* XueKai(薛凯), Software Engineer, BeiJing.China
* Xun Wang, PhD Student, Technische Universität Berlin
* Xun Zhang, Designer and Web Developer, Github user
* YaFeng Wu,Software Architect, HangZhou.China
* Yamcer(Fan Yang), Software Engineer, Freelancer
* Yan Cui, Solution Architect, Microsoft
* Yang He(杨赫) Software Engineer, Student, Number 1 High School Affiliated to TJU.
* Yang Mao, Student, University of Applied Sciences Emden/Leer
* Yang Zhang(张阳),Software Engineer, Chengdu, China
* YangJun, CEO, Beijing Kirastar Technology Co., Ltd.
* Yann Achard, Senior Software Engineer, Microsoft
* Yao Yuan, AI Scientist, Pensees Ptd Ltd, Singapore
* Yaoze Ye, Student
* Yi Yang, Product Manager, Suning Corporation
* Yibo Wei, Mobile Developer, SAP
* Yichen Lu(卢亦尘), Software Developer, Credit Suisse
* Yidan Mao(毛义丹), Software Engineer
* Yifu Meng, Front-end Developer
* Yihang Wang(王一航), Student/Cyber Security, Harbin Institute of Technology
* Yihong Zhang, Student, University of Washington
* Yihuai Liang(梁义怀), Student/BigData, Pusan National University
* Yijie Deng, Student, University of Washington
* Yin Kwan Lai, Software Developer, Manchester, UK
* Yinghao Liu, Senior Software Engineer, ThoughtWorks
* Yin Xia, Student/Data Analyst, Shanghai Maritime University
* Yixiang Yang, Student, University of Melbourne
* Yon Liu, Software Engineer, Wesoft
* Yongbao Wang, Enginner, Suning
* Yu Chen(陈宇), Software Engineer, Xique Life
* Yu Fei(余斐), Software Engineer
* Yu Ge (葛雨), Software Engineer, Jinan, Shandong
* Yu Guanhua, Software Developer
* Yu Guanhua (于冠华), Software Engineer Student
* Yu Li, Axinfu.Inc
* Yu Li, Software Engineer, Shanghai DaoCloud Network Technology Co., Ltd.
* Yuan Guo, Senior Software Engineer, App Annie
* Yuchao Zhou, Software Engineer, Microsoft
* Yuchen Lei, Chief Engineer of Collegiate Programming Contest Association, China University of Geosciences
* Yuchong Pan, Software Engineering Intern (x2), Microsoft, & Student, University of British Columbia
* YuHao Tao (陶宇豪), Student
* Yuns Sun, Fontend Software Coder, Freelancer
* Yutao Gu, Developer, Undergraduate student
* Zachary Laborde, Software Developer, IBM
* Zaijing Wang(王在京), Unity3d Game Client Developer, Jinggle Inc
* ZedeX Zhao, Director of Product, PingAn Group
* Zehao Chen(陈泽浩), Software Engineer, Enjoylife
* Zeng Wei (曾伟), Student
* Zenghuang Wang, Software Engineer, Guangzhou.China
* Zhang Yan Qiang (张言强), 杭州鑫火信息科技
* Zhang Yiwang, Student
* ZhaoYu Jiang, Full-Stack Software Engineer, Computer Science Student
* Zhaowei Zhong (钟兆玮), Student
* Zhengshuai PENG, Software Engineer, Github User, Shanghai. China
* Zhentao Tang, Software Engineer, Nanjing.China
* Zhenwei Chen, Software Engineer
* Zhicheng Huang(黄智程), College of Mobile Communications, Chongqing University of Posts and Telecommunications
* Zhida Liu, Software Engineer, Enjoylife
* Zhipeng Fu, PhD Candidate, Nanjing University
* Zhonghua Qiang, Software Engineer, Sofu
* Zhong Wei (钟威), PhD Student at Rochester Institute of Technology
* Zhongyu Hwang, Android Software Engineer, Chongqing Genesis Network Technology Co.,Ltd
* Zhou Quan, Software Engineer, SmartRecruiters
* Zhuoran Wu, Machine Learning Engineer, Octi.tv
* Zhou Quan, Software Engineer, SmartRecruiters
* Zhou Zach, Software Engineer, Shanghai.China
* Ziang Chen, Software Engineer
* ZiHang Gao, Software Engineer, IReader
* ZiHong Jing, Software Engineer, freelancer,Beijing
* Zion Chen, Software Engineer, SAP
* Ziyi Li(李子毅), Student
* Zixiang Wang, Student, Hefei University of Technology
* ZJ Wang, Software Engineer， China Unicom, China
* Zongbao Feng, Engineer, Baidu
* Zora Tung, Software Engineer, Google
* Zuokun Ouyang, PhD Student, Université d'Orléans
* BaiChao Liu(刘百超), High School Student
* Zhuo Quan(全卓), Software Engineer, Zhengzhou, China
","['MSWorkers', 'Tedko', 'kattgu7', '88250', 'Lax', 'xty', 'LipYoung', 'linzhary', 'mu-yu', 'crossPQW', 'windyujp', 'yueguoguo', 'steambap', 'alphadl', 'AF74776', 'EnchongWang', 'ff98sha', 'hejhj', 'jasonWangsk', 'naimen', 'shangsharon', 'tiangeluo', 'wxy954072132', 'xcw-coder', 'xrwang', 'zhaolion', 'zhuweiyou', 'iMemento', 'hycinth22', 'wangjunsheng', 'nickchan-uu', 'ff4415', 'umaim', 'yuchong-pan', 'Charles7c', 'ChandlerHarden', 'danyow', 'benweizhu', 'cwbcheng', 'ChungZH', 'skullface', 'MCDC1', 'miguelgfierro', 'djzhao627', 'deepgrace', 'chinaq', 'waynebaby', 'DolorHunter', 'ShenChuan95', 'simplexidev', 'StalinCheung', 'renshunli', 'mickeyouyou', 'disksing', 'dddDarrenQin', 'cyan33', 'cmullins77', 'charonzhang', 'bmiwcy', 'yangjian102621', 'alizeegod', 'yunyu950908', 'yuchaozh', 'Zionchen', 'cdoco', 'zhouzach', 'TooYoungTooSimp', 'liuhenghui', 'lipsuper', 'llwwang', 's010s', 'lbt0612', 'labrusca', 'jy01325228', 'gozsir', 'GalvinGao', 'kasumi-ikka', 'ifmung', 'iamlaobie', 'hyspace', 'huangsijun17', 'hltj', 'imlinhanchao', 'WillTheGameGuy', 'wjc911', 'real-journalism-matters', 'w32zhong', 'cest-la-v', 'VinVendetta', 'usagiryu', 'tksmai', 'armaan-handa', 'tim-hub', 'TigerHix', 'ThomasLiuSeu', 'tiantsing', 'tata9001', 'Germlord', 'w0yne', 'ashton2914', 'zhaoweizhong', 'ZedeX', 'YongbaoWang', 'zi-l', 'ClarkYang91', 'yihozhang', 'Litash', 'yoing945', 'SomeoneDistant', 'mu1one', 'yangjun1994', 'fifsky', '94kai', 'cjim8889', 'WuZhuoran', 'KashingLiu', 'Yixi', 'stevenjoezhang', 'zh0uquan', 'yukitsu-ren', 'yaozeye', 'yahoozzzzzzz', 'xunwang633', 'xiaomengHuang', 'baoziv587', 'xeodou', 'fsx950223', 'warmcheng', 'wangzj00', 'wangjie0513', 'vvvv3', 'margox', 'aladdin-add', 'Jiang-Xuan', 'sannnyu', 'WangYihang', 'saleemshenlin', 'shuiRong', 'kernelbin', 'TsingfunLee', 'ldqk', 'xuwenzhi', 'ZhangZihe', 'zhangx528', 'andot', 'xiafanglongfei', 'TenviLi', 'vickitardif', 'hatedplayer', 'PandaBlessing', 'fengshenyuan', 'nekocode', 'moonrailgun', 'moeworld', 'mhabibal13', 'broven', 'matuyoyi', 'lzn27', 'jo-jordan', 'luddite1989', 'liziyi0914', 'liyujiang-gzu', 'lixiaoyang1992', 'liuyibo123', 'tzt1994', 'alectian', 'txdy077345', 'Ka1evi', 'sunhahong', 'ssicefox', 'soenggam', 'smh2028', 'shuaicj', 'PengXing', 'daysdays', 'roth1002', 'ranjanp', 'radishlee', 'qazmko1029', 'purp1e', 'JaredEatsBugs', 'Garfill', 'gabLaroche', 'itellboy', 'Fa1c0nSec', 'gavin971', 'ErcinDedeoglu', 'zcteo', 'flxxyz', 'EINDEX', 'duzy', 'dexhunter', 'tambling', 'zdy023', 'cbwang2016', 'ddawx123', 'pujiaxun', 'jasminexie', 'jaredonline', 'jambran', 'jamtur01', 'jritch', 'JackEggie', 'wangjiecloud', 'fenghuanfun', 'Horrypotter', 'rustin-bot', 'Heronalps', 'exherb', 'harrisonfeng', 'shisoft', 'realGuantum', 'lineralpha', 'apnovo', 'BensonLaur', 'Enginebeck', 'bamwang', 'AngryPowman', 'L04DB4L4NC3R', 'q-e-p', 'aylai', 'alexhuangster', 'admoores', 'apiad', '1582130940', 'AlainOUYANG', 'adamyi', 'ATRiiX', '1212087', 'Damon0820', 'ConanChou', 'chunliu', 'cherriecheng', 'Chen-Hailin', 'windoze', 'cmdavis4', 'Chao1155', 'Ccccche', 'gladwearefriends', 'CangHaiQingYue', 'buxiaoyang', 'bcanseco', 'CrazyChenzi', 'liu316484231', 'binarywang', 'SamChou19815', 'robert197', 'RicoLiu', 'czero1995', 'kuyeduwu', 'BrisWhite', 'QuantumGhost', 'phodal', 'bartcubbins', 'OldLiu001', 'YijieDeng', 'iostyle', 'bermannoah', 'NickyPan', 'YugeCse', 'Mocha25', 'fongchinghinunsw', 'hukaibaihu', 'liuzhida33', 'yangbryant', 'dingsong0', 'simonzhaoms', 'kiawin', 'shevisj', 'ShawnXxy', 'Shawfy', 'shadowglen', 'SethLin', 'sh474', 'jks15satoshi', 'SANTOSHKVNSP', 'shengtaolee', 'AD1024', 'Knove', 'zhangkeyao', 'kevinlee104', 'kangqiwang', 'Justmemoryl', 'xjose97x', 'jhuashao', 'martinezjose', 'jony4', 'JonathanTR', 'jktomer', 'papadavis47', 'joeyxworks', 'jctechhub', 'jagross', 'VeterinaryChen', 'Michael18811380328', 'MateusGabi', 'MarcSteven', 'Luxroid', 'LucasMMBB', 'lsongdev', 'LovingCoder', '011011100010110101101111', 'l-qiang', 'coooliang', 'LNKLEO', 'linroid', 'optimusleobear', 'LaPluses', 'LQYcode', 'wksuper']",0,,0.72,0,,"# Contribution Guidelines

Your contributions are welcome!

## Sign to Support

One of the main ways you can contribute to this repository is by signing (and starring!) the repo to show your support. Follow these steps to sign:

1. Click on the edit icon of the README.md
2. Add your name and affiliation as you would like it to appear
3. Create a pull request:
    - Name the branch `signed-<your-username>` and ""propose file change""
    - Add `MSWorkers` as a reviewer

> You can also email <msworkers4good@protonmail.com> with your name and affiliation as you would like it to appear and we can add it for you.

Please also considering starring the
[996.ICU](https://github.com/996ICU/996.icu) repository to show support and contribute directly to the original GitHub project.

## Translation

Microsoft and GitHub are global companies with workers all around the world. While English and Chinese are the primary languages we support, we welcome translations of the letter into other languages. To do so, make a pull request with your translated version of the letter under `README_<language_code>.md`. For example, the Spanish translated version may look like `README_ES.md`. 

When making a translated version, only translate the contents of the letter and not the signatures themselves. Signatures should only go into the main `README.md`.

When creating the pull request for a language translation, please add a native speaker of the language you are translating to to review the contents of the pull request.

If you plan on translating the `README.md`, we ask that you consider yourself a native speaker and writer of the language you are translating to. Please only translate _from_ the English or Chinese version of the letter as we want to make sure that translations are as accurate as possible. 

## Other edits

We welcome other edits. Feel free to update the contents of the letter or even add new ideas to it. Pull requests to the contents of the letter will be heavily scrutinized during the review process and we cannot guarantee that most changes will be accepted. However, we are always happy to have those discussions.
",,,,125,,AlloSphere-Research-Group,
40328394,MDEwOlJlcG9zaXRvcnk0MDMyODM5NA==,angr,angr/angr,0,angr,https://github.com/angr/angr,A powerful and user-friendly binary analysis platform!,0,2015-08-06 21:46:55+00:00,2025-03-06 21:08:32+00:00,2025-03-06 19:24:48+00:00,http://angr.io,65675,7802,7802,Python,1,1,1,0,0,0,1095,0,0,543,bsd-2-clause,1,0,0,public,1095,543,7802,master,1,1,"# angr

[![Latest Release](https://img.shields.io/pypi/v/angr.svg)](https://pypi.python.org/pypi/angr/)
[![Python Version](https://img.shields.io/pypi/pyversions/angr)](https://pypi.python.org/pypi/angr/)
[![PyPI Statistics](https://img.shields.io/pypi/dm/angr.svg)](https://pypistats.org/packages/angr)
[![License](https://img.shields.io/github/license/angr/angr.svg)](https://github.com/angr/angr/blob/master/LICENSE)

angr is a platform-agnostic binary analysis framework.
It is brought to you by [the Computer Security Lab at UC Santa Barbara](https://seclab.cs.ucsb.edu), [SEFCOM at Arizona State University](https://sefcom.asu.edu), their associated CTF team, [Shellphish](https://shellphish.net), the open source community, and **[@rhelmot](https://github.com/rhelmot)**.

## Project Links
Homepage: https://angr.io

Project repository: https://github.com/angr/angr

Documentation: https://docs.angr.io

API Documentation: https://api.angr.io/en/latest/

## What is angr?

angr is a suite of Python 3 libraries that let you load a binary and do a lot of cool things to it:

- Disassembly and intermediate-representation lifting
- Program instrumentation
- Symbolic execution
- Control-flow analysis
- Data-dependency analysis
- Value-set analysis (VSA)
- Decompilation

The most common angr operation is loading a binary: `p = angr.Project('/bin/bash')` If you do this in an enhanced REPL like IPython, you can use tab-autocomplete to browse the [top-level-accessible methods](https://docs.angr.io/docs/toplevel) and their docstrings.

The short version of ""how to install angr"" is `mkvirtualenv --python=$(which python3) angr && python -m pip install angr`.

## Example

angr does a lot of binary analysis stuff.
To get you started, here's a simple example of using symbolic execution to get a flag in a CTF challenge.

```python
import angr

project = angr.Project(""angr-doc/examples/defcamp_r100/r100"", auto_load_libs=False)

@project.hook(0x400844)
def print_flag(state):
    print(""FLAG SHOULD BE:"", state.posix.dumps(0))
    project.terminate_execution()

project.execute()
```

# Quick Start

- [Install Instructions](https://docs.angr.io/introductory-errata/install)
- Documentation as [HTML](https://docs.angr.io/) and sources in the angr [Github repository](https://github.com/angr/angr/tree/master/docs)
- Dive right in: [top-level-accessible methods](https://docs.angr.io/core-concepts/toplevel)
- [Examples using angr to solve CTF challenges](https://docs.angr.io/examples).
- [API Reference](https://angr.io/api-doc/)
- [awesome-angr repo](https://github.com/degrigis/awesome-angr)
","['ltfish', 'zardus', 'rhelmot', 'kereoz', 'salls', 'NickStephens', 'mborgerson', 'badnack', 'twizmwazin', 'tyb0807', 'thrsten', 'jmgrosen', 'angr-bot', 'pre-commit-ci[bot]', 'mahaloz', 'dnivra', 'subwire', 'ronnychevalier', 'Pamplemousse', 'fmagin', 'Lukas-Dresel', 'odell89', 'github-actions[bot]', 'acama', 'Phat3', 'schieb', 'domenukk', 'Kyle-Kyle', 'bannsec', 'axt', 'antoniobianchi333', 'danse-macabre', 'nebirhos', 'ekilmer', 'zwimer', 'conand', 'Cl4sm', 'degrigis', 'bennofs', 'P1kachu', 'tiffanyb', 'AOS002', 'ercoppa', 'bluesadi', 'm1ghtym0', 'xxr0ss', 'JinBlack', 'mohitrpatil', 'yuzeming', 'saullocarvalho', 'Owlz', 'demoray', 'ArtemShypotilov', 'ekse', 'adamdoupe', 'CodeMaxx', 'edmcman', 'sam-b', 'capysix', '5lipper', 'sraboy', 'NicolaasWeideman', 'DennyDai', 'mephi42', 'iamahuman', 'lockshaw', 'drone29a', 'hwu71', 'Alexeyan', '3vilWind', 'Adersh97', 'sei-eschwartz', 'Kingloko', 'HexRoman', 'Markakd', 'farosato', 'intranautic', 'stef', 'syheliel', 'r00tus3r', 'Atipriya', 'matthewpruett', '0mp', 'extremecoders-re', 'novafacing', 'nescio007', 'mor619dx', 'ulugbekna', 'MatthewShao', 'Bonnie1256', 'lks9', 'lowks', 'FantasqueX', 'ocean1', 'tunefish', 'sam-f0', 'f-prettyland', 'symflood', 'JsHuang', 'Mic92', 'adamgrimm99', 'NyaMisty', 'moshekaplan', 'amlweems', 'sschritt', 'loverics', 'Luluno01', 'agnosticlines', 'aheinric', 'bkosciarz', 'clslgrnc', 'ducorduck', 'ljjgdfs', 'mauricesvp', 'simplevuln', 'Icegrave0391', 'Diff-fusion', 'jasperla', 'KevOrr', 'mbrattain', 'nmeum', 'CharlyBVo', 'cq674350529', 'adrianherrera', 'LiptonB', 'akibnizam', 'cwgreene', 'ConnorNelson', 'cdisselkoen', 'DanielBotnik', 'amatus', 'Manouchehri', 'EvelynVusky', 'zx2c4', 'themaks', 'qsphan', 'riyadparvez', 'saagarjha', 'woadey', 'CFSworks', 'SolalPirelli', 'zeroSteiner', 'TodAmon', 'toshipiazza', 'trentn', 'tgduckworth', 'W4terf1re', 'qwestduck', 'zhouxuan009', 'aflorea-2k', 'wuruoyu', 'rickyz', 'za233', 'antifob', 'pdcsec', 'Phasip', 'Pascal-0x90', 'nigel', 'ruaronicola', 'covanam', 'lordmoses', 'mikenawrocki', 'embg', 'MarSoft', 'idiomaticrefactoring', 'zd99921', 'yaroslav-o', 'wwwzbwcom', 'trz19991228', 'tracquangthinh', 'thebluegreeny', 'ctfhacker', 'tdube', 'swthorn', 'ri-char', 'trufae', 'otibsa', 'mcfx', 'koko-ng', 'honghai0924', 'minroco', 'hrx1', 'f4c31e55', 'eggplants', 'damienmaier', 'd05004', 'ch4rli3kop', 'CCDV2', 'canpadawan', 'candymate', 'analyst1001', 'EarthCompass', 'digitalisx', 'disconnect3d', 'ttdennis', 'chorankates', 'niftic', 'tunz', 'sch3m4', 'commial', 'BackTrackCRoot', 'LimitOrderBook', 'bdemick', '0xbc', 'capuanob', 'Archfx', 'aeflores', 'afaerber', 'AndyXan', 'andreafioraldi', 'gkso', 'AlaRduTP', 'AlLongley', 'sciencemanx', 'moubctez', 'Spirotot', '3553x', 'f0rki', 'mdxn', 'mksavic', 'kordood', 'Ordoviz', 'Airtnp', 'Kylir', 'afflux', 'stevenskevin', 'kbittick3', 'kbittick', 'ArionMiles', 'jomanchu-twosix', 'jkrshnmenon', 'JJK96', 'pickerj', 'Invincibl-e', 'benquike', 'henrikhorluck', 'crhf', 'h3lpful', 'gsingh93', 'grant-h', 'FelixMartel', 'SidekickLx', 'erhlee-bird', 'emmanuel-ferdman']",1,,0.77,0,,,"Security
========

angr is meant to be able to function as fully secure environment for analyzing code of any kind in its default configuration.
As a result, we take sandbox escapes - opportunities for guest code to manipulate the host environment without the analysis author explicitly allowing it - very seriously.
If you read all the documentation, you should be able to deploy angr to analyze untrusted code without worrying about it.

If you find a sandbox escape bug of any sort by this definition, please let us know through a private channel.
You can contact the core developers through their emails at audrey@rhelmot.io and fishw@asu.edu.
","blank_issues_enabled: false
contact_links:
  - name: Join our Slack community
    url: https://angr.io/invite/
    about: For questions and help with angr, you are invited to join the angr Slack community
",,184,,UCSB-VRL,
143238763,MDEwOlJlcG9zaXRvcnkxNDMyMzg3NjM=,Cheatsheet-God,OlivierLaflamme/Cheatsheet-God,0,OlivierLaflamme,https://github.com/OlivierLaflamme/Cheatsheet-God,Penetration Testing Reference Bank - OSCP / PTP & PTX  Cheatsheet,0,2018-08-02 03:41:17+00:00,2025-03-07 04:47:09+00:00,2024-12-12 23:39:00+00:00,,801,5023,5023,,1,1,1,1,0,0,1232,0,0,4,,1,0,0,public,1232,4,5023,master,1,,"<p align=""center"">
  <img src=""https://user-images.githubusercontent.com/25066959/67897219-5128a480-fb34-11e9-8cca-423a9dd6973f.png"" width=""256"" title=""Github Logo"">

![STARS](https://img.shields.io/github/stars/OlivierLaflamme/Cheatsheet-God?style=social) 
![FORKS](https://img.shields.io/github/forks/OlivierLaflamme/Cheatsheet-God?style=social)
![INFO](https://img.shields.io/badge/Cheatsheet-OSCP%2FPTX-red) 
![CC](https://img.shields.io/badge/license-cc--by--sa--4.0--Licence-blue) 
![COMMIT](https://img.shields.io/github/last-commit/OlivierLaflamme/Cheatsheet-God)
![SIZE](https://img.shields.io/github/repo-size/OlivierLaflamme/Cheatsheet-God)

```diff
+ UPDATE: Added my huge link of bookmarks / references ❤️  

```
*Do you have a million bookmarks saved? Do all of those bookmarks contain unique information? Github repos starred for later?*


Well this is a compilation of all of these resources into a single repo known as **Cheatsheet-God**. No more need for bookmarked links. No need to open a web browser. Its all here for you.

This is a collection of resources, scripts and easy to follow how-to's. I have been gathering (and continuing to gather) in preparation for the OSCP as well as for general pentesting. Feel free to use however you want! 



All contributions are welcomed! If you feel like you can contribute and make these documents more complete, please do! I'll acknowledge you.



### If you would like to improve anything, and add to this repo, PLEASE DO!

Here's what you do:

1. Create Issue Request describing your `enhancement`
2. Fork this repository
3. Push some code to your fork
4. Come back to this repository and open a PR
5. After some review, get that PR merged to master
6. Make sure to update Issue Request so that I can credit you! You ROCK!

Feel free to also open an issue with any questions, help wanted, or requests!

## Acknowledgments
* Inspiration: Making a cheatsheet god would be proud of using.
* Hat tip to anyone who ever contributed :shipit:


&nbsp;
-> Much thanks to [MrTsRex](https://github.com/MrTsRex) for Cheatsheet_Windows.txt enumerating Windows version vulnerabilities
&nbsp; 

&nbsp;
-> Much thanks to [susmithaaa](https://github.com/susmithaaa) for his contribution to Cheatsheet_PenTesting.txt password attacks section 
&nbsp;

&nbsp;
-> Much thanks to [akshaycbor](https://github.com/akshaycbor) for his contribution to Cheatsheet_MobileAppTesting.txt regarding apk repackaging instructions
&nbsp;


## More 
#### WeChat Official Account
A_Can_Of_Tuna  
![Webp net-resizeimage](https://user-images.githubusercontent.com/25066959/68775780-0c554100-05fd-11ea-8ed2-3d7d6ff37576.jpg)


## Get Some Practice  
 [Hack The Box](https://www.hackthebox.eu/)  
 [Attack Defense 1000+ Labs!](https://attackdefense.com/)<br>
 [VulnHub](https://www.vulnhub.com/)<br>
 [Root.me](https://www.root-me.org/)<br>
 [Penetration Testing Practice Lab / Vulnerable Apps/Systems ](https://www.amanhardikar.com/mindmaps/Practice.html)<br>
 [Vulhub](https://vulhub.org/)<br>
 [Vulapps](http://vulapps.evalbug.com/)<br>
 [Vulnspy](https://www.vulnspy.com/)<br>
 [Upload-Labs](https://github.com/c0ny1/upload-labs)<br>
 [TryHackMe](https://tryhackme.com/)<br>
 [HackThisSite](https://www.hackthissite.org/)<br>
 [PentesterLab](https://pentesterlab.com/exercises)<br>
 [HellBound Hackers](https://hbh.sh/home)<br>
 [PortSwigger Web Security Academy Labs](https://portswigger.net/web-security)<br>

## BLOGS   
https://scriptkidd1e.wordpress.com/oscp-journey/  
http://www.securitysift.com/offsec-pwb-oscp/  
http://ch3rn0byl.com/down-with-oscp-yea-you-know-me/  
http://www.techexams.net/forums/security-certifications/110760-oscp-jollyfrogs-tale.html  
http://hackingandsecurity.blogspot.com   
Http://carnal0wnage.blogspot.com/  
Http://www.mcgrewsecurity.com/  
Http://www.gnucitizen.org/blog/  
Http://www.darknet.org.uk/  
Http://spylogic.net/  
Http://taosecurity.blogspot.com/  
Http://www.room362.com/  
Http://blog.sipvicious.org/  
Http://blog.portswigger.net/  
Http://pentestmonkey.net/blog/  
Http://jeremiahgrossman.blogspot.com/  
Http://i8jesus.com/  
Http://blog.c22.cc/  
Http://www.skullsecurity.org/blog/  
Http://blog.metasploit.com/  
Http://www.darkoperator.com/  
Http://blog.skeptikal.org/  
Http://preachsecurity.blogspot.com/  
Http://www.tssci-security.com/  
Http://www.gdssecurity.com/l/b/  
Http://websec.wordpress.com/  
Http://bernardodamele.blogspot.com/  
Http://laramies.blogspot.com/  
Http://www.spylogic.net/  
Http://blog.andlabs.org/  
Http://xs-sniper.com/blog/  
Http://www.commonexploits.com/  
Http://www.sensepost.com/blog/  
Http://wepma.blogspot.com/  
Http://exploit.co.il/  
Http://securityreliks.wordpress.com/  
Http://www.madirish.net/index.html  
Http://sirdarckcat.blogspot.com/  
Http://reusablesec.blogspot.com/  
Http://myne-us.blogspot.com/  
Http://www.notsosecure.com/  
Http://blog.spiderlabs.com/  
Http://www.corelan.be/  
Http://www.digininja.org/  
Http://www.pauldotcom.com/  
Http://www.attackvector.org/  
Http://deviating.net/  
Http://www.alphaonelabs.com/  
Http://www.smashingpasswords.com/  
Http://wirewatcher.wordpress.com/  
Http://gynvael.coldwind.pl/  
Http://www.nullthreat.net/  
Http://www.question-defense.com/  
Http://archangelamael.blogspot.com/  
Http://memset.wordpress.com/  
Http://sickness.tor.hu/  
Http://punter-infosec.com/  
Http://www.securityninja.co.uk/  
Http://securityandrisk.blogspot.com/  
Http://esploit.blogspot.com/  
Http://www.pentestit.com/  
## FORUMS    
Http://sla.ckers.org/forum/index.php  
Http://www.ethicalhacker.net/  
Http://www.backtrack-linux.org/forums/  
Http://www.elitehackers.info/forums/  
Http://www.hackthissite.org/forums/index.php   
Http://securityoverride.com/forum/index.php  
Http://www.iexploit.org/  
Http://bright-shadows.net/  
Http://www.governmentsecurity.org/forum/  
Http://forum.intern0t.net/  
## MAGAZINES  
Http://www.net-security.org/insecuremag.php  
Http://hakin9.org/  
## VIDEO  
Http://www.hackernews.com/  
Http://www.securitytube.net/  
Http://www.irongeek.com/i.php?page=videos/aide-winter-2011  
Http://avondale.good.net/dl/bd/  
Http://achtbaan.nikhef.nl/27c3-stream/releases/mkv/  
http://www.youtube.com/user/ChRiStIaAn008  
http://www.youtube.com/user/HackingCons  
https://www.youtube.com/channel/UCa6eh7gCkpPo5XXUDfygQQA  
https://www.youtube.com/channel/UCCkVMojdBWS-JtH7TliWkVg  
https://www.youtube.com/channel/UCW6MNdOsqv2E9AjQkv9we7A  
https://www.youtube.com/channel/UCFmjA6dnjv-phqrFACyI8tw  
https://www.youtube.com/channel/UClcE-kVhqyiHCcjYwcpfj9w  
https://www.youtube.com/user/RootOfTheNull  
https://www.youtube.com/channel/UCMACXuWd2w6_IEGog744UaA  
## METHODOLOGIES  
http://www.vulnerabilityassessment.co.uk/Penetration%20Test.html  
http://www.pentest-standard.org/index.php/Main_Page  
http://projects.webappsec.org/w/page/13246978/Threat-Classification  
http://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project  
Http://www.social-engineer.org/  
## PRESENTATIONS  
Http://www.spylogic.net/2009/10/enterprise-open-source-intelligence-gathering-part-1-social-networks/  
http://www.spylogic.net/2009/10/enterprise-open-source-intelligence-gathering-%E2%80%93-part-2-blogs-message-boards-and-metadata/  
Http://www.spylogic.net/2009/10/enterprise-open-source-intelligence-gathering-part-3-monitoring/  
http://www.slideshare.net/Laramies/tactical-information-gathering  
Http://www.sans.org/reading_room/whitepapers/privacy/document_metadata_the_silent_killer__32974  
Http://infond.blogspot.com/2010/05/toturial-footprinting.html  
## PEOPLE AND ORGANIZATIONAL  
Http://www.spokeo.com/  
Http://www.123people.com/  
Http://www.xing.com/  
Http://www.zoominfo.com/search  
Http://pipl.com/  
Http://www.zabasearch.com/  
Http://www.searchbug.com/default.aspx  
Http://theultimates.com/  
Http://skipease.com/  
Http://addictomatic.com/  
Http://socialmention.com/  
Http://entitycube.research.microsoft.com/  
Http://www.yasni.com/  
Http://tweepz.com/  
Http://tweepsearch.com/  
Http://www.glassdoor.com/index.htm  
Http://www.jigsaw.com/  
http://searchwww.sec.gov/EDGARFSClient/jsp/EDGAR_MainAccess.jsp  
Http://www.tineye.com/  
Http://www.peekyou.com/  
Http://picfog.com/  
Http://twapperkeeper.com/index.php  
## INFRASTRUCTURE  
Http://uptime.netcraft.com/  
Http://www.serversniff.net/  
Http://www.domaintools.com/  
Http://centralops.net/co/  
Http://hackerfantastic.com/  
Http://whois.webhosting.info/  
Https://www.ssllabs.com/ssldb/analyze.html  
Http://www.clez.net/  
Http://www.my-ip-neighbors.com/  
Http://www.shodanhq.com/  
Http://www.exploit-db.com/google-dorks/  
Http://www.hackersforcharity.org/ghdb/  
EXPLOITS AND ADVISORIES   
Http://www.exploit-db.com/  
Http://www.cvedetails.com/  
Http://www.packetstormsecurity.org/  
http://www.securityforest.com/wiki/index.php/Main_Page  
Http://www.securityfocus.com/bid  
Http://nvd.nist.gov/  
Http://osvdb.org/  
http://www.nullbyte.org.il/Index.html  
Http://secdocs.lonerunners.net/  
http://www.phenoelit-us.org/whatSAP/index.html  
Http://secunia.com/  
Http://cve.mitre.org/  
CHEATSHEETS AND SYNTAX   
Http://www.cheat-sheets.org/  
Http://blog.securitymonks.com/2009/08/15/whats-in-your-folder-security-cheat-sheets/  
## AGILE HACKING  
Http://www.gnucitizen.org/blog/agile-hacking-a-homegrown-telnet-based-portscanner/  
Http://blog.commandlinekungfu.com/  
Http://www.securityaegis.com/simple-yet-effective-directory-bruteforcing/  
Http://isc.sans.edu/diary.html?storyid=2376  
Http://isc.sans.edu/diary.html?storyid=1229  
Http://ss64.com/nt/  
Http://pauldotcom.com/2010/02/running-a-command-on-every-mac.html  
Http://synjunkie.blogspot.com/2008/03/command-line-ninjitsu.html  
Http://www.zonbi.org/2010/06/09/wmic-the-other-other-white-meat/  
Http://rstcenter.com/forum/22324-hacking-without-tools-windows.rst  
http://www.coresecurity.com/files/attachments/Core_Define_and_Win_Cmd_Line.pdf  
http://www.scribd.com/Penetration-Testing-Ninjitsu2-Infrastructure-and-Netcat-without-Netcat/d/3064507  
Http://www.pentesterscripting.com/  
Http://www.sans.org/reading_room/whitepapers/hackers/windows-script-host-hack-windows_33583  
http://www.blackhat.com/presentations/bh-dc-10/Bannedit/BlackHat-DC-2010-Bannedit-Advanced-Command-Injection-Exploitation-1-wp.pdf  
## OS AND SCRIPTS   
http://en.wikipedia.org/wiki/IPv4_subnetting_reference  
Http://www.nixtutor.com/linux/all-the-best-linux-cheat-sheets/  
Http://shelldorado.com/shelltips/beginner.html  
Http://www.linuxsurvival.com/  
http://mywiki.wooledge.org/BashPitfalls  
Http://rubular.com/  
Http://www.iana.org/assignments/port-numbers  
Http://www.robvanderwoude.com/ntadmincommands.php  
Http://www.nixtutor.com/linux/all-the-best-linux-cheat-sheets/  
## TOOLS  
Http://www.sans.org/security-resources/sec560/netcat_cheat_sheet_v1.pdf  
http://www.secguru.com/files/cheatsheet/nessusNMAPcheatSheet.pdf  
http://sbdtools.googlecode.com/files/hping3_cheatsheet_v1.0-ENG.pdf  
http://sbdtools.googlecode.com/files/Nmap5%20cheatsheet%20eng%20v1.pdf  
Http://www.sans.org/security-resources/sec560/misc_tools_sheet_v1.pdf  
http://rmccurdy.com/scripts/Metasploit%20meterpreter%20cheat%20sheet%20reference.html  
Http://h.ackack.net/cheat-sheets/netcat  
## DISTROS  
Http://www.backtrack-linux.org/  
Http://www.matriux.com/  
Http://samurai.inguardians.com/  
http://www.owasp.org/index.php/Category:OWASP_Live_CD_Project  
Https://pentoo.ch/  
Http://www.hackfromacave.com/articles_and_adventures/katana_v2_release.html  
Http://www.piotrbania.com/all/kon-boot/  
Http://www.linuxfromscratch.org/  
Http://sumolinux.suntzudata.com/  
Http://blog.0x0e.org/2009/11/20/pentesting-with-an-ubuntu-box/#comments  
Http://www.backbox.org/  
## LABS ISOS AND VMS  
Http://sourceforge.net/projects/websecuritydojo/  
http://code.google.com/p/owaspbwa/wiki/ProjectSummary  
Http://heorot.net/livecds/  
Http://informatica.uv.es/~carlos/docencia/netinvm/  
Http://www.bonsai-sec.com/en/research/moth.php  
Http://blog.metasploit.com/2010/05/introducing-metasploitable.html  
Http://pynstrom.net/holynix.php  
Http://gnacktrack.co.uk/download.php    
Http://sourceforge.net/projects/lampsecurity/files/  
Https://www.hacking-lab.com/news/newspage/livecd-v4.3-available.html  
Http://sourceforge.net/projects/virtualhacking/files/  
Http://www.badstore.net/   
Http://www.irongeek.com/i.php?page=security/mutillidae-deliberately-vulnerable-php-owasp-top-10  
Http://www.dvwa.co.uk/  
Http://sourceforge.net/projects/thebutterflytmp/  
## VULNERABLE SOFTWARE  
Http://www.oldapps.com/  
Http://www.oldversion.com/  
Http://www.exploit-db.com/webapps/  
Http://code.google.com/p/wavsep/downloads/list  
http://www.owasp.org/index.php/Owasp_SiteGenerator  
Http://www.mcafee.com/us/downloads/free-tools/hacmebooks.aspx  
Http://www.mcafee.com/us/downloads/free-tools/hacme-casino.aspx  
Http://www.mcafee.com/us/downloads/free-tools/hacmeshipping.aspx  
Http://www.mcafee.com/us/downloads/free-tools/hacmetravel.aspx  
## TEST SITES   
Http://www.webscantest.com/  
http://crackme.cenzic.com/Kelev/view/home.php  
http://zero.webappsecurity.com/banklogin.asp?serviceName=FreebankCaastAccess&templateName=prod_sel.forte&source=Freebank&AD_REFERRING_URL=http://www.Freebank.com  
Http://testaspnet.vulnweb.com/  
Http://testasp.vulnweb.com/  
Http://testphp.vulnweb.com/  
Http://demo.testfire.net/  
Http://hackme.ntobjectives.com/  
## EXPLOITATION INTRO  
Http://myne-us.blogspot.com/2010/08/from-0x90-to-0x4c454554-journey-into.html  
Http://www.mgraziano.info/docs/stsi2010.pdf  
Http://www.abysssec.com/blog/2010/05/past-present-future-of-windows-exploitation/  
Http://www.ethicalhacker.net/content/view/122/2/  
http://code.google.com/p/it-sec-catalog/wiki/Exploitation  
Http://x9090.blogspot.com/2010/03/tutorial-exploit-writting-tutorial-from.html  
Http://ref.x86asm.net/index.html  
## REVERSE ENGINEERING & MALWARE  
http://www.woodmann.com/TiGa/idaseries.html  
Http://www.binary-auditing.com/  
Http://visi.kenshoto.com/  
Http://www.radare.org/y/  
Http://www.offensivecomputing.net/  
## PASSWORDS AND HASHES  
Http://www.irongeek.com/i.php?page=videos/password-exploitation-class  
Http://cirt.net/passwords  
Http://sinbadsecurity.blogspot.com/2008/10/ms-sql-server-password-recovery.html  
Http://www.foofus.net/~jmk/medusa/medusa-smbnt.html  
Http://www.foofus.net/?page_id=63  
Http://hashcrack.blogspot.com/  
Http://www.nirsoft.net/articles/saved_password_location.html  
Http://www.onlinehashcrack.com/  
Http://www.md5this.com/list.php?  
Http://www.virus.org/default-password  
Http://www.phenoelit-us.org/dpl/dpl.html  
Http://news.electricalchemy.net/2009/10/cracking-passwords-in-cloud.html  
## WORDLISTS   
Http://contest.korelogic.com/wordlists.html  
http://packetstormsecurity.org/Crackers/wordlists/  
http://www.skullsecurity.org/wiki/index.php/Passwords  
Http://www.ericheitzman.com/passwd/passwords/  
## PASS THE HASH  
Http://www.sans.org/reading_room/whitepapers/testing/pass-the-hash-attacks-tools-mitigation_33283  
Http://www.sans.org/reading_room/whitepapers/testing/crack-pass-hash_33219  
Http://carnal0wnage.blogspot.com/2008/03/using-pash-hash-toolkit.html  
## MITM  
Http://www.giac.org/certified_professionals/practicals/gsec/0810.php  
http://www.linuxsecurity.com/docs/PDF/dsniff-n-mirror.pdf  
Http://www.cs.uiuc.edu/class/sp08/cs498sh/slides/dsniff.pdf  
Http://www.techvibes.com/blog/a-hackers-story-let-me-tell-you-just-how-easily-i-can-steal-your-personal-data  
http://www.mindcenter.net/uploads/ECCE101.pdf  
Http://toorcon.org/pres12/3.pdf  
http://media.techtarget.com/searchUnifiedCommunications/downloads/Seven_Deadliest_UC_Attacks_Ch3.pdf  
Http://packetstormsecurity.org/papers/wireless/cracking-air.pdf  
Http://www.blackhat.com/presentations/bh-europe-03/bh-europe-03-valleri.pdf  
http://www.oact.inaf.it/ws-ssri/Costa.pdf  
Http://www.defcon.org/images/defcon-17/dc-17-presentations/defcon-17-sam_bowne-hijacking_web_2.0.pdf  
http://mcafeeseminar.com/focus/downloads/Live_Hacking.pdf  
http://www.seanobriain.com/docs/PasstheParcel-MITMGuide.pdf  
http://www.more.net/sites/default/files/2010JohnStrandKeynote.pdf  
http://www.leetupload.com/database/Misc/Papers/Asta%20la%20Vista/18.Ettercap_Spoof.pdf  
http://bandwidthco.com/whitepapers/netforensics/arp/EtterCap%20ARP%20Spoofing%20&%20Beyond.pdf  
http://bandwidthco.com/whitepapers/netforensics/arp/Fun%20With%20EtterCap%20Filters.pdf  
http://www.iac.iastate.edu/iasg/libarchive/0910/The_Magic_of_Ettercap/The_Magic_of_Ettercap.pdf  
Http://articles.manugarg.com/arp_spoofing.pdf  
http://academy.delmar.edu/Courses/ITSY2430/eBooks/Ettercap(ManInTheMiddleAttack-tool).pdf  
http://www.ucci.it/docs/ICTSecurity-2004-26.pdf  
http://web.mac.com/opticrealm/iWeb/asurobot/My%20Cyber%20Attack%20Papers/My%20Cyber%20Attack%20Papers_files/ettercap_Nov_6_2005-1.pdf  
Http://blog.spiderlabs.com/2010/12/thicknet.html  
Http://www.hackyeah.com/2010/10/ettercap-filters-with-metasploit-browser_autopwn/  
Http://www.go4expert.com/forums/showthread.php?t=11842  
Http://www.irongeek.com/i.php?page=security/ettercapfilter  
Http://openmaniak.com/ettercap_filter.php  
Http://www.irongeek.com/i.php?page=videos/dns-spoofing-with-ettercap-pharming  
Http://www.irongeek.com/i.php?page=videos/ettercap-plugins-find-ip-gw-discover-isolate  
Http://www.irongeek.com/i.php?page=videos/ettercapfiltervid1  
Http://spareclockcycles.org/2010/06/10/sergio-proxy-released/  
## TOOLS OSINT  
http://www.edge-security.com/theHarvester.php  
Http://www.mavetju.org/unix/dnstracer-man.php  
Http://www.paterva.com/web5/  
## Metadata  
Http://www.sans.org/reading_room/whitepapers/privacy/document-metadata-silent-killer_32974  
Http://lcamtuf.coredump.cx/strikeout/  
Http://www.sno.phy.queensu.ca/~phil/exiftool/  
Http://www.edge-security.com/metagoofil.php  
Http://www.darkoperator.com/blog/2009/4/24/metadata-enumeration-with-foca.html  
## GOOGLE HACKING  
Http://www.stachliu.com/index.php/resources/tools/google-hacking-diggity-project/  
Http://midnightresearch.com/projects/search-engine-assessment-tool/#downloads   
Http://sqid.rubyforge.org/#next  
http://voidnetwork.org/5ynL0rd/darkc0de/python_script/dorkScan.html  
## WEB  
Http://www.bindshell.net/tools/beef  
Http://blindelephant.sourceforge.net/  
Http://xsser.sourceforge.net/  
Http://sourceforge.net/projects/rips-scanner/   
Http://www.divineinvasion.net/authforce/  
Http://andlabs.org/tools.html#sotf  
http://www.taddong.com/docs/Browser_Exploitation_for_Fun&Profit_Taddong-RaulSiles_Nov2010_v1.1.pdf  
Http://carnal0wnage.blogspot.com/2007/07/using-sqid-sql-injection-digger-to-look.html  
Http://code.google.com/p/pinata-csrf-tool/  
Http://xsser.sourceforge.net/#intro  
Http://www.contextis.co.uk/resources/tools/clickjacking-tool/  
Http://packetstormsecurity.org/files/view/69896/unicode-fun.txt  
Http://sourceforge.net/projects/ws-attacker/files/  
Https://github.com/koto/squid-imposter  
## ATTACK STRINGS  
Http://code.google.com/p/fuzzdb/  
http://www.owasp.org/index.php/Category:OWASP_Fuzzing_Code_Database#tab=Statements  
## SHELLS  
Http://sourceforge.net/projects/yokoso/  
Http://sourceforge.net/projects/ajaxshell/  
## SCANNERS  
Http://w3af.sourceforge.net/  
Http://code.google.com/p/skipfish/  
Http://sqlmap.sourceforge.net/  
Http://sqid.rubyforge.org/#next  
http://packetstormsecurity.org/UNIX/scanners/XSSscan.py.txt  
http://code.google.com/p/fimap/wiki/WindowsAttack  
Http://code.google.com/p/fm-fsf/  
## PROXIES Burp  
Http://www.sans.org/reading_room/whitepapers/testing/fuzzing-approach-credentials-discovery-burp-intruder_33214  
Http://www.gdssecurity.com/l/b/2010/08/10/constricting-the-web-the-gds-burp-api/  
Http://sourceforge.net/projects/belch/files/    
Http://www.securityninja.co.uk/application-security/burp-suite-tutorial-repeater-and-comparer-tools    
Http://blog.ombrepixel.com/    
Http://andlabs.org/tools.html#dser     
Http://feoh.tistory.com/22    
Http://www.sensepost.com/labs/tools/pentest/reduh    
http://www.owasp.org/index.php/OWASP_WebScarab_NG_Project    
Http://intrepidusgroup.com/insight/mallory/  
Http://www.fiddler2.com/fiddler2/  
http://websecuritytool.codeplex.com/documentation?referringTitle=Home  
http://translate.google.com/translate?hl=en&sl=es&u=http://xss.codeplex.com/releases/view/43170&prev=/search%3Fq%3Dhttp://www.hackingeek.com/2010/08/x5s-encuentra-fallos-xss-lfi-rfi-en-tus.html%26hl%3Den&rurl=translate.google.com&twu=1  
## SOCIAL ENGINEERING  
Http://www.secmaniac.com/  
## PASSWORD  
Http://nmap.org/ncrack/  
Http://www.foofus.net/~jmk/medusa/medusa.html  
Http://www.openwall.com/john/  
Http://ophcrack.sourceforge.net/  
Http://blog.0x3f.net/tool/keimpx-in-action/  
Http://code.google.com/p/keimpx/  
Http://sourceforge.net/projects/hashkill/  
## METASPLOIT  
Http://www.indepthdefense.com/2009/02/reverse-pivots-with-metasploit-how-not.html  
http://code.google.com/p/msf-hack/wiki/WmapNikto  
Http://www.indepthdefense.com/2009/01/metasploit-visual-basic-payloads-in.html   
Http://seclists.org/metasploit/  
Http://pauldotcom.com/2010/03/nessus-scanning-through-a-meta.html  
Http://meterpreter.illegalguy.hostzi.com/  
Http://blog.metasploit.com/2010/03/automating-metasploit-console.html  
Http://www.workrobot.com/sansfire2009/561.html  
Http://www.securitytube.net/video/711  
http://en.wikibooks.org/wiki/Metasploit/MeterpreterClient#download  
Http://vimeo.com/16852783  
Http://milo2012.wordpress.com/2009/09/27/xlsinjector/  
Http://www.fastandeasyhacking.com/  
Http://trac.happypacket.net/  
http://www.blackhat.com/presentations/bh-dc-10/Ames_Colin/BlackHat-DC-2010-colin-david-neurosurgery-with-meterpreter-wp.pdf  
http://www.blackhat.com/presentations/bh-dc-10/Egypt/BlackHat-DC-2010-Egypt-UAV-slides.pdf  
http://www.offensive-security.com/metasploit-unleashed/Metasploit_Unleashed_Information_Security_Training  
Http://www.irongeek.com/i.php?page=videos/metasploit-class  
Http://www.ethicalhacker.net/component/option,com_smf/Itemid,54/topic,6158.0/  
Http://vimeo.com/16925188  
Http://www.ustream.tv/recorded/13396511  
Http://www.ustream.tv/recorded/13397426  
Http://www.ustream.tv/recorded/13398740  
## MSF Exploits or Easy  
Http://www.nessus.org/plugins/index.php?view=single&id=12204
Http://www.nessus.org/plugins/index.php?view=single&id=11413
Http://www.nessus.org/plugins/index.php?view=single&id=18021
Http://www.nessus.org/plugins/index.php?view=single&id=26918
Http://www.nessus.org/plugins/index.php?view=single&id=34821
Http://www.nessus.org/plugins/index.php?view=single&id=22194
Http://www.nessus.org/plugins/index.php?view=single&id=34476
Http://www.nessus.org/plugins/index.php?view=single&id=25168
Http://www.nessus.org/plugins/index.php?view=single&id=19408
Http://www.nessus.org/plugins/index.php?view=single&id=21564
Http://www.nessus.org/plugins/index.php?view=single&id=10862
Http://www.nessus.org/plugins/index.php?view=single&id=26925
Http://www.nessus.org/plugins/index.php?view=single&id=29314
Http://www.nessus.org/plugins/index.php?view=single&id=23643
Http://www.nessus.org/plugins/index.php?view=single&id=12052
Http://www.nessus.org/plugins/index.php?view=single&id=12052
Http://www.nessus.org/plugins/index.php?view=single&id=34477
Http://www.nessus.org/plugins/index.php?view=single&id=15962
Http://www.nessus.org/plugins/index.php?view=single&id=42106
Http://www.nessus.org/plugins/index.php?view=single&id=15456
Http://www.nessus.org/plugins/index.php?view=single&id=21689
Http://www.nessus.org/plugins/index.php?view=single&id=12205
Http://www.nessus.org/plugins/index.php?view=single&id=22182
Http://www.nessus.org/plugins/index.php?view=single&id=26919
Http://www.nessus.org/plugins/index.php?view=single&id=26921
Http://www.nessus.org/plugins/index.php?view=single&id=21696
Http://www.nessus.org/plugins/index.php?view=single&id=40887
Http://www.nessus.org/plugins/index.php?view=single&id=10404
Http://www.nessus.org/plugins/index.php?view=single&id=18027
Http://www.nessus.org/plugins/index.php?view=single&id=19402
Http://www.nessus.org/plugins/index.php?view=single&id=11790
Http://www.nessus.org/plugins/index.php?view=single&id=12209
Http://www.nessus.org/plugins/index.php?view=single&id=10673
## NSE  
Http://www.securitytube.net/video/931  
Http://nmap.org/nsedoc/  
## NET SCANNERS AND SCRIPTS  
Http://nmap.org/  
Http://asturio.gmxhome.de/software/sambascan2/i.html  
Http://www.softperfect.com/products/networkscanner/  
Http://www.openvas.org/  
Http://tenable.com/products/nessus  
Http://www.rapid7.com/vulnerability-scanner.jsp  
Http://www.eeye.com/products/retina/community  
## POST EXPLOITATION  
Http://www.awarenetwork.org/home/rattle/source/python/exe2bat.py  
Http://www.phx2600.org/archive/2008/08/29/metacab/  
Http://www.room362.com/blog/2011/9/6/post-exploitation-command-lists.html  
## NETCAT  
Http://readlist.com/lists/insecure.org/nmap-dev/1/7779.html  
Http://www.radarhack.com/tutorial/ads.pdf  
http://www.infosecwriters.com/text_resources/pdf/Netcat_for_the_Masses_DDebeer.pdf  
Http://www.sans.org/security-resources/sec560/netcat_cheat_sheet_v1.pdf  
Http://www.dest-unreach.org/socat/  
Http://www.antionline.com/archive/index.php/t-230603.html  
Http://technotales.wordpress.com/2009/06/14/netcat-tricks/  
Http://seclists.org/nmap-dev/2009/q1/581  
Http://www.terminally-incoherent.com/blog/2007/08/07/few-useful-netcat-tricks/  
http://www.inguardians.com/research/docs/Skoudis_pentestsecrets.pdf  
Http://gse-compliance.blogspot.com/2008/07/netcat.html  
## SOURCE INSPECTION  
Http://www.justanotherhacker.com/projects/graudit.html  
Http://code.google.com/p/javasnoop/  
## FIREFOX ADDONS  
https://addons.mozilla.org/en-US/firefox/addon/wappalyzer/?src=collection
https://addons.mozilla.org/en-US/firefox/addon/web-developer/?src=collection
https://addons.mozilla.org/en-CA/firefox/addon/cookie-quick-manager/
https://addons.mozilla.org/en-CA/firefox/addon/hackbartool/  
## TOOL LISTINGS  
Http://packetstormsecurity.org/files/tags/tool  
http://tools.securitytube.net/index.php?title=Main_Page  
## TRAINING/CLASSES SEC/HACKING  
Http://pentest.cryptocity.net/  
Http://www.irongeek.com/i.php?page=videos/network-sniffers-class  
http://samsclass.info/124/124_Sum09.shtml  
Http://www.cs.ucsb.edu/~vigna/courses/cs279/  
Http://crypto.stanford.edu/cs142/  
Http://crypto.stanford.edu/cs155/  
Http://cseweb.ucsd.edu/classes/wi09/cse227/  
Http://www-inst.eecs.berkeley.edu/~cs161/sp11/  
http://security.ucla.edu/pages/Security_Talks  
Http://www.cs.rpi.edu/academics/courses/spring10/csci4971/  
Http://cr.yp.to/2004-494.html  
Http://www.ece.cmu.edu/~dbrumley/courses/18732-f09/  
Https://noppa.tkk.fi/noppa/kurssi/t-110.6220/luennot  
Http://stuff.mit.edu/iap/2009/#websecurity  
## PROGRAMMING Python  
Http://code.google.com/edu/languages/google-python-class/index.html  
http://www.swaroopch.com/notes/Python_en: Table_of_Contents  
http://www.thenewboston.com/?cat=40&pOpen=tutorial  
Http://showmedo.com/videotutorials/python  
Http://www.catonmat.net/blog/learning-python-programming-language-through-video-lectures/  
## PROGRAMMING Ruby  
Http://www.tekniqal.com/  
##  OTHER MISC  
Http://www.cs.sjtu.edu.cn/~kzhu/cs490/  
Https://noppa.tkk.fi/noppa/kurssi/t-110.6220/luennot/  
http://i-web.iu-tokyo.ac.jp/edu/training/ss/lecture/new-documents/Lectures/  
Http://resources.infosecinstitute.com/  
Http://vimeo.com/user2720399  
## WEB VECTORS SQLI  
Http://pentestmonkey.net/blog/mssql-sql-injection-cheat-sheet/   
Http://isc.sans.edu/diary.html?storyid=9397   
Http://ferruh.mavituna.com/sql-injection-cheatsheet-oku/  
Http://www.evilsql.com/main/index.php  
Http://xd-blog.com.ar/descargas/manuales/bugs/full-mssql-injection-pwnage.html  
http://securityoverride.com/articles.php?article_id=1&article=The_Complete_Guide_to_SQL_Injections  
Http://websec.wordpress.com/2010/03/19/exploiting-hard-filtered-sql-injections/  
Http://sqlzoo.net/hack/  
Http://www.sqlteam.com/article/sql-server-versions  
Http://www.krazl.com/blog/?p=3  
http://www.owasp.org/index.php/Testing_for_MS_Access  
http://web.archive.org/web/20101112061524/http://seclists.org/pen-test/2003/May/0074.html  
http://web.archive.org/web/20080822123152/http://www.webapptest.org/ms-access-sql-injection-cheat-sheet-EN.html  
http://www.youtube.com/watch?v=WkHkryIoLD0  
http://layerone.info/archives/2009/Joe%20McCray%20-%20Advanced%20SQL%20Injection%20-%20L1%202009.pdf  
Http://vimeo.com/3418947  
Http://sla.ckers.org/forum/read.php?24,33903  
Http://websec.files.wordpress.com/2010/11/sqli2.pdf  
Http://old.justinshattuck.com/2007/01/18/mysql-injection-cheat-sheet/  
Http://ha.ckers.org/sqlinjection/  
http://lab.mediaservice.net/notes_more.php?id=MSSQL  
## WEB VECTORS UPLOAD TRICKS  
Http://www.google.com/#hl=en&q=bypassing+upload+file+type&start=40&sa=N&fp=a2bb30ecf4f91972  
Http://blog.skeptikal.org/2009/11/adobe-responds-sort-of.html  
Http://blog.insicdesigns.com/2009/01/secure-file-upload-in-php-web-applications/  
Http://perishablepress.com/press/2006/01/10/stupid-htaccess-tricks/  
Http://ex.ploit.net/f20/tricks-tips-bypassing-image-uploaders-t3hmadhatt3r-38/  
Http://www.ravenphpscripts.com/article2974.html  
Http://www.acunetix.com/cross-site-scripting/scanner.htm  
Http://www.vupen.com/english/advisories/2009/3634  
Http://msdn.microsoft.com/en-us/library/aa478971.aspx  
Http://dev.tangocms.org/issues/237  
http://seclists.org/fulldisclosure/2006/Jun/508  
Http://www.gnucitizen.org/blog/cross-site-file-upload-attacks/  
http://www.ipolicynetworks.com/technology/files/TikiWiki_jhot.php_Script_File_Upload_Security_Bypass_Vulnerability.html  
http://shsc.info/FileUploadSecurity  
## WEB VECTORS LFI/RFI  
Http://pastie.org/840199  
Http://websec.wordpress.com/2010/02/22/exploiting-php-file-inclusion-overview/  
Http://www.notsosecure.com/folder2/2010/08/20/lfi-code-exec-remote-root/?utm_source=twitterfeed&utm_medium=twitter  
Http://labs.neohapsis.com/2008/07/21/local-file-inclusion-%E2%80%93-tricks-of-the-trade/  
Http://www.digininja.org/blog/when_all_you_can_do_is_read.php  
## WEB VECTORS XSS  
Http://www.infosecwriters.com/hhworld/hh8/csstut.htm  
http://www.technicalinfo.net/papers/CSS.html  
Http://msmvps.com/blogs/alunj/archive/2010/07/07/1773441.aspx  
Http://forum.intern0t.net/web-hacking-war-games/112-cross-site-scripting-attack-defense-guide.html  
https://media.blackhat.com/bh-eu-10/presentations/Lindsay_Nava/BlackHat-EU-2010-Lindsay-Nava-IE8-XSS-Filters-slides.pdf  
Http://sirdarckcat.blogspot.com/2009/08/our-favorite-xss-filters-and-how-to.html  
Http://www.securityaegis.com/filter-evasion-houdini-on-the-wire/  
Http://heideri.ch/jso/#javascript  
Http://www.reddit.com/r/xss/  
Http://sla.ckers.org/forum/list.php?2  
## COLDFUSION  
Http://www.gnucitizen.org/blog/coldfusion-directory-traversal-faq-cve-2010-2861/  
http://zastita.com/02114/Attacking_ColdFusion..html  
Http://www.nosec.org/2010/0809/629.html  
http://h30507.www3.hp.com/t5/Following-the-White-Rabbit-A/Adobe-ColdFusion-s-Directory-Traversal-Disaster/ba-p/81964  
http://cfunited.com/2009/files/presentations/254_ShlomyGantz_August2009_HackProofingColdFusion.pdf  
## SHAREPOINT  
http://www.ethicalhacker.net/component/option,com_smf/Itemid,54/topic,6131.msg32678/#msg32678   
## LOTUS  
http://blog.ombrepixel.com/post/2009/05/06/Lotus-Notes/Domino-Security  
http://seclists.org/pen-test/2002/Nov/43  
Http://www.sectechno.com/2010/07/12/hacking-lotus-domino/?  
## JBOSS  
http://www.nruns.com/_downloads/Whitepaper-Hacking-jBoss-using-a-Browser.pdf  
Http://blog.mindedsecurity.com/2010/04/good-bye-critical-jboss-0day.html  
## VMWARE WEB  
Http://www.metasploit.com/modules/auxiliary/scanner/http/vmware_server_dir_trav  
## ORACLE APP SERVERS  
Http://www.hideaway.net/2007/07/hacking-oracle-application-servers.html  
http://www.owasp.org/index.php/Testing_for_Oracle   
Http://www.ngssoftware.com/services/software-products/internet-security/orascan.aspx  
http://www.ngssoftware.com/services/software-products/Database-Security/NGSSQuirreLOracle.aspx  
Http://www.ngssoftware.com/papers/hpoas.pdf  
## SAP  
Http://www.onapsis.com/research.html#bizploit   
Http://marc.info/?l=john-users&m=121444075820309&w=2  
http://www.phenoelit-us.org/whatSAP/index.html  
## WIRELESS  
Http://code.google.com/p/pyrit/  
## CAPTURE THE FLAG/WARGAMES  
Http://intruded.net/  
Http://smashthestack.org/   
Http://flack.hkpco.kr/  
Http://ctf.hcesperer.org/  
Http://ictf.cs.ucsb.edu/  
Http://capture.thefl.ag/calendar/  
## MISC/UNSORTED  
http://www.ikkisoft.com/stuff/SMH_XSS.txt  
Http://securestate.blogspot.com/2010/08/xfs-101-cross-frame-scripting-explained.html?utm_source=twitterfeed&utm_medium=twitter
Http://whatthefuckismyinformationsecuritystrategy.com/   
Http://video.google.com/videoplay?docid=4379894308228900017&q=owasp#   
http://video.google.com/videoplay?docid=4994651985041179755&ei=_1k4TKj-PI-cqAPioJnKDA&q=deepsec#  
Http://www.sensepost.com/blog/4552.html  
Http://blog.zenone.org/2009/03/pci-compliance-disable-sslv2-and-weak.html  
Http://threatpost.com/en_us/blogs/hd-moore-metasploit-exploitation-and-art-pen-testing-040210   
Http://carnal0wnage.attackresearch.com/node/410   
Http://www.cs.ucsb.edu/~adoupe/static/black-box-scanners-dimva2010.pdf   
http://www.spy-hunter.com/Database_Pen_Testing_ISSA_March_25_V2.pdf   
Http://perishablepress.com/press/2006/01/10/stupid-htaccess-tricks/   


","['OlivierLaflamme', 'susmithaaa', 'its0x08', 'akshaycbor', 'yhlong0', 'remonsec', 'PtPhoenix', 'MrTsRex', 'd4rkc0nd0r']",0,,0.69,0,,"If you would like to improuve anything, and add to this repo, PLEASE DO!

Here's what you do:

Here's what you do:

1. Create Issue Request describing your enhancement
2. Fork this repository
3. Push some code to your fork
4. Come back to this repository and open a PR
5. After some review, get that PR merged to master
6. Make sure to update Issue Request so that I can credit you and link your github account to the README! 

You ROCK!
Feel free to also open an issue with any questions, help wanted, or requests!
",,,,240,,NCEAS,
554523373,R_kgDOIQ1a7Q,open_flamingo,mlfoundations/open_flamingo,0,mlfoundations,https://github.com/mlfoundations/open_flamingo,An open-source framework for training large multimodal models.,0,2022-10-20 00:32:35+00:00,2025-03-06 13:51:09+00:00,2024-08-31 23:11:03+00:00,,7719,3835,3835,Python,1,1,1,1,0,0,297,0,0,49,mit,1,0,0,public,297,49,3835,main,1,1,"# 🦩 OpenFlamingo

[![PyPI version](https://badge.fury.io/py/open_flamingo.svg)](https://badge.fury.io/py/open_flamingo)

[Paper](https://arxiv.org/abs/2308.01390) | Blog posts: [1](https://laion.ai/blog/open-flamingo/), [2](https://laion.ai/blog/open-flamingo-v2/) | [Demo](https://huggingface.co/spaces/openflamingo/OpenFlamingo)

Welcome to our open source implementation of DeepMind's [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)! 

In this repository, we provide a PyTorch implementation for training and evaluating OpenFlamingo models.
If you have any questions, please feel free to open an issue. We also welcome contributions!

# Table of Contents
- [Installation](#installation)
- [Approach](#approach)
  * [Model architecture](#model-architecture)
- [Usage](#usage)
  * [Initializing an OpenFlamingo model](#initializing-an-openflamingo-model)
  * [Generating text](#generating-text)
- [Training](#training)
  * [Dataset](#dataset)
- [Evaluation](#evaluation)
- [Future plans](#future-plans)
- [Team](#team)
- [Acknowledgments](#acknowledgments)
- [Citing](#citing)

# Installation

To install the package in an existing environment, run 
```
pip install open-flamingo
```

or to create a conda environment for running OpenFlamingo, run
```
conda env create -f environment.yml
```

To install training or eval dependencies, run one of the first two commands. To install everything, run the third command.
```
pip install open-flamingo[training]
pip install open-flamingo[eval]
pip install open-flamingo[all]
```

There are three `requirements.txt` files: 
- `requirements.txt` 
- `requirements-training.txt`
- `requirements-eval.txt`

Depending on your use case, you can install any of these with `pip install -r <requirements-file.txt>`. The base file contains only the dependencies needed for running the model.

## Development

We use pre-commit hooks to align formatting with the checks in the repository. 
1. To install pre-commit, run
    ```
    pip install pre-commit
    ```
    or use brew for MacOS
    ```
    brew install pre-commit
    ```
2. Check the version installed with
    ```
    pre-commit --version
    ```
3. Then at the root of this repository, run
    ```
    pre-commit install
    ```
Then every time we run git commit, the checks are run. If the files are reformatted by the hooks, run `git add` for your changed files and `git commit` again

# Approach
OpenFlamingo is a multimodal language model that can be used for a variety of tasks. It is trained on a large multimodal dataset (e.g. Multimodal C4) and can be used to generate text conditioned on interleaved images/text. For example, OpenFlamingo can be used to generate a caption for an image, or to generate a question given an image and a text passage. The benefit of this approach is that we are able to rapidly adapt to new tasks using in-context learning.

## Model architecture
OpenFlamingo combines a pretrained vision encoder and a language model using cross attention layers. The model architecture is shown below.

![OpenFlamingo architecture](docs/flamingo.png) 
Credit: [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)

# Usage
## Initializing an OpenFlamingo model
We support pretrained vision encoders from the [OpenCLIP](https://github.com/mlfoundations/open_clip) package, which includes OpenAI's pretrained models. 
We also support pretrained language models from the `transformers` package, such as [MPT](https://huggingface.co/models?search=mosaicml%20mpt), [RedPajama](https://huggingface.co/models?search=redpajama), [LLaMA](https://huggingface.co/models?search=llama), [OPT](https://huggingface.co/models?search=opt), [GPT-Neo](https://huggingface.co/models?search=gpt-neo), [GPT-J](https://huggingface.co/models?search=gptj), and [Pythia](https://huggingface.co/models?search=pythia) models.

``` python
from open_flamingo import create_model_and_transforms

model, image_processor, tokenizer = create_model_and_transforms(
    clip_vision_encoder_path=""ViT-L-14"",
    clip_vision_encoder_pretrained=""openai"",
    lang_encoder_path=""anas-awadalla/mpt-1b-redpajama-200b"",
    tokenizer_path=""anas-awadalla/mpt-1b-redpajama-200b"",
    cross_attn_every_n_layers=1,
    cache_dir=""PATH/TO/CACHE/DIR""  # Defaults to ~/.cache
)
```

## Released OpenFlamingo models
We have trained the following OpenFlamingo models so far.

|# params|Language model|Vision encoder|Xattn interval*|COCO 4-shot CIDEr|VQAv2 4-shot Accuracy|Weights|
|------------|--------------|--------------|----------|-----------|-------|----|
|3B| anas-awadalla/mpt-1b-redpajama-200b | openai CLIP ViT-L/14 | 1 | 77.3 | 45.8 |[Link](https://huggingface.co/openflamingo/OpenFlamingo-3B-vitl-mpt1b)|
|3B| anas-awadalla/mpt-1b-redpajama-200b-dolly | openai CLIP ViT-L/14 | 1 | 82.7 | 45.7 |[Link](https://huggingface.co/openflamingo/OpenFlamingo-3B-vitl-mpt1b-langinstruct)|
|4B| togethercomputer/RedPajama-INCITE-Base-3B-v1 | openai CLIP ViT-L/14 | 2 | 81.8 | 49.0 | [Link](https://huggingface.co/openflamingo/OpenFlamingo-4B-vitl-rpj3b)|
|4B| togethercomputer/RedPajama-INCITE-Instruct-3B-v1 | openai CLIP ViT-L/14 | 2 | 85.8 | 49.0 | [Link](https://huggingface.co/openflamingo/OpenFlamingo-4B-vitl-rpj3b-langinstruct)|
|9B| anas-awadalla/mpt-7b | openai CLIP ViT-L/14 | 4 | 89.0 | 54.8 | [Link](https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b)|

*\* Xattn interval refers to the `--cross_attn_every_n_layers` argument.*

Note: as part of our v2 release, we have deprecated a previous LLaMA-based checkpoint. However, you can continue to use our older checkpoint using the new codebase.

## Downloading pretrained weights

To instantiate an OpenFlamingo model with one of our released weights, initialize the model as above and use the following code.

```python
# grab model checkpoint from huggingface hub
from huggingface_hub import hf_hub_download
import torch

checkpoint_path = hf_hub_download(""openflamingo/OpenFlamingo-3B-vitl-mpt1b"", ""checkpoint.pt"")
model.load_state_dict(torch.load(checkpoint_path), strict=False)
```

## Generating text
Below is an example of generating text conditioned on interleaved images/text. In particular, let's try few-shot image captioning.

``` python
from PIL import Image
import requests
import torch

""""""
Step 1: Load images
""""""
demo_image_one = Image.open(
    requests.get(
        ""http://images.cocodataset.org/val2017/000000039769.jpg"", stream=True
    ).raw
)

demo_image_two = Image.open(
    requests.get(
        ""http://images.cocodataset.org/test-stuff2017/000000028137.jpg"",
        stream=True
    ).raw
)

query_image = Image.open(
    requests.get(
        ""http://images.cocodataset.org/test-stuff2017/000000028352.jpg"", 
        stream=True
    ).raw
)


""""""
Step 2: Preprocessing images
Details: For OpenFlamingo, we expect the image to be a torch tensor of shape 
 batch_size x num_media x num_frames x channels x height x width. 
 In this case batch_size = 1, num_media = 3, num_frames = 1,
 channels = 3, height = 224, width = 224.
""""""
vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]
vision_x = torch.cat(vision_x, dim=0)
vision_x = vision_x.unsqueeze(1).unsqueeze(0)

""""""
Step 3: Preprocessing text
Details: In the text we expect an <image> special token to indicate where an image is.
 We also expect an <|endofchunk|> special token to indicate the end of the text 
 portion associated with an image.
""""""
tokenizer.padding_side = ""left"" # For generation padding tokens should be on the left
lang_x = tokenizer(
    [""<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of""],
    return_tensors=""pt"",
)


""""""
Step 4: Generate text
""""""
generated_text = model.generate(
    vision_x=vision_x,
    lang_x=lang_x[""input_ids""],
    attention_mask=lang_x[""attention_mask""],
    max_new_tokens=20,
    num_beams=3,
)

print(""Generated text: "", tokenizer.decode(generated_text[0]))
```

# Training
We provide training scripts in `open_flamingo/train`. We provide an example Slurm script in `open_flamingo/scripts/run_train.py`, as well as the following example command:
```
torchrun --nnodes=1 --nproc_per_node=4 open_flamingo/train/train.py \
  --lm_path anas-awadalla/mpt-1b-redpajama-200b \
  --tokenizer_path anas-awadalla/mpt-1b-redpajama-200b \
  --cross_attn_every_n_layers 1 \
  --dataset_resampled \
  --batch_size_mmc4 32 \
  --batch_size_laion 64 \
  --train_num_samples_mmc4 125000\
  --train_num_samples_laion 250000 \
  --loss_multiplier_laion 0.2 \
  --workers=4 \
  --run_name OpenFlamingo-3B-vitl-mpt1b \
  --num_epochs 480 \
  --warmup_steps  1875 \
  --mmc4_textsim_threshold 0.24 \
  --laion_shards ""/path/to/shards/shard-{0000..0999}.tar"" \
  --mmc4_shards ""/path/to/shards/shard-{0000..0999}.tar"" \
  --report_to_wandb
```

*Note: The MPT-1B [base](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)  and [instruct](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b-dolly) modeling code does not accept the `labels` kwarg or compute cross-entropy loss directly within `forward()`, as expected by our codebase. We suggest using a modified version of the MPT-1B models found [here](https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b) and [here](https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b-dolly).*

For more details, see our [training README](https://github.com/mlfoundations/open_flamingo/tree/main/open_flamingo/train).


# Evaluation
An example evaluation script is at `open_flamingo/scripts/run_eval.sh`. Please see our [evaluation README](https://github.com/mlfoundations/open_flamingo/tree/main/open_flamingo/eval) for more details.


To run evaluations on OKVQA you will need to run the following command:
```
import nltk
nltk.download('wordnet')
```


# Future plans
- [ ] Add support for video input

# Team

OpenFlamingo is developed by:

[Anas Awadalla*](https://anas-awadalla.streamlit.app/), [Irena Gao*](https://i-gao.github.io/), [Joshua Gardner](https://homes.cs.washington.edu/~jpgard/), [Jack Hessel](https://jmhessel.com/), [Yusuf Hanafy](https://www.linkedin.com/in/yusufhanafy/), [Wanrong Zhu](https://wanrong-zhu.com/), [Kalyani Marathe](https://sites.google.com/uw.edu/kalyanimarathe/home?authuser=0), [Yonatan Bitton](https://yonatanbitton.github.io/), [Samir Gadre](https://sagadre.github.io/), [Shiori Sagawa](https://cs.stanford.edu/~ssagawa/), [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ&hl=en), [Simon Kornblith](https://simonster.com/), [Pang Wei Koh](https://koh.pw/), [Gabriel Ilharco](https://gabrielilharco.com/), [Mitchell Wortsman](https://mitchellnw.github.io/), [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/).

The team is primarily from the University of Washington, Stanford, AI2, UCSB, and Google.

# Acknowledgments
This code is based on Lucidrains' [flamingo implementation](https://github.com/lucidrains/flamingo-pytorch) and David Hansmair's [flamingo-mini repo](https://github.com/dhansmair/flamingo-mini). Thank you for making your code public! We also thank the [OpenCLIP](https://github.com/mlfoundations/open_clip) team as we use their data loading code and take inspiration from their library design.

We would also like to thank [Jean-Baptiste Alayrac](https://www.jbalayrac.com) and [Antoine Miech](https://antoine77340.github.io) for their advice, [Rohan Taori](https://www.rohantaori.com/), [Nicholas Schiefer](https://nicholasschiefer.com/), [Deep Ganguli](https://hai.stanford.edu/people/deep-ganguli), [Thomas Liao](https://thomasliao.com/), [Tatsunori Hashimoto](https://thashim.github.io/), and [Nicholas Carlini](https://nicholas.carlini.com/) for their help with assessing the safety risks of our release, and to [Stability AI](https://stability.ai) for providing us with compute resources to train these models.

# Citing
If you found this repository useful, please consider citing:

```
@article{awadalla2023openflamingo,
  title={OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models},
  author={Anas Awadalla and Irena Gao and Josh Gardner and Jack Hessel and Yusuf Hanafy and Wanrong Zhu and Kalyani Marathe and Yonatan Bitton and Samir Gadre and Shiori Sagawa and Jenia Jitsev and Simon Kornblith and Pang Wei Koh and Gabriel Ilharco and Mitchell Wortsman and Ludwig Schmidt},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}
```

```
@software{anas_awadalla_2023_7733589,
  author = {Awadalla, Anas and Gao, Irena and Gardner, Joshua and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Jitsev, Jenia and Kornblith, Simon and Koh, Pang Wei and Ilharco, Gabriel and Wortsman, Mitchell and Schmidt, Ludwig},
  title = {OpenFlamingo},
  month        = mar,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.1.1},
  doi          = {10.5281/zenodo.7733589},
  url          = {https://doi.org/10.5281/zenodo.7733589}
}
```

```
@article{Alayrac2022FlamingoAV,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.14198}
}
```
","['anas-awadalla', 'jpgard', 'i-gao', 'mitchellnw', 'riotonzuk', 'yonatanbitton', 'chris-alexiuk-1', 'yassouali', 'isaac-chung', 'kalyani7195', 'triakshunn', 'loftusa', 'chris-alexiuk', 'DavidMChan', 'gabrielilharco', 'HUGHNew', 'siddk', 'VegB', 'ElegantLin']",1,,0.69,0,,,,"blank_issues_enabled: true
contact_links:
  - name: About the OpenFlamingo Project
    url: https://laion.ai/blog/open-flamingo-v2/
    about: General information about OpenFlamingo, privacy policy, and FAQ
  - name: Blank issue
    url: https://github.com/mlfoundations/open_flamingo/issues/new
    about: Please note that bug reports and feature requests should be used in most cases instead
",,48,,move-ucsb,
17144859,MDEwOlJlcG9zaXRvcnkxNzE0NDg1OQ==,python-sortedcontainers,grantjenks/python-sortedcontainers,0,grantjenks,https://github.com/grantjenks/python-sortedcontainers,"Python Sorted Container Types: Sorted List, Sorted Dict, and Sorted Set",0,2014-02-24 17:44:20+00:00,2025-03-06 13:04:21+00:00,2024-03-08 17:47:09+00:00,http://www.grantjenks.com/docs/sortedcontainers/,78483,3685,3685,Python,1,1,1,1,0,0,207,0,0,34,other,1,0,0,public,207,34,3685,master,1,,"Python Sorted Containers
========================

`Sorted Containers`_ is an Apache2 licensed `sorted collections library`_,
written in pure-Python, and fast as C-extensions.

Python's standard library is great until you need a sorted collections
type. Many will attest that you can get really far without one, but the moment
you **really need** a sorted list, sorted dict, or sorted set, you're faced
with a dozen different implementations, most using C-extensions without great
documentation and benchmarking.

In Python, we can do better. And we can do it in pure-Python!

.. code-block:: python

    >>> from sortedcontainers import SortedList
    >>> sl = SortedList(['e', 'a', 'c', 'd', 'b'])
    >>> sl
    SortedList(['a', 'b', 'c', 'd', 'e'])
    >>> sl *= 10_000_000
    >>> sl.count('c')
    10000000
    >>> sl[-3:]
    ['e', 'e', 'e']
    >>> from sortedcontainers import SortedDict
    >>> sd = SortedDict({'c': -3, 'a': 1, 'b': 2})
    >>> sd
    SortedDict({'a': 1, 'b': 2, 'c': -3})
    >>> sd.popitem(index=-1)
    ('c', -3)
    >>> from sortedcontainers import SortedSet
    >>> ss = SortedSet('abracadabra')
    >>> ss
    SortedSet(['a', 'b', 'c', 'd', 'r'])
    >>> ss.bisect_left('c')
    2

All of the operations shown above run in faster than linear time. The above
demo also takes nearly a gigabyte of memory to run. When the sorted list is
multiplied by ten million, it stores ten million references to each of ""a""
through ""e"". Each reference requires eight bytes in the sorted
container. That's pretty hard to beat as it's the cost of a pointer to each
object. It's also 66% less overhead than a typical binary tree implementation
(e.g. Red-Black Tree, AVL-Tree, AA-Tree, Splay-Tree, Treap, etc.) for which
every node must also store two pointers to children nodes.

`Sorted Containers`_ takes all of the work out of Python sorted collections -
making your deployment and use of Python easy. There's no need to install a C
compiler or pre-build and distribute custom extensions. Performance is a
feature and testing has 100% coverage with unit tests and hours of stress.

.. _`Sorted Containers`: http://www.grantjenks.com/docs/sortedcontainers/
.. _`sorted collections library`: http://www.grantjenks.com/docs/sortedcontainers/

Testimonials
------------

**Alex Martelli**, `Fellow of the Python Software Foundation`_

""Good stuff! ... I like the `simple, effective implementation`_ idea of
splitting the sorted containers into smaller ""fragments"" to avoid the O(N)
insertion costs.""

**Jeff Knupp**, `author of Writing Idiomatic Python and Python Trainer`_

""That last part, ""fast as C-extensions,"" was difficult to believe. I would need
some sort of `Performance Comparison`_ to be convinced this is true. The author
includes this in the docs. It is.""

**Kevin Samuel**, `Python and Django Trainer`_

I'm quite amazed, not just by the code quality (it's incredibly readable and
has more comment than code, wow), but the actual amount of work you put at
stuff that is *not* code: documentation, benchmarking, implementation
explanations. Even the git log is clean and the unit tests run out of the box
on Python 2 and 3.

**Mark Summerfield**, a short plea for `Python Sorted Collections`_

Python's ""batteries included"" standard library seems to have a battery
missing. And the argument that ""we never had it before"" has worn thin. It is
time that Python offered a full range of collection classes out of the box,
including sorted ones.

`Sorted Containers`_ is used in popular open source projects such as:
`Zipline`_, an algorithmic trading library from Quantopian; `Angr`_, a binary
analysis platform from UC Santa Barbara; `Trio`_, an async I/O library; and
`Dask Distributed`_, a distributed computation library supported by Continuum
Analytics.

.. _`Fellow of the Python Software Foundation`: https://en.wikipedia.org/wiki/Alex_Martelli
.. _`simple, effective implementation`: http://www.grantjenks.com/docs/sortedcontainers/implementation.html
.. _`author of Writing Idiomatic Python and Python Trainer`: https://jeffknupp.com/
.. _`Python and Django Trainer`: https://www.elephorm.com/formateur/kevin-samuel
.. _`Python Sorted Collections`: http://www.qtrac.eu/pysorted.html
.. _`Zipline`: https://github.com/quantopian/zipline
.. _`Angr`: https://github.com/angr/angr
.. _`Trio`: https://github.com/python-trio/trio
.. _`Dask Distributed`: https://github.com/dask/distributed

Features
--------

- Pure-Python
- Fully documented
- Benchmark comparison (alternatives, runtimes, load-factors)
- 100% test coverage
- Hours of stress testing
- Performance matters (often faster than C implementations)
- Compatible API (nearly identical to older blist and bintrees modules)
- Feature-rich (e.g. get the five largest keys in a sorted dict: d.keys()[-5:])
- Pragmatic design (e.g. SortedSet is a Python set with a SortedList index)
- Developed on Python 3.10
- Tested with CPython 3.7, 3.8, 3.9, 3.10, 3.11, 3.12 and PyPy3
- Tested on Linux, Mac OSX, and Windows

.. image:: https://github.com/grantjenks/python-sortedcontainers/workflows/integration/badge.svg
   :target: http://www.grantjenks.com/docs/sortedcontainers/

.. image:: https://github.com/grantjenks/python-sortedcontainers/workflows/release/badge.svg
   :target: http://www.grantjenks.com/docs/sortedcontainers/

Quickstart
----------

Installing `Sorted Containers`_ is simple with `pip
<https://pypi.org/project/pip/>`_::

    $ pip install sortedcontainers

You can access documentation in the interpreter with Python's built-in `help`
function. The `help` works on modules, classes and methods in `Sorted
Containers`_.

.. code-block:: python

    >>> import sortedcontainers
    >>> help(sortedcontainers)
    >>> from sortedcontainers import SortedDict
    >>> help(SortedDict)
    >>> help(SortedDict.popitem)

Documentation
-------------

Complete documentation for `Sorted Containers`_ is available at
http://www.grantjenks.com/docs/sortedcontainers/

User Guide
..........

The user guide provides an introduction to `Sorted Containers`_ and extensive
performance comparisons and analysis.

- `Introduction`_
- `Performance Comparison`_
- `Load Factor Performance Comparison`_
- `Runtime Performance Comparison`_
- `Simulated Workload Performance Comparison`_
- `Performance at Scale`_

.. _`Introduction`: http://www.grantjenks.com/docs/sortedcontainers/introduction.html
.. _`Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance.html
.. _`Load Factor Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance-load.html
.. _`Runtime Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance-runtime.html
.. _`Simulated Workload Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance-workload.html
.. _`Performance at Scale`: http://www.grantjenks.com/docs/sortedcontainers/performance-scale.html

Community Guide
...............

The community guide provides information on the development of `Sorted
Containers`_ along with support, implementation, and history details.

- `Development and Support`_
- `Implementation Details`_
- `Release History`_

.. _`Development and Support`: http://www.grantjenks.com/docs/sortedcontainers/development.html
.. _`Implementation Details`: http://www.grantjenks.com/docs/sortedcontainers/implementation.html
.. _`Release History`: http://www.grantjenks.com/docs/sortedcontainers/history.html

API Documentation
.................

The API documentation provides information on specific functions, classes, and
modules in the `Sorted Containers`_ package.

- `Sorted List`_
- `Sorted Dict`_
- `Sorted Set`_

.. _`Sorted List`: http://www.grantjenks.com/docs/sortedcontainers/sortedlist.html
.. _`Sorted Dict`: http://www.grantjenks.com/docs/sortedcontainers/sorteddict.html
.. _`Sorted Set`: http://www.grantjenks.com/docs/sortedcontainers/sortedset.html

Talks
-----

- `Python Sorted Collections | PyCon 2016 Talk`_
- `SF Python Holiday Party 2015 Lightning Talk`_
- `DjangoCon 2015 Lightning Talk`_

.. _`Python Sorted Collections | PyCon 2016 Talk`: http://www.grantjenks.com/docs/sortedcontainers/pycon-2016-talk.html
.. _`SF Python Holiday Party 2015 Lightning Talk`: http://www.grantjenks.com/docs/sortedcontainers/sf-python-2015-lightning-talk.html
.. _`DjangoCon 2015 Lightning Talk`: http://www.grantjenks.com/docs/sortedcontainers/djangocon-2015-lightning-talk.html

Resources
---------

- `Sorted Containers Documentation`_
- `Sorted Containers at PyPI`_
- `Sorted Containers at Github`_
- `Sorted Containers Issue Tracker`_

.. _`Sorted Containers Documentation`: http://www.grantjenks.com/docs/sortedcontainers/
.. _`Sorted Containers at PyPI`: https://pypi.org/project/sortedcontainers/
.. _`Sorted Containers at Github`: https://github.com/grantjenks/python-sortedcontainers
.. _`Sorted Containers Issue Tracker`: https://github.com/grantjenks/python-sortedcontainers/issues

Sorted Containers License
-------------------------

Copyright 2014-2024 Grant Jenks

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
","['grantjenks', 'medecau', 'belm0', 'jonathaneunice', 'timgates42', 'adamchainz', 'asottile', 'felixonmars', 'hugovk', 'jakob-keller', 'jwilk', 'Muon', 'mhils', 'nathanielobrown', 'Wilfred', 'wbolster', 'Asayu123', 'bamartin125', 'eukaryote', 'ismailmaj', 'tortarino']",0,,0.71,0,,,,,,35,,CARDAMOM-framework,
72790118,MDEwOlJlcG9zaXRvcnk3Mjc5MDExOA==,awesome-hacking,jekil/awesome-hacking,0,jekil,https://github.com/jekil/awesome-hacking,Awesome hacking is an awesome collection of hacking tools.,0,2016-11-03 21:51:08+00:00,2025-03-05 22:46:36+00:00,2024-07-02 09:43:40+00:00,https://awesomehacking.org,1650,3190,3190,Python,1,1,1,0,1,0,576,0,0,2,,1,0,0,public,576,2,3190,master,1,,,"['jekil', 'gavynriebau', 'fabacab', 'vlad-s', 'ceeac']",0,,0.73,0,,,,,,144,,ucsbieee,
90966138,MDEwOlJlcG9zaXRvcnk5MDk2NjEzOA==,medical-imaging-datasets,sfikas/medical-imaging-datasets,0,sfikas,https://github.com/sfikas/medical-imaging-datasets,A list of Medical imaging datasets.,0,2017-05-11 10:14:19+00:00,2025-03-06 10:10:39+00:00,2024-09-12 13:11:08+00:00,,47,2350,2350,,1,1,1,1,0,0,422,0,0,1,,1,0,0,public,422,1,2350,master,1,,"# medical-imaging-datasets

* A list of Medical imaging datasets. Source : https://sites.google.com/site/aacruzr/image-datasets
* An additional, possibly overlapping list can be found at : https://github.com/beamandrew/medical-data

### Multimodal databases

* Center for Invivo Microscopy (CIVM), Embrionic and Neonatal Mouse (H&E, MR) http://www.civm.duhs.duke.edu/devatlas/ 
user guide: http://www.civm.duhs.duke.edu/devatlas/UserGuide.pdf
* LONI image data archive https://ida.loni.usc.edu/services/Menu/IdaData.jsp?project=
* Radiology (Ultrasound, Mammographs, X-Ray, CT, MRI, fMRI, etc.)
* Collaborative Informatics and Neuroimaging Suite (COINS) https://portal.mrn.org/micis/index.php?subsite=dx
* The Cancer Imaging Archive (TCIA) http://www.cancerimagingarchive.net/ (Collections)
* Alzheimer’s Disease Neuroimaging Initiative (ADNI) http://adni.loni.ucla.edu/
* The Open Access Series of Imaging Studies (OASIS) http://www.oasis-brains.org/
* Breast Cancer Digital Repository https://bcdr.eu/
* DDSM: Digital Database for Screening Mammography http://marathon.csee.usf.edu/Mammography/Database.html
* The Mammographic Image Analysis Society (MIAS) mini-database http://peipa.essex.ac.uk/info/mias.html
* Mammography Image Databases 100 or more images of mammograms with ground truth. Additional images available by request, and links to several other mammography databases are provided http://marathon.csee.usf.edu/Mammography/Database.html
* NLM HyperDoc Visible Human Project color, CAT and MRI image samples - over 30 images http://www.nlm.nih.gov/research/visible/visible_human.html
* CT Scans for Colon Cancer https://wiki.cancerimagingarchive.net/display/Public/CT+COLONOGRAPHY#e88604ec5c654f60a897fa77906f88a6
* [[BreastScreening](http://breastscreening.github.io/)] UTA4: Breast Cancer Medical Imaging DICOM Files Dataset & Resources (MG, US and MRI) https://github.com/MIMBCD-UI/dataset-uta4-dicom
* [[MIMBCD-UI](http://mimbcd-ui.github.io/)] UTA7: Breast Cancer Medical Imaging DICOM Files Dataset & Resources (MG, US and MRI) https://github.com/MIMBCD-UI/dataset-uta7-dicom
* [[Facebook AI + NYU FastMRI](https://fastmri.org/dataset/)] includes two types of MRI scans: knee MRIs and the brain (neuro) MRIs, containing training, validation, and masked test sets. Also includes PyTorch data loaders in open-sourced [GitHub Repository](https://github.com/facebookresearch/fastMRI/)
* BCNB: Early Breast Cancer Core-Needle Biopsy WSI Dataset, https://bupt-ai-cz.github.io/BCNB/, https://github.com/bupt-ai-cz/BALNMP#bcnb-dataset
* National Cancer Institute Imaging Data Commons (IDC) https://portal.imaging.datacommons.cancer.gov/explore/

### Histology, Histopathology (H&E, IHQ) & Microscopy (Cell, Cytology, Biology, Protein, Molecular, Fluorescence)

* Pap Smear image database #1 (""SIPAKMed"") https://www.cse.uoi.gr/~marina/sipakmed.html
* Pap Smear image database #2 https://www.cs.uoi.gr/~marina/data_set_TITB.zip
* Pap Smear image database #3 http://mde-lab.aegean.gr/index.php/downloads
* The Cancer Genome Atlas (TCGA) http://cancergenome.nih.gov/ https://tcga-data.nci.nih.gov/tcga/
* International Cancer Genome Consortium http://icgc.org, (Data portal) http://dcc.icgc.org/
* Stanford Tissue Microarray Database (TMA) http://tma.im
* MITOS dataset http://www.ipal.cnrs.fr/event/icpr-2012
* DPA’s Whole Slide Imaging Repository https://digitalpathologyassociation.org/whole-slide-imaging-repository
* ITK Analysis of Large Histology Datasets http://www.na-mic.org/Wiki/index.php/ITK_Analysis_of_Large_Histology_Datasets
* Histology Photo Album http://www.histology-world.com/photoalbum/thumbnails.php?album=52
* Slide Library of Virtual pathology, University of Leeds http://www.virtualpathology.leeds.ac.uk/
* BDGP images from the FlyExpress database www.flyexpress.net
* The UCSB Bio-Segmentation Benchmark dataset http://www.bioimage.ucsb.edu/research/biosegmentation
* Histology (CIMA) dataset http://cmp.felk.cvut.cz/~borovji3/?page=dataset
* ANHIR dataset https://anhir.grand-challenge.org/
* Genome RNAi dataset http://www.genomernai.org/
* Chinese Hamster Ovary cells (CHO) dataset http://www.chogenome.org/data.html
* Allen Brain Atlas http://www.brain-map.org/
* 1000 Functional Connectomes Project http://fcon_1000.projects.nitrc.org/
* The Cell Centered Database (CCDB) https://library.ucsd.edu/dc/collection/bb5940732k
* The Encyclopedia of DNA Elements (ENCODE) http://genome.ucsc.edu/ENCODE/ 
user guide: http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1001046
* The Human Protein Atlas: http://www.proteinatlas.org/
* El Salvador Atlas of Gastrointestinal VideoEndoscopy Images and Videos of hi-res of studies taken from Gastrointestinal Video endoscopy http://www.gastrointestinalatlas.com/
* BCNB: Early Breast Cancer Core-Needle Biopsy WSI Dataset, https://bupt-ai-cz.github.io/BCNB/, https://github.com/bupt-ai-cz/BALNMP#bcnb-dataset
* BCI: Breast Cancer Immunohistochemical Image Generation Dataset, https://bupt-ai-cz.github.io/BCI/, https://github.com/bupt-ai-cz/BCI

### Databases you can use for benchmarking 

* http://peipa.essex.ac.uk/benchmark/databases/
* http://mulan.sourceforge.net/datasets-mlc.html
* Datasets reporting formats for pathologists http://www.rcpath.org/publications-media/publications/datasets

### State of the art / Challenges

* Grand Challenges in Medical Image Analysis https://grand-challenge.org/
* Challenges in global health and development problems https://grandchallenges.org/#/map
* Current state of the art of most used computer vision datasets: Who is the best at X? http://rodrigob.github.io/are_we_there_yet/build/
* Automatic Non-rigid Histological Image Registration (ANHIR) challenge https://anhir.grand-challenge.org/
","['sfikas', 'camlloyd', 'federikovi', 'super233', 'FMCalisto', 'fedorov', 'gabrielziegler3', 'Borda', 'radroof22', 'SantJay']",0,,0.73,0,,,,,,60,,geometric-intelligence,
196211092,MDEwOlJlcG9zaXRvcnkxOTYyMTEwOTI=,fun-with-computer-graphics,zheng95z/fun-with-computer-graphics,0,zheng95z,https://github.com/zheng95z/fun-with-computer-graphics,"This is a collection of computer graphics related courses, books, tutorials, articles, blogs, resources, researcher homepages, lab homepages, video channels, open source projects, websites, etc.",0,2019-07-10 13:32:46+00:00,2025-03-04 20:22:00+00:00,2022-03-31 17:36:14+00:00,,78,2296,2296,,0,0,1,0,0,1,308,0,0,0,cc0-1.0,1,0,0,public,308,0,2296,master,1,,"<h1 align=""center"">fun-with-computer-graphics</h1>
<h5 align=""center"">Hi, you have just found fun-with-computer-graphics!</h5>

This is a collection of computer graphics related courses, books, tutorials, articles, blogs, resources, researcher homepages, lab homepages, video channels, open source projects, websites, etc.

Here are some useful tips for you to find what you want:

- You can now find the autogenerate table of contents in the top left corner (provided by GitHub).

## Computer Graphics

### Open-source Renderers

| Open-source Renderers                                        |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [mmp](https://github.com/mmp)/**[pbrt-v3](https://github.com/mmp/pbrt-v3)** | The renderer described in the third edition of ""Physically Based Rendering: From Theory To Implementation"", by Matt Pharr, Wenzel Jakob, and Greg Humphreys. |
| [mmp](https://github.com/mmp)/**[pbrt-v4](https://github.com/mmp/pbrt-v4)** | This is an early release of pbrt-v4, the rendering system that will be described in the (eventually) forthcoming fourth edition of *Physically Based Rendering: From Theory to Implementation*. |
| [mitsuba-renderer](https://github.com/mitsuba-renderer)/**[mitsuba](https://github.com/mitsuba-renderer/mitsuba)** | Mitsuba is a research-oriented rendering system in the style of PBRT, from which it derives much inspiration. |
| [mitsuba-renderer](https://github.com/mitsuba-renderer)/**[mitsuba2](https://github.com/mitsuba-renderer/mitsuba2)** | Mitsuba 2 is a research-oriented rendering system written in portable C++17. |
| [shiinamiyuki](https://github.com/shiinamiyuki)/**[AkariRender](https://github.com/shiinamiyuki/AkariRender)** | AkariRender is a highly modular CPU/GPU physically based renderer written in C++17. |
| [AirGuanZ](https://github.com/AirGuanZ)/**[Atrc](https://github.com/AirGuanZ/Atrc)** | Offline rendering lab based on ray tracing.                  |
| [Mike-Leo-Smith](https://github.com/Mike-Leo-Smith)/**[LuisaRender](https://github.com/Mike-Leo-Smith/LuisaRender)** | High-Performance Renderer on GPU.                            |
| [neverfelly](https://github.com/neverfelly)/**[misaki-render](https://github.com/neverfelly/misaki-render)** | A modular physically-based photorealistic global illumination renderer. |
| [FaithZL](https://github.com/FaithZL)/**[Paladin](https://github.com/FaithZL/Paladin)** |                                                              |
| [shiinamiyuki](https://github.com/shiinamiyuki)/**[minpt](https://github.com/shiinamiyuki/minpt)** | Small yet (almost) complete modern path tracer.              |
| [JiayinCao](https://github.com/JiayinCao)/**[SORT](https://github.com/JiayinCao/SORT)** | Simple Open-source Ray Tracer.                               |
| [g1n0st](https://github.com/g1n0st)/**[AyaRay](https://github.com/g1n0st/AyaRay)** | A Modern C++ Windows-platform physically based renderer developing by Chang Yu. |
| [tunabrain](https://github.com/tunabrain)/**[tungsten](https://github.com/tunabrain/tungsten)** | High performance physically based renderer in C++11.         |
| [google](https://github.com/google)/**[filament](https://github.com/google/filament)** | Filament is a real-time physically-based renderer written in C++. It is mobile-first, but also multi-platform. |
| [appleseedhq](https://github.com/appleseedhq)/**[appleseed](https://github.com/appleseedhq/appleseed)** | A modern open source rendering engine for animation and visual effects. |
| [jbikker](https://github.com/jbikker)/**[lighthouse2](https://github.com/jbikker/lighthouse2)** | Lighthouse 2 framework for real-time ray tracing.            |
| [harskish](https://github.com/harskish)/**[fluctus](https://github.com/harskish/fluctus)** | An interactive OpenCL wavefront path tracer.                 |

### Courses 

| Courses                                                      |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [GAMES101: 现代计算机图形学入门](https://sites.cs.ucsb.edu/~lingqi/teaching/games101.html)<br>([bilibili](https://www.bilibili.com/video/av90798049)) | **GAMES101**<br>本课程将全面而系统地介绍现代计算机图形学的四大组成部分：（1）光栅化成像，（2）几何表示，（3）光的传播理论，以及（4）动画与模拟。每个方面都会从基础原理出发讲解到实际应用，并介绍前沿的理论研究。通过本课程，你可以学习到计算机图形学背后的数学和物理知识，并锻炼实际的编程能力。<br>Taught by [@Lingqi Yan](https://sites.cs.ucsb.edu/~lingqi/). |
| [GAMES202: 高质量实时渲染](https://sites.cs.ucsb.edu/~lingqi/teaching/games202.html)<br/>([bilibili](https://www.bilibili.com/video/BV1YK4y1T7yY)) | **GAMES202**<br/>本课程将全面地介绍现代实时渲染中的关键问题与解决方法。由于实时渲染 (>30 FPS) 对速度要求极高，因此本课程的关注点将是在苛刻的时间限制下，人们如何打破速度与质量之间的权衡，同时保证实时的高速度与照片级的真实感。<br/>Taught by [@Lingqi Yan](https://sites.cs.ucsb.edu/~lingqi/). |
| [Rendering Algorithms (Fall21)](https://cs87-dartmouth.github.io/Fall2021/) | **Dartmouth**<br>This class focuses on advanced 3D graphics techniques for realistic image synthesis. You will learn how light interacts with objects in the real world, and how to translate the underlying math and physics into practical algorithms for rendering photorealistic images.<br>Taught by [@Wojciech Jarosz](https://cs.dartmouth.edu/~wjarosz/?tdsourcetag=s_pctim_aiomsg). |
| [Introduction to Computer Graphics](https://sites.cs.ucsb.edu/~lingqi/teaching/cs180.html) | **UCSB CS180**<br>This course is an introduction to the foundations of three-dimensional computer graphics. Topics covered include 2D and 3D transformations, Rasterization based interactive 3D graphics, shading and reflectance models, texture mapping, geometric modeling using Bézier and B-Spline curves, ray tracing, and animation. There will be an emphasis on both the mathematical and geometric aspects of graphics, as well as the ability to write fully functional 3D graphics programs.<br>Taught by [@Lingqi Yan](https://sites.cs.ucsb.edu/~lingqi/). |
| [Real-Time High Quality Rendering](http://www.cs.ucsb.edu/~lingqi/teaching/cs291a.html) | **UCSB CS291A**<br>In this course, we will review the history and some of the recent ideas that seek to bridge the gap between realism and interactivity. We will focus on the use of complex lighting and shading within limited computation time. Specifically, topics will cover programmable shaders, real-time shadows, interactive global illumination, image-based rendering, precomputed rendering, adaptive sampling and reconstruction, and real-time ray tracing.<br/>Taught by [@Lingqi Yan](https://sites.cs.ucsb.edu/~lingqi/). |
| [Introduction to Offline Rendering](https://sites.cs.ucsb.edu/~lingqi/teaching/cs190I.html) | **UCSB CS190I**<br>This course will teach you everything about offline rendering, so you will be able to write a fully functional industry-level renderer (such as Disney's Hyperion and Pixar's RenderMan) that produces stunning graphics. Topics in this course will cover the physics of light, the rendering equation, Monte Carlo integration, path tracing, physically-based reflectance models, participating media, other advanced light transport methods, production rendering approaches, and so on.<br>Taught by [@Lingqi Yan](https://sites.cs.ucsb.edu/~lingqi/). |
| [TU Wien Rendering/Ray Tracing Course](https://www.cg.tuwien.ac.at/courses/Rendering/VU.SS2018.html)<br>([YouTube](https://www.youtube.com/watch?v=pjc1QAI6zS0&list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi)) | **TU WIEN Rendering**<br>This course aims to give an overview of basic and state-of-the-art methods of rendering. Offline methods such as ray and path tracing, photon mapping and many other algorithms are introduced and various refinement are explained.<br>Taught by [@Károly Zsolnai-Fehér](https://users.cg.tuwien.ac.at/zsolnai/about/). |
| [Computer Graphics (Fall18)](https://canvas.dartmouth.edu/courses/30008) | **Dartmouth**<br/>This course provides a broad introduction to the mathematical and programmatic foundations of computer graphics, including modeling, rendering (drawing), and animating three-dimensional scenes.<br/>Taught by [@Wojciech Jarosz](https://cs.dartmouth.edu/~wjarosz/?tdsourcetag=s_pctim_aiomsg). |
| [Introduction to Computer Graphics and Imaging](https://web.stanford.edu/class/cs148/index.html) | **Stanford CS148**<br>This is the introductory prerequisite course in the computer graphics sequence which introduces students to the technical concepts behind creating synthetic computer generated images. |
| [Interactive Computer Graphics](http://cs248.stanford.edu/)  | **Stanford CS248**<br/>This course provides a comprehensive introduction to computer graphics, focusing on fundamental concepts and techniques, as well as their cross-cutting relationship to multiple problem domains in interactive graphics (such as rendering, animation, geometry, image processing). |
| [Image Synthesis Techniques](http://graphics.stanford.edu/courses/cs348b/) | **Stanford CS348b**<br>This course provides a broad overview of the theory and practice of making photo-realistic imagery. Rendering is treated as a problem in modeling and simulating the physics of light and appearance. |
| [Computer Graphics](http://15462.courses.cs.cmu.edu)         | **CMU 15-462/662**<br>This course provides a comprehensive introduction to computer graphics. Focuses on fundamental concepts and techniques, and their cross-cutting relationship to multiple problem domains in graphics (rendering, animation, geometry, imaging). |
| [Computer Graphics - AS 19](https://cgl.ethz.ch/teaching/cg19/home.php) | **ETH**<br>This course covers some of the fundamental concepts of modern computer graphics. The main topics of the course are modeling and rendering. During the course, we will discuss how digital 3D scenes are represented and modeled, and how a realistic image can be generated from a digital representation of a 3D scene.<br>Taught by [Computer Graphics Laboratory (CGL)](https://cgl.ethz.ch/teaching/cg19/home.php). |
| [Physically-based Simulation - AS 19](https://cgl.ethz.ch/teaching/simulation19/home.php) | **ETH**<br>Physically-based simulations are fundamental to many applications of computer graphics, including 3D video games, animated movies and films, or virtual surgery. This course introduces the physical concepts as well as the numerical methods required for simulating deformable objects, fluids, rigid bodies, and other physical systems. The material covered in this lecture ranges from simple mass-spring systems to advanced topics such as finite elements.<br>Taught by [Computer Graphics Laboratory (CGL)](https://cgl.ethz.ch/teaching/cg19/home.php). |
| [COS 426 Computer Graphics (Spring19)](https://www.cs.princeton.edu/courses/archive/spring19/cos426/index.php) | **Princeton COS 426**<br>This course will study topics in computer graphics, covering methods in image processing, modeling, rendering, and animation. |
| [COS 526 Advanced Computer Graphics (Fall16)](https://www.cs.princeton.edu/courses/archive/fall16/cos526/index.php) | **Princeton COS 526**<br>This course will study advanced topics in computer graphics, covering methods in computational photography, geometric modeling, photorealistic rendering, and other topics in computer graphics. |
| [Physically Based Rendering and Material Appearance Modelling](http://courses.compute.dtu.dk/02941/) | **DTU 02941**<br>This course takes its outset in the appearance of real world materials. The goal is to get as close as possible to replicating the appearance of real materials by computer graphical rendering based on mathematical/physical models. |
| [Interactive 3D Graphics by Autodesk](https://www.udacity.com/course/interactive-3d-graphics--cs291) | **Udacity CS291**<br>This class will teach you about the basic principles of 3D computer graphics: meshes, transforms, cameras, materials, lighting, and animation.<br>Taught by [@Eric Haines](https://erich.realtimerendering.com/) |
| [Computer Graphics and Imaging](https://cs184.eecs.berkeley.edu/sp19) | **Berkeley cs184/284a**<br>This course provides a broad introduction to the fundamentals of computer graphics. The main areas covered are modeling, rendering, animation and imaging. Topics include 2D and 3D transformations, drawing to raster displays, sampling, texturing, antialiasing, geometric modeling, ray tracing and global illumination, animation, cameras, image processing and computational imaging. There will be an emphasis on mathematical and geometric aspects of graphics, and the ability to write complete 3D graphics programs. |
| [Introduction To Computer Graphics](https://cs.brown.edu/courses/cs123/index.shtml) | **Brown CS123**<br>This course offers an in-depth exploration of fundamental concepts in 2D and 3D computer graphics. It introduces 2D raster graphics techniques, including scan conversion, simple image processing, interaction techniques and user interface design. The bulk of the course is devoted to 3D modeling, geometric transformations, and 3D viewing and rendering. A sequence of assignments culminates in a simple geometric modeler and ray tracer. C++ and the graphics library OGL are used throughout the course, as is shader programming on the GPU, taught from the first lab onwards. The final project is typically a small group project spec'd and implemented by the group using shaders to create special effects. |
| [Introduction To Computer Graphics](http://www.cs.cornell.edu/courses/cs4620/2019fa/) | **Cornell CS4620**<br>The study of creating manipulating, and using visual images in the computer. |

### SIGGRAPH Courses

| SIGGRAPH Courses                                             |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Path tracing in production](https://jo.dreggn.org/path-tracing-in-production/) | This is the web page for the SIGGRAPH courses on path tracing in production. |

### Books

| Books                                                        |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Fundamentals of Computer Graphics, 4th Edition               | Drawing on an impressive roster of experts in the field, **Fundamentals of Computer Graphics, Fifth Edition** offers an ideal resource for computer course curricula as well as a user-friendly personal or professional reference. |
| Computer Graphics: Principles and Practice, 3rd Edition<br>([book website](cgpp.net)) | In this book, we explain the principles, as well as the mathematics, underlying computer graphics--knowledge that is essential for successful work both now and in the future. Early chapters show how to create 2D and 3D pictures right away, supporting experimentation. Later chapters, covering a broad range of topics, demonstrate more sophisticated approaches. Sections on current computer graphics practice show how to apply given principles in common situations, such as how to approximate an ideal solution on available hardware, or how to represent a data structure more efficiently. Topics are reinforced by exercises, programming problems, and hands-on projects. |
| Real-Time Rendering, 4th Edition<br>([book website](http://www.realtimerendering.com/)) | This edition discusses current, practical rendering methods used in games and other applications. It also presents a solid theoretical framework and relevant mathematics for the field of interactive computer graphics, all in an approachable style. New to this edition: new chapter on VR and AR as well as expanded coverage of Visual Appearance, Advanced Shading, Global Illumination, and Curves and Curved Surfaces. |
| Physically Based Rendering: From Theory To Implementation, Third Edition<br>([read for free](http://www.pbr-book.org/)) | *Physically Based Rendering* describes both the mathematical theory behind a modern photorealistic rendering system as well as its practical implementation. A method known as “literate programming” combines human-readable documentation and source code into a single reference that is specifically designed to aid comprehension. The ideas and software in this book show the reader how to design and employ a full-featured rendering system capable of creating stunning imagery. |
| Robust Monte Carlo Methods for Light Transport Simulation<br>([thesis website](https://graphics.stanford.edu/papers/veach_thesis/)) | Eric Veach. PhD Thesis, Stanford University, 1997<br>Nearly 20 years later, this monster thesis is *still* relevant when it comes to developing rendering algorithms. Introduces Monte Carlo rendering methods, multiple importance sampling, bidirectional path tracing, Metropolis Light Transport |
| Advanced Global Illumination<br>([authors' site](http://graphics.cs.kuleuven.be/publications/AGI2E/index.html), [Google Books sample](https://books.google.com/books?id=TB1jDAAAQBAJ&printsec=frontcover)) | This book provides the reader with a fundamental understanding of global illumination algorithms. It discusses a broad class of algorithms for realistic image synthesis and introduces a theoretical basis for the algorithms presented. |
| 《Ray Tracing in One Weekend》series <br>([read for free](https://raytracing.github.io/)) | [Peter Shirley](https://twitter.com/Peter_shirley)'s *The Ray Tracing in One Weekend* series of books |
| 《Ray Tracing Gems》<br>([book website](http://raytracinggems.com/)) | This book is a collection of articles focused on ray tracing techniques for serious practitioners. Like other ""gems"" books, it focuses on subjects commonly considered too advanced for introductory texts, yet rarely addressed by research papers. |

### Tutorials

| Tutorials                                                    |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Nori 2](https://wjakob.github.io/nori/)                     | Nori is a minimalistic ray tracer written in C++. It runs on Windows, Linux, and Mac OS and provides a foundation for the homework assignments in the course [Advanced Computer Graphics](http://rgl.epfl.ch/courses/ACG17) taught at EPFL. |
| [darts](https://cs87-dartmouth.github.io/Fall2021/darts-overview.html) | darts is a minimalistic skeleton for a Monte Carlo ray tracer, written in C++17. The name is an acronym for *The Dartmouth Academic Ray Tracer Skeleton* while also being a nod to the random dart-throwing-like process involved in Monte Carlo ray tracing. It runs on recent versions of Windows, Linux, and macOS and provides the foundation for the programming assignments we'll be doing in this class. |
| [The Graphics Codex](http://graphicscodex.com/)              | by [Morgan McGuire](https://twitter.com/CasualEffects)       |
| [Scratchapixel 2.0](http://www.scratchapixel.com/index.php)  | 32 lessons, 166 chapters, 450,000 words, C++ source code     |
| [Windows渲染引擎入门](https://www.zhihu.com/column/c_1465096004047822849) | by [MaxwellGeng](https://www.zhihu.com/people/maxwellgeng)   |
| [Vulkan 渲染器开发实战小师班](https://zhuanlan.zhihu.com/p/478021889) | by [SaeruHikari](https://www.zhihu.com/people/SaeruHikari)   |
| [Learn Vulkan](https://learnvulkan.com/book/)                |                                                              |
| [Vulkan Tutorial](https://vulkan-tutorial.com/)              |                                                              |
| Learn OpenGL<br>([English](https://learnopengl.com/), [Chinese](https://learnopengl-cn.github.io/)) | The aim of LearnOpenGL is to show you all there is to modern OpenGL in an easy-to-understand fashion with clear examples, while also providing a useful reference for later studies. |
| [Joey de Vries OpenGL Tutorials](http://learnopengl.com/)    | OpenGL                                                       |
| [Pixar in a Box: Rendering](https://www.khanacademy.org/partner-content/pixar/rendering) | by Pixar                                                     |
| [The Book of Shaders](https://thebookofshaders.com/)         |                                                              |
| [3d-game-shaders-for-beginners](https://github.com/lettier/3d-game-shaders-for-beginners) |                                                              |
| [Daily Pathtracer](http://aras-p.info/blog/2018/03/28/Daily-Pathtracer-Part-0-Intro/) | by [Aras Pranckevičius](http://aras-p.info/)                 |
| [Tiny renderer or how OpenGL works](https://github.com/ssloy/tinyrenderer/wiki) | software rendering in 500 lines of code                      |
| [Rasterization in One Weekend](https://tayfunkayhan.wordpress.com/2018/11/24/rasterization-in-one-weekend/) | by [Tayfun Kayhan](https://tayfunkayhan.wordpress.com/)      |

### Articles

| Articles                                                     |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [什么是计算机图形学](http://staff.ustc.edu.cn/~lgliu/Resources/CG/What_is_CG.htm)<br>[什么是深度学习？](http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.html)<br/>[数学在计算机图形学中的应用](http://staff.ustc.edu.cn/~lgliu/Resources/SummerSchool/USTC-summer-school.html) | by [刘利刚](http://staff.ustc.edu.cn/~lgliu)                 |
| [系统的学习计算机图形学，有哪些不同阶段的书籍的推荐？](https://www.zhihu.com/question/26720808) | Q&A                                                          |
| [现阶段应该怎么学习计算机图形学呢？](https://www.zhihu.com/question/26341836) | Q&A                                                          |
| [光线追踪与实时渲染的未来](https://zhuanlan.zhihu.com/p/34851503) | by [Edward Liu](http://behindthepixels.io/about/)            |
| 基于物理着色<br>([1](https://zhuanlan.zhihu.com/p/20091064), [2](https://zhuanlan.zhihu.com/p/20119162), [3](https://zhuanlan.zhihu.com/p/20122884), [4](https://zhuanlan.zhihu.com/p/21247702)) | by [Edward Liu](http://behindthepixels.io/about/)            |
| [How to Start Learning Computer Graphics Programming](https://erkaman.github.io/posts/beginner_computer_graphics.html)<br>([Chinese](https://zhuanlan.zhihu.com/p/55518151)) | by [Eric Arnebäck](https://erkaman.github.io/index.html)     |
| [Finding Your Home in Game Graphics Programming](https://alextardif.com/LearningGraphics.html) | by Alex Tardif                                               |
| [Bidirectional Path Tracing](https://rendering-memo.blogspot.com/2016/03/bidirectional-path-tracing-1-kickoff.html) series | by [Wei-Feng Wayne Huang](https://www.blogger.com/profile/01056048230170636241) |
| [BSSRDF Importance Sampling](https://rendering-memo.blogspot.com/2015/01/bssrdf-importance-sampling-1-kickoff.html) series | by [Wei-Feng Wayne Huang](https://www.blogger.com/profile/01056048230170636241) |

### Blogs

| Blogs                                                        | [(Back to TOC)](#fwg)                                        |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [A Graphics Guy's Note](https://agraphicsguynotes.com/)      | Jiayin Cao's some random notes about computer graphics.      |
| [Rendering Memo](https://rendering-memo.blogspot.com/)       | [Wei-Feng Wayne Huang](https://www.blogger.com/profile/01056048230170636241), Walt Disney Animation Studio rendering software developer. |
| [Self Shadow](https://blog.selfshadow.com/)                  | Walt Disney Animation Studio rendering software developer. I worked on few film production renderers[@self_shadow](https://twitter.com/self_shadow) has been collecting [Siggraph courses/papers links](https://blog.selfshadow.com/categories/conference/) for many years:  Especially interesting are the Physically Based Shading in Theory and Practice course presentations. |
| [Code & Visuals](https://blog.yiningkarlli.com/)             | Yining Karl Li, computer graphics senior software engineer at [Walt Disney Animation Studios](http://www.disneyanimation.com/) working on Disney's in-house production physically based renderer, [Hyperion.](https://www.disneyanimation.com/technology/hyperion) |
| [INTERPLAY OF LIGHT](https://interplayoflight.wordpress.com/) | ""This blog is my scratchpad for graphics techniques I try and experiment with.""<br>by [@Kostas Anagnostou](https://twitter.com/KostasAAA) |
| [Alan Zucconi](https://www.alanzucconi.com/)                 | author of the book *Unity 2018 Shaders and Effects Cookbook* |
| [Linden Reid](https://lindenreid.wordpress.com/)             | Procedural geometry & graphics tutorials<br>A game developer at Blizzard |
| [Harold Serrano](https://www.haroldserrano.com/home/)        | Creator of the [Untold Engine](https://www.untoldengine.com/) |
| [Behind the Pixels](http://behindthepixels.io/)              | Edward(Shiqiu) Liu, a Senior Real Time Rendering Engineer at NVIDIA<br> |
| [iquilezles](http://www.iquilezles.org/index.html)           | Inigo Quilez                                                 |
| [JOEY DE VRIES](https://joeydevries.com/#home)               | Joey, author of [learnopengl.com](https://learnopengl.com/)  |
| [Coding Labs](http://www.codinglabs.net/default.aspx)        |                                                              |
| [TYLER HOBBS](https://tylerxhobbs.com/)                      |                                                              |
| [HUMUS](http://humus.name/)                                  |                                                              |
| [Icare3D Blog](https://blog.icare3d.org/)                    |                                                              |

### Resources pages

| Resources pages                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Real-Time Rendering Resources](http://www.realtimerendering.com/) | Resources for Real-Time Rendering                            |
| [Ke-Sen Huang's Home Page](http://kesen.realtimerendering.com/) | A collection of CG papers (from SIGGRAPH, Asia, EG, PG, etc.) |
| [Rendering Resources](https://benedikt-bitterli.me/resources/) | This page offers 32 different 3D scenes that you can use for free in your rendering research, publications and classes. |
| [Graphics Programming weekly](https://jendrikillner.bitbucket.io/#posts) | Update per week, by [@Jendrik Illner](https://jendrikillner.bitbucket.io/) |
| [McGuire Computer Graphics Archive](https://casual-effects.com/data/) | A collection of models.                                      |
| [Technically Art](https://halisavakis.com/category/technically-art/) | by [@Harry Alisavakis](https://halisavakis.com/)             |
| [Open-Source Real-Time Rendering Engines and Libs](https://konstantinkhomyakov3d.github.io/real-time-engines/) |                                                              |
| [GDCVault](https://www.gdcvault.com/)                        | GDC Vault is a trove of in-depth design, technical and inspirational talks and slides from the influencers of the game development industry, taken from over 20 years of the worldwide Game Developers Conferences. |
| [Graphics Research Tools](https://developer.nvidia.com/graphics-research-tools) | by Nvidia                                                    |
| [mattdesl/graphics-resources](mattdesl/graphics-resources)   | A list of graphic programming resources                      |
| [Readings on The State of The Art in Rendering](https://interplayoflight.wordpress.com/2018/09/30/readings-on-the-state-of-the-art-in-rendering/) | by [Kostas Anagnostou](https://twitter.com/KostasAAA)        |
| [Readings on Physically Based Rendering](https://interplayoflight.wordpress.com/2013/12/30/readings-on-physically-based-rendering/) | by [Kostas Anagnostou](https://twitter.com/KostasAAA)        |
| [Advances in Real-Time Rendering in 3D Graphics and Games](http://advances.realtimerendering.com/) | the well-established series of SIGGRAPH courses<br>covering late-breaking work and advances in real-time computer graphics |
| [terkelg/awesome-creative-coding](https://github.com/terkelg) | Creative Coding: Generative Art, Data visualization, Interaction Design, Resources |
| [eug/awesome-opengl](https://github.com/eug/awesome-opengl)  | A curated list of awesome OpenGL libraries, debuggers and resources |
| [vinjn/awesome-vulkan](https://github.com/vinjn/awesome-vulkan) | A curated list of awesome Vulkan libraries, debuggers and resources. Inspired by [awesome-opengl](https://github.com/eug/awesome-opengl) and other awesome-... stuff. |
| [ericjang/awesome-graphics](https://github.com/ericjang/awesome-graphics) | This is a curated list of computer graphics tutorials and resources. |
| [Real-time Rendering Blogs](http://svenandersson.se/2014/realtime-rendering-blogs.html) |                                                              |

### Researchers

| Researchers                                                  |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Benedikt Bitterli](https://benedikt-bitterli.me/index.html) | A PhD at Dartmouth College.                                  |
| [Kun Xu (徐昆)](https://cg.cs.tsinghua.edu.cn/people/~kun/)  | Associate professor at Graphics and Geometric Computing Group, Tsinghua University. |
| [Lingqi Yan (闫令琪)](http://www.cs.ucsb.edu/~lingqi/?tdsourcetag=s_pctim_aiomsg#bio) | Assistant Professor at UC Santa Barbara.                     |
| [Ligang Liu (刘利刚)](http://staff.ustc.edu.cn/~lgliu/)      | A Professor at Graphics & Geometric Computing Laboratory (GCL), school of mathematical sciences USTC. |
| [Matt Pharr](https://pharr.org/matt/)                        | A research scientist at NVIDIA Research.                     |
| [Morgan McGuire](https://www.cs.williams.edu/~morgan/#courses) | A Professor of Computer Science at Williams College.         |
| [Pradeep Sen](https://web.ece.ucsb.edu/~psen/)               | A Professor at UC Santa Barbara.                             |
| [Ravi Ramamoorthi](http://cseweb.ucsd.edu/~ravir/)           | Professor at UC San Diego.                                   |
| [Thomas Müller](https://tom94.net/index.php)                 | A senior research scientist at NVIDIA Zürich.                |
| [Toshiya Hachisuka](https://www.ci.i.u-tokyo.ac.jp/~hachisuka/) | An Associate Professor at The University of Tokyo.           |
| [Wenzel Jakob](https://people.epfl.ch/265226)                | An assistant professor leading the Realistic Graphics Lab at EPFL's School of Computer and Communication Sciences. |
| [Wojciech Jarosz](https://cs.dartmouth.edu/~wjarosz/)        | An Assistant Professor at Dartmouth College.                 |

###  Labs

| Labs                                                         |                   |
| ------------------------------------------------------------ | ----------------- |
| [Graphics & Geometric Computing Group](https://cg.cs.tsinghua.edu.cn/) | at Tsinghua Univ. |
| [UCSB MIRAGE Lab](http://graphics.cvc.ucsb.edu/)             | at UCSB           |
| [Visual Computing Lab](http://vcl.cs.dartmouth.edu/#about)   | at Dartmouth      |
| [Utah Graphics Lab](https://graphics.cs.utah.edu/#u)         | at Utah           |
| [Realistic Graphics Lab](http://rgl.epfl.ch/)                | at EPFL           |
| [Computer Graphics Lab](https://cgl.ethz.ch/)                | at ETH Zürich     |
| [Graphics Lab](http://graphics.stanford.edu/)                | at Stanford       |
| [Cornell Graphics and Vision Group](https://rgb.cs.cornell.edu/) | at Cornell        |
| [Princeton ImageX Labs](https://pixl.cs.princeton.edu/)      | at Princeton      |
| [Carnegie Mellon Graphics Lab](http://graphics.cs.cmu.edu/)  | at CMU            |
| [MIT Computer Graphics Group](http://graphics.csail.mit.edu/) | at MIT            |
| [Center for Visual Computing](http://visualcomputing.ucsd.edu/) | at UCSD           |

### Video Channels

| Video Channels                                               |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [ACMSIGGRAPH](https://www.youtube.com/channel/UCbaxUExGKrH2zxY4AkY9wCg) |                                                              |
| [The Cherno](https://www.youtube.com/user/TheChernoProject)  | C++,<br/>Game Engine,<br/>...                                |
| [ChiliTomatoNoodle](https://www.youtube.com/user/ChiliTomatoNoodle) | DirectX & C++ Game Programming,<br>3D Programming Fundamentals,<br>... |
| [MIT OpenCourseWare](https://www.youtube.com/user/MIT)       |                                                              |

###  Open-source Projects

| Projects                                                     |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Intel® Embree](https://www.embree.org/)                     | Intel® Embree is a collection of high-performance ray tracing kernels, developed at Intel. |
| [The Mesa 3D Graphics Library](https://www.mesa3d.org/intro.html) | The Mesa project began as an open-source implementation of the [OpenGL](https://www.opengl.org/) specification - a system for rendering interactive 3D graphics. |
| [The Advanced Rendering Toolkit](https://cgg.mff.cuni.cz/ART/) | ART is a command-line system for physically based image synthesis. |
| [TAICHI](http://taichi.graphics/)                            | TAICHI: Open-source computer graphics library                |
| [Intel® Embree](https://embree.github.io/)                   | Intel® Embree is a collection of high-performance ray tracing kernels, developed at Intel |
| [MERL BRDF Database](http://www.merl.com/brdf/)              | The MERL BRDF database contains reflectance functions of 100 different materials |
| [yocto-gl](https://github.com/xelatihy/yocto-gl)             | Tiny C++ Libraries for Data-Driven Physically-based Graphics |
| [id-Software](https://github.com/id-Software)                | id-Software公司的所有游戏的开源代码                          |
| [WebGL Fluid Simulation](https://paveldogreat.github.io/WebGL-Fluid-Simulation/) |                                                              |
| [Scotty3D](https://github.com/cmu462/Scotty3D)               | 3D graphics software for mesh editing, path tracing, and animation |

###  Websites

| Websites                                                     |      |
| ------------------------------------------------------------ | ---- |
| [Shadertoy](https://www.shadertoy.com/)                      |      |
| [CSRankings: Computer Science Rankings](http://csrankings.org/#/index?graph&world) |      |

### Essential Mathematics

| Essential Mathematics                                        |                                              |
| ------------------------------------------------------------ | -------------------------------------------- |
| [Probability Theory for Physically Based Rendering Part 1](https://jacco.ompf2.com/2019/12/11/probability-theory-for-physically-based-rendering/), [Part 2](https://jacco.ompf2.com/2019/12/13/probability-theory-for-physically-based-rendering-part-2/) | by [Jacco Bikker.](https://jacco.ompf2.com/) |
| [Immersive linear Algebra](http://immersivemath.com/ila/index.html) |                                              |

","['infancy', 'nikoong', 'zheng95z', 'setoye', 'limberc']",0,,0.65,0,,,,,,90,,ucsb-cs,
50352391,MDEwOlJlcG9zaXRvcnk1MDM1MjM5MQ==,Neural-Networks-on-Silicon,fengbintu/Neural-Networks-on-Silicon,0,fengbintu,https://github.com/fengbintu/Neural-Networks-on-Silicon,This is originally a collection of papers on neural network accelerators. Now it's more like my selection of research on deep learning and computer architecture.,0,2016-01-25 13:31:31+00:00,2025-03-07 00:55:03+00:00,2025-01-06 14:57:13+00:00,,817,1909,1909,,1,1,1,1,0,1,384,0,0,0,,1,0,0,public,384,0,1909,master,1,,,"['fengbintu', 'kentaroy47', 'waterbearbee', 'Aayush-Ankit', 'abhishektyaagi', 'qinkunbao', 'sung-kim']",0,,0.56,0,,,,,,300,,CLIVAC,
216189313,MDEwOlJlcG9zaXRvcnkyMTYxODkzMTM=,Awesome-Federated-Machine-Learning,innovation-cat/Awesome-Federated-Machine-Learning,0,innovation-cat,https://github.com/innovation-cat/Awesome-Federated-Machine-Learning,"Everything about federated learning, including research papers, books, codes, tutorials, videos and beyond",0,2019-10-19 10:35:34+00:00,2025-03-07 05:05:37+00:00,2024-05-30 21:21:42+00:00,,421,1879,1879,,0,1,1,1,0,0,277,0,0,5,,1,0,0,public,277,5,1879,master,1,,,"['innovation-cat', 'fayelis', 'TrustworthyFL', 'alexey-gruzdev', 'amin-nejad']",0,,0.69,0,,,,,,41,,ucsb-coast-lab,
519771271,R_kgDOHvsUhw,awesome-quarto,mcanouil/awesome-quarto,0,mcanouil,https://github.com/mcanouil/awesome-quarto,"A curated list of Quarto talks, tools, examples & articles! Contributions welcome!",0,2022-07-31 12:34:01+00:00,2025-03-06 12:48:18+00:00,2025-02-03 07:22:34+00:00,,620,1822,1822,,1,0,1,0,0,0,122,0,0,5,cc0-1.0,1,0,0,public,122,5,1822,main,1,,"<!--lint disable double-link-->

<div align=""center"">

<!--lint ignore no-dead-urls-->

# <a href=""https://quarto.org"" target=""_blank"" rel=""noopener noreferrer""><img src=""media/media.png"" alt-text=""Awesome list logo with the Quarto logo with text, which is a pair of sunglasses with below the text 'awesome' and then the Quarto logo in blue and grey blue as per Quarto's logo."" width=""75%""/></a></br>[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

The most up to date curated list of [Quarto®](https://quarto.org) docs, talks, tools, examples & articles the internet has to offer.

[Quarto®](https://quarto.org) is an open-source scientific and technical publishing system built on [Pandoc]([Pandoc](https://pandoc.org/)).

</div>

---

Contributions of any kind are welcome, just follow the [guidelines](.github/CONTRIBUTING.md) by either:

- Filling a [suggestion issue](https://github.com/mcanouil/awesome-quarto/issues/new?assignees=mcanouil&labels=&template=suggestion.yml) (easier).
- Opening a [pull request](https://github.com/mcanouil/awesome-quarto/compare).

---

## Contents

- [Official documentation \& quickstarts](#official-documentation--quickstarts)
- [Tutorials \& workshops](#tutorials--workshops)
- [Blog posts](#blog-posts)
- [Talks and videos](#talks-and-videos)
- [Supported editors](#supported-editors)
- [Libraries/Packages/Scripts](#librariespackagesscripts)
- [Continuous integration / Continuous deployment](#continuous-integration--continuous-deployment)
- [Extensions](#extensions)
- [Real-life examples](#real-life-examples)
  - [Presentations formats](#presentations-formats)
  - [Websites formats](#websites-formats)
  - [Book formats](#book-formats)
  - [Other formats](#other-formats)
- [Follow](#follow)
  - [Official](#official)
  - [Community](#community)

## Official documentation & quickstarts

- [Documentation: Quarto documentation](https://quarto.org/) - Official Quarto Documentation.
- [GitHub: Quarto GitHub repository](https://github.com/quarto-dev) - Official Quarto GitHub repository.
- [Tutorial: Hello, Quarto](https://quarto.org/docs/get-started/hello/) - Official ""Hello, Quarto"" tutorial.
- [Tutorial: Computations](https://quarto.org/docs/get-started/computations/) - Official ""Computations"" tutorial.
- [Tutorial: Authoring](https://quarto.org/docs/get-started/authoring/) - Official ""Authoring"" tutorial.
- [Service: Quarto Pub](https://quartopub.com/) - Create documents, websites, presentations, books, and blogs in Quarto, then securely publish them to the web with the Quarto CLI, the easiest way to publish and share on the web.

## Tutorials & workshops

- [Tutorial: The ultimate guide to starting a Quarto blog](https://albert-rapp.de/posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html) - An in-depth guide on how to start blogging with Quarto.
- [Tutorial: Creating a blog with Quarto in 10 steps](https://beamilz.com/posts/2022-06-05-creating-a-blog-with-quarto/en/) - A blog post introducing a new Quarto blog and the steps to create your own blog with Quarto.
- [Tutorial: Making shareable docs with Quarto](https://openscapes.github.io/quarto-website-tutorial/) - A tutorial to make website with Quarto.
- [Workshop: From R Markdown to Quarto](https://rstudio-conf-2022.github.io/rmd-to-quarto/) - A workshop for those who want to take their R Markdown skills and expertise and apply them in Quarto, the next generation of R Markdown.
- [Workshop: Getting started with Quarto](https://rstudio-conf-2022.github.io/get-started-quarto/) - ""Get started with Quarto"" workshop materials for rstudio::conf(2022).
- [Workshop: Quarto, a library to run them all?](https://warwickcim.github.io/quarto-workshop/slides/slides.html) - Workshop at [RSECon'22](https://rsecon2022.society-rse.org/), led by Carlos Cámara, James Tripp and Cagatay Turkay (materials: <https://github.com/WarwickCIM/quarto-workshop>).
- [Tutorial: Creating your personal website using Quarto](https://ucsb-meds.github.io/creating-quarto-websites/) - A step-by-step guide for setting up a personal website using Quarto by Samantha Csik.
- [Tutorial: Customizing Quarto Websites - Make your website stand out using SASS & CSS](https://ucsb-meds.github.io/customizing-quarto-websites/) - Slides by Samantha Csik about using SASS and CSS to customise HTML Quarto website (materials: <https://github.com/UCSB-MEDS/customizing-quarto-websites>).
- [Workshop: Quartaki — 6 hour introduction to Quarto](https://drmowinckels.github.io/quartaki/) - Using R and RStudio by [Athanasia Mo Mowinckel](https://github.com/drmowinckels) covering basic markdown, html reports, citation & cross-refs, pdf and journal templates and Reveal.js presentations.
- [Workshop: Mi primer blog con Quarto](https://perezp44.github.io/taller.primer.blog/) - A workshop in Spanish by Pedro J. Pérez to create a blog with Quarto (materials: <https://github.com/perezp44/taller.primer.blog>).
- [Tutorial: Creating Quarto Journal Article Templates](https://christophertkenny.com/posts/2023-07-01-creating-quarto-journal-articles/) - An in-depth blog post detailing the process for converting journal LaTeX templates into Quarto templates.
- [Tutorial: Personal Website using Jupyter Notebook and Quarto](https://adtarie.net/posts/007-quarto-python-tutorial/) - A Python-oriented step-by-step tutorial on how to create a website using Quarto.
- [Tutorial: Create Your Website with Quarto](https://www.marvinschmitt.com/blog/website-tutorial-quarto/) - A tutorial on how to create a website using Quarto by Marvin Schmitt.
- [Tutorial: Publish a Quarto website with Netlify](https://jadeyryan.com/blog/publish-quarto-website/) - A comprehensive blog post walking through how to create a Quarto website, connect it to GitHub, deploy & publish it with Netlify by Jadey Ryan.
- [Workshop: Parameterized Reports with Quarto](https://jadeyryan.quarto.pub/rladies-dc-quarto-params/) - A 2-hour code-along workshop to learn parameterized reporting with `quarto` and `purrr` to generate multiple format outputs (materials: <https://github.com/jadeynryan/parameterized-quarto-workshop>).
- [Tutorials: Quarto Visual Editor and RStudio](https://youtube.com/playlist?list=PLEzw67WWDg80-fT1hq2IZf7D62tRmKy8f&si=ECVfFIH1bg6_JfjS) - A YouTube playlist of short videos tutorials for beginners to work with Quarto Visual Editor and RStudio by Andy Field.
- [Quarto for amsmath LaTeX users](https://nmfs-opensci.github.io/quarto-amsmath) - Notes focused on addressing issues related to using amsmath LaTeX for numbered equations and fancy math in Quarto books, specifically for HTML and PDF rendering.
- [Tutorial: A step-by-step guide to parameterized reporting in R using Quarto](https://rfortherestofus.com/2024/06/parameterized-reporting-quarto) - A walkthrough of how to use parameterized reporting with Quarto with videos.

## Blog posts

- [We don't talk about Quarto](https://www.apreshill.com/blog/2022-04-we-dont-talk-about-quarto/) - A blog post introducing to Quarto publishing software by Alison Presmanes Hill.
- [Quarto tip a day](https://mine-cetinkaya-rundel.github.io/quarto-tip-a-day/) - A website made with Quarto highlighting a tip for Quarto every day as a blog post.
- [Announcing Quarto, a new scientific and technical publishing system](https://www.rstudio.com/blog/announcing-quarto-a-new-scientific-and-technical-publishing-system/) - Blog post by J.J. Allaire announcing the launch of Quarto, a new open-source scientific and technical publishing system.
- [Interactive Molecular Content](https://www.valencekjell.com/posts/2022-08-13-interactive/) - A blog post showing how to embed interactive content (*i.e.*, molecular visualisation) in webpages with Quarto using Bokeh, 3DMol.js and NGL.
- [Slidecraft 101: Colors and Fonts](https://www.emilhvitfeldt.com/post/slidecraft-colors-fonts/) - A blog post about ""The art of putting together slides that are functional and aesthetically pleasing"" using Quarto presentation format by Emil Hvitfeldt.
- [Making Slides in Quarto with Reveal.js](https://meghan.rbind.io/blog/quarto-slides/) - A blog post about making slides in Quarto with Reveal.js and how to use emojis or customise the slides by Meghan Hall.
- [A beginner's guide to using Observable JavaScript, R, and Python with Quarto](https://www.infoworld.com/article/3674789/a-beginners-guide-to-using-observable-javascript-r-and-python-with-quarto.html) - This article shows you how to set up a Quarto document to use Observable JavaScript, including how to pass data from R or Python to an Observable code chunk.
- [Six Productivity Hacks for Quarto](https://www.rstudio.com/blog/6-productivity-hacks-for-quarto/) - A blog post showing six tips from the re-use content across documents, the insertion of Pandoc divs and spans to continuous deployment with GitHub Actions.
- [How to add some personality to your Quarto Blog](https://www.ddanieltan.com/posts/blogtips/) - A blog post sharing some of the added features and tweaks users can make on top of the standard blog templates to inject some personality into their blog.
- [Use R to Generate a Quarto Blogpost](https://themockup.blog/posts/2022-11-08-use-r-to-generate-a-quarto-blogpost/) - A blog post about using R to generate Quarto blog post by Tom Mock.
- [Adding Subscriptions to a Quarto Site](https://forbo7.github.io/forblog/posts/7_blog_subscriptions.html) - A blog post about how to add a subscription form to your Quarto blog.
- [I'm an R user: Quarto or R Markdown?](https://www.jumpingrivers.com/blog/quarto-rmarkdown-comparison/) - A blog post comparing Quarto and R Markdown from an R user perspective by [Jumping Rivers](https://www.jumpingrivers.com/).
- [Quarto for the Python User](https://www.jumpingrivers.com/blog/quarto-for-python-users/) - A blog post introducing Quarto to Python user for creating reports.
- [How to publish your Quarto document/book/website as a Docker container?](https://mickael.canouil.fr/posts/2023-05-07-quarto-docker/) - A blog post describing how to publish your Quarto document/book/website as a Docker container by Mickaël Canouil.
- [How to have images for both light and dark theme?](https://mickael.canouil.fr/posts/2023-05-30-quarto-light-dark/) - A blog post describing how to have images for both light and dark theme by Mickaël Canouil.
- [Making Pretty PDFs with Quarto](https://nrennie.rbind.io/blog/making-pretty-pdf-quarto/) - A blog post showing how to customise the styling of PDF documents, and save the styling into a Quarto extension to make it easier to reuse and share.
- [How to self-publish a technical book on Leanpub and Amazon using Quarto](https://www.brodrigues.co/blog/2023-06-29-book_quarto/) - This blog post explains which settings to use to compile an Epub for Leanpub and a print-ready PDF for Amazon's self-publishing service (KDP).
- [Hello Quarto: Porting my Website from Hugo Apéro](https://silviacanelon.com/blog/2023-09-29-hello-quarto/) - A blog post detailing a user's experience of porting a blogdown Hugo Apéro site to Quarto, with content including design ideas, CSS tips, HTML partials, setting up redirects, and others.
- [Seven Tips for Creating Quarto Reveal.js Presentations](https://remlapmot.github.io/post/2025/quarto-revealjs-tips/) - A blog post showing few tips on how to customise Reveal.js slides with R/knitr, HTML/CSS, and Quarto native options.

## Talks and videos

- [Reproducible authoring with Quarto](https://www.youtube.com/watch?v=6p4vOKS6Xls) - 2022 Toronto Workshop on Reproducibility with Mine Çetinkaya-Rundel (slides: <https://mine-cetinkaya-rundel.github.io/2022-repro-toronto/>).
- [Reproducible Publications with Julia and Quarto](https://www.youtube.com/watch?v=Y1uKNO32H_I) - JuliaCon 2022 with J.J. Allaire (slides: <https://jjallaire.github.io/quarto-juliacon-2022>).
- [A Conversation about Quarto](https://www.youtube.com/watch?v=azVAl343CIU) - [Openscapes](https://www.openscapes.org/) Community Talk: Hello Quarto!
- [Tutorial: How to style your Quarto blog without knowing a lot of HTML/CSS](https://www.youtube.com/watch?v=ErRX8plZpQE) - This is a video tutorial on styling your Quarto blog even if you lack a strong foundation of HTML/CSS.
- [Workshop: Welcome to Quarto 2-hour Workshop](https://www.youtube.com/watch?v=yvi5uXQMvu4) - RStudio Meetup: Welcome to Quarto 2-hour Workshop by Tom Mock (slides: <https://jthomasmock.github.io/quarto-2hr-webinar/>).
- [Quarto for the Curious](https://www.rstudio.com/conference/2022/talks/quarto-for-rmarkdown-users/) - A Quarto overview given by Tom Mock at RStudio::conf(2022) (materials: <https://thomasmock.quarto.pub/quarto-curious/>).
- [Hello Quarto: Share • Collaborate • Teach • Reimagine](https://www.rstudio.com/conference/2022/keynotes/collaborate-with-quarto/) - Keynote by Mine Çetinkaya-Rundel & Julia Stewart Lowndes highlighting how they leverage Quarto in open-science at RStudio::conf(2022) (materials: <https://github.com/mine-cetinkaya-rundel/hello-quarto>).
- [Websites & Books & Blogs, oh my! Creating Rich Content with Quarto](https://www.rstudio.com/conference/2022/talks/sessions/quarto-deep-dive/websites-books-blogs-quarto/) - Talk by Devin Pastoor at RStudio::conf(2022) showing some of the formats available in Quarto and how it is easy to focus on contents while Quarto takes care of the rest.
- [Literate Programming With Jupyter Notebooks and Quarto](https://www.rstudio.com/conference/2022/talks/literate-programming-quarto/) - Talk by Hamel Husain at RStudio::conf(2022) describing the integration between [Nbdev](https://github.com/fastai/nbdev) and Quarto (materials: <https://github.com/fastai/nbdev-demo>).
- [These are a few of my favorite things](https://www.rstudio.com/conference/2022/talks/my-favorite-things-quarto-presentations/) - Talk by Tracy Teal at RStudio::conf(2022) highlighting some of the features of Quarto presentation, such as multiple columns, speaker notes and mode, transitions, *etc.*
- [Building a Blog with Quarto](https://www.youtube.com/watch?v=CVcvXfRyfE0) - RStudio Meetup: Building a Blog with Quarto by Isabella Velásquez (materials: <https://ivelasq.quarto.pub/building-a-blog-with-quarto/>).
- [Beautiful Reports and Presentations with Quarto](https://www.youtube.com/watch?v=hbf7Ai3jnxY) - RStudio Meetup: Beautiful Reports and Presentations with Quarto by Tom Mock (materials: <https://github.com/jthomasmock/quarto-reporting>).
- [Introduction to Quarto](https://www.youtube.com/watch?v=y6_xMIBKuP4) - R-Ladies St. Louis: Introduction to Quarto by Isabella Velásquez (materials: <https://github.com/ivelasq/2022-10-27_intro-to-quarto>).
- [Quarto YouTube Playlist](https://www.youtube.com/playlist?list=PLDqZV53PcnYxnBYuEdSBxnOwdKLGaoKGg) - A YouTube playlist of videos about Quarto and Pandoc by Eli Holmes.
- [Create your Data Science Portfolio with Quarto](https://www.youtube.com/watch?v=xtSFXtDf4cM) - In this video by Deepsha Menghani, learn how you can easily create a Data Science Portfolio website and deploy it instantly with the help of Quarto (materials: <https://deepshamenghani.quarto.pub/portfolio-with-quarto-workshop/#/title-slide>).
- [A Coffee with Quarto and Neovim](https://youtube.com/playlist?list=PLabWm-zCaD1axcMGvf7wFxJz8FZmyHSJ7) - A YouTube playlist showing you how to use Quarto in Neovim by [Jannik Buhr](https://jmbuhr.de/).
- [How to style your Quarto blog without knowing a lot of HTML/CSS?](https://www.youtube.com/watch?v=ErRX8plZpQE) - This is a video tutorial on styling your Quarto blog even if you lack a strong foundation of HTML/CSS by Albert Rapp.
- [Quarto for Academics](https://www.youtube.com/watch?v=EbAAmrB0luA) - This video highlights some of Quarto's features that are especially useful for academics, as educators and as researchers by Mine Çetinkaya-Rundel.
- [Quarto Dashboards](https://www.youtube.com/watch?v=_VGJIPRGTy4) - This video highlights the new dashboard feature arriving in Quarto 1.4 by Charles Teague.
- [Parameterized Quarto reports improve understanding of soil health](https://www.youtube.com/watch?v=lbE5uOqfT70) - posit::conf(2023) talk by Jadey Ryan provides an example workflow of creating parameterized reports with HTML and MS Word outputs (materials: <https://jadeynryan.github.io/2023_posit-parameterized-quarto/>).

## Supported editors

- [Emacs](https://github.com/quarto-dev/quarto-emacs) - [Emacs](https://www.gnu.org/software/emacs/) mode for Quarto.
- [Neovim](https://github.com/quarto-dev/quarto-nvim) - [Neovim](https://neovim.io/) tools to work with Quarto.
- [Sublime Text](https://github.com/quarto-dev/quarto-sublime) - [Sublime Text](https://www.sublimetext.com/) plugin to work with Quarto.
- [Vim](https://github.com/quarto-dev/quarto-vim) - [Vim](https://www.vim.org/) plugin to work with Quarto.
- [Visual Studio Code](https://github.com/quarto-dev/quarto-vscode) - [Visual Studio Code](https://code.visualstudio.com/) extension for Quarto.
- [Scrivener](https://forum.literatureandlatte.com/t/scrivener-quarto-a-technical-academic-publishing-workflow/129769) - Quarto support to [Scrivener](https://www.literatureandlatte.com/) via Scrivener Template.
- [RStudio](https://posit.co/products/open-source/rstudio/) - RStudio IDE by [Posit PBC](https://posit.co/) natively supports Quarto.
- [Positron](https://positron.posit.co/) - A next-generation extensible, polyglot data science IDE built by Posit PBC.

## Libraries/Packages/Scripts

- [Julia](https://github.com/quarto-dev/quarto-julia) - [Julia](https://julialang.org/) interface package to Quarto CLI.
- [Python](https://github.com/quarto-dev/quarto-python) - [Python](https://www.python.org/) interface package to Quarto CLI.
- [R](https://github.com/quarto-dev/quarto-r) - [R](https://www.r-project.org/) interface package to Quarto CLI.
- [ecodown](https://github.com/edgararuiz/ecodown) - Turn R package documentation `pkgdown` website into a Quarto website.
- [Simulate colorblindness](https://rpubs.com/ijlyttle/quarto-cvd-widget) - Observable widget to simulate colorblindness for your whole document.
- [quartostamp](https://github.com/matt-dray/quartostamp) - An R package containing an [RStudio Addin](https://rstudio.github.io/rstudioaddins/) to insert some useful divs and classes into your Quarto `revealjs` document.
- [ohq2quarto](https://github.com/hrbrmstr/ohq2quarto) - A Rust-based command line utility to turn any [ObservableHQ](https://observablehq.com/) notebook into a Quarto project.
- [Quartize](https://github.com/hrbrmstr/reveal-qmd) - A Chrome extension to transform any [ObservableHQ](https://observablehq.com/) notebook into a list of downloadable `FileAttachment`s and an in-page Quarto source document.
- [RStudio & VSCode snippets](https://gist.github.com/jthomasmock/11acebd4448f171f786e01397df34116) - RStudio & VSCode snippets to ease typesetting with Quarto.
- [matrix BOT](https://github.com/rgomez90/matrix-bot) - A little bot for the [matrix-network](https://matrix.org/) that listens for some Quarto files and returns the PDF into the matrix channel.
- [babelquarto](https://docs.ropensci.org/babelquarto/) - R package to help set up, and render, multilingual Quarto books (see also [babeldown](https://docs.ropensci.org/babeldown/articles/quarto.html)).
- [quartodoc](https://github.com/machow/quartodoc) - A Python module that lets you quickly generate Python package API reference documentation using Markdown and Quarto.
- [altdoc](https://github.com/etiennebacher/altdoc) - Alternative to pkgdown to document R packages.
- [surveydown](https://surveydown.org/) - A platform for making markdown-based surveys with Quarto, Shiny, and Supabase.

## Continuous integration / Continuous deployment

- [Quarto GitHub Actions](https://github.com/quarto-dev/quarto-actions) - Official Quarto GitHub Actions allowing to setup, render, and deploy Quarto projects via GitHub Actions.
- [Quarto Website with GitHub Actions](https://tarleb.com/posts/quarto-with-gh-pages/) - Publish a Quarto website automatically every time it is updated via GitHub Actions.
- [Quarto Devcontainer Feature](https://github.com/rocker-org/devcontainer-features/tree/main/src/quarto-cli) - Add Quarto CLI to your [Development Containers](https://containers.dev/) as a feature.
- [Publish a Quarto project in 6 minutes](https://www.youtube.com/watch?v=arzBRW5XIkg) - A short video tutorial on how to publish a Quarto project to GitHub Pages with GitHub Actions without any local rendering.

## Extensions

- [Quarto.org Extensions Listing](https://quarto.org/docs/extensions/) - Quarto extensions listing from <https://quarto.org>.
- [Awesome Quarto Extensions Listing](https://m.canouil.dev/quarto-extensions/) - Quarto extensions listing from Awesome Quarto.

## Real-life examples

### Presentations formats

- [Advanced Introduction to R (French)](https://m.canouil.dev/radvanced/) - See slides [here](https://github.com/mcanouil/radvanced).
- [Streamlining with R](https://github.com/meghall06/personal-website/blob/master/static/slides/NEAIR/NEAIR.qmd) - See slides [here](https://meghan.rbind.io/slides/neair/neair.html).
- [An educator's perspective of the tidyverse](https://github.com/mine-cetinkaya-rundel/tidyperspective/blob/main/talks/dagstat-2022.qmd) - See slides [here](https://mine-cetinkaya-rundel.github.io/tidyperspective/talks/dagstat-2022.html).
- [An anthology of experimental designs](https://github.com/emitanaka/talks/tree/master/Toronto2022) - See slides [here](https://emitanaka.org/slides/toronto2022/).
- [The untold story of palmerpenguins](https://github.com/apreshill/palmerpenguins-useR-2022) - See slides [here](https://apreshill.github.io/palmerpenguins-useR-2022/).
- [Outrageously efficient EDA](https://github.com/jthomasmock/arrow-dplyr) - See slides [here](https://jthomasmock.github.io/arrow-dplyr/).
- [Improvements in textrecipes](https://github.com/emilhvitfeldt/talk-useR2022-textrecipes/) - See slides [here](https://emilhvitfeldt.github.io/talk-useR2022-textrecipes/).
- [Quarto: Create Beautiful Documents with R, Python, Julia and Observable (Runapp 2022 talk)](https://github.com/jimjam-slam/talk-runapp-quarto-2022) - See slides [here](https://runapp2022.talks.jamesgoldie.dev/).
- [Iframes Gallery](https://github.com/EmilHvitfeldt/quarto-iframe-examples) - A gallery of iframes that could be used in Quarto `revealjs` format.
- [Continental-scale biodiversity data assessment using the Atlas of Living Australia](https://github.com/shandiya/VicBioCon2023) - Slides for éVictorian Biodiversity Conference 2023 (See slides [here](https://shandiya.quarto.pub/vicbiocon2023/)).

### Websites formats

- [quarto.org](https://github.com/quarto-dev/quarto-web) - The Quarto documentation website.
- [rlille.fr](https://github.com/RLille/rlille.fr) - The R Lille (R User Group) website using Quarto.
- [R-Manuals](https://github.com/rstudio/r-manuals) - R Manuals rewritten with Quarto.
- [Quarto tip a day](https://github.com/mine-cetinkaya-rundel/quarto-tip-a-day) - Website/blog highlighting a tip for Quarto every day.
- [Documentation website from Jupyter Notebook](https://github.com/aeturrell/skimpy) - Quarto used to generate a website from a Jupyter notebook containing Python module documentation.
- [Program Evaluation for Public Service (course)](https://github.com/andrewheiss/evalf22.classes.andrewheiss.com) - Website for graduate-level course on program evaluation and causal inference using R, built with Quarto.
- [Bioconductor Community Blog](https://github.com/Bioconductor/biocblog) - A Quarto Blog for Bioconductor community.
- [R for Social Scientists workshop](https://github.com/pittmethods/r4ss) - A Quarto website for a workshop which includes Quarto Reveal JS presentations embedded in it.
- [AffCom Lab Website](https://github.com/jmgirard/affcomlab) - A research lab Quarto Blog/website using custom listing pages for people and publications.
- [Quantum Jitter](https://github.com/cgoo4/quantumjitter) - A Quarto website / blog with a custom theme (adapted from flatly / darkly), day / night landing page and a novel 404 page.
- [Andrew Heiss's website](https://github.com/andrewheiss/ath-quarto) - Andrew Heiss's website with custom EJS format, footer, 404 page, (S)CSS, and many more customisations.
- [Ella Kaye's website](https://github.com/EllaKaye/ellakaye.co.uk) - Ella Kaye's website with Bootstrap Grid card home page layout, CSS animation in navigation bar, and light/dark mode.
- [Quering with PRQL](https://github.com/eitsupi/querying-with-prql) - Docusaurus website using computations via `Jupyter` and `knitr` and multiple languages (PRQL, SQL, R, Python, *etc.*).
- [Real World Data Science](https://realworlddatascience.net/) - The Royal Statistical Society website, built with Quarto, features a custom design (based on the Lux Bootswatch theme), with a customised navbar and homepage layout.
- [Silvia Canelón's website](https://silviacanelon.com) - Silvia Canelón's website customized to match the style of the Hugo Apéro blogdown theme.
- [NASCENT-PERU website](https://nascent-peru.github.io/) - A multi-lingual (English/Spanish) website for a scientific research project using the [babelquarto](https://github.com/ropensci-review-tools/babelquarto) package.
- [rainbowR website](https://rainbowr.org) - rainbowR is a community for LGBTQ+ folks who code in R - its website has a rainbow colour theme, custom syntax highlighting for both light and dark modes, and some fun customisations in the navbar.
- [Marten Walk's website](https://martenw.com/) - Academic website of Marten Walk, using a custom theme inspired by the Financial Times, with modern look and many customisations (*i.e.*, custom fonts, custom graphs, etc.).
- [Skimpy documentation](https://aeturrell.github.io/skimpy/) - Skimpy documentation made using `quartodoc`.

### Book formats

- [R for Data Science, 2E](https://github.com/hadley/r4ds/) - ""R for Data Science"" book second edition (see <https://r4ds.hadley.nz/>).
- [R Packages, 2E](https://github.com/hadley/r-pkgs/) - ""R Packages"" book second edition (see <https://r-pkgs.org/>).
- [mlr3book](https://github.com/mlr-org/mlr3book/tree/main/book/) - Book on the [`mlr3`](https://mlr3.mlr-org.com/) packages ecosystem (see <https://mlr3book.mlr-org.com/>).
- [Introduction to Data Analysis with R](https://jmbuhr.de/dataintro/) - Introductory course with videos and lecture scripts as a Quarto book format.
- [Python for Data Analysis, 3E](https://github.com/wesm/pydata-book) - ""Python for Data Analysis"" book third edition (see <https://wesmckinney.com/book/>).

### Other formats

<!--lint disable awesome-list-item-->
<!--lint disable double-link-->

## Follow

<!-- list people worth following on social sites (Twitter, LinkedIn, GitHub, YouTube etc.) -->

### Official

- [@quarto-dev](https://github.com/quarto-dev) - GitHub organisation containing Quarto CLI, IDE plugins/extension, etc.
- [@quarto-ext](https://github.com/quarto-ext) - GitHub organisation containing extensions developed/maintained by Quarto's team.
- [@quarto-journals](https://github.com/quarto-journals) - GitHub organisation containing journals templates developed/maintained by Quarto's team.
- [J.J. Allaire (\@jjallaire)](https://github.com/jjallaire/) - Member of Quarto core team (Twitter: [\@fly_upside_down](https://twitter.com/fly_upside_down)).
- [Christophe Dervieux (\@cderv)](https://github.com/cderv) - Member of Quarto core team (Twitter: [\@chrisderv](https://twitter.com/chrisderv); Mastodon: [\@cderv\@fosstodon.org](https://fosstodon.org/@cderv)).
- [Carlos Scheidegger (\@cscheid)](https://github.com/cscheid) - Member of Quarto core team (Twitter: [\@scheidegger](https://twitter.com/scheidegger); Mastodon: [\@scheidegger\@mastodon.social](https://mastodon.social/@scheidegger)).
- [Charles Teague (\@dragonstyle)](https://github.com/dragonstyle) - Member of Quarto core team (Twitter: [\@dragonstyle](https://twitter.com/dragonstyle)).
- [Yihui Xie (\@yihui)](https://github.com/yihui) - Member of Quarto core team (Twitter: [\@xieyihui](https://twitter.com/xieyihui)).
- [Tom Mock (\@jthomasmock)](https://github.com/jthomasmock) - Customer Enablement & Quarto Product Manager (Twitter: [\@thomas_mock](https://twitter.com/thomas_mock); Mastodon: [\@thomas_mock\@fosstodon.org](https://fosstodon.org/@thomas_mock)).

### Community

- [Mickaël Canouil (\@mcanouil)](https://github.com/mcanouil) - Maintainer of Awesome Quarto list (Twitter: [\@MickaelCanouil](https://twitter.com/MickaelCanouil); Mastodon: [\@MickaelCanouil\@fosstodon.org](https://fosstodon.org/@MickaelCanouil)).

*Who else should we be following!?*

## Contributing

Thanks goes to these [contributors](https://github.com/mcanouil/awesome-quarto/graphs/contributors)!
","['mcanouil', 'jmbuhr', 'javedali99', 'finnoh', 'jadeynryan', 'hrbrmstr', 'AlbertRapp', 'andrewpbray', 'ArthurData', 'drmowinckels', 'blenback', 'ccamara', 'EllaKaye', 'grantmcdermott', 'jbwhit', 'juba', 'kelly-sovacool', 'lawwu', 'malcolmbarrett', 'mavam', 'mikemahoney218', 'pitmonticone', 'spcanelon', 'dependabot[bot]', 'shafayetShafee', 'zachcp']",0,,0.73,0,"# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity and
orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards
of acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies
when an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at [INSERT CONTACT
METHOD]. All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0,
available at https://www.contributor-covenant.org/version/2/0/
code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at https://
www.contributor-covenant.org/translations.
","# Contribution Guidelines

Please note that this project is released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md).  
By participating in this project you agree to abide by its terms.

## Pull Requests

ALWAYS create a new branch with your proposed changes.  
Thank you!

## Adding an new Item

- Try to fit your item into an existing sections.  
  Or suggest a new section and explain why your item does not fit into an existing one and what would be the interest of your new section.
- Add a new item to the bottom of the list in a section.
- If a duplicate item exists, discuss why the new item should replace it.
- Check your spelling & grammar.
- The item must follow this format:
  ```
  - [item name](https link) - Description beginning with capital, ending in period.
  ```
",,Directory exists,,63,,emlab-ucsb,
494658282,R_kgDOHXvi6g,Awesome-FL,youngfish42/Awesome-FL,0,youngfish42,https://github.com/youngfish42/Awesome-FL,"Comprehensive and timely academic information on federated learning (papers, frameworks, datasets, tutorials, workshops)",0,2022-05-21 02:20:25+00:00,2025-03-07 03:03:37+00:00,2024-12-30 03:42:37+00:00,https://youngfish42.github.io/Awesome-FL,4046,1634,1634,Python,1,1,1,1,1,1,193,0,0,0,cc-by-sa-4.0,1,0,0,public,193,0,1634,main,1,,,"['youngfish42', 'beiyuouo', 'Shmily1368', 'tenderzada', 'Sprinter1999', 'yh-yao', 'Li-Hongcheng', 'Moep90', 'MatZaharia', 'AlvinIsonomia', 'JinheonBaek', 'Armandolando', 'lokinko', 'xbfu', '6lyc']",0,,0.69,0,,,,,,40,,scalableinternetservicesarchive,
733901924,R_kgDOK750ZA,3D-Gaussian-Splatting-Papers,Awesome3DGS/3D-Gaussian-Splatting-Papers,0,Awesome3DGS,https://github.com/Awesome3DGS/3D-Gaussian-Splatting-Papers,3D高斯论文，持续更新，欢迎交流讨论。,0,2023-12-20 11:47:05+00:00,2025-03-07 04:13:35+00:00,2025-03-07 03:10:21+00:00,,2667,1614,1614,Python,1,1,1,1,0,1,60,0,0,3,,1,0,0,public,60,3,1614,main,1,1,,"['gqk', 'ArisWayne', 'lck666666', 'Florian-Barthel', 'Ysz2022', 'Yuxuan-W']",0,,0.67,0,,,,,,56,,whatevery1says,
600295047,R_kgDOI8fGhw,Web3Bugs,ZhangZhuoSJTU/Web3Bugs,0,ZhangZhuoSJTU,https://github.com/ZhangZhuoSJTU/Web3Bugs,Demystifying Exploitable Bugs in Smart Contracts,0,2023-02-11 03:29:56+00:00,2025-03-07 06:33:36+00:00,2024-05-12 15:13:06+00:00,,152818,1598,1598,Solidity,1,1,1,1,0,1,223,0,0,9,,1,0,0,public,223,9,1598,main,1,,"# Demystifying Exploitable Bugs in Smart Contracts <a href=""https://openai.com/product/dall-e-2""><img src=""resources/logo.png"" alt=""Logo"" align=""right"" width=""82""/></a>

[![integrity validation](https://github.com/ZhangZhuoSJTU/Web3Bugs/actions/workflows/validate.yml/badge.svg)](https://github.com/ZhangZhuoSJTU/Web3Bugs/actions/workflows/validate.yml)

<p>
<a href=""papers/icse23.pdf""> <img title="""" src=""resources/paper.jpg"" alt=""loading-ag-167"" align=""right"" width=""200""></a>

This project aims to provide a valuable resource for Web3 developers and security analysts by facilitating their understanding of exploitable bugs in smart contracts. We conduct a thorough analysis of exploitable bugs extracted from [code4rena](https://code4rena.com/) and classify each bug according to its nature.

Our initial research suggests that a notable proportion of exploitable bugs in smart contracts are functional bugs, which cannot be detected using simple and general oracles like reentrancy. We aim to raise awareness about the significance of such bugs and encourage practitioners to develop more sophisticated and nuanced automatic semantical oracles to detect them.
</p>

<br>

> 𝙰 𝚜𝚒𝚐𝚗𝚒𝚏𝚒𝚌𝚊𝚗𝚝 𝚗𝚞𝚖𝚋𝚎𝚛 𝚘𝚏 𝚎𝚡𝚙𝚕𝚘𝚒𝚝𝚊𝚋𝚕𝚎 𝚋𝚞𝚐𝚜 𝚒𝚗 𝚜𝚖𝚊𝚛𝚝 𝚌𝚘𝚗𝚝𝚛𝚊𝚌𝚝𝚜 𝚏𝚊𝚕𝚕 𝚞𝚗𝚍𝚎𝚛 𝚝𝚑𝚎 𝚌𝚊𝚝𝚎𝚐𝚘𝚛𝚢 𝚘𝚏 𝚏𝚞𝚗𝚌𝚝𝚒𝚘𝚗𝚊𝚕 𝚋𝚞𝚐𝚜, 𝚠𝚑𝚒𝚌𝚑 𝚌𝚊𝚗𝚗𝚘𝚝 𝚋𝚎 𝚍𝚎𝚝𝚎𝚌𝚝𝚎𝚍 𝚞𝚜𝚒𝚗𝚐 𝚜𝚒𝚖𝚙𝚕𝚎 𝚊𝚗𝚍 𝚐𝚎𝚗𝚎𝚛𝚊𝚕 𝚘𝚛𝚊𝚌𝚕𝚎𝚜.

<br>

Please be aware that __this repository is currently undergoing active development, and the data may change over time due to ongoing code4rena contests__.

## Dataset Description

### Folder Structure

The dataset is organized into four folders:

+ [papers/](papers/): contains our ICSE23 paper summarizing our preliminary results, as well as the supplementary material for the paper.
+ [results/](results/): contains the bug classification in [bugs.csv](results/bugs.csv) and the description for each contest in [contests.csv](results/contests.csv).
+ [contracts/](contracts/): contains all the smart contracts that we examined, using the version at the time of the contest.
+ [reports/](reports/): contains all the reports provided by code4rena.

### Bug Labels

We classify the surveyed bugs into three main categories based on their nature: 

+ Out-of-scope bugs (denoted by __O__)
+ Bugs with simple and general testing oracles (denoted by __L__)
+ Bugs that require high-level semantical oracles (denoted by __S__)

As classifying functional bugs can be ambiguous, we welcome suggestions to improve our classification standards. You can find more detailed label information in our [documentation](docs/standard.md), and we encourage you to refer to our current classification [guidelines](docs/standard.md#process) for more information.

## Recommended Security Analysis Tools

Our goal is to create a comprehensive list of vulnerability detection techniques that will be a valuable resource for Web3 developers and security analysts. We will focus on two main categories:

+ Vulnerability detection techniques that prioritize the development of semantical oracles for smart contracts.
+ Publicly available security analysis tools that can be used for auditing

<span style=""color:red""><strong>We warmly welcome any additional suggestions or contributions from the community to help expand and improve the list. </strong></span> 

### Vulnerability Detection with Automatic Semantical Oracles

We believe that future web3 security efforts will prioritize identifying functional bugs and developing corresponding oracles. To this end, we intend to compile a list of techniques that provide guidance in the creation of automatic semantic oracles. These techniques will be sourced from various materials, such as peer-reviewed research papers, pre-prints, industry tools, and online resources.

| Technique                                                                                                                                          | Bug Category   |
| :------------------------------------------------------------------------------------------------------------------------------------------------- | :------------- |
| [Finding Permission Bugs in Smart Contracts with Role Mining](https://personal.ntu.edu.sg/yi_li/files/Liu2022FPB.pdf)                              | Access Control |
| [AChecker: Statically Detecting Smart Contract Access Control Vulnerabilities](https://people.ece.ubc.ca/mjulia/publications/ACheckerICSE2023.pdf) | Access Control |
| [Towards Automated Verification of Smart Contract Fairness](https://personal.ntu.edu.sg/yi_li/files/Liu2020TAV.pdf) | Fairness Property |
| [Clockwork Finance: Automated Analysis of Economic Security in Smart Contracts](https://www.computer.org/csdl/proceedings-article/sp/2023/933600a622/1He7Yru4ls4) | TBD |
| [Confusum Contractum: Confused Deputy Vulnerabilities in Ethereum Smart Contracts](https://seclab.cs.ucsb.edu/files/publications/gritti23confusum.pdf) | Confused Deputy |
| [Not your Type! Detecting Storage Collision Vulnerabilities in Ethereum Smart Contracts](https://seclab.cs.ucsb.edu/files/publications/ruaro24crush.pdf)    | Storage Collision |


### Publicly Available Security Analysis Techniques

This section will include open-source techniques that are publicly available and currently in active development. These techniques can be used either directly by Web3 developers and security analysts or as building blocks for other tools. We give priority to source-code level techniques, which are better suited for Web3 development and auditing contexts.

| Technique                                                                        | Developer(s)                                               | Description                                  | Security-related Keywords                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| :------------------------------------------------------------------------------- | :--------------------------------------------------------- | :------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Slither](https://github.com/crytic/slither)                                     | [Trail of Bits](https://www.trailofbits.com/)              | Static Analysis Framework                    | [Vulnerability Detectors](https://github.com/crytic/slither/blob/master/trophies.md), [SlithIR](https://github.com/crytic/slither/wiki/SlithIR)                                                                                                                                                                                                                                                                                                                                                                                                                          |
| [Aderyn](https://github.com/Cyfrin/aderyn)                                       | [Cyfrin](https://www.cyfrin.io/)                           | Static Analysis Framework                    | Static Analyzer, Custom Detectors, Markdown Reports                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [Foundry](https://github.com/foundry-rs/foundry)                                 | [Paradigm](https://www.paradigm.xyz/)                      | Development Toolchain                        | [Fuzzing](https://book.getfoundry.sh/forge/fuzz-testing), [Stateful Fuzzing (Invariant Testing)](https://book.getfoundry.sh/forge/invariant-testing#invariant-testing), [Differential Testing](https://book.getfoundry.sh/forge/differential-ffi-testing#differential-testing)                                                                                                                                                                                                                                                                                           |
| [Echidna](https://github.com/crytic/echidna)                                     | [Trail of Bits](https://www.trailofbits.com/)              | Fuzzer                                       | [Fuzzing](https://github.com/crytic/echidna#echidna-a-fast-smart-contract-fuzzer-)                                                                                                                                                                                                                                                                                                 , [Stateful Fuzzing (Invariant Testing)](https://github.com/crytic/echidna#writing-invariants), [CI/CD](https://github.com/crytic/echidna#using-echidna-in-a-github-actions-workflow) |
| [Optik](https://github.com/crytic/optik)                                         | [Trail of Bits](https://www.trailofbits.com/)              | Hybrid Fuzzer (Symbolic Execution + Fuzzing) | Fuzzing, Stateful Fuzzing, Symbolic Execution                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| [Woke](https://github.com/Ackee-Blockchain/woke)                                 | [Ackee Blockchain](https://ackeeblockchain.com/)           | Development Toolchain                        | [Cross-chain Testing](https://ackeeblockchain.com/woke/docs/2.1.0/testing-framework/cross-chain-testing/#relaying-events), [Invariant Testing](https://ackeeblockchain.com/woke/docs/2.1.0/testing-framework/fuzzing/), [Vulnerability Detectors](https://ackeeblockchain.com/woke/docs/devel/detectors/), [IR](https://ackeeblockchain.com/woke/docs/devel/api-reference/ir/abc/)                                                                                                                                                                                       |
| [4naly3er](https://github.com/Picodes/4naly3er)                                  | [Picodes](https://twitter.com/thePicodes)                  | Static Scanner                               | [Code4rena Pre-content Testing](https://docs.code4rena.com/roles/wardens/submission-policy#automated-findings-ineligible)                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| [Manticore](https://github.com/trailofbits/manticore)                            | [Trail of Bits](https://www.trailofbits.com/)              | Symbolic Execution Tool                      | Symbolic Execution, [Property Testing](https://manticore.readthedocs.io/en/latest/verifier.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| [Halmos](https://github.com/a16z/halmos)                                         | [a16z](https://github.com/a16z)                            | Symbolic Bounded Model Checker               | Symbolic Execution, Bound Checker                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [Solidity SMTChecker](https://docs.soliditylang.org/en/latest/smtchecker.html)   | [Ethereum Foundation](https://ethereum.org/en/foundation/) | Formal Verification by Symbolic Execution     | Solidity, Formal Verification, Symbolic Execution                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [Mythril](https://github.com/ConsenSys/mythril)                                  | [Consensys](https://consensys.net/)                        | Symbolic Execution Tool                       | Symbolic Execution, [On-Chain Analysis](https://mythril-classic.readthedocs.io/en/develop/security-analysis.html#analyzing-on-chain-contracts), [Vulnerability Detectors](https://mythril-classic.readthedocs.io/en/develop/analysis-modules.html), Taint Analysis |
| [Pyrometer](https://github.com/nascentxyz/pyrometer) __[WIP]__                       | [Nascent](https://www.nascent.xyz/)                        | Symbolic Execution Tool  | Symbolic Execution, Abstract Interpretation |
| [greed](https://github.com/ucsb-seclab/greed)                                        | [UCSB Seclab](https://seclab.cs.ucsb.edu/)                 | Static/Symbolic Analysis Framework |  Symbolic Execution, Bound Checker, Static Analyses, Property Testing
| [ethpwn](https://github.com/ethpwn/ethpwn)                                           | [ethpwn](https://github.com/ethpwn/)                       | Dynamic analysis/Debugging                | EVM simulations, EVM debugging

<details>
<summary>In addition, we curate a catalogue of security utilities applicable to smart contract programming languages beyond Solidity.</summary></br>

| Technique                                                                        | Language                                               | Description                                  | Security-related Keywords                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| :------------------------------------------------------------------------------- | :--------------------------------------------------------- | :------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Move Prover](https://github.com/move-language/move/tree/main/language/move-prover) | [Move](https://github.com/move-language) | Formal Specification and Verification     | Formal Verification |
</details>

### Valuable Resources for Web3 Security

<details>
<summary>This section comprises a compilation of resources that pertain to web3 security.</summary></br>

| Resource | Keywords |
| :-------- | :------- |
| [Academic Smart Contract Papers](https://github.com/hzysvilla/Academic_Smart_Contract_Papers) | Academic Paper List |
| [DeFi Hacks Reproduce - Foundry](https://github.com/SunWeb3Sec/DeFiHackLabs) | Attack Replication |
| [Smart Contract Security Verification Standard](https://github.com/ComposableSecurity/SCSVS) | Security Checklist  |
| [Awesome MythX Smart Contract Security Tools](https://github.com/muellerberndt/awesome-mythx-smart-contract-security-tools) | Security Analysis Service |
| [Common Security Properties of Smart Contracts](https://github.com/crytic/properties) | Security Compliance Suite |
| [Immunefi PoC Templates](https://github.com/immunefi-team/forge-poc-templates) | PoC Templates |
| [Awesome MEV Resources](https://github.com/0xalpharush/awesome-MEV-resources) | MEV Resources |
| [Front-Running Attack Benchmark Construction and Vulnerability Detection Technique Evaluation](https://arxiv.org/abs/2212.12110) | Front-Running Dataset |
| [Ultimate DeFi & Blockchain Research Base](https://github.com/OffcierCia/ultimate-defi-research-base) | Blockchain Security All-in-One |
| [Common Fork Bugs](https://github.com/YAcademy-Residents/defi-fork-bugs) | Exploit Dataset |
</details>

## Contributing

We welcome all types of contributions to our project, including but not limited to:

+ <span style=""color:red""><strong>Suggesting new reference techniques for smart contract security analysis.</strong></span>
+ Adding newly disclosed code4rena contest bugs.
+ Suggesting improvements to the classification standard
+ Correcting mislabeled bugs
+ Filling in any missing defillama entities in the `results/contests.csv`

Further details can be found in our [contribution guidelines](docs/contribution.md).

## Cite

If you are using our dataset for an academic publication, we would really appreciate a citation to the following work:

```
@inproceedings{DBLP:conf/icse/ZhangZXL23,
  author       = {Zhuo Zhang and
                  Brian Zhang and
                  Wen Xu and
                  Zhiqiang Lin},
  title        = {Demystifying Exploitable Bugs in Smart Contracts},
  booktitle    = {{ICSE}},
  pages        = {615--627},
  publisher    = {{IEEE}},
  year         = {2023}
}
```

## Clarification

Please refer to our classification [documentation](docs/classification.md).

## Acknowledgments

We would like to extend our sincere thanks to [code4rena](https://code4rena.com/) for making this valuable information publicly available.

<details>
<summary>Our appreciation also goes out to the following contributors for their valuable input.</summary></br>

+ [__ItsNio__](https://github.com/niothefirst)
+ [__Wen Xu__](https://github.com/tarafans)
+ [__Patrick Collins__](https://github.com/PatrickAlphaC)
+ [__Meng Xu__](https://twitter.com/meng_xu_cs)
+ [__y3s0n__](https://github.com/CharesFang)
+ [__William Aaron Cheung__](https://github.com/Troublor)
+ [__soaphorn__](https://github.com/soaphorn)
+ [__Fabio Gritti__](https://github.com/degrigis)
+ [__Yannick__](https://twitter.com/MillusPontius)

</details>
","['ZhangZhuoSJTU', 'NioTheFirst', 'Troublor', '0xScratch', 'Mill1995', 'PatrickAlphaC', 'degrigis', 'omahs', 'CharesFang']",0,,0.7,0,,,,Directory exists,,33,,,
104403768,MDEwOlJlcG9zaXRvcnkxMDQ0MDM3Njg=,OpenFermion,quantumlib/OpenFermion,0,quantumlib,https://github.com/quantumlib/OpenFermion,The electronic structure package for quantum computers.,0,2017-09-21 22:10:28+00:00,2025-03-06 00:17:18+00:00,2025-03-06 00:17:14+00:00,https://quantumai.google/openfermion,49247,1557,1557,Python,1,0,1,0,0,0,382,0,0,84,apache-2.0,1,0,0,public,382,84,1557,master,1,1,".. image:: https://raw.githubusercontent.com/quantumlib/OpenFermion/refs/heads/master/docs/images/logo_horizontal.svg
   :alt: OpenFermion logo
   :width: 75%
   :align: center

.. |ci| image:: https://img.shields.io/github/actions/workflow/status/quantumlib/openfermion/ci.yaml?style=flat-square&logo=GitHub&label=CI
   :alt: Continuous integration status badge
   :target: https://github.com/quantumlib/OpenFermion/actions/workflows/ci.yaml

.. |python| image:: https://img.shields.io/badge/Python-3.10+-fcbc2c.svg?style=flat-square&logo=python&logoColor=white
   :alt: Compatible with Python versions 3.10 and higher
   :target: https://www.python.org/downloads/

.. |license| image:: https://img.shields.io/badge/License-Apache%202.0-3c60b1.svg?logo=opensourceinitiative&logoColor=white&style=flat-square
   :alt: Licensed under the Apache 2.0 license
   :target: https://github.com/quantumlib/OpenFermion/blob/main/LICENSE

.. |version| image:: https://img.shields.io/pypi/v/OpenFermion.svg?logo=semantic-release&logoColor=white&label=Release&style=flat-square&color=fcbc2c
   :alt: OpenFermion project on PyPI
   :target: https://pypi.org/project/OpenFermion

.. |downloads| image:: https://img.shields.io/pypi/dm/openfermion?logo=PyPI&logoColor=white&style=flat-square&label=Downloads
   :alt: OpenFermion downloads per month from PyPI
   :target: https://img.shields.io/pypi/dm/OpenFermion

.. class:: margin-top
.. class:: centered

|
| |ci| |python| |license| |version| |downloads|

.. |vertspace| image:: https://upload.wikimedia.org/wikipedia/commons/archive/c/ca/20200404084254%211x1.png
   :alt: Blank space
   :width: 1px
   :height: 30px

|vertspace| OpenFermion is an open source library for compiling and analyzing quantum
algorithms to simulate fermionic systems, including quantum chemistry. Among
other functionalities, it features data structures and tools for obtaining and
manipulating representations of fermionic and qubit Hamiltonians. For more
information, see our `release paper <https://arxiv.org/abs/1710.07629>`__.

You can run the interactive Jupyter Notebooks in |Colab|_ or |MyBinder|_.

.. |Colab| replace:: Colab
.. _Colab: https://colab.research.google.com/github/quantumlib/OpenFermion

.. |MyBinder| replace:: MyBinder
.. _MyBinder:  https://mybinder.org/v2/gh/quantumlib/OpenFermion/master?filepath=examples

Installation and Documentation
==============================

Installing the latest **stable** OpenFermion requires the Python package
installer `pip <https://pip.pypa.io>`__. Make sure that you are using an
up-to-date version of it.

Documentation can be found at `quantumai.google/openfermion <https://quantumai.google/openfermion>`__ and the following links:

* `Installation <https://quantumai.google/openfermion/install>`__

* `API Docs <https://quantumai.google/reference/python/openfermion/all_symbols>`__

* `Tutorials <https://quantumai.google/openfermion/tutorials/intro_to_openfermion>`__

Currently, OpenFermion is tested on Mac, Windows, and Linux. We recommend using Mac or Linux because
the electronic structure plugins are only compatible on these platforms. However,
for those who would like to use Windows, or for anyone having other difficulties
with installing OpenFermion or its plugins, we have provided a Docker image
and usage instructions in the
`docker folder <https://github.com/quantumlib/OpenFermion/tree/master/docker>`__.
The Docker image provides a virtual environment with OpenFermion and select plugins pre-installed.
The Docker installation should run on any operating system.

Developer install
-----------------

To install the latest version of OpenFermion (in development mode), run
the following commands:

.. code-block:: shell

  git clone https://github.com/quantumlib/OpenFermion
  cd OpenFermion
  python -m pip install -e .

Library install
---------------

To install the latest PyPI release as a library (in user mode), run
the following commands:

.. code-block:: shell

  python -m pip install --user openfermion

Plugins
=======

OpenFermion relies on modular plugin libraries for significant functionality.
Specifically, plugins are used to simulate and compile quantum circuits and to perform
classical electronic structure calculations.
Follow the links below to learn more!

High-performance simulators
---------------------------

* `OpenFermion-FQE <https://github.com/quantumlib/OpenFermion-FQE>`__ is
  a high-performance emulator of fermionic quantum evolutions specified
  by a sequence of fermion operators, which can exploit fermionic
  symmetries such as spin and particle number.

Circuit compilation plugins
---------------------------

* `Forest-OpenFermion <https://github.com/rigetticomputing/forestopenfermion>`__ to support integration with `Forest <https://www.rigetti.com/forest>`__.

* `SFOpenBoson <https://github.com/XanaduAI/SFOpenBoson>`__ to support integration with `Strawberry Fields <https://github.com/XanaduAI/strawberryfields>`__.

Electronic structure package plugins
------------------------------------

* `OpenFermion-Psi4 <http://github.com/quantumlib/OpenFermion-Psi4>`__ to support integration with `Psi4 <http://psicode.org>`__.

* `OpenFermion-PySCF <http://github.com/quantumlib/OpenFermion-PySCF>`__ to support integration with `PySCF <https://github.com/sunqm/pyscf>`__.

* `OpenFermion-Dirac <https://github.com/bsenjean/Openfermion-Dirac>`__ to support integration with `DIRAC <http://diracprogram.org/doku.php>`__.

* `OpenFermion-QChem <https://github.com/qchemsoftware/OpenFermion-QChem>`__ to support integration with `Q-Chem <https://www.q-chem.com>`__.

How to contribute
=================

We'd love to accept your contributions and patches to OpenFermion.
There are a few small guidelines you need to follow.
Contributions to OpenFermion must be accompanied by a Contributor License
Agreement (CLA).
You (or your employer) retain the copyright to your contribution; the CLA
this simply gives us permission to use and redistribute your contributions as part of the project.
Head over to https://cla.developers.google.com/
to see your current agreements on file or to sign a new one.

All submissions, including submissions by project members, require review.
We use GitHub pull requests for this purpose. Consult
`GitHub Help <https://help.github.com/articles/about-pull-requests/>`__ for
more information on using pull requests.
Furthermore, please make sure your new code comes with extensive tests!
We use automatic testing to make sure all pull requests pass tests and do not
decrease overall test coverage by too much. Make sure you adhere to our style
guide. Just have a look at our code for clues. We mostly follow
`PEP 8 <https://www.python.org/dev/peps/pep-0008/>`_ and use
the corresponding `linter <https://pypi.python.org/pypi/pep8>`_ to check for it.
Code should always come with documentation, which is generated automatically and can be found
`here <http://openfermion.readthedocs.io/en/latest/openfermion.html>`_.

We use `Github issues <https://github.com/quantumlib/OpenFermion/issues>`__
for tracking requests and bugs. Please post questions to the
`Quantum Computing Stack Exchange <https://quantumcomputing.stackexchange.com/>`__ with an 'openfermion' tag.

Authors
=======

`Ryan Babbush <http://ryanbabbush.com>`__ (Google),
`Jarrod McClean <http://jarrodmcclean.com>`__ (Google),
`Nicholas Rubin <https://github.com/ncrubin>`__ (Google),
`Kevin Sung <https://github.com/kevinsung>`__ (University of Michigan),
`Ian Kivlichan <http://aspuru.chem.harvard.edu/ian-kivlichan/>`__ (Harvard),
`Dave Bacon <https://github.com/dabacon>`__ (Google),
`Xavier Bonet-Monroig <https://github.com/xabomon>`__  (Leiden University),
`Yudong Cao <https://github.com/yudongcao>`__ (Harvard),
`Chengyu Dai <https://github.com/jdaaph>`__ (University of Michigan),
`E. Schuyler Fried <https://github.com/schuylerfried>`__ (Harvard),
`Craig Gidney <https://github.com/Strilanc>`__ (Google),
`Brendan Gimby <https://github.com/bgimby>`__ (University of Michigan),
`Pranav Gokhale <https://github.com/singular-value>`__ (University of Chicago),
`Thomas Häner <https://github.com/thomashaener>`__ (ETH Zurich),
`Tarini Hardikar <https://github.com/TariniHardikar>`__ (Dartmouth),
`Vojtĕch Havlíček <https://github.com/VojtaHavlicek>`__ (Oxford),
`Oscar Higgott <https://github.com/oscarhiggott>`__ (University College London),
`Cupjin Huang <https://github.com/pertoX4726>`__ (University of Michigan),
`Josh Izaac <https://github.com/josh146>`__ (Xanadu),
`Zhang Jiang <https://ti.arc.nasa.gov/profile/zjiang3>`__ (NASA),
`William Kirby <https://williammkirby.com>`__ (Tufts University),
`Xinle Liu <https://github.com/sheilaliuxl>`__ (Google),
`Sam McArdle <https://github.com/sammcardle30>`__ (Oxford),
`Matthew Neeley <https://github.com/maffoo>`__ (Google),
`Thomas O'Brien <https://github.com/obriente>`__ (Leiden University),
`Bryan O'Gorman <https://ti.arc.nasa.gov/profile/bogorman>`__ (UC Berkeley, NASA),
`Isil Ozfidan <https://github.com/conta877>`__ (D-Wave Systems),
`Max Radin <https://github.com/max-radin>`__ (UC Santa Barbara),
`Jhonathan Romero <https://github.com/jromerofontalvo>`__ (Harvard),
`Daniel Sank <https://github.com/DanielSank>`__ (Google),
`Nicolas Sawaya <https://github.com/nicolassawaya>`__ (Harvard),
`Bruno Senjean <https://github.com/bsenjean>`__ (Leiden University),
`Kanav Setia <https://github.com/kanavsetia>`__ (Dartmouth),
`Hannah Sim <https://github.com/hsim13372>`__ (Harvard),
`Damian Steiger <https://github.com/damiansteiger>`__ (ETH Zurich),
`Mark Steudtner <https://github.com/msteudtner>`__  (Leiden University),
`Qiming Sun <https://github.com/sunqm>`__ (Caltech),
`Wei Sun <https://github.com/Spaceenter>`__ (Google),
`Daochen Wang <https://github.com/daochenw>`__ (River Lane Research),
`Chris Winkler <https://github.com/quid256>`__ (University of Chicago),
`Fang Zhang <https://github.com/fangzh-umich>`__ (University of Michigan) and
`Emiel Koridon <https://github.com/Emieeel>`__ (Leiden University).

How to cite
===========
When using OpenFermion for research projects, please cite:

    Jarrod R McClean, Nicholas C Rubin, Kevin J Sung, Ian D Kivlichan, Xavier Bonet-Monroig,
    Yudong Cao, Chengyu Dai, E Schuyler Fried, Craig Gidney, Brendan Gimby, Pranav Gokhale,
    Thomas Häner, Tarini Hardikar, Vojtěch Havlíček, Oscar Higgott, Cupjin Huang, Josh Izaac,
    Zhang Jiang, Xinle Liu, Sam McArdle, Matthew Neeley, Thomas O'Brien, Bryan O'Gorman,
    Isil Ozfidan, Maxwell D Radin, Jhonathan Romero, Nicolas P D Sawaya, Bruno Senjean,
    Kanav Setia, Sukin Sim, Damian S Steiger, Mark Steudtner, Qiming Sun, Wei Sun, Daochen Wang,
    Fang Zhang, and Ryan Babbush
    *OpenFermion: The Electronic Structure Package for Quantum Computers*.
    `Quantum Science and Technology 5.3 (2020): 034014 <https://iopscience.iop.org/article/10.1088/2058-9565/ab8ebc/meta>`__.

We are happy to include future contributors as authors on later releases.

Disclaimer
==========

Copyright 2017 The OpenFermion Developers.
This is not an official Google product.
","['kevinsung', 'babbush', 'mhucka', 'ncrubin', 'dependabot[bot]', 'jarrodmcc', 'fdmalone', 'obriente', 'dstrain115', 'bryano', 'sheilaliuxl', 'dabacon', 'conta877', 'vtomole', 'mpharrigan', 'lamberta', 'Strilanc', 'max-radin', 'rmlarose', 'xabomon', 'viathor', 'bsenjean', 'josh146', 'MichaelBroughton', 'Spaceenter', 'yashk2810', 'tanujkhattar', 'subzjee', 'sammcardle30', 'singular-value', 'hsim13372', 'Emieeel', 'quid256', 'balopat', 'alexfleury', 'mapmeld', 'pavoljuhas', 'philipp-q', 'r-imai-67', 'StefanoPolla', 'gyu-don', 'TariniHardikar', 'wmkirby1', 'Wuggins', 'augustehirth', 'bgimby', 'brettkoonce', 'cvjjm', 'cvmxn1', 'daochenw', 'fangzh-umich', 'ggd87', 'lilies', 'nicolassawaya', 'oscarhiggott', 'aleksey-uvarov', 'jdaaph', 'xcz011', 'DanielSank', 'epifanovsky', 'snow0369', 'hay-k', 'ica574', 'jamesquantum', 'johnchildren', 'jjgoings', 'kwkbtr', 'jlmayfield', 'loriab', 'mafaldaramoa', 'MarkDaoust', 'cvsik', 'WhiteSymmetry', 'mstechly', 'NHDaly']",1,,0.69,353,"# Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, gender identity and expression, level of
experience, education, socio-economic status, nationality, personal appearance,
race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

*   Using welcoming and inclusive language
*   Being respectful of differing viewpoints and experiences
*   Gracefully accepting constructive criticism
*   Focusing on what is best for the community
*   Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

*   The use of sexualized language or imagery and unwelcome sexual attention or
    advances
*   Trolling, insulting/derogatory comments, and personal or political attacks
*   Public or private harassment
*   Publishing others' private information, such as a physical or electronic
    address, without explicit permission
*   Other conduct which could reasonably be considered inappropriate in a
    professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, or to ban temporarily or permanently any
contributor for other behaviors that they deem inappropriate, threatening,
offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

This Code of Conduct also applies outside the project spaces when the Project
Stewards have a reasonable belief that an individual's behavior may have a
negative impact on the project or its community.

## Conflict Resolution

We do not believe that all conflict is bad; healthy debate and disagreement
often yield positive results. However, it is never okay to be disrespectful or
to engage in behavior that violates the project’s Code of Conduct.

If you see someone violating the Code of Conduct, you are encouraged to address
the behavior directly with those involved. Many issues can be resolved quickly
and easily, and this gives people more control over the outcome of their
dispute. If you are unable to resolve the matter for any reason, or if the
behavior is threatening or harassing, report it. We are dedicated to providing
an environment where participants feel welcome and safe.

Reports should be directed to quantumai-oss-maintainers@googlegroups.com,
the project stewards at Google Quantum AI. They will then work with a committee
consisting of representatives from the Open Source Programs Office and the
Google Open Source Strategy team. If for any reason you are uncomfortable
reaching out to the Project Stewards, please email opensource@google.com.

We will investigate every complaint, but you may not receive a direct response.
We will use our discretion in determining when and how to follow up on reported
incidents, which may range from not taking action to permanent expulsion from
the project and project-sponsored spaces. We will notify the accused of the
report and provide them an opportunity to discuss it before any action is taken.
The identity of the reporter will be omitted from the details of the report
supplied to the accused. In potentially harmful situations, such as ongoing
harassment or threats to anyone's safety, we may take action without notice.

## Attribution

This Code of Conduct is adapted from the Contributor Covenant, version 1.4,
available at
https://www.contributor-covenant.org/version/1/4/code-of-conduct.html
","# How to contribute

We'd love to accept your patches and contributions to this project. We do have
some guidelines to follow, covered in this document, but don't be concerned
about getting everything right the first time! Create a pull request (discussed
below) and we'll nudge you in the right direction.

## Before you begin

### Sign our Contributor License Agreement

Contributions to this project must be accompanied by a [Contributor License
Agreement](https://cla.developers.google.com/about) (CLA). You (or your
employer) retain the copyright to your contribution; the CLA simply gives us
permission to use and redistribute your contributions as part of the project.
Please visit https://cla.developers.google.com/ to see your current agreements
on file or to sign a new one. You generally only need to submit a Google CLA
once, so if you've already submitted one (even if it was for a different
project), you probably don't need to do it again.

> [!WARNING]
> Please note carefully clauses [#5](https://cla.developers.google.com/about/google-corporate#:~:text=You%20represent%20that%20each%20of%20Your%20Contributions%20is%20Your%20original%20creation)
> and [#7](https://cla.developers.google.com/about/google-corporate#:~:text=Should%20You%20wish%20to%20submit%20work%20that%20is%20not%20Your%20original%20creation%2C%20You%20may%20submit%20it%20to%20Google%20separately)
> in the CLA. Any code that you contribute to this project must be **your**
> original creation. Code generated by artificial intelligence tools **does
> not** qualify as your original creation.

### Review our community guidelines

We have a [code of conduct](CODE_OF_CONDUCT.md) to make the project an open and
welcoming community environment. Please make sure to read and abide by the code
of conduct.

## Contribution process

All submissions, including submissions by project members, require review. We
use the tools provided by GitHub for [pull
requests](https://docs.github.com/articles/about-pull-requests) for this
purpose. The preferred manner for submitting pull requests is to
[fork](https://docs.github.com/articles/fork-a-repo) the repository, create a
new [git branch](https://docs.github.com/articles/about-branches) in this fork
to do your work, and when ready, create a pull request from your branch to the
main project repository.

We follow the [Google coding style](https://google.github.io/styleguide/) for
the programming languages used in this project, with only a few deviations. The
[`.editorconfig`](.editorconfig) file in this repository defines the settings
we use.

### Repository forks

1.  Fork the OpenFermion repository (you can use the _Fork_ button in upper
    right corner of the [repository
    page](https://github.com/quantumlib/OpenFermion)). Forking creates a new
    GitHub repo at the location `https://github.com/USERNAME/OpenFermion`, where
    `USERNAME` is your GitHub user name.

1.  Clone (using `git clone`) or otherwise download your forked repository to
    your local computer, so that you have a local copy where you can do your
    development work using your preferred editor and development tools.

1.  Check out the `main` branch and create a new [git
    branch](https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell)
    from `main`:

    ```shell
    git checkout main -b YOUR_BRANCH_NAME
    ```

    where `YOUR_BRANCH_NAME` is the name of your new branch.

### Development environment installation

Please refer to the section _Developer install_ of the [installation
instructions](docs/install.md) for information about how to set up a local copy
of the software for development.

### Tests and test coverage

Existing tests must continue to pass (or be updated) when changes are
introduced, and code should be covered by tests. We use
[pytest](https://docs.pytest.org) to run our tests and
[pytest-cov](https://pytest-cov.readthedocs.io) to compute coverage. We use the
scripts [`./check/pytest`](./check/pytest) and
[`./check/pytest-and-incremental-coverage`](./check/pytest-and-incremental-coverage)
to run these programs with custom configurations for this project.

We don't require 100% coverage, but any uncovered code must be annotated with `#
pragma: no cover`. To ignore coverage of a single line, place `# pragma: no
cover` at the end of the line. To ignore coverage for an entire block, start the
block with a `# pragma: no cover` comment on its own line.

### Lint

Code should meet common style standards for Python and be free of error-prone
constructs. We use [Pylint](https://www.pylint.org/) to check for code lint, and
the script [`./check/pylint`](./check/pylint) to run it. When Pylint produces a
false positive, it can be silenced with annotations. For example, the annotation
`# pylint: disable=unused-import` would silence a warning about an unused
import.

### Type annotations

Code should have [type annotations](https://www.python.org/dev/peps/pep-0484/).
We use [mypy](http://mypy-lang.org/) to check that type annotations are correct,
and the script [`./check/mypy`](./check/mypy) to run it. When type checking
produces a false positive, it can be silenced with annotations such as `# type:
ignore`.

### Pull requests and code reviews

1.  If your local copy has drifted out of sync with the `main` branch of the
    main OpenFermion repo, you may need to merge the latest changes into your
    branch. To do this, first update your local `main` and then merge your local
    `main` into your branch:

    ```shell
    # Track the upstream repo (if your local repo hasn't):
    git remote add upstream https://github.com/quantumlib/OpenFermion.git

    # Update your local main.
    git fetch upstream
    git checkout main
    git merge upstream/main
    # Merge local main into your branch.
    git checkout YOUR_BRANCH_NAME
    git merge main
    ```

    If git reports conflicts during one or both of these merge processes, you
    may need to [resolve the merge conflicts](
    https://docs.github.com/articles/about-merge-conflicts) before continuing.

1.  Finally, push your changes to your fork of the OpenFermion repo on GitHub:

    ```shell
    git push origin YOUR_BRANCH_NAME
    ```

1.  Now when you navigate to the OpenFermion repository on GitHub
    (https://github.com/quantumlib/OpenFermion), you should see the option to
    create a new [pull
    requests](https://help.github.com/articles/about-pull-requests/) from your
    forked repository. Alternatively, you can create the pull request by
    navigating to the ""Pull requests"" tab near the top of the page, and
    selecting the appropriate branches.

1.  A reviewer from the OpenFermion team will comment on your code and may ask
    for changes. You can perform the necessary changes locally, commit them to
    your branch as usual, and then push changes to your fork on GitHub following
    the same process as above. When you do that, GitHub will update the code in
    the pull request automatically.
","# Reporting security issues

The OpenFermion developers and community take security bugs in OpenFermion
seriously. We appreciate your efforts to responsibly disclose your findings,
and will make every effort to acknowledge your contributions.

Please **do not** use GitHub issues to report security vulnerabilities; GitHub
issues are public, and doing so could allow someone to exploit the information
before the problem can be addressed. Instead, please use the GitHub [""Report a
Vulnerability""](https://github.com/quantumlib/OpenFermion/security/advisories/new)
interface from the _Security_ tab of the OpenFermion repository.

Please report security issues in third-party modules to the person or team
maintaining the module rather than the OpenFermion project stewards, unless you
believe that some action needs to be taken with OpenFermion in order to guard
against the effects of a security vulnerability in a third-party module.

## Responses to security reports

The project stewards at Google Quantum AI will send a response indicating the
next steps in handling your report. After the initial reply to your report, the
project stewards will keep you informed of the progress towards a fix and full
announcement, and may ask for additional information or guidance.

## Additional points of contact

Please contact the project stewards at Google Quantum AI via email at
quantum-oss-maintainers@google.com if you have questions or other concerns. If
for any reason you are uncomfortable reaching out to the project stewards,
please email opensource@google.com instead.
",,,110,,,
437832906,R_kgDOGhjMyg,training-collection,sib-swiss/training-collection,0,sib-swiss,https://github.com/sib-swiss/training-collection,Collection of bioinformatics training materials,0,2021-12-13 10:41:18+00:00,2025-02-23 01:40:58+00:00,2025-02-23 01:40:55+00:00,,217,1006,1006,Python,1,1,1,1,0,0,184,0,0,1,other,1,0,0,public,184,1,1006,main,1,1,,"['GeertvanGeest', 'WandrilleD', 'abotzki', 'hrhotz', 'robinengler']",0,,0.67,0,,,,,,44,,,
375769411,MDEwOlJlcG9zaXRvcnkzNzU3Njk0MTE=,ctf-archives,sajjadium/ctf-archives,0,sajjadium,https://github.com/sajjadium/ctf-archives,CTF Archives: Collection of CTF Challenges.,0,2021-06-10 16:53:52+00:00,2025-03-07 05:48:45+00:00,2025-03-04 04:22:07+00:00,https://twitter.com/sajjadium,23967101,980,980,Python,1,1,1,1,0,0,143,0,0,2,mit,1,0,0,public,143,2,980,main,1,,,"['kinasant', 'sajjadium', 'wxrdnx']",0,,0.74,0,,,,,,24,,,
134979079,MDEwOlJlcG9zaXRvcnkxMzQ5NzkwNzk=,dynamic-analysis,analysis-tools-dev/dynamic-analysis,0,analysis-tools-dev,https://github.com/analysis-tools-dev/dynamic-analysis,"⚙️ A curated list of dynamic analysis tools and linters for all programming languages, binaries, and more.",0,2018-05-26 16:52:14+00:00,2025-03-07 04:45:54+00:00,2025-03-03 11:38:19+00:00,https://analysis-tools.dev,1042,977,977,Markdown,1,0,1,0,0,0,107,0,0,0,mit,1,0,0,public,107,0,977,master,1,1,"<!-- 🚨🚨 DON'T EDIT THIS FILE DIRECTLY. Edit `data/tools.yml` instead. 🚨🚨 -->

<a href=""https://analysis-tools.dev/"">
  <img alt=""Analysis Tools Website"" src=""https://raw.githubusercontent.com/analysis-tools-dev/assets/master/static/redesign.svg"" />
</a>

This repository lists **dynamic analysis tools** for all programming languages, build tools, config files and more. The focus is on tools which improve code quality such as linters and formatters.
The official website, [analysis-tools.dev](https://analysis-tools.dev/) is based on this repository and adds rankings, user comments, and additional resources like videos for each tool.

[![Website](https://img.shields.io/badge/Website-Online-2B5BAE)](https://analysis-tools.dev)
![CI](https://github.com/analysis-tools-dev/dynamic-analysis/workflows/CI/badge.svg)

## Sponsors

This project would not be possible without the generous support of our sponsors.

<table>
   <tr>
      <td>
         <a href=""https://bugprove.com"">
            <picture >
               <source width=""200px"" media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/analysis-tools-dev/assets/master/static/sponsors/bugprove-dark.svg"">
               <img width=""200px"" alt=""BugProve"" src=""https://raw.githubusercontent.com/analysis-tools-dev/assets/master/static/sponsors/bugprove-light.svg"">
            </picture>
         </a>
      </td>
      <td>
         <a href=""https://www.betterscan.io"">
            <picture >
               <source width=""200px"" media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/analysis-tools-dev/assets/master/static/sponsors/betterscan-dark.svg"">
               <img width=""200px"" alt=""Betterscan"" src=""https://raw.githubusercontent.com/analysis-tools-dev/assets/master/static/sponsors/betterscan-light.svg"">
            </picture>
         </a>
      </td>
      <td>
         <a href=""https://www.pixee.ai/"">
            <picture >
               <source width=""200px"" media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/analysis-tools-dev/assets/master/static/sponsors/pixee-light.png"">
               <img width=""200px"" alt=""Pixee"" src=""https://raw.githubusercontent.com/analysis-tools-dev/assets/master/static/sponsors/pixee-dark.png"">
            </picture>
         </a>
      </td>
      <td>
         <a href=""https://coderabbit.ai"">
            <img width=""200px"" src=""https://raw.githubusercontent.com/analysis-tools-dev/assets/master/static/sponsors/code-rabbit.svg"" />
         </a>
      </td>
      <td>
         <a href=""https://semgrep.dev/"">
            <img width=""200px"" src=""https://raw.githubusercontent.com/analysis-tools-dev/assets/master/static/sponsors/semgrep.svg"" />
         </a>
      </td>
      <td>
         <a href=""https://offensive360.com/"">
            <img width=""200px"" src=""https://raw.githubusercontent.com/analysis-tools-dev/assets/master/static/sponsors/offensive360.png"" />
         </a>
      </td>
   </tr>
</table>

If you also want to support this project, head over to our [Github sponsors page](https://github.com/sponsors/analysis-tools-dev).

## Meaning of Symbols:

- :copyright: stands for proprietary software. All other tools are Open Source.
- :information_source: indicates that the community does not recommend to use this tool for new projects anymore. The icon links to the discussion issue.
- :warning: means that this tool was not updated for more than 1 year, or the repo was archived.

Pull requests are very welcome!  
Also check out the sister project, [awesome-static-analysis](https://github.com/mre/awesome-static-analysis).

## Table of Contents

#### [Programming Languages](#programming-languages-1)

<details>
 <summary>Show languages</summary>
  <!-- Please use HTML syntax here so that it works for Github and mkdocs -->
  <ul>
    <li><a href=""#dotnet"">.NET</a></li>
    <li><a href=""#c"">C</a></li>
    <li><a href=""#cpp"">C++</a></li>
    <li><a href=""#go"">Go</a></li>
    <li><a href=""#java"">Java</a></li>
    <li><a href=""#javascript"">JavaScript</a></li>
    <li><a href=""#php"">PHP</a></li>
    <li><a href=""#python"">Python</a></li>
    <li><a href=""#ruby"">Ruby</a></li>
    <li><a href=""#rust"">Rust</a></li>
    <li><a href=""#sql"">SQL</a></li>
    <li><a href=""#vbasic"">Visual Basic</a></li>
    <li><a href=""#zig"">Zig</a></li>
    </ul>
</details>

#### [Multiple languages](#multiple-languages-1)

#### [Other](#other-1)



- [API](#api)
  

- [Binaries](#binary)
  

- [Bytecode/IR](#bytecode)
  

- [Cloud](#cloud)
  

- [Containers](#container)
  

- [Laravel](#laravel)
  

- [Security/DAST](#security)
  

- [Web](#web)
  

- [WebAssembly](#webassembly)
  

- [XML](#xml)
  

---

## Programming Languages

<h2 id=""dotnet"">.NET</h2>



- [Microsoft IntelliTest](https://docs.microsoft.com/en-us/visualstudio/test/intellitest-manual/getting-started?view=vs-2019) — Generate a candidate suite of tests for your .NET code.
  

- [Pex and Moles](https://www.microsoft.com/en-us/research/project/pex-and-moles-isolation-and-white-box-unit-testing-for-net/) — Pex automatically generates test suites with high code coverage using automated white box analysis.
  

<h2 id=""c"">C</h2>



- [CHAP](https://github.com/vmware/chap) — Analyzes un-instrumented ELF core files for leaks, memory growth, and corruption. It helps explain memory growth, can identify some forms of corruption, and  supplements a debugger by giving the status of various memory locations.
  

- [KLEE](https://github.com/klee/klee) — Symbolic virtual machine built on top of the LLVM compiler infrastructure.
  

- [LDRA](https://ldra.com) :copyright: — A tool suite including dynamic analysis and test to various standards can ensure test coverage to 100% op-code, branch & decsion coverage.
  

- [LLVM/Clang Sanitizers](https://github.com/google/sanitizers) — <ul> <li><a href=""https://github.com/google/sanitizers/wiki/AddressSanitizer"">AddressSanitizer</a> - A memory error detector for C/C++</li> <li><a href=""https://github.com/google/sanitizers/wiki/MemorySanitizer"">MemorySanitizer</a> - A detector of uninitialized memory reads in C/C++ programs.</li> <li><a href=""https://github.com/google/sanitizers/wiki/ThreadSanitizerCppManual"">ThreadSanitizer</a> - A data race detector for C/C++</li> </ul>
  

- [Valgrind](https://valgrind.org/) — An instrumentation framework for building dynamic analysis tools.
  

<h2 id=""cpp"">C++</h2>



- [CHAP](https://github.com/vmware/chap) — Analyzes un-instrumented ELF core files for leaks, memory growth, and corruption. It helps explain memory growth, can identify some forms of corruption, and  supplements a debugger by giving the status of various memory locations.
  

- [KLEE](https://github.com/klee/klee) — Symbolic virtual machine built on top of the LLVM compiler infrastructure.
  

- [LDRA](https://ldra.com) :copyright: — A tool suite including dynamic analysis and test to various standards can ensure test coverage to 100% op-code, branch & decsion coverage.
  

- [LLVM/Clang Sanitizers](https://github.com/google/sanitizers) — <ul> <li><a href=""https://github.com/google/sanitizers/wiki/AddressSanitizer"">AddressSanitizer</a> - A memory error detector for C/C++</li> <li><a href=""https://github.com/google/sanitizers/wiki/MemorySanitizer"">MemorySanitizer</a> - A detector of uninitialized memory reads in C/C++ programs.</li> <li><a href=""https://github.com/google/sanitizers/wiki/ThreadSanitizerCppManual"">ThreadSanitizer</a> - A data race detector for C/C++</li> </ul>
  

- [Valgrind](https://valgrind.org/) — An instrumentation framework for building dynamic analysis tools.
  

<h2 id=""go"">Go</h2>



- [statsviz](https://github.com/arl/statsviz) — Instant live visualization of your Go application runtime statistics in the browser. It plots heap usage, MSpans/MCaches, Object counts, Goroutines and GC/CPU fraction.
  

<h2 id=""java"">Java</h2>



- [Java PathFinder](https://github.com/javapathfinder/jpf-core) — An extensible software model checking framework for Java bytecode programs.
  

- [Parasoft Jtest](https://www.parasoft.com/products/jtest) :copyright: — Jtest is an automated Java software testing and static analysis product that is made by Parasoft. The product includes technology for Data-flow analysis Unit test-case generation and execution, static analysis, regression testing, code coverage, and runtime error detection.
  

<h2 id=""javascript"">JavaScript</h2>



- [Iroh.js](https://github.com/maierfelix/Iroh) — A dynamic code analysis tool for JavaScript. Iroh allows to record your code flow in realtime, intercept runtime informations and manipulate program behaviour on the fly.
  

- [Jalangi2](https://github.com/Samsung/jalangi2) — Jalangi2 is a popular framework for writing dynamic analyses for JavaScript.
  

<h2 id=""php"">PHP</h2>



- [Enlightn](https://www.laravel-enlightn.com/) — A static and dynamic analysis tool for Laravel applications that provides recommendations to improve the performance, security and code reliability of Laravel apps. Contains 120 automated checks.
  

<h2 id=""python"">Python</h2>



- [CrossHair](https://github.com/pschanely/CrossHair) — Symbolic execution engine for testing Python contracts.
  

- [DynaPyt](https://github.com/sola-st/DynaPyt) — DynaPyt is a framework for writing dynamic analyses for Python. The analyses can also modify runtime values to alter the execution.
  

- [icontract](https://github.com/Parquery/icontract) — Design-by-contract library supporting behavioral subtyping
There is also a wider tooling around the icontract library such as  a linter (pyicontract-lint) and a plug-in for Sphinx (sphinx-icontract).
  

- [Scalene](https://github.com/emeryberger/scalene) — A high-performance, high-precision CPU and memory profiler for Python
  

- [typo](https://github.com/aldanor/typo) — Runtime Type Checking for Python 3.
  

<h2 id=""ruby"">Ruby</h2>



- [suture](https://github.com/testdouble/suture) — A Ruby gem that helps you refactor your legacy code  by the result of some old behavior with a new version.
  

<h2 id=""rust"">Rust</h2>



- [cargo-careful](https://github.com/RalfJung/cargo-careful) — Execute Rust code carefully, with extra checking along the way. It builds the standard library with debug assertions.
Here are some of the checks this enables:
* `get_unchecked` in slices performs bounds checks * `copy`, `copy_nonoverlapping`, and `write_bytes` check that pointers are aligned and non-null and (if applicable) non-overlapping `{NonNull,NonZero*,...}::new_unchecked` check that the value is valid * plenty of internal consistency checks in the collection types * mem::zeroed and the deprecated mem::uninitialized panic if the type does not allow that kind of initialization
  

- [hyperfine](https://github.com/sharkdp/hyperfine) — A command-line benchmarking tool It features statistical analysis across multiple runs, support for arbitrary shell commands, constant feedback about the benchmark progress and current estimates, warmup runs, a simple and expressive syntax, and more.
  

- [loom](https://github.com/tokio-rs/loom) — Concurrency permutation testing tool for Rust.  It runs a test many times, permuting the possible concurrent executions of that test.
  

- [MIRI](https://github.com/rust-lang/miri) — An interpreter for Rust's mid-level intermediate representation, which can detect certain classes of undefined behavior like out-of-bounds memory accesses and use-after-free.
  

- [puffin](https://github.com/EmbarkStudios/puffin) — Instrumentation profiler for Rust.
  

- [rust-san](https://github.com/japaric/rust-san) — How-to sanitize your Rust code with built-in Rust dynamic analyzers
  

- [stuck](https://github.com/jonhoo/stuck) — provides a visualization for quickly identifying common bottlenecks in running, asynchronous, and concurrent applications.
  

<h2 id=""sql"">SQL</h2>



- [WhiteHat Sentinel Dynamic](https://www.synopsys.com/software-integrity/security-testing/dast.html) :copyright: — Part of the WhiteHat Application Security Platform. Dynamic application security scanner that covers the OWASP Top 10.
  

<h2 id=""vbasic"">Visual Basic</h2>



- [VB Watch](https://www.aivosto.com/vbwatch.html) :copyright: — Profiler, Protector and Debugger for VB6. Profiler measures performance and test coverage. Protector implements robust error handling. Debugger helps monitor your executables.
  

<h2 id=""zig"">Zig</h2>



- [poop](https://github.com/andrewrk/poop) — Performance Optimizer Observation Platform This command line tool uses Linux's `perf_event_open` functionality to compare the performance of multiple commands with a colorful terminal user interface. It is similar to `hyperfine`.
  

## Multiple languages



- [allocscope](https://github.com/matt-kimball/allocscope) — allocscope is a tool for tracking down where the most egregiously large allocations are occurring in a C, C++ or Rust codebase. It is particularly intendend to be useful for developers who want to get a handle on excessive allocations and are working in a large codebase with multiple contributors with allocations occuring in many modules or libraries.
  

- [bytehound](https://github.com/koute/bytehound) — A memory profiler for Linux. Can be used to analyze memory leaks, see where exactly the memory is being consumed, identify temporary allocations and investigate excessive memory fragmentation.
  

- [CASR](https://crates.io/crates/casr) — Crash Analysis and Severity Report.
  

- [Code Pulse](http://code-pulse.com/) — Code Pulse is a free real-time code coverage tool for penetration testing activities by OWASP and Code Dx ([GitHub](https://github.com/codedx/codepulse)).
  

- [Sydr](https://sydr-fuzz.github.io/) :copyright: — Continuous Hybrid Fuzzing and Dynamic Analysis for Security Development Lifecycle.
  

## Other



<h2 id=""api"">API</h2>



- [Smartbear](https://smartbear.com/) :copyright: — Test automation and performance testing platform
  

<h2 id=""binary"">Binaries</h2>



- [angr](https://github.com/angr/angr) — Platform agnostic binary analysis framework from UCSB.
  

- [BOLT](https://github.com/facebookincubator/BOLT) — Binary Optimization and Layout Tool - A linux command-line utility used for optimizing performance of binaries  with profile guided permutation of linking to improve cache efficiency
  

- [Dr. Memory](https://drmemory.org/) — Dr. Memory is a memory monitoring tool capable of identifying memory-related programming errors ([Github](https://github.com/DynamoRIO/drmemory)).
  

- [DynamoRIO](http://www.dynamorio.org/) — Is a runtime code manipulation system that supports code transformations on any part of a program, while it executes.
  

- [llvm-propeller](https://github.com/google/llvm-propeller) — Profile guided hot/cold function splitting to improve cache efficiency. An alternative to BOLT by Facebook
  

- [Pin Tools](https://software.intel.com/en-us/articles/pin-a-dynamic-binary-instrumentation-tool) — A dynamic binary instrumentation tool and a platform for creating analysis tools.
  

- [TRITON](https://triton.quarkslab.com/) — Dynamic Binary Analysis for x86 binaries.
  

<h2 id=""bytecode"">Bytecode/IR</h2>



- [souper](https://github.com/google/souper) — optimize LLVM IR with SMT solvers
  

<h2 id=""cloud"">Cloud</h2>



- [prowler](https://prowler.pro) — Prowler is an Open Source security tool to perform AWS and Azure security best practices assessments, audits, incident response, continuous monitoring, hardening and forensics readiness.
It contains hundreds of controls covering CIS, PCI-DSS, ISO27001, GDPR, HIPAA, FFIEC, SOC2, AWS FTR, ENS and custom security frameworks.
  

<h2 id=""container"">Containers</h2>



- [cadvisor](https://github.com/google/cadvisor) — Analyzes resource usage and performance characteristics of running containers.
  

<h2 id=""laravel"">Laravel</h2>



- [Enlightn](https://www.laravel-enlightn.com/) — A static and dynamic analysis tool for Laravel applications that provides recommendations to improve the performance, security and code reliability of Laravel apps. Contains 120 automated checks.
  

<h2 id=""security"">Security/DAST</h2>



- [AppScan Standard](https://www.hcltechsw.com/products/appscan) :copyright: — HCL's AppScan is a dynamic application security testing suite (previously by IBM)
  

- [Enlightn](https://www.laravel-enlightn.com/) — A static and dynamic analysis tool for Laravel applications that provides recommendations to improve the performance, security and code reliability of Laravel apps. Contains 120 automated checks.
  

- [WhiteHat Sentinel Dynamic](https://www.synopsys.com/software-integrity/security-testing/dast.html) :copyright: — Part of the WhiteHat Application Security Platform. Dynamic application security scanner that covers the OWASP Top 10.
  

<h2 id=""web"">Web</h2>



- [Smartbear](https://smartbear.com/) :copyright: — Test automation and performance testing platform
  

<h2 id=""webassembly"">WebAssembly</h2>



- [Wasabi](https://github.com/danleh/wasabi) — Wasabi is a framework for writing dynamic analyses for WebAssembly, written in JavaScript.
  

<h2 id=""xml"">XML</h2>



- [WhiteHat Sentinel Dynamic](https://www.synopsys.com/software-integrity/security-testing/dast.html) :copyright: — Part of the WhiteHat Application Security Platform. Dynamic application security scanner that covers the OWASP Top 10.
  

## License

[![CC0](https://i.creativecommons.org/p/zero/1.0/88x31.png)](https://creativecommons.org/publicdomain/zero/1.0/)

To the extent possible under law, [Matthias Endler](https://endler.dev) has waived all copyright and related or neighboring rights to this work.
The underlying source code used to format and display that content is licensed under the MIT license.

Title image [Designed by Freepik](http://www.freepik.com).","['dependabot[bot]', 'mre', 'dependabot-preview[bot]', 'AristoChen', 'AryazE', 'Harvester57', 'MadhuNimmo', 'm-zakeri', 'lujiefsi', 'jakubsacha', 'GadgetSteve', 'pschanely', 'philzook58', 'spekulatius', 'paras-malhotra', 'MichaelHinrichs', 'arthaud', 'mristin', 'chadbrewbaker', 'SweetVishnya']",0,,0.7,0,,"# How to add a new tool to the list

Please feel free to open a pull request if you know of a dynamic analysis tool that
is not mentioned here.  
If you're in doubt if a tool is a good fit for the list, **don't open an issue,
but create a pull request right away** because that's easier to handle. Thanks!
:smiley:

### Requirements

Each tool on the list should be

- actively maintained (more than one contributor)
- actively used (have **more than 20 stars on Github or similar impact**)
- relatively mature (project exists for at least three months)

### Format

The main `README.md` is just a rendered version of the data. To add a new tool,
please create a file in the `data/tools` directory.

- Make each tool description as precise as possible.  
  Please limit the description to **500 characters**.
- By default, we assume that the tool is open source.
  If a tool is proprietary, add `proprietary: true`.
- Please add as many tags as possible. You can choose from the tags
  in `data/tags.yml` If a tool does not match any existing tag, feel
  free to add a new tag.

Finally, create a pull request with all your changes.
You can call `make render` to check for errors before.  
This is optional, because it will also be done when creating
a pull request.

# How to mark a tool as unmaintained/deprecated

Sometimes a tool becomes unmaintained and there's nothing wrong with that.  
After all, a tool can still be very valuable to the community - even without
frequent updates.  
However, since it is one of the goals of this project to allow people to make an
informed decision on what is the best tool for the job, we are marking
unmaintained or deprecated tools.
[Here](https://github.com/mre/awesome-static-analysis/issues/223) is a nice
discussion about why we think this is necessary. If you find a tool, which is
unmaintained, please add `deprecated: true` to the entry in `data/tools/` and
create a pull request in which you provide an objective explanation as to why
you think the tool should be marked deprecated. Every deprecation will be
handled on a case-by-case basis.

**Thanks for helping out!** :tada:
",,,,31,,,
30662167,MDEwOlJlcG9zaXRvcnkzMDY2MjE2Nw==,TheiaSfM,sweeneychris/TheiaSfM,0,sweeneychris,https://github.com/sweeneychris/TheiaSfM,An open source library for multiview geometry and structure from motion,0,2015-02-11 18:15:17+00:00,2025-03-05 17:23:04+00:00,2023-04-03 19:08:29+00:00,,31871,920,920,C++,1,1,1,1,0,0,280,0,0,46,other,1,0,0,public,280,46,920,master,1,,"Copyright 2015-2016 Chris Sweeney (sweeney.chris.m@gmail.com)
UC Santa Barbara

What is this library?
---------------------

Theia is an end-to-end structure-from-motion library that was created by Chris
Sweeney. It is designed to be very efficient, scalable, and accurate. All
steps of the pipeline are designed to be modular so that code is easy to read
and easy to extend.

Please see the Theia website for detailed information, including instructions
for building Theia and full documentation of the library. The website is
currently located at http://www.theia-sfm.org

Contact Information
-------------------

Questions, comments, and bug reports can be sent to the Theia mailing list:
theia-vision-library@googlegroups.com

Citing this library
-------------------

If you are using this library for academic research or publications we ask that
you please cite this library as:

    @misc{theia-manual,
      author = {Chris Sweeney},
      title = {Theia Multiview Geometry Library: Tutorial \& Reference},
      howpublished = ""\url{http://theia-sfm.org}"",
    }
","['sweeneychris', 'vfragoso', 'urbste', 'nuernber', 'pmoulon', 'SeyoungPyo', 'KindDragon', 'holynski', 'nosleduc', 'stoyanovd', 'oadoad', 'bjornpiltz', 'rajvishah', 'cqd123123', 'zhixy', 'tomrunia', 'tompaynter03', 'klemmster', 'rsanchezsaez', 'oleg-alexandrov', 'nrondaud', 'Meekohi', 'kvalev', 'iago-suarez', 'groundmelon', 'Anstow', 'davesque', 'cbalint13']",1,,0.75,0,,,,,,73,,,
161059470,MDEwOlJlcG9zaXRvcnkxNjEwNTk0NzA=,TEE-reversing,enovella/TEE-reversing,0,enovella,https://github.com/enovella/TEE-reversing,A curated list of public TEE resources for learning how to reverse-engineer and achieve trusted code execution on ARM devices,0,2018-12-09 16:36:09+00:00,2025-03-04 14:03:50+00:00,2024-07-08 09:56:27+00:00,,21908,916,916,,1,1,1,1,0,0,106,0,0,0,,1,0,0,public,106,0,916,master,1,,"# TEE Basics & General

- Introduction to Trusted Execution Environment: ARM's TrustZone
	- https://blog.quarkslab.com/introduction-to-trusted-execution-environment-arms-trustzone.html

- Introduction to TEE (original title: TEEを中心とするCPUセキュリティ機能の動向 )
	- https://seminar-materials.iijlab.net/iijlab-seminar/iijlab-seminar-20181120.pdf

- Attacking the ARM's TrustZone
	- https://blog.quarkslab.com/attacking-the-arms-trustzone.html

- ARM TrustZone Security Whitepaper
	- http://infocenter.arm.com/help/topic/com.arm.doc.prd29-genc-009492c/PRD29-GENC-009492C_trustzone_security_whitepaper.pdf

- Web Site ARM TrustZone
	- https://developer.arm.com/ip-products/security-ip/trustzone

- TrustZone Explained: Architectural Features and Use Cases
	- http://sefcom.asu.edu/publications/trustzone-explained-cic2016.pdf

- Trustworthy Execution on Mobile Devices
	- https://netsec.ethz.ch/publications/papers/paper-hyperphone-TRUST-2012.pdf

- Demystifying ARM Trustzone : A Comprehensive Survey
	- https://www.researchgate.net/profile/Nuno_Santos9/publication/330696364_Demystifying_Arm_TrustZone_A_Comprehensive_Survey/links/5c6ff1a792851c6950379cdd/Demystifying-Arm-TrustZone-A-Comprehensive-Survey.pdf

- Understanding Trusted Execution Environments and Arm TrustZone (by Azeria)
	- https://azeria-labs.com/trusted-execution-environments-tee-and-trustzone/

- SoK: Understanding the Prevailing Security Vulnerabilities in TrustZone-assisted TEE Systems
	- https://www.cs.purdue.edu/homes/pfonseca/papers/sp2020-tees.pdf

- Giving Mobile Security the Boot (by Jonathan Levin)
	- https://papers.put.as/papers/ios/2016/TrustZone.pdf

- The ARMs race to TrustZone (by Jonathan Levin)
	- http://technologeeks.com/files/TZ.pdf

# TEE Exploits/Security Analysis

## HiSilicon/Huawei (TrustedCore)

- Exploiting Trustzone on Android (BH-US 2015) by Di Shen(@returnsme)
	- https://www.blackhat.com/docs/us-15/materials/us-15-Shen-Attacking-Your-Trusted-Core-Exploiting-Trustzone-On-Android-wp.pdf

- EL3 Tour : Get the Ultimate Privilege of Android Phone (Infiltrate19)
	- https://speakerdeck.com/hhj4ck/el3-tour-get-the-ultimate-privilege-of-android-phone
	- Paper : [infiltrate.pdf](https://github.com/enovella/TEE-reversing/blob/master/Papers/infiltrate.pdf)
	- video: https://vimeo.com/335948808

- Nailgun: Break the privilege isolation in ARM devices (PoC #2 only)
	- https://github.com/ningzhenyu/nailgun

- Nick Stephens : how does someone unlock your phone with nose. (give big picture of NWd <> SWd communications and exploits) GeekPwn 2016
	- https://fr.slideshare.net/GeekPwnKeen/nick-stephenshow-does-someone-unlock-your-phone-with-nose

## Qualcomm (QSEE)

- Reflections on Trusting TrustZone (2014)
	- https://www.blackhat.com/docs/us-14/materials/us-14-Rosenberg-Reflections-on-Trusting-TrustZone.pdf

- Getting arbitrary code execution in TrustZone's kernel from any context (28/03/2015)
	- http://bits-please.blogspot.com/2015/03/getting-arbitrary-code-execution-in.html

- Exploring Qualcomm's TrustZone implementation (04/08/2015)
	- http://bits-please.blogspot.com/2015/08/exploring-qualcomms-trustzone.html

- Full TrustZone exploit for MSM8974 (10/08/2015)
	- http://bits-please.blogspot.com/2015/08/full-trustzone-exploit-for-msm8974.html

- TrustZone Kernel Privilege Escalation (CVE-2016-2431)
	- http://bits-please.blogspot.com/2016/06/trustzone-kernel-privilege-escalation.html

- War of the Worlds - Hijacking the Linux Kernel from QSEE
	- http://bits-please.blogspot.com/2016/05/war-of-worlds-hijacking-linux-kernel.html

- QSEE privilege escalation vulnerability and exploit (CVE-2015-6639)
	- http://bits-please.blogspot.com/2016/05/qsee-privilege-escalation-vulnerability.html

- Exploring Qualcomm's Secure Execution Environment (26/04/2016)
	- http://bits-please.blogspot.com/2016/04/exploring-qualcomms-secure-execution.html

- Android privilege escalation to mediaserver from zero permissions (CVE-2014-7920 + CVE-2014-7921)
	- http://bits-please.blogspot.com/2016/01/android-privilege-escalation-to.html

- Trust Issues: Exploiting TrustZone TEEs (24 July 2017)
	- https://googleprojectzero.blogspot.com/2017/07/trust-issues-exploiting-trustzone-tees.html

- Breaking Bad. Reviewing Qualcomm ARM64 TZ and HW-enabled Secure Boot on Android (4-9.x)
	- https://github.com/bkerler/slides_and_papers/blob/master/QualcommCrypto.pdf

- Technical Advisory: Private Key Extraction from Qualcomm Hardware-backed Keystores CVE-2018-11976 (NCC)
	- https://www.nccgroup.trust/us/our-research/private-key-extraction-qualcomm-keystore/

- Qualcomm TrustZone Integer Signedness bug (12/2014)
	- https://fredericb.info/2014/12/qpsiir-80-qualcomm-trustzone-integer.html

- The road to Qualcomm TrustZone apps fuzzing (RECON Montreal 2019)
	- https://cfp.recon.cx/media/tz_apps_fuzz.pdf

- Downgrade Attack on TrustZone
	- http://ww2.cs.fsu.edu/~ychen/paper/downgradeTZ.pdf

### Motorola (Qualcomm SoC)

- Unlocking the Motorola Bootloader (10/02/2016)
	- http://bits-please.blogspot.com/2016/02/unlocking-motorola-bootloader.html

### HTC (Qualcomm SoC)

- Here Be Dragons: Vulnerabilities in TrustZone (14/08/2014)
	- https://atredispartners.blogspot.com/2014/08/here-be-dragons-vulnerabilities-in.html

## Trustonic (Kinibi & MobiCore)

- Unbox Your Phone: Parts I, II & III
	- https://medium.com/taszksec/unbox-your-phone-part-i-331bbf44c30c
	- https://medium.com/taszksec/unbox-your-phone-part-ii-ae66e779b1d6
	- https://medium.com/taszksec/unbox-your-phone-part-iii-7436ffaff7c7
	- https://github.com/puppykitten/tbase
	- https://github.com/puppykitten/tbase/blob/master/unboxyourphone_ekoparty.pdf

- KINIBI TEE: Trusted Application Exploitation (2018-12-10)
	- https://www.synacktiv.com/posts/exploit/kinibi-tee-trusted-application-exploitation.html

- TEE Exploitation on Samsung Exynos devices by Eloi Sanfelix: Parts I, II, III, IV
	- https://labs.bluefrostsecurity.de/blog/2019/05/27/tee-exploitation-on-samsung-exynos-devices-introduction/
	- https://labs.bluefrostsecurity.de/files/TEE.pdf
	- video: (Infiltrate 2019) https://vimeo.com/335947683

- Breaking Samsung's ARM TrustZone (BlackHat USA 2019)
	- slides: https://i.blackhat.com/USA-19/Thursday/us-19-Peterlin-Breaking-Samsungs-ARM-TrustZone.pdf
	- video: https://www.youtube.com/watch?v=uXH5LJGRwXI&list=PLH15HpR5qRsWrfkjwFSI256x1u2Zy49VI&index=30

- Launching feedback-driven fuzzing on TrustZone TEE (HITBGSEC2019)
	- https://gsec.hitb.org/materials/sg2019/D2%20-%20Launching%20Feedback-Driven%20Fuzzing%20on%20TrustZone%20TEE%20-%20Andrey%20Akimov.pdf

- A Deep Dive into Samsung's trustzone
	- (Part 1 - intro) https://blog.quarkslab.com/a-deep-dive-into-samsungs-trustzone-part-1.html
	- (Part 2 - fuzzing TAs) https://blog.quarkslab.com/a-deep-dive-into-samsungs-trustzone-part-2.html
	- (Part 3 - exploiting EL3) https://blog.quarkslab.com/a-deep-dive-into-samsungs-trustzone-part-3.html

## Samsung (TEEGRIS)

- Breaking TEE Security :
	- (Part 1 - Intro) https://www.riscure.com/blog/tee-security-samsung-teegris-part-1
	- (Part 2 - Exploiting TAs) https://www.riscure.com/blog/tee-security-samsung-teegris-part-2
	- (Part 3 - EoP TAs > TOS) https://www.riscure.com/blog/tee-security-samsung-teegris-part-3

- Reverse-engineering Samsung Exynos 9820 bootloader and TZ by @astarasikov
	- http://allsoftwaresucks.blogspot.com/2019/05/reverse-engineering-samsung-exynos-9820.html

- Bug Hunting S21’s 10ADAB1E FW (OffensiveCon 2022)
	- https://www.dropbox.com/s/2f14ga52jguu5cy/OffensiveCon%202022%20-%20Bug%20Hunting%20S21s%2010ADAB1E%20FW.pdf?dl=0

## Apple (Secure Enclave)

- Demystifying the Secure Enclave Processor by Tarjei Mandt, Mathew Solnik, and David Wang
	- http://mista.nu/research/sep-paper.pdf
	- *slides* https://www.blackhat.com/docs/us-16/materials/us-16-Mandt-Demystifying-The-Secure-Enclave-Processor.pdf

## Intel (Intel SGX)

- Intel SGX Explained by Victor Costan and Srinivas Devadas
	- https://css.csail.mit.edu/6.858/2017/readings/costan-sgx.pdf


# TEE Fuzzing

- PARTEMU: Enabling Dynamic Analysis of Real-World TrustZone Software Using Emulation
	- https://people.eecs.berkeley.edu/~rohanpadhye/files/partemu-usenixsec20.pdf

- The Road to Qualcomm TrustZone Apps Fuzzing
	- https://research.checkpoint.com/the-road-to-qualcomm-trustzone-apps-fuzzing/
	- https://cfp.recon.cx/media/tz_apps_fuzz.pdf

- Launching feedback-driven fuzzing on TrustZone TEE (HITB GSEC 2019 Singapore)
	- slides: https://gsec.hitb.org/materials/sg2019/D2%20-%20Launching%20Feedback-Driven%20Fuzzing%20on%20TrustZone%20TEE%20-%20Andrey%20Akimov.pdf
	- video: https://www.youtube.com/watch?v=yb7KGznzczs

- Fuzzing Embedded (Trusted) Operating Systems Using AFL (Martijn Bogaard | nullcon Goa 2019) OP-TEE
	- slides: https://nullcon.net/website/archives/pdf/bangalore-2019/fuzzing-embedded-(trusted)-operating-systems%20using-AFL.pdf
	- video: https://www.youtube.com/watch?v=AZhxZlwZ160
	- webinar: https://www.youtube.com/watch?time_continue=12&v=ROyD9RTMePA

- SAN19-225 Fuzzing embedded (trusted) operating systems using AFL (Martijn Bogaard) OP-TEE
	- video: https://www.youtube.com/watch?v=7bYAwaJ7WZw


# TEE Secure Boot

- Reverse Engineering Samsung S6 SBOOT - Part I & II
	- https://blog.quarkslab.com/reverse-engineering-samsung-s6-sboot-part-i.html
	- https://blog.quarkslab.com/reverse-engineering-samsung-s6-sboot-part-ii.html

- Secure initialization of TEEs: when secure boot falls short (EuskalHack 2017)
	- https://www.riscure.com/uploads/2017/08/euskalhack_2017_-_secure_initialization_of_tees_when_secure_boot_falls_short.pdf

- Amlogic S905 SoC: bypassing the (not so) Secure Boot to dump the BootROM
	- https://fredericb.info/2016/10/amlogic-s905-soc-bypassing-not-so.html#amlogic-s905-soc-bypassing-not-so

- Qualcomm Secure Boot and Image Authentication Technical Overview
	- https://www.qualcomm.com/documents/secure-boot-and-image-authentication-technical-overview-v20

- Breaking Samsung's Root of Trust - Exploiting Samsung Secure Boot (BlackHat 2020)
	- https://teamt5.org/en/posts/blackhat-s-talk-breaking-samsung-s-root-of-trust-exploiting-samsung-secure-boot/

- Overview of Secure Boot state in the ARM-based SoCs (Hardware-Aided Trusted Computing devroom - Maciej Pijanowski- FOSDEM 2021)
	- https://archive.fosdem.org/2021/schedule/event/tee_arm_secboot/attachments/paper/4635/export/events/attachments/tee_arm_secboot/paper/4635/Overview_of_Secure_Boot_in_Arm_based_SoCs.pdf

- Dive-Into-Android-TA-BugHunting-And-Fuzzing (Kanxue SDC 2023)
        - https://github.com/guluisacat/MySlides/blob/main/KanxueSDC2023/%E3%80%90%E8%AE%AE%E9%A2%98%E3%80%91%E6%B7%B1%E5%85%A5Android%E5%8F%AF%E4%BF%A1%E5%BA%94%E7%94%A8%E6%BC%8F%E6%B4%9E%E6%8C%96%E6%8E%98.pdf

# TEE Videos

- Ekoparty-13 (2017) Daniel Komaromy - Unbox Your Phone - Exploring and Breaking Samsung's TrustZone SandBoxes
	- video: https://www.youtube.com/watch?v=L2Mo8WcmmZo
	- slides: https://github.com/puppykitten/tbase/blob/master/unboxyourphone_ekoparty.pdf

- Daniel Komaromy - Enter The Snapdragon (2014-10-11)
	- https://www.youtube.com/watch?v=2wJRnewVE-g

- BSides DC 2018 & DerbiCon VIII - On the nose: Bypassing Huaweis Fingerprint Authentication by Exploiting the TrustZone by Nick Stephens
	- https://www.youtube.com/watch?v=QFFhdqP7Dxg
	- https://www.youtube.com/watch?v=MdoGCXGHGnY

- An infestation of dragons: Exploring vulnerabilities in the ARM TrustZone architecture by Josh Thomas and Charles Holmes Android Security Symposium in Vienna, Austria, 9-11 September 2015
	- https://www.youtube.com/watch?v=vxNGgOR-iVM

- Android and trusted execution environments by Jan-Erik Ekberg (Trustonic) at the Android Security Symposium in Vienna, Austria, 9-11 September 2015
	- https://www.youtube.com/watch?v=5542lEk3OAM

- 34C3 2017 - Console Security - Switch by Plutoo, Derrek and Naehrwert
	- https://media.ccc.de/v/34c3-8941-console_security_-_switch

- 34C3 2017 - TrustZone is not enough by Pascal Cotret
	- https://media.ccc.de/v/34c3-8831-trustzone_is_not_enough

- RootedCON 2017 - What your mother never told you about Trusted Execution Environment... by José A. Rivas
	- *audio Spanish original* https://www.youtube.com/watch?v=lzrIzS84mdk
	- *English translation* https://www.youtube.com/watch?v=Lzb5OfE1M7s

- BH US 2015 - Fingerprints On Mobile Devices: Abusing And Leaking
	- https://www.youtube.com/watch?v=7NkojB9gLXM

- No ConName 2015 - (Un)Trusted Execution Environments by Pau Oliva
	- video: *audio Spanish only* https://vimeo.com/150787883
	- slides: https://t.co/vFATxEa7sy

- BH US 2014 - Reflections on Trusting TrustZone by Dan Rosenberg
	- https://www.youtube.com/watch?v=7w40mS5yLjc

- ARM TrustZone for dummies by Tim Hummels
	- https://www.youtube.com/watch?v=ecBByjwny3s

# Microarchitectural attacks applied to TEE

- ARMageddon: Cache attacks on mobile devices
    - [Paper] https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_lipp.pdf
    - [Related tool] https://github.com/IAIK/armageddon

- Cache storage channels: Alias-driven attacks and verified countermeasures.
    - https://www.kth.se/polopoly_fs/1.641701.1550155969!/R.Guanciale.pdf

- 34C3 - Microarchitectural Attacks on Trusted Execution Environments
    - https://media.ccc.de/v/34c3-8950-microarchitectural_attacks_on_trusted_execution_environments

- TruSpy: Cache side-channel information leakage from the secure world on ARM devices
    - https://eprint.iacr.org/2016/980.pdf


# Tools

## Emulate

- QEMU Support for Exynos9820 S-Boot
	- https://github.com/astarasikov/qemu

- Emulating Exynos 4210 BootROM in QEMU
	- https://fredericb.info/2018/03/emulating-exynos-4210-bootrom-in-qemu.html#emulating-exynos-4210-bootrom-in-qemu

## Reverse

- TZAR unpacker
	- https://gist.github.com/astarasikov/f47cb7f46b5193872f376fa0ea842e4b#file-unpack_startup_tzar-py

- IDA MCLF Loader
	- https://github.com/ghassani/mclf-ida-loader

- Ghidra MCLF Loader
	- https://github.com/NeatMonster/mclf-ghidra-loader

# Other useful resources

- ARM Trusted Firmware: reference implementation of secure world for Cortex A and Cortex M
	- https://www.trustedfirmware.org/

- OP-TEE: open source ARM TrusZone based TEE
	- https://www.op-tee.org/

- Trust Issues: Exploiting TrustZone TEEs by Project Zero Team
	- https://googleprojectzero.blogspot.com/2017/07/trust-issues-exploiting-trustzone-tees.html

- Boomerang: Exploiting the Semantic Gap in Trusted Execution Environments (A.Machiry) 2017
	- https://pdfs.semanticscholar.org/f62b/db9f1950329f59dc467238737d2de1a1bac4.pdf (slides)
	- http://sites.cs.ucsb.edu/~cspensky/pdfs/ndss17-final227.pdf (paper)
	- https://github.com/ucsb-seclab/boomerang (tool)

- TEE research (Some useful IDA and Ghidra plugins for TEE research)
	- https://github.com/bkerler/tee_research

","['enovella', 'FrenchYeti', 'jarv-git', 'Resery']",0,,0.72,0,,,,,,48,,,
65838440,MDEwOlJlcG9zaXRvcnk2NTgzODQ0MA==,thesisdown,ismayc/thesisdown,0,ismayc,https://github.com/ismayc/thesisdown,An updated R Markdown thesis template using the bookdown package,0,2016-08-16 17:05:41+00:00,2025-02-09 02:21:24+00:00,2024-10-31 19:00:14+00:00,,3575,812,812,TeX,1,1,1,1,1,0,359,0,0,16,other,1,0,0,public,359,16,812,master,1,,"
<!-- README.md is generated from README.Rmd via `devtools::build_readme()`. Please edit README.Rmd -->

# thesisdown <img src=""man/figures/thesisdown_hex.png"" align=""right"" width=""200""/>

This project was inspired by the
[bookdown](https://github.com/rstudio/bookdown) package and is an
updated version of my Senior Thesis template in the `reedtemplates`
package [here](https://github.com/ismayc/reedtemplates). It was
originally designed to only work with the Reed College LaTeX template,
but has since been adapted to work with many different institutions by
many different individuals. Check out the [**Customizing thesisdown to
your
institution**](https://github.com/ismayc/thesisdown#customizing-thesisdown-to-your-institution)
section below for examples.

Currently, the PDF and gitbook versions are fully-functional. The word
and epub versions are developmental, have no templates behind them, and
are essentially calls to the appropriate functions in bookdown.

If you are new to working with `bookdown`/`rmarkdown`, please read over
the documentation available in the `gitbook` template at
<https://ismayc.github.io/thesisdown_book>.

The current output for the four versions is here:

- [PDF](https://github.com/ismayc/thesisdown_book/blob/master/thesis.pdf)
  (Generating LaTeX file is available
  [here](https://github.com/ismayc/thesisdown_book/blob/master/thesis.tex)
  with other files in the [book
  directory](https://github.com/ismayc/thesisdown_book/tree/master).)
- [Word](https://github.com/ismayc/thesisdown_book/blob/master/thesis.docx)
- [ePub](https://github.com/ismayc/thesisdown_book/blob/master/thesis.epub)
- [gitbook](https://ismayc.github.io/thesisdown_book)

Under the hood, the Reed College LaTeX template is used to ensure that
documents conform precisely to submission standards. At the same time,
composition and formatting can be done using lightweight
[markdown](https://rmarkdown.rstudio.com/authoring_basics.html) syntax,
and **R** code and its output can be seamlessly included using
[rmarkdown](https://rmarkdown.rstudio.com).

## Customizing thesisdown to your institution

In an ideal world, this package would support a variety of different
LaTeX templates from a wide range of institutions and we’d love to get
it there at some point. Until that time, realize that this was designed
to only work with the Reed College LaTeX template but others have
adapted it to work with their institutions. Here are some that have
customized it to fit their needs. It is recommended you review how they
changed the files by comparing their repositories to this one and then
make tweaks to yours as needed. Feel free to file an issue on this repo
if you have questions/troubles.

Have you created a thesisdown template for your institution and would
like to have it included here? Make a PR [similar to the commit done to
include
`jayhawkdown`](https://github.com/ismayc/thesisdown/commit/760113a076767cf67b6e22339e398bd3f15305c5).
I’ll review it and merge it in. Let’s keep the list going!

|College/University                                                        |Repository                          |Based on                            |
|:-------------------------------------------------------------------------|:-----------------------------------|:-----------------------------------|
|American University                                                       |[SimonHeuberger/eagledown](https://github.com/SimonHeuberger/eagledown)|[benmarwick/huskydown](https://github.com/benmarwick/huskydown)|
|Boğaziçi University, the Institute of Graduate Studies in Social Sciences |[serhatcevikel/boundown](https://github.com/serhatcevikel/boundown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Brock University                                                          |[brentthorne/brockdown](https://github.com/brentthorne/brockdown)|[zkamvar/beaverdown](https://github.com/zkamvar/beaverdown)|
|Coventry University                                                       |[tomislavmedak/coventrydown](https://github.com/tomislavmedak/coventrydown)|[ulyngs/oxforddown](https://github.com/ulyngs/oxforddown)|
|Drexel University                                                         |[tbradley1013/dragondown](https://github.com/tbradley1013/dragondown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Duke University                                                           |[mine-cetinkaya-rundel/thesisdowndss](https://github.com/mine-cetinkaya-rundel/thesisdowndss)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|École Doctorale de Mathématiques Hadamard                                 |[abichat/hadamardown](https://github.com/abichat/hadamardown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Graduate Institute of International and Development Studies               |[jhollway/iheiddown](https://github.com/jhollway/iheiddown)|[ulyngs/oxforddown](https://github.com/ulyngs/oxforddown)|
|Heidelberg University, Faculty of Biosciences                             |[nkurzaw/heididown](https://github.com/nkurzaw/heididown)|[phister/huwiwidown](https://github.com/phister/huwiwidown)|
|Humboldt University of Berlin                                             |[phister/huwiwidown](https://github.com/phister/huwiwidown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Kansas State University                                                   |[emraher/wildcatdown](https://github.com/emraher/wildcatdown)|[benmarwick/huskydown](https://github.com/benmarwick/huskydown)|
|Macquarie University                                                      |[thomas-fung/thesisdownmq](https://github.com/thomas-fung/thesisdownmq)|[mine-cetinkaya-rundel/thesisdowndss](https://github.com/mine-cetinkaya-rundel/thesisdowndss)|
|Massachusetts Institute of Technology                                     |[ratatstats/manusdown](https://github.com/ratatstats/manusdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|McMaster University                                                       |[paezha/macdown](https://github.com/paezha/macdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Monash University                                                         |[masiraji/monashthesisdown](https://github.com/masiraji/monashthesisdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Oregon State University                                                   |[zkamvar/beaverdown](https://github.com/zkamvar/beaverdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Oxford University                                                         |[davidplans/oxdown](https://github.com/davidplans/oxdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Queen's University                                                        |[eugenesit/gaelsdown](https://github.com/eugenesit/gaelsdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Smith College                                                             |[SmithCollege-SDS/pioneerdown](https://github.com/SmithCollege-SDS/pioneerdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Southampton University                                                    |[dr-harper/sotonthesis](https://github.com/dr-harper/sotonthesis)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Stanford University                                                       |[mhtess/treedown](https://github.com/mhtess/treedown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|TU Wien                                                                   |[ben-schwen/robotdown](https://github.com/ben-schwen/robotdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Universidade Federal de Santa Catarina                                    |[lfpdroubi/ufscdown](https://github.com/lfpdroubi/ufscdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Universidade Federal do Ceará                                             |[damarals/ufcdown](https://github.com/damarals/ufcdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Universidade Federal do Rio de Janeiro                                    |[COPPE-UFRJ/coppedown](https://github.com/COPPE-UFRJ/coppedown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Universiteit van Amsterdam                                                |[lcreteig/amsterdown](https://github.com/lcreteig/amsterdown)|[benmarwick/huskydown](https://github.com/benmarwick/huskydown)|
|University College London                                                 |[benyohaiphysics/thesisdownUCL](https://github.com/benyohaiphysics/thesisdownUCL)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Arizona                                                     |[kelseygonzalez/beardown](https://github.com/kelseygonzalez/beardown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Auckland                                                    |[d-scanzi/UOAdown](https://github.com/d-scanzi/UOAdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Bergen                                                      |[SaltyRydM/bergendown](https://github.com/SaltyRydM/bergendown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Bristol                                                     |[mattlee821/bristolthesis](https://github.com/mattlee821/bristolthesis)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of California, Davis                                           |[ryanpeek/aggiedown](https://github.com/ryanpeek/aggiedown)|[DanOvando/gauchodown](https://github.com/DanOvando/gauchodown)|
|University of California, Santa Barbara                                   |[DanOvando/gauchodown](https://github.com/DanOvando/gauchodown)|[benmarwick/huskydown](https://github.com/benmarwick/huskydown)|
|University of Florida                                                     |[ksauby/thesisdownufl](https://github.com/ksauby/thesisdownufl)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Freiburg                                                    |[vivekbhr/doctorRbite](https://github.com/vivekbhr/doctorRbite)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Guelph                                                      |[sebsciarra/guelphdown](https://github.com/sebsciarra/guelphdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Kansas                                                      |[wjakethompson/jayhawkdown](https://github.com/wjakethompson/jayhawkdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Manchester                                                  |[juliov/uomthesisdown](https://github.com/JulioV/uomthesisdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Maryland, College Park                                      |[ImNotaGit/thesisdown](https://github.com/ImNotaGit/thesisdown/tree/umd)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Minnesota                                                   |[zief0002/gopherdown](https://github.com/zief0002/gopherdown)|[wjakethompson/jayhawkdown](https://github.com/wjakethompson/jayhawkdown)|
|University of New South Wales                                             |[rensa/unswthesisdown](https://github.com/rensa/unswthesisdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Salzburg                                                    |[irmingard/salzburgthesisdown](https://github.com/irmingard/salzburgthesisdown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|University of Toronto                                                     |[mattwarkentin/torontodown](https://github.com/mattwarkentin/torontodown)|[zkamvar/beaverdown](https://github.com/zkamvar/beaverdown)|
|University of Washington                                                  |[benmarwick/huskydown](https://github.com/benmarwick/huskydown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Université Paris-Saclay                                                   |[abichat/hadamardown](https://github.com/abichat/hadamardown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|
|Youngstown State University                                               |[gjkerns/penguindown](https://github.com/gjkerns/penguindown)|[ismayc/thesisdown](https://github.com/ismayc/thesisdown)|

### Using thesisdown from Chester’s GitHub

Special thanks to [Ben Marwick](https://github.com/benmarwick) for
helping to add a lot more clarity to the directions below from the
[README of his spin-off `huskydown`
package](https://github.com/benmarwick/huskydown/blob/master/README.md).

Using {thesisdown} has some prerequisites which are described below. To
compile PDF documents using **R**, you are going to need to have LaTeX
installed. By far the easiest way to install LaTeX on any platform is
with the [tinytex](https://yihui.name/tinytex/) R package:

``` r
install.packages(c('tinytex', 'rmarkdown'))
tinytex::install_tinytex()
# after restarting RStudio, confirm that you have LaTeX with
tinytex:::is_tinytex()
```

You may need to install a few extra LaTeX packages on your first attempt
to knit as well. Here is one such example of how to do so:

``` r
tinytex::tlmgr_install(""babel-portuges"")
```

To use {thesisdown} from
[RStudio](https://www.rstudio.com/products/rstudio/download/):

1.  Ensure that you have already installed LaTeX and the fonts described
    above, and are using the latest version of
    [RStudio](https://www.rstudio.com/products/rstudio/download/). You
    can use `thesisdown` without RStudio. For example, you can write the
    Rmd files in your favorite text editor
    (e.g. [Atom](https://atom.io/),
    [Notepad++](https://notepad-plus-plus.org/)). But RStudio is
    probably the easiest tool for writing both R code and text in your
    thesis. It also provides a nice way to build your thesis while
    editing. We’ll proceed assuming that you have decided to use the
    RStudio workflow.

2.  Install the {bookdown} and {thesisdown} packages. Note that
    {thesisdown} is not available on CRAN at the moment and that’s why
    `install.packages(""thesisdown"")` won’t work. Use
    `remotes::install_github()` as shown below instead to install the
    package.

    ``` r
    if (!require(""remotes"")) 
      install.packages(""remotes"", repos = ""https://cran.rstudio.org"")
    remotes::install_github(""rstudio/bookdown"")
    remotes::install_github(""ismayc/thesisdown"")
    ```

          Note that you may need to restart RStudio at this point for
the following dialog to show up.

3.  Get started with the {thesisdown} template. There are two options
    for doing so.

- 3a) **RECOMMENDED** Create a new RStudio project with a {thesisdown}
  template.

  In RStudio, click on **File** \> **New Project** \> **New Directory**.
  Then select **Thesis Project using thesisdown** from the dropdown that
  will look something like the image below. You’ll see the graduation
  cap as the icon on the left for the appropriate project type.

  ![](https://raw.githubusercontent.com/ismayc/thesisdown/master/docs/reference/figures/thesis_proj.png)

  Next, give your project a name and specify where you’d like the files
  to appear. In the screenshot below, the project name is `my_thesis`
  and it will appear as a new folder on my Desktop.

  ![](https://raw.githubusercontent.com/ismayc/thesisdown/master/docs/reference/figures/thesis_proj_name.png)

  If you got this far, skip over step 3b which is the older version of
  getting the template. It might force you to change some of the
  directories to get knitting to work and has some other limitations as
  well. That’s why step 3a is recommended.

- 3b) Use the **New R Markdown** dialog to select **Thesis**:

  ![](https://raw.githubusercontent.com/ismayc/thesisdown/master/docs/reference/figures/thesis_rmd.png)

  Note that this will currently only **Knit** if you name the directory
  `index` as shown above. This guarantees that `index.html` is generated
  correctly for the Gitbook version of the thesis.

4.  After choosing which type of output you’d like in the YAML at the
    top of `index.Rmd`, **Knit** the `index.Rmd` file to get the book in
    PDF or HTML formats.

### Day-to-day writing of your thesis

You need to edit the individual chapter R Markdown files to write your
thesis. It’s recommended that you version control your thesis using
GitHub if possible. RStudio can also easily sync up with GitHub to make
the process easier. While writing, you should `git commit` your work
frequently, after every major activity on your thesis. For example,
every few paragraphs or section of text, and after major step of
analysis development. You should `git push` at the end of each work
session before you leave your computer or change tasks. For a gentle,
novice-friendly guide to getting starting with using Git with R and
RStudio, see <https://happygitwithr.com/>.

## Rendering

To render your thesis into a PDF, open `index.Rmd` in RStudio and then
click the “knit” button. To change the output formats between PDF,
gitbook and Word, look at the `output:` field in `index.Rmd` and
comment-out the formats you don’t want.

The PDF file of your thesis will be deposited in the `_book/` directory,
by default.

## Components

The following components are ones you should edit to customize your
thesis:

### `_bookdown.yml`

This is the main configuration file for your thesis. You can change the
name of your outputted file here for your thesis and other options about
your thesis here.

### `index.Rmd`

This file contains all the meta information that goes at the beginning
of your document. You’ll need to edit the top portion of this file (the
YAML) to put your name on the first page, the title of your thesis, etc.
Note that you need to have at least one chapter start in the `index.Rmd`
file for the build to work. For the template, this is done with
`# Introduction` in the example from the template.

### `01-chap1.Rmd`, `02-chap2.Rmd`, etc.

These are the Rmd files for each chapter in your dissertation. Write
your thesis in these. If you’re writing in RStudio, you may find the
[wordcount addin](https://github.com/benmarwick/wordcountaddin) useful
for getting word counts and readability statistics in R Markdown
documents.

### `bib/`

Store your bibliography (as bibtex files) here. We recommend using the
[citr addin](https://github.com/crsh/citr) and
[Zotero](https://www.zotero.org/) to efficiently manage and insert
citations.

### `csl/`

Specific style files for bibliographies should be stored here. A good
source for citation styles is
<https://github.com/citation-style-language/styles#readme>.

### `figure/` and `data/`

Store your figures and data here and reference them in your R Markdown
files. See the [bookdown book](https://bookdown.org/yihui/bookdown/) for
details on cross-referencing items using R Markdown.
","['ismayc', 'nicksolomon', 'abichat', 'Samasaur1', 'eugenesit', 'masiraji', 'shirdekel', 'simonpcouch', 'mralbu', 'sebsciarra', 'keurcien', 'tbradley1013', 'tomislavmedak', 'thomas-fung', 'nkurzaw', 'mattlee821', 'ben-schwen', 'serhatcevikel', 'ssayols', 'SaltyRydM', 'ruaridhw', 'pcastellanoescuder', 'phinguyen44', 'mmahmoudian', 'mavogel', 'lcreteig', 'ImNotaGit', 'JulioV', 'trashbirdecology', 'jhollway', 'Bisaloo', 'gjkerns', 'eliocamp', 'damarals', 'zief0002', 'rudeboybert']",0,,0.61,0,,,,Directory exists,,26,,,
163022594,MDEwOlJlcG9zaXRvcnkxNjMwMjI1OTQ=,awesome-computational-neuroscience,eselkin/awesome-computational-neuroscience,0,eselkin,https://github.com/eselkin/awesome-computational-neuroscience,A list of schools and researchers in computational neuroscience,0,2018-12-24 20:47:35+00:00,2025-03-05 15:34:29+00:00,2024-08-02 18:19:50+00:00,,388,784,784,,1,1,1,1,0,0,79,0,0,6,cc0-1.0,1,0,0,public,79,6,784,master,1,,,"['HussainAther', 'eselkin', 'andrewjmcgehee', 'mschrimpf']",0,,0.69,0,,,,,,32,,,
237835599,MDEwOlJlcG9zaXRvcnkyMzc4MzU1OTk=,hydro-sdk,hydro-sdk/hydro-sdk,0,hydro-sdk,https://github.com/hydro-sdk/hydro-sdk,"Author Flutter experiences in Typescript. No native bridge, no V8. Just Dart. From runtime to virtual machine.",0,2020-02-02 20:57:05+00:00,2025-03-05 09:23:38+00:00,2024-11-30 06:09:39+00:00,,122985,688,688,Dart,1,1,1,1,1,1,36,0,0,173,mit,1,0,0,public,36,173,688,master,1,1,"# Hydro-SDK
![Logo](https://github.com/chgibb/hydro-sdk/blob/master/img/socialImage.png)

Author Flutter experiences in Typescript. No native bridge, no V8. Just Dart. From runtime to virtual machine.

# Sponsored by
Possibly you! See [sponsoring Hydro-SDK](https://github.com/sponsors/hydro-sdk)
# Documentation and Tutorials 
https://hydro-sdk.io/
# Features
## Hot Reload, Incremental Compilation
![Action Gif](https://github.com/hydro-sdk/counter-app/blob/master/media/action-gif.gif)

## Function Maps
![Function maps screenshot](https://github.com/chgibb/hydro-sdk/blob/master/img/sourceMapScreenShot.png)

# Roadmap
Hydro-SDK is a software development kit (SDK) enabling Flutter developers to write portions of their app (or their entire app) using Typescript. It is a project with one large, ambitious goal. ""Become React Native for Flutter"".
It aims to do that by:

1. Decoupling the API surface of Flutter from the Dart programming language.
2. Decoupling the development time experience of Flutter from the Dart programming language.
3. Providing first-class support for over-the-air distribution of code.
4. Providing an ecosystem of packages from pub.dev, automatically projected to supported languages and published to other package systems.

Hydro-SDK is currently only suitable for simple content. A few dozen Flutter widgets are supported. Content built with Hydro-SDK can be updated over the air using Hydro-SDK's built-in [codepush](https://hydro-sdk.io/blog/fluttering-over-the-air). Content can be authored in a similar manner as in Dart with hot-reload and limited IDE debugging support.

## Near Term
### API Support
Continue expanding available Dart and Flutter APIs with automatic language projection (ALP) using [Structured Wrapper and Interface generator for Dart (SWID)](https://github.com/hydro-sdk/hydro-sdk/tree/master/lib/swid).  This effort is where the overwhelming amount of focus currently is. Having a stable and high quality ALP will allow for the re-use of existing packages from `pub.dev` in Typescript. This will also allow for expanding support for authoring Hydro-SDK content to other programming languages.

This work is tracked at a high-level in the [Binding Generator project](https://github.com/hydro-sdk/hydro-sdk/projects/5) and in other sub-projects.
This work is required to expand support for Flutter 2.8 (and 2.10) and is directly related to the following umbrella issues.
- [☂️ Support Flutter 2.8](https://github.com/hydro-sdk/hydro-sdk/issues/773)
- [☂️ Cables](https://github.com/hydro-sdk/hydro-sdk/issues/684)

## Medium Term
### Developer Ergonomics  
Improving developer ergonomics with more affective testing workflows and support for Typescript language features like `async` / `await` interop with Dart.
This is being scoped and tracked at a high level in umbrella issues.
- [☂️ Improved Test Ergonomics](https://github.com/hydro-sdk/hydro-sdk/issues/843)
- [☂️ Typescript 4](https://github.com/hydro-sdk/hydro-sdk/issues/844)
### Performance
Establishing performance baselines through benchmarking. Some efforts to improve performance. This is being scoped and tracked at a high level in umbrella issues.
- [☂️ Optimizing Lua Compiler](https://github.com/hydro-sdk/hydro-sdk/issues/797)
- [☂️ Size Benchmarks](https://github.com/hydro-sdk/hydro-sdk/issues/846)
- [☂️ CPU and Memory Benchmarks](https://github.com/hydro-sdk/hydro-sdk/issues/845)

## Long Term Projects
## Developer Ergonomics
Lower the barrier to entry for developers with non-mobile backgrounds that are interested in trying out Flutter and Hydro-SDK.
- [☂️ Component Preview](https://github.com/hydro-sdk/hydro-sdk/issues/763)
## Services
### Registry
A closed-source component registry for first-class support for hosting and running over-the-air (OTA) update packages built using the open-source Hydro-SDK. This service is already live as a basic [MVP](https://registry.hydro-sdk.io/#/).

### Registry Value Adds
In the future, building paid features such as targeted distribution, analytics, feature flagging and logging directly into Registry are expected to be a viable path to monetization.

### Language Support
Hydro-SDK is language agnostic. In the future, it should be possible to support authoring content in other programming languages to attract users from other ecosystems (Flixel, .NET, Xamarin).
### Tier 1 (hot-reload, programmatic debugging, function maps)
- [&check;] Typescript  
    https://github.com/TypeScriptToLua/TypeScriptToLua
### Tier 2 (hot-reload, programmatic debugging)
- [ ] Haxe  
    https://github.com/HaxeFoundation/haxe    
- [ ] C#  
    https://github.com/yanghuan/CSharp.lua 

# Prior Art for Common Flutter Runtime
- Dartlua, Andre Lipke https://github.com/PixelToast/dartlua
- React Native, Facebook https://reactnative.dev/
- LuaViewSdk, Alibaba https://github.com/alibaba/LuaViewSDK
- ILRuntime, Ourpalm https://github.com/Ourpalm/ILRuntime
- Flutterscript, Charles Lowell https://github.com/cowboyd/flutterscript
- Lisp in Dart, Suzuki Hisao https://github.com/nukata/lisp-in-dart
- widget_extensions https://github.com/canewsin/widget_extensions
- xamarin.flutter https://github.com/adamped/xamarin.flutter

# Prior Art for Structured Wrapper and Interface generator for Dart
- Dartle, https://docs.google.com/document/d/1Ei0ZIqdqNjxTHoGB3Ay6SWQg3DMSsKKWl70XoBUCFTA/edit
- Pigeon, https://github.com/flutter/packages/tree/master/packages/pigeon
- Djinni, https://github.com/dropbox/djinni
- Uniffi-rs, https://github.com/mozilla/uniffi-rs
- TS to CSharp, https://github.com/mono/TsToCSharp


# Interesting Links and Resources
- John C. Reynolds, Definitional Interpreters for Higher-Order Programming Languages https://surface.syr.edu/cgi/viewcontent.cgi?article=1012&context=lcsmith_other
- Andrey Mokhov, Neil Mitchell, Simon Peyton Jones, Build Systems a la carte: Theory and Practice https://www.cambridge.org/core/journals/journal-of-functional-programming/article/build-systems-a-la-carte-theory-and-practice/097CE52C750E69BD16B78C318754C7A4
- Fabio Mascarenhas de Queiroz, Optimized Compilation of a Dynamic Language to a Managed Runtime Environment http://www.lua.inf.puc-rio.br/publications/mascarenhas09optimized.pdf
- Michael Schroder, Optimizing Lua Using Run-time Type Specialization https://www.complang.tuwien.ac.at/anton/praktika-fertig/schroeder/thesis.pdf
- Satoru Kawahara, Optimizing Lua VM Bytecode Using Global Dataflow Analysis https://nymphium.github.io/pdf/opeth_report.pdf
- Haichuan Wang, Compiler and Runtime Techniques for Optimizing Dynamic Scripting Languages https://www.ideals.illinois.edu/bitstream/handle/2142/78638/WANG-DISSERTATION-2015.pdf?sequence=1&isAllowed=y
- Kevin Williams, Jason McCandless, David Gregg, Dynamic Interpretation for Dynamic Scripting Languages https://sites.cs.ucsb.edu/~ckrintz/papers/TCD-CS-2009-37.pdf
- Dibyendu Majumdar, Ravi Lua 5.3 bytecode reference https://the-ravi-programming-language.readthedocs.io/en/latest/lua_bytecode_reference.html
- Kein-Hong Man, A No-Frills Introduction to Lua 5.1 VM Instructions http://luaforge.net/docman/83/98/ANoFrillsIntroToLua51VMInstructions.pdf
- Rust Programming Language Request for Comments 2603, Symbol Name Mangling https://github.com/rust-lang/rfcs/blob/master/text/2603-rust-symbol-name-mangling-v0.md
- Itanium C++ Application Binary Interface Specification https://itanium-cxx-abi.github.io/cxx-abi/abi.html
- Yan Dong Zonz, Wen Hui-chao, Exploration and practice of Flutter packet size management https://tech.meituan.com/2020/09/18/flutter-in-meituan.html
- Shangxian, Meituan takeaway Flutter dynamic practice https://tech.meituan.com/2020/06/23/meituan-flutter-flap.html
- Vyacheslav Egorov, 10 Years of Dart (Slides)
    https://mrale.ph/talks/vmil2020/
","['chgibb', 'dependabot[bot]', 'dependabot-preview[bot]', 'snyk-bot', 'waveform-bot', 'tilk48', 'ibrahimkhiq']",0,,0.73,249170,,,,,,11,,,
9760917,MDEwOlJlcG9zaXRvcnk5NzYwOTE3,shellnoob,reyammer/shellnoob,0,reyammer,https://github.com/reyammer/shellnoob,A shellcode writing toolkit,0,2013-04-30 00:13:15+00:00,2025-03-02 16:40:07+00:00,2022-03-15 18:56:41+00:00,,62,662,662,Python,1,1,1,1,0,0,120,0,0,4,mit,1,0,0,public,120,4,662,master,1,,"# ShellNoob

Writing shellcodes has always been super fun, but some parts are extremely
boring and error prone. Focus only on the fun part, and use **ShellNoob**! 

For a quick overview, check the slides for the Black Hat Arsenal talk:
[link](https://media.blackhat.com/us-13/Arsenal/us-13-Fratantonio-ShellNoob-Slides.pdf)

Want to contribute? Feature request? Bug report? Swears? **All** feedback is
welcome!! (But some kind of feedback is more welcome than others :-)).

Feel free to ping me on twitter [@reyammer](https://twitter.com/reyammer) or to
email me at yanick[AT]cs.ucsb.edu any questions!


## Contributors & Acknowledgments

- Levente Polyak ([@anthraxx42](https://twitter.com/anthraxx42))
    - added Python 3 support
    - bug fixes
- @ToolsWatch & Black Hat crews
    - They gave me a chance to show off my tool :D


## News

- *01/21/2014* - ShellNoob 2.1 is out! It comes with full support for Python 3 and tons of bug fixes. Full credits go to Levente Polyak!

- *07/29/2013* - ShellNoob 2.0 is out!

- *06/08/2013* - ShellNoob got accepted at Black Hat Arsenal! See announcement here: [link](http://www.blackhat.com/us-13/arsenal.html#Fratantonio).


## Features

- convert shellcode between different formats and sources. Formats currently supported: asm, bin, hex, obj, exe, C, python, ruby, pretty, safeasm, completec, shellstorm. (All details in the ""Formats description"" section.)
- interactive asm-to-opcode conversion (and viceversa) mode. This is useful when you cannot use specific bytes in the shellcode and you want to figure out if a specific assembly instruction will cause problems.
- support for both ATT & Intel syntax. Check the ```--intel``` switch.
- support for 32 and 64 bits (when playing on x86\_64 machine). Check the ```--64``` switch.
- resolve syscall numbers, constants, and error numbers (now implemented for real! :-)).
- portable and easily deployable (it only relies on gcc/as/objdump and python). It is just *one self-contained python script*, and it supports both Python2.7+ and Python3+.
- in-place development: you run ShellNoob directly on the target architecture!
- built-in support for Linux/x86, Linux/x86\_64, Linux/ARM, FreeBSD/x86, FreeBSD/x86\_64.
- ""*prepend breakpoint*"" option. Check the ```-c``` switch.
- read from stdin / write to stdout support (use ""-"" as filename)
- uber cheap debugging: check the ```--to-strace``` and ```--to-gdb``` option!
- Use ShellNoob as a Python module in your scripts! Check the ""ShellNoob as a library"" section.
- Verbose mode shows the low-level steps of the conversion: useful to debug / understand / learn!
- Extra plugins: binary patching made easy with the ```--file-patch```, ```--vm-patch```, ```--fork-nopper``` options! (all details below)


## Use Cases

### Built-in help
```bash
$ ./shellnoob.py -h
shellnoob.py [--from-INPUT] (input_file_path | - ) [--to-OUTPUT] [output_file_path | - ]
shellnoob.py -c (prepend a breakpoint (Warning: only few platforms/OS are supported!)
shellnoob.py --64 (64 bits mode, default: 32 bits)
shellnoob.py --intel (intel syntax mode, default: att)
shellnoob.py -q (quite mode)
shellnoob.py -v (or -vv, -vvv)
shellnoob.py --to-strace (compiles it & run strace)
shellnoob.py --to-gdb (compiles it & run gdb & set breakpoint on entrypoint)

Standalone ""plugins""
shellnoob.py -i [--to-asm | --to-opcode ] (for interactive mode)
shellnoob.py --get-const <const>
shellnoob.py --get-sysnum <sysnum>
shellnoob.py --get-errno <errno>
shellnoob.py --file-patch <exe_fp> <file_offset> <data> (in hex). (Warning: tested only on x86/x86_64)
shellnoob.py --vm-patch <exe_fp> <vm_address> <data> (in hex). (Warning: tested only on x86/x86_64)
shellnoob.py --fork-nopper <exe_fp> (this nops out the calls to fork(). Warning: tested only on x86/x86_64)

""Installation""
shellnoob.py --install [--force] (this just copies the script in a convinient position)
shellnoob.py --uninstall [--force]

Supported INPUT format: asm, obj, bin, hex, c, shellstorm
Supported OUTPUT format: asm, obj, exe, bin, hex, c, completec, python, bash, ruby, pretty, safeasm
All combinations from INPUT to OUTPUT are supported!
```

### Installation (only if you want)
```bash
$ ./shellnoob.py --install
```
This will just copy the script to /usr/local/bin/snoob. That's it. (Run ```./shellnoob.py --uninstall``` to undo).

### Convert shellcode from/to different formats with a uber flexible CLI.
```bash
$ snoob --from-asm shell.asm --to-bin shell.bin
```

Some equivalent alternatives (the tool will try to guess what you want given the file extension..)  
```bash
$ snoob --from-asm shell.asm --to-bin
$ snoob shell.asm --to-bin
$ snoob shell.asm --to-bin - > shell.bin
$ cat shell.asm | snoob --from-asm - --to-bin - > shell.bin
```

### Formats description
- ""asm"" - standard assembly. ATT syntax by default, use ```--intel``` to use Intel syntax. (see ""asm as output"" section for more details)
- ""bin"" - raw binary ('\\x41\\x42\\x43\\x44')
- ""hex"" - raw binary encoded in hex ('41424344')
- ""obj"" - an ELF
- ""exe"" - an executable ELF
- ""c"" - something ready to embed in a C program.
- ""python"", ""bash"", ""ruby"" - same here.
- ""completec"" - compilable C that properly set the memory as RWX (to support self-modifying shellcodes)
- ""safeasm"" - assembly that is 100% assemblable: sometimes objdump's output, from which the ""asm"" is taken, is not assemblable. This will output the ""raw"" bytes (in .byte notation) so that it's assemblable by ""as"".
- ""shellstorm"" - The ```--from-shellstorm``` switch takes as argument a <shellcode_id>. ShellNoob will grab the selected shellcode from the shell-storm shellcode DB, and it will convert it to the selected
  format.


### Easy debugging
```bash
$ snoob -c shell.asm --to-exe shell
$ gdb -q shell
$ run
Reading symbols from ./shell...(no debugging symbols found)...done.
(gdb) run
Starting program: ./shell

Program received signal SIGTRAP, Trace/breakpoint trap.
0x08048055 in ?? ()
(gdb) 
```

Or you can use the new ```--to-strace``` and ```--to-gdb``` switches!
```bash
$ snoob open-read-write.asm --to-strace
Converting open-read-write.asm (asm) into /tmp/tmpBaQbzP (exe)
execve(""/tmp/tmpBaQbzP"", [""/tmp/tmpBaQbzP""], [/* 97 vars */]) = 0
[ Process PID=12237 runs in 32 bit mode. ]
open(""/tmp/secret"", O_RDONLY)           = 3
read(3, ""thesecretisthedolphin\n"", 255) = 22
write(1, ""thesecretisthedolphin\n"", 22thesecretisthedolphin
) = 22
_exit(0)  
```

```bash
$ snoob open-read-write.asm --to-gdb
Converting open-read-write.asm (asm) into /tmp/tmpZdImWw (exe)
Reading symbols from /tmp/tmpZdImWw...(no debugging symbols found)...done.
(gdb) Breakpoint 1 at 0x8048054
(gdb)
```
Note how ShellNoob automatically sets a breakpoint on the entry point!

### Get syscall numbers, constants and errno
```bash
$ snoob --get-sysnum read
i386 ~> 3
x86_64 ~> 0
$ snoob --get-sysnum fork
i386 ~> 2
x86_64 ~> 57
```
```bash
$ snoob --get-const O_RDONLY
O_RDONLY ~> 0
$ snoob --get-const O_CREAT
O_CREAT ~> 64
$ snoob --get-const EINVAL
EINVAL ~> 22
```
```bash
$ snoob --get-errno EINVAL
EINVAL ~> Invalid argument
$ snoob --get-errno 22
22 ~> Invalid argument
$ snoob --get-errno EACCES
EACCES ~> Permission denied
$ snoob --get-errno 13
13 ~> Permission denied
```

### Interactive mode
```bash
$ ./shellnoob.py -i --to-opcode
asm_to_opcode selected
>> mov %eax, %ebx
mov %eax, %ebx ~> 89c3
>> 
```
```bash
./shellnoob.py -i --to-asm
opcode_to_asm selected
>> 89c3
89c3 ~> mov %eax,%ebx
>>
```

### ShellNoob as a library
```python
$ python
>>> from shellnoob import ShellNoob
>>> sn = ShellNoob(flag_intel=True)

>>> sn.asm_to_hex('nop; mov ebx,eax; xor edx,edx')
'9089c331d2'
>>> sn.hex_to_inss('9089c331d2')
['nop', 'mov ebx,eax', 'xor edx,edx']

>>> sn.do_resolve_syscall('fork')
i386 ~> 2
x86_64 ~> 57
```

### Asm as ouput format
When ""asm"" is the output format, ShellNoob will try its best. Objdump is used as disassembler, but its output is not bullet-proof.
ShellNoob tries to augment the disasm by adding the bytes (.byte notation), and, when appropriate, it will display the equivalent in ASCII (.ascii notation). This is useful when you want to modify/assemble the output of objdump but you need to do a quick fix.

Example with the .byte notation:
```
jmp 0x37              # .byte 0xeb,0x35      
pop %ebx              # .byte 0x5b          
mov %ebx,%eax         # .byte 0x89,0xd8      
add $0xb,%eax         # .byte 0x83,0xc0,0x0b 
xor %ecx,%ecx         # .byte 0x31,0xc9      
```

Example with the .ascii notation:
```
das                   # .ascii ""/""
je 0xac               # .ascii ""tm""
jo 0x70               # .ascii ""p/""
jae 0xa8              # .ascii ""se""
arpl %si,0x65(%edx)   # .ascii ""cre""
je 0xa0               # .ascii ""tX
```

## License

ShellNoob is release under the MIT license. Check the COPYRIGHT file.
","['reyammer', 'anthraxx', 'ret2libc']",1,,0.78,0,,,,,,40,,,
255867718,MDEwOlJlcG9zaXRvcnkyNTU4Njc3MTg=,AIChip_Paper_List,BRTResearch/AIChip_Paper_List,0,BRTResearch,https://github.com/BRTResearch/AIChip_Paper_List,,0,2020-04-15 09:27:42+00:00,2025-03-03 09:00:59+00:00,2021-01-13 01:48:25+00:00,,4590,602,602,,1,1,1,1,1,0,117,0,0,3,,1,0,0,public,117,3,602,master,1,1,,"['sjtujnf', 'lliuBR', 'ghzgt', 'basicmi', 'PsychArch']",0,,0.56,0,,,,,,38,,,
648450331,R_kgDOJqaRGw,sft_datasets,chaoswork/sft_datasets,0,chaoswork,https://github.com/chaoswork/sft_datasets,"开源SFT数据集整理,随时补充",0,2023-06-02 02:13:57+00:00,2025-03-06 18:39:36+00:00,2023-06-02 02:15:45+00:00,,4,494,494,,1,1,1,1,0,0,41,0,0,2,,1,0,0,public,41,2,494,master,1,,"# 开源SFT数据集整理



| 数据集                                                                                 | 数目     | Lang  | Task  | Gen | 类型                                      | 来源                    | 链接                                                                                       |
|-------------------------------------------------------------------------------------- |-------- |----- |----- |--- |----------------------------------------- |----------------------- |------------------------------------------------------------------------------------------ |
| [belle\_cn](https://huggingface.co/BelleGroup)                                         | 1079517  | CN    | TS/MT | SI  | 通用指令，数学推理，对话                  | text-davunci-003        | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/belle_cn)             |
| [firefly](https://github.com/yangjianxin1/Firefly)                                     | 1649398  | CN    | MT    | COL | 23种nlp任务                               | 收集中文数据集，人工书写指令模板 | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/firefly)              |
| [GAOKAO](https://github.com/OpenLMLab/GAOKAO-Bench)                                    | 2785     | CN    | MT    | COL | 高考中的多选，填空等问题                  | 人工标注的数据集的收集  | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/GAOKAO)               |
| [COIG](https://huggingface.co/datasets/BAAI/COIG)                                      | 298428   | CN    | MT    | COL | 考试，翻译，价值观指令数据集搜集，基于知识图谱的反事实对话 | 自动化工具+人工验证     | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/COIG)                 |
| [pCLUE](https://github.com/CLUEbenchmark/pCLUE)                                        | 1200705  | CN    | MT    |     | 73个Prompt,分类，推理，关键词识别，阅读理解等9个NLP任务 |                         | [下载](https://github.com/CLUEbenchmark/pCLUE/tree/main/datasets)                          |
| [CSL](https://github.com/ydli-ai/CSL)                                                  | 396209   | CN    | MT    |     | 40万中文论文元数据，26个Prompt            |                         | [下载](https://drive.google.com/file/d/1xEDgtqHU4qm0Sp-dKjc5KerAmWydmh3-/view?usp=sharing) |
| [CNewSum](https://dqwang122.github.io/projects/CNewSum/)                               | 304307   | CN    | TS    |     | 字节与UCSB发布的中文摘要数据集            |                         | [下载](https://drive.google.com/u/0/uc?id=1A_YcQ3cBAI7u9iVIoCeVLLgwU7UUzHHv&export=download) |
| [Coco-cn](https://github.com/li-xirong/coco-cn)                                        |          | CN    | TS    |     | 图文多模态                                |                         | [下载](https://github.com/li-xirong/coco-cn)                                               |
| [news\_commentary](https://huggingface.co/datasets/news_commentary/viewer/en-zh/train) | 69200    | EN/CN | TS    |     | 中英文翻译数据                            |                         | [下载](https://huggingface.co/datasets/news_commentary/viewer/en-zh/train)                 |
| [Chain of Thought](https://github.com/google-research/FLAN)                            | 74771    | EN/CN | MT    | HG  | CoT相关任务                               | 人在现有数据集上标注CoT | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Chain-of-Thought)     |
| [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)                              | 37175    | EN/CN | TS    | MIX | 对话评估                                  | gpt-3.5 或 人工         | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/HC3)                  |
| [instinwild](https://github.com/XueFuzhao/InstructionWild)                             | 52191    | EN/CN | MT    | SI  | 生成，开放域问答，头脑风暴                | text-davunci-003        | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/instinwild)           |
| [Alpaca\_GPT4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)             | 52002    | EN/CN | MT    | SI  | 通用指令                                  | GPT-4 生成的Alpaca数据  | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/alpacaGPT4)           |
| [MOSS](https://github.com/OpenLMLab/MOSS)                                              | 1583595  | EN/CN | SI    |     |                                           |                         | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/MOSS)                 |
| [LLMZoo](https://github.com/FreedomIntelligence/LLMZoo)                                |          | ML    |       |     |                                           |                         | [下载](https://huggingface.co/datasets/FreedomIntelligence/phoenix-sft-data-v1/tree/main)  |
| [Guanaco](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)               | 534610   | ML    | MT    | SI  | 多种nlp任务                               | text-davinci-003        | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Guanaco)              |
| [Natural Instructions](https://github.com/allenai/natural-instructions)                | 5040134  | ML    | MT    | COL | 多种nlp任务                               | 人工标注的数据集的收集  | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Natural-Instructions) |
| [xP3](https://huggingface.co/datasets/bigscience/xP3)                                  | 78883588 | ML    | MT    | COL | 多种nlp任务                               | 人工标注的数据集的收集  | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/xP3)                  |
| [alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                 | 52002    | EN    | MT    | SI  | 通用指令                                  | text-davinci-003        | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/alpaca)               |
| [GPT4all](https://github.com/nomic-ai/gpt4all)                                         | 806199   | EN    | MT    | COL | 代码，故事，对话                          | GPT-3.5-turbo 蒸馏      | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/GPT4all)              |
| [GPTeacher](https://github.com/teknium1/GPTeacher)                                     | 29013    | EN    | MT    | SI  | 通用，角色扮演，工具指令                  | GPT-4 & toolformer      | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/GPTeacher)            |
| [prosocial dialog](https://huggingface.co/datasets/allenai/prosocial-dialog)           | 165681   | EN    | TS    | MIX | 对话                                      | GPT-3改写问题，人工回复 | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/prosocial-dialog)     |
| [finance\_en](https://huggingface.co/datasets/gbharti/finance-alpaca)                  | 68912    | EN    | TS    | COL | 金融领域问答                              | GPT3.5                  | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/)                     |
| [instruct](https://huggingface.co/datasets/swype/instruct)                             | 888969   | EN    | MT    | COL | GPT4All，Alpaca和开源数据集的增强         | 使用AllenAI提供的nlp增强工具 | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/instruct)             |
| [Code Alpaca](https://github.com/sahil280114/codealpaca)                               | 20022    | EN    | SI    | SI  | 代码生成，编辑，优化                      | text-davinci-003        | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/CodeAlpaca)           |
| [webGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)                    | 18994    | EN    | TS    | MIX | 信息检索问答                              | fine-tuned GPT-3 + 人工评估 | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/webGPT)               |
| [dolly 2.0](https://github.com/databrickslabs/dolly)                                   | 15015    | EN    | TS    | HG  | 公开、封闭式问答、信息抽取、摘要生成、开放式构思、分类以及创意写作七类任务 | 人工标注                | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/dolly)                |
| [baize](https://github.com/project-baize/baize-chatbot)                                | 653699   | EN    | MT    | COL | Alpaca和多种问答任务                      | 人工标注的数据集的收集  | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/baize)                |
| [hh-rlhf](https://github.com/anthropics/hh-rlhf)                                       | 284517   | EN    | TS    | MIX | 对话                                      | RLHF models             | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/hh-rlhf)              |
| [OIG(part)](https://laion.ai/blog/oig-dataset/)                                        | 49237    | EN    | MT    | COL | 多种nlp任务                               | 人工标注的数据集的收集和数据增强 | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/OIG)                  |
| [camel](https://github.com/lightaime/camel)                                            | 760620   | EN    | MT    | SI  | 物理生物化学编程，数学，社会等领域的角色扮演对话人工标注的数据集的收集 | gpt-3.5-turbo 生成      | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/camel)                |
| [FLAN-Muffin](https://huggingface.co/datasets/Muennighoff/flan)                        | 1764800  | EN    | MT    | COL | 60种nlp任务                               | 人工标注的数据集的收集  | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/FLAN-Muffin)          |
| [GPT4Tools](https://github.com/StevenGrove/GPT4Tools)                                  | 71446    | EN    | MT    | SI  | a collection of tool-related instructions | gpt-3.5-turbo           | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/gpt4tools)            |
| [ShareChat](https://huggingface.co/datasets/RyokoAI/ShareGPT52K)                       | 1663241  | EN    | MT    | MIX | general instruct                          | 收集ShareGPT            | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/ShareGPT)             |
| [Auto CoT](https://github.com/amazon-science/auto-cot)                                 |          | EN    |       |     |                                           |                         | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Auto-CoT)             |
| [ultrachat](https://github.com/thunlp/UltraChat)                                       | 28247446 | EN    |       |     |                                           |                         | [下载](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/ultrachat)            |
| [StackLLaMA](https://huggingface.co/datasets/lvwerra/stack-exchange-paired)            | todo     | EN    |       |     |                                           |                         |                                                                                            |
",['chaoswork'],0,,0.7,0,,,,,,2,,,
405294488,MDEwOlJlcG9zaXRvcnk0MDUyOTQ0ODg=,database-system-readings,Sunt-ing/database-system-readings,0,Sunt-ing,https://github.com/Sunt-ing/database-system-readings,:yum: A curated reading list about database systems,0,2021-09-11 05:43:08+00:00,2025-01-17 15:50:48+00:00,2022-05-03 04:49:22+00:00,,13878,466,466,,1,1,1,1,0,0,31,0,0,98,mit,1,0,0,public,31,98,466,main,1,,"# Introduction
This is a curated reading list about database systems, including personal digests of books/papers/blogs.

Booklist can be found in README, while paper and blog digests are located in the issue list. 

In the issue list, a closed issue means that the digest about it was finished, while an open one means that the digest is to-be-appeared or on-the-flying. 

For each paper digest, I try to answer the following questions: 
- What problem does the paper solve? Is it important? 
- How does it solve the problem? Any approaches new?
- What could be improved? 
- Anything interesting about it?


# Booklist
- Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems
- Database Internals: A Deep Dive into How Distributed Data Systems Work
- Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing
- Stream Processing with Apache Flink
- Database System Concepts, 5th Edition
- Readings in Database Systems, 5th Edition
- Computer Systems: A Programmer's Perspective, 3th Edition
- The PhD Grind

<!-- You may also find [Other Books](https://github.com/Sunt-ing/database-system-readings/blob/main/Other%20Books.md).
 -->

# Referenced Reading Lists
## General 
- [:star:] [MIT 6.824 Distributed System Course](https://pdos.csail.mit.edu/6.824/schedule.html)
- [:star:] Yuanli Wang at BU: [Sys Reading](https://github.com/pentium3/sys_reading/issues)
- [:star:] Andy Pavlo at CMU: [Seminar 2021](https://db.cs.cmu.edu/seminar2021/)
- [:star:] Xiangfeng Zhu at UW: [Reading List](https://xzhu0027.gitbook.io/blog/reading-list)
- CMU: [CMU 15-721 Database Systems Reading List (Spring 2016)](https://15721.courses.cs.cmu.edu/spring2016/schedule.html)
- Lianke Qin at UCSB: [Paper](https://github.com/brucechin/paper)
- Reynold Xin at Databricks: [DB Readings](https://github.com/rxin/db-readings)
- UIUC: [https://systems-seminar-uiuc.github.io/fall20/index.html](https://systems-seminar-uiuc.github.io/fall20/index.html)
- UIUC: [https://docs.google.com/document/d/1gWQ_Uk60zIH6PvP1P4NYzz4TvrKWGCnltySBxwkradM/edit](https://docs.google.com/document/d/1gWQ_Uk60zIH6PvP1P4NYzz4TvrKWGCnltySBxwkradM/edit)
- NYU: [NYU System Seminar](https://github.com/fruffy/nyu-systems-seminar)
- UMass: [Systems Lunch](https://systems-lunch.org/)
- UCSD: [http://www.sysnet.ucsd.edu/classes/cse294/sp20/](http://www.sysnet.ucsd.edu/classes/cse294/sp20/)
- Cornell: [http://www.cs.cornell.edu/courses/cs7490/2020fa/](http://www.cs.cornell.edu/courses/cs7490/2020fa/)
- UW: [playlists](https://www.youtube.com/c/uwcse/playlists)
- [http://dancres.github.io/Pages/](http://dancres.github.io/Pages/)
- [Distributed System Readings](https://github.com/feilengcui008/distributed_system_readings)
- [Database Techiques Everyone Should Know](https://blog.acolyer.org/2016/01/03/database-techiques-everyone-should-know/)
- [https://github.com/mosharaf/eecs598/tree/w19-bigdata-ai](https://github.com/mosharaf/eecs598/tree/w19-bigdata-ai)
- Blogs of Dan Abadi at UMD: [DBMS Musings](http://dbmsmusings.blogspot.com/)
- Classical material on distributed systems: [Awesome Distributed Systems](https://github.com/theanalyst/awesome-distributed-systems)
- Alaikexisi at AWS (in Chinese): [https://www.zhihu.com/column/c_158208519](https://www.zhihu.com/column/c_158208519)
- Chinese translation set of classic papers in distributed system field: [http://duanple.com/?p=170](http://duanple.com/?p=170)

## System + Machine Learning
- Umich: [https://github.com/mosharaf/eecs598](https://github.com/mosharaf/eecs598)
- [https://github.com/mcanini/SysML-reading-list](https://github.com/mcanini/SysML-reading-list)
- [https://remziarpacidusseau.wixsite.com/mlos](https://remziarpacidusseau.wixsite.com/mlos)

## System Reliability
- JHU: [https://www.cs.jhu.edu/~huang/cs817/spring21/](https://www.cs.jhu.edu/~huang/cs817/spring21/)
- JHU: [https://www.cs.jhu.edu/~huang/cs624/spring21/syllabus.html](https://www.cs.jhu.edu/~huang/cs624/spring21/syllabus.html)
- UMich: [Summer School 2020](https://web.eecs.umich.edu/~manosk/summer-school-2020.html)
- [Systems Reading](https://github.com/lorin/systems-reading)

## In-Memory Databases
- CMU advanced database systems course
  - [course videos](https://www.youtube.com/watch?v=m72mt4VN9ik&list=PLSE8ODhjZXja7K1hjZ01UTVDnGQdx5v5U)
  - [course reading list](https://15721.courses.cs.cmu.edu/spring2019/schedule.html#jan-14-2019)

## Streaming Systems
- [:star:] Vasia at BU: 
  - [course DSPA 21](https://vasia.github.io/dspa21/readings.html)
  - [course DSPA 20](https://vasia.github.io/dspa20/readings.html)
- Jamie Brandon: [Scattered Thoughts](https://scattered-thoughts.net/)
- Dunith Dhanushka: [https://dunith.medium.com/](https://dunith.medium.com/)
- [streaming readings (in Chinese)](https://github.com/lw-lin/streaming-readings)


",['Sunt-ing'],0,,0.7,0,,,,,,8,,,
229741,MDEwOlJlcG9zaXRvcnkyMjk3NDE=,right_aws,flexera-public/right_aws,0,flexera-public,https://github.com/flexera-public/right_aws,RightScale Amazon Web Services Ruby Gems,0,2009-06-17 16:37:34+00:00,2025-01-11 06:45:28+00:00,2022-08-08 22:59:07+00:00,,1173,450,450,Ruby,1,1,0,1,0,0,175,0,0,66,mit,1,0,0,public,175,66,450,master,1,1,#ERROR!,"['trbryan', 'ktheory', 'tve', 'cleicar', 'dominicm', 'aboisvert', 'sergeyenin', 'AE9RB', 'jakedouglas', 'jashkenas', 'rnash-rs', 'rubysolo', 'klochner', 'findchris', 'careo', 'dvrensk', 'jschneiderhan', 'kevmoo', 'martinos', 'mauricio', 'pieter', 'rbroemeling', 'rgeyer', 'seanxiesx', 'Sergyenko', 'soleun', 'mirakui']",0,,0.71,0,,,,,,92,,,
186561683,MDEwOlJlcG9zaXRvcnkxODY1NjE2ODM=,Learn_Physics_in_2_Months,llSourcell/Learn_Physics_in_2_Months,0,llSourcell,https://github.com/llSourcell/Learn_Physics_in_2_Months,"This is the curriculum for ""Learn Physics in 2 Months"" by Siraj Raval on Youtube",0,2019-05-14 06:46:54+00:00,2025-02-18 15:13:47+00:00,2020-05-07 14:44:44+00:00,,9,447,447,,1,1,1,1,0,0,119,0,0,3,,1,0,0,public,119,3,447,master,1,,"# Learn_Physics_in_2_Months

This is the curriculum for ""Learn Physics in 2 Months"" by Siraj Raval on Youtube. [This](https://youtu.be/RGD6KQ6bRS8) video.

##### Why Learn Physics?
- It's fundamentally helped us build modern civilization. 
- It's used in Quantum Computing
- It's used in Computer Engineering
- It's used for innovation in general (thinking in first principles)

# Month 1

## Week 1 Math Review

##### Linear Algebra 
- Study Guide
https://www.souravsengupta.com/cds2016/lectures/Savov_Notes.pdf
- Playlist
https://www.youtube.com/watch?v=kjBOesZCoqc&index=1&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab
#### Calculus 
- Study Guide
http://tutorial.math.lamar.edu/pdf/Calculus_Cheat_Sheet_All.pdf 
- Playlist
https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr
##### Probability 
- Study Guide
https://static1.squarespace.com/static/54bf3241e4b0f0d81bf7ff36/t/55e9494fe4b011aed10e48e5/1441352015658/probability_cheatsheet.pdf
##### Statistics 
- Study Guide
http://web.mit.edu/~csvoss/Public/usabo/stats_handout.pdf

## Week 2 Classical Mechanics
- Lectures
https://www.youtube.com/watch?v=ApUFtLCrU90&list=PL47F408D36D4CF129
- Study Guide
http://www.maths.liv.ac.uk/TheorPhys/people/staff/jgracey/math228/formula.pdf
- Final Exam 
http://galileo.phys.virginia.edu/classes/321.jvn.fall02/Fin2002s.pdf

## Week 3 Statistical Mechanics
- Lectures
 https://www.youtube.com/watch?v=D1RzvXDXyqA&t=619s
- Study Guide
https://pdfs.semanticscholar.org/a4d6/cd309dd005c4e30c8a4dbe3ed4c377de32ec.pdf
- Final Exam 
http://www.phys.ttu.edu/~cmyles/Phys5305/Exams/Phys5305%20Final%20Exam%20Spring2009.PDF 

## Week 4 Electromagnetism
- Lectures
https://www.youtube.com/watch?v=x1-SibwIPM4&list=PLyQSN7X0ro2314mKyUiOILaOC2hk6Pc3j&index=2
- Study Guide
http://www.phys.nthu.edu.tw/~thschang/notes/EM02.pdf
- Final Exam 
http://web.mit.edu/8.02/www/Spring02/exams/final-sol4.pdf

# Month 2 

## Week 5 Particle Physics
- Lectures
https://www.coursera.org/learn/particle-physics
- Study Guide
https://www.nikhef.nl/~i93/Master/PP1/2011/Lectures/Lecture.pdf 
- Final Exam 
http://hitoshi.berkeley.edu/129A/final-sol.pdf

## Week 6 Theory of Relativity
- Lectures
https://www.youtube.com/watch?v=JRZgW1YjCKk&list=PLXLSbKIMm0kh6XsMSCEMnM02kEoW_8x-f 
- Study Guide
https://arxiv.org/pdf/gr-qc/9712019.pdf 
- Final Exam 
https://courses.physics.ucsd.edu/2015/Winter/physics225b/hw4-sols.pdf 

## Week 7 Quantum Mechanics

- Lectures
https://www.youtube.com/watch?v=ZcpwnozMh2U
https://www.edx.org/course/quantum-mechanics-everyone-georgetownx-phyx-008-01x 
- Study Guide
https://ocw.mit.edu/courses/physics/8-04-quantum-physics-i-spring-2013/lecture-notes/MIT8_04S13_Lec01.pdf
- Final Exam 
http://www.physics.rutgers.edu/~haule/501/sol_final_2015.pdf 

## Week 8 Quantum Field Theory

- Lectures
https://www.youtube.com/watch?v=IGHvf9BwkDY&list=PLbMVogVj5nJQ3slQodXQ5cSEtcp4HbNFc 
- Study Guide
https://web.physics.ucsb.edu/~mark/ms-qft-DRAFT.pdf- 
Final Exam 
http://www-personal.umich.edu/~jbourj/peskin/Quantum%20Field%20Theory%20II%20homeworks.pdf



*Additional Resources:*   
- Join the [""Wizards"" Slack channel](http://wizards.herokuapp.com/ ""Herokuapp.com""). Join the ""Physics"" channel.
",['llSourcell'],0,,0.61,0,,,,,,11,,,
526473051,R_kgDOH2FXWw,compiler-and-arch,KnowingNothing/compiler-and-arch,0,KnowingNothing,https://github.com/KnowingNothing/compiler-and-arch,"A list of tutorials, paper, talks, and open-source projects for emerging compiler and architecture",0,2022-08-19 05:20:40+00:00,2025-02-28 02:13:59+00:00,2025-01-15 09:46:19+00:00,,239,436,436,,1,1,1,1,0,0,35,0,0,0,,1,0,0,public,35,0,436,main,1,,,"['KnowingNothing', 'Light-of-Hers', 'akothen', 'jokmingwong', 'gulang2019', 'xiupingcui', 'yintao-he', 'zhang677', 'erjanmx', 'LeiWang1999', 'rachitnigam', 'haoxiaochen', 'KuangjuX', 'uv-xiao']",0,,0.66,0,,,,,,26,,,
165630835,MDEwOlJlcG9zaXRvcnkxNjU2MzA4MzU=,Awesome-Visual-Captioning,forence/Awesome-Visual-Captioning,0,forence,https://github.com/forence/Awesome-Visual-Captioning,This repository focus on Image Captioning & Video Captioning & Seq-to-Seq Learning & NLP,0,2019-01-14 09:12:59+00:00,2025-01-04 06:09:17+00:00,2022-11-14 03:24:29+00:00,,1243,413,413,,1,1,1,1,0,1,50,0,0,1,,1,0,0,public,50,1,413,master,1,,"# Awesome-Visual-Captioning[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

## Table of Contents
- [ECCV-2022](#ECCV-2022)
- [CVPR-2022](#CVPR-2022)
- [AAAI-2022](#AAAI-2022)
- [IJCAI-2022](#IJCAI-2022)
- [NeurIPS-2021](#NeurIPS-2021)  
- [ACMMM-2021](#ACMMM-2021)    
- [ICCV-2021](#ICCV-2021)  
- [ACL-2021](#ACL-2021)  
- [CVPR-2021](#CVPR-2021)  
- [AAAI-2021](#AAAI-2021)
- [ACMMM-2020](#ACMMM-2020)
- [NeurIPS-2020](#NeurIPS-2020)
- [ECCV-2020](#ECCV-2020)
- [CVPR-2020](#CVPR-2020)
- [ACL-2020](#ACL-2020)
- [AAAI-2020](#AAAI-2020)
- [ACL-2019](#ACL-2019)
- [NeurIPS-2019](#NeurIPS-2019)
- [ACMMM-2019](#ACMMM-2019)   
- [ICCV-2019](#ICCV-2019)
- [CVPR-2019](#CVPR-2019)
- [AAAI-2019](#AAAI-2019)

## Paper Roadmap
### ECCV-2022
- ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-Verified Image-Caption Associations for MS-COCO  
- Object-Centric Unsupervised Image Captioning  
- D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding  
- StyleBabel: Artistic Style Tagging and Captioning  
- MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes     
- GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and Retrieval  
- Explicit Image Caption Editing  
- GRIT: Faster and Better Image Captioning Transformer Using Dual Visual Features  
- Unifying Event Detection and Captioning as Sequence Generation via Pre-training  

### CVPR-2022
**Image Captioing**
- DeeCap: Dynamic Early Exiting for Efficient Image Captioning   
- Injecting Visual Concepts into End-to-End Image Captioning   
- DIFNet: Boosting Visual Information Flow for Image Captioning   
- Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning   
- Quantifying Societal Bias Amplification in Image Captioning    
- Show, Deconfound and Tell: Image Captioning with Causal Inference    
- Scaling Up Vision-Language Pretraining for Image Captioning   
- VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning   
- Comprehending and Ordering Semantics for Image Captioning    
- Alleviating Emotional bias in Affective Image Captioning by Contrastive Data Collection    
- NOC-REK: Novel Object Captioning with Retrieved Vocabulary from External Knowledge    
- NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image Caption Generation Models   

**Video Captioing**
- End-to-end Generative Pretraining for Multimodal Video Captioning    
- SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning    
- Hierarchical Modular Network for Video Captioning   


### AAAI-2022
**Image Captioing**
- Image Difference Captioning with Pre-Training and Contrastive Learning   
- Attention-Aligned Transformer for Image Captioning   
- Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models    
- MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-Based Image Captioning    
- UNISON: Unpaired Cross-Lingual Image Captioning    
- End-to-End Transformer Based Model for Image Captioning  

### IJCAI-2022
**Image Captioing**
- ER-SAN: Enhanced-Adaptive Relation Self-Attention Network for Image Captioning    
- S2 Transformer for Image Captioning    

**Video Captioning**
- GL-RG: Global-Local Representation Granularity for Video Captioning   


### NeurIPS-2021
**Video Captioning**
- Multi-modal Dependency Tree for Video Captioning [[paper]](https://openreview.net/pdf?id=sW40wkwfsZp)  

### ACMMM-2021
**Image Captioning**
- Distributed Attention for Grounded Image Captioning     
- Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning  
- Semi-Autoregressive Image Captioning   
- Question-controlled Text-aware Image Captioning   
- Triangle-Reward Reinforcement Learning: A Visual-Linguistic Semantic Alignment for Image Captioning   
- Group-based Distinctive Image Captioning with Memory Attention   
- Direction Relation Transformer for Image Captioning    
- Scene Graph with 3D Information for Change Captioning   
- Similar Scenes Arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning   

**Video Captioning**
- State-aware Video Procedural Captioning    
- Discriminative Latent Semantic Graph for Video Captioning   
- Sensor-Augmented Egocentric-Video Captioning with Dynamic Modal Attention   
- Multi-Perspective Video Captioning.   
- Hybrid Reasoning Network for Video-based Commonsense Captioning   

### ICCV-2021
**Image Captioning** 
- Partial Off-Policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Partial_Off-Policy_Learning_Balance_Accuracy_and_Diversity_for_Human-Oriented_Image_ICCV_2021_paper.pdf)  
- Viewpoint-Agnostic Change Captioning With Cycle Consistency [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Viewpoint-Agnostic_Change_Captioning_With_Cycle_Consistency_ICCV_2021_paper.pdf)  
- Understanding and Evaluating Racial Biases in Image Captioning [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Understanding_and_Evaluating_Racial_Biases_in_Image_Captioning_ICCV_2021_paper.pdf)  
- Auto-Parsing Network for Image Captioning and Visual Question Answering [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Auto-Parsing_Network_for_Image_Captioning_and_Visual_Question_Answering_ICCV_2021_paper.pdf)  
- In Defense of Scene Graphs for Image Captioning [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Nguyen_In_Defense_of_Scene_Graphs_for_Image_Captioning_ICCV_2021_paper.pdf)  
- Describing and Localizing Multiple Changes With Transformers [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Qiu_Describing_and_Localizing_Multiple_Changes_With_Transformers_ICCV_2021_paper.pdf)  
- Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Bai_Explain_Me_the_Painting_Multi-Topic_Knowledgeable_Art_Description_Generation_ICCV_2021_paper.pdf)  

**Video Captioning**
- Motion Guided Region Message Passing for Video Captioning [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Motion_Guided_Region_Message_Passing_for_Video_Captioning_ICCV_2021_paper.pdf)   
- End-to-End Dense Video Captioning With Parallel Decoding [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_End-to-End_Dense_Video_Captioning_With_Parallel_Decoding_ICCV_2021_paper.pdf)  

### ACL-2021
**Image Captioning** 
- Control Image Captioning Spatially and Temporally [[paper]](https://aclanthology.org/2021.acl-long.157.pdf)  
- SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis [[paper]](https://arxiv.org/pdf/2106.01444.pdf) [[code]](https://github.com/JoshuaFeinglass/SMURF)  
- Enhancing Descriptive Image Captioning with Natural Language Inference [[paper]](https://aclanthology.org/2021.acl-short.36/)  
- UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning [[paper]](https://arxiv.org/pdf/2106.14019.pdf)  
- Semantic Relation-aware Difference Representation Learning for Change Captioning [[paper]](https://aclanthology.org/2021.findings-acl.6/)  

**Video Captioning**
- Hierarchical Context-aware Network for Dense Video Event Captioning [[paper]](https://aclanthology.org/2021.acl-long.156.pdf)  
- Video Paragraph Captioning as a Text Summarization Task [[paper]](https://aclanthology.org/2021.acl-short.9.pdf)  
- O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning [[paper]](https://aclanthology.org/2021.findings-acl.24.pdf)  

### CVPR-2021
**Image Captioning**  
- Connecting What to Say With Where to Look by Modeling Human Attention Traces. [[paper]](https://arxiv.org/pdf/2105.05964.pdf) [[code]](https://github.com/facebookresearch/connect-caption-and-trace)  
- Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles. [[paper]](https://arxiv.org/pdf/2103.05121.pdf)  
- Improving OCR-Based Image Captioning by Incorporating Geometrical Relationship. [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Improving_OCR-Based_Image_Captioning_by_Incorporating_Geometrical_Relationship_CVPR_2021_paper.pdf)  
- Image Change Captioning by Learning From an Auxiliary Task. [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Hosseinzadeh_Image_Change_Captioning_by_Learning_From_an_Auxiliary_Task_CVPR_2021_paper.pdf)  
- Scan2Cap: Context-aware Dense Captioning in RGB-D Scans. [[paper]](https://arxiv.org/pdf/2012.02206.pdf) [[code]](https://github.com/daveredrum/Scan2Cap)  
- Towards Bridging Event Captioner and Sentence Localizer for Weakly Supervised Dense Event Captioning. [paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Towards_Bridging_Event_Captioner_and_Sentence_Localizer_for_Weakly_Supervised_CVPR_2021_paper.pdf) 
- TAP: Text-Aware Pre-Training for Text-VQA and Text-Caption. [[paper]](https://arxiv.org/pdf/2012.04638.pdf)  
- Towards Accurate Text-Based Image Captioning With Content Diversity Exploration. [[paper]](https://arxiv.org/pdf/2105.03236.pdf)  
- FAIEr: Fidelity and Adequacy Ensured Image Caption Evaluation. [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_FAIEr_Fidelity_and_Adequacy_Ensured_Image_Caption_Evaluation_CVPR_2021_paper.pdf)   
- RSTNet: Captioning With Adaptive Attention on Visual and Non-Visual Words. [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_RSTNet_Captioning_With_Adaptive_Attention_on_Visual_and_Non-Visual_Words_CVPR_2021_paper.pdf)  
- Human-Like Controllable Image Captioning With Verb-Specific Semantic Roles. [[paper]](https://arxiv.org/pdf/2103.12204.pdf)  

**Video Captioning**
- Open-Book Video Captioning With Retrieve-Copy-Generate Network. [[paper]](https://arxiv.org/pdf/2103.05284.pdf)  
- Towards Diverse Paragraph Captioning for Untrimmed Videos. [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Towards_Diverse_Paragraph_Captioning_for_Untrimmed_Videos_CVPR_2021_paper.pdf)  

### AAAI-2021  
**Image Captioning**   
- Partially Non-Autoregressive Image Captioning. [[code]](https://github.com/feizc/PNAIC/tree/master)  
- Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network. [[paper]](https://arxiv.org/pdf/2012.07061.pdf)   
- Object Relation Attention for Image Paragraph Captioning [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/16423)   
- Dual-Level Collaborative Transformer for Image Captioning. [[paper]](https://arxiv.org/pdf/2101.06462.pdf) [[code]](https://github.com/luo3300612/image-captioning-DLCT)    
- Memory-Augmented Image Captioning [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/16220)  
- Image Captioning with Context-Aware Auxiliary Guidance. [[paper]](https://arxiv.org/pdf/2012.05545.pdf)  
- Consensus Graph Representation Learning for Better Grounded Image Captioning. [[paper]](https://www.aaai.org/AAAI21Papers/AAAI-3680.ZhangW.pdf)  
- FixMyPose: Pose Correctional Captioning and Retrieval. [[paper]](https://arxiv.org/pdf/2104.01703.pdf) [[code]](https://github.com/hyounghk/FixMyPose)  [[website]](https://fixmypose-unc.github.io/)  
- VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning [[paper]](https://arxiv.org/pdf/2009.13682)  

**Video Captioning**
- Non-Autoregressive Coarse-to-Fine Video Captioning. [[paper]](https://arxiv.org/pdf/1911.12018.pdf)  
- Semantic Grouping Network for Video Captioning. [[paper]](https://arxiv.org/pdf/2102.00831.pdf) [[code]](https://github.com/hobincar/SGN)    
- Augmented Partial Mutual Learning with Frame Masking for Video Captioning. [[paper]](https://www.aaai.org/AAAI21Papers/AAAI-9714.LinK.pdf)  

### ACMMM-2020
**Image Captioning**
- Structural Semantic Adversarial Active Learning for Image Captioning. `oral` [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413885)  
- Iterative Back Modification for Faster Image Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413901)  
- Bridging the Gap between Vision and Language Domains for Improved Image Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3414004)  
- Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413859)  
- Improving Intra- and Inter-Modality Visual Relation for Image Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413877)  
- ICECAP: Information Concentrated Entity-aware Image Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413576)  
- Attacking Image Captioning Towards Accuracy-Preserving Target Words Removal. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3414009)  
- Multimodal Attention with Image Text Spatial Relationship for OCR-Based Image Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413753)  

**Video Captioning**
- Controllable Video Captioning with an Exemplar Sentence. `oral` [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413908)  
- Poet: Product-oriented Video Captioner for E-commerce. `oral` [[paper]](https://arxiv.org/abs/2008.06880)  
- Learning Semantic Concepts and Temporal Alignment for Narrated Video Procedural Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413498)  
- Relational Graph Learning for Grounded Video Description Generation. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413746)  

### NeurIPS-2020
- Prophet Attention: Predicting Attention with Future Attention for Improved Image Captioning. [[paper]](https://proceedings.neurips.cc/paper/2020/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf)   
- RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning.  [[paper]](https://proceedings.neurips.cc/paper/2020/file/c2964caac096f26db222cb325aa267cb-Paper.pdf)  
- Diverse Image Captioning with Context-Object Split Latent Spaces. [[paper]](https://papers.nips.cc/paper/2020/file/24bea84d52e6a1f8025e313c2ffff50a-Paper.pdf)  

### ECCV-2020
**Image Captioning**
- Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets. `oral` [[paper]](https://arxiv.org/pdf/2007.06877.pdf)
- In-Home Daily-Life Captioning Using Radio Signals. `oral` [[paper]](https://arxiv.org/pdf/2008.10966.pdf) [[website]](http://rf-diary.csail.mit.edu/)
- TextCaps: a Dataset for Image Captioning with Reading Comprehension. `oral` [[paper]](https://arxiv.org/pdf/2003.12462.pdf) [[website]](https://textvqa.org/textcaps) [[code]](https://github.com/facebookresearch/mmf/tree/master/projects/m4c_captioner)
- SODA: Story Oriented Dense Video Captioning Evaluation Framework. [[paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123510511.pdf)
- Towards Unique and Informative Captioning of Images. [[paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520613.pdf) 
- Learning Visual Representations with Caption Annotations. [[paper]](https://arxiv.org/pdf/2008.01392.pdf) [[website]](https://europe.naverlabs.com/research/computer-vision-research-naver-labs-europe/icmlm/)
- Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards. [[paper]](https://arxiv.org/pdf/2008.02693.pdf)
- Length Controllable Image Captioning. [[paper]](https://arxiv.org/pdf/2007.09580.pdf) [[code]](https://github.com/bearcatt/LaBERT)
-	Comprehensive Image Captioning via Scene Graph Decomposition. [[paper]](https://arxiv.org/pdf/2007.11731.pdf) [[website]](http://pages.cs.wisc.edu/~yiwuzhong/Sub-GC.html)
- Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning. [[paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590562.pdf)
- Captioning Images Taken by People Who Are Blind. [[paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620409.pdf)
- Learning to Generate Grounded Visual Captions without Localization Supervision. [[paper]](https://arxiv.org/pdf/1906.00283.pdf) [[code]](https://github.com/chihyaoma/cyclical-visual-captioning)

**Video Captioning**
- Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos. `Spotlight` [[paper]](https://arxiv.org/pdf/2007.14164.pdf) [[code]](https://github.com/xuewyang/Fashion_Captioning)
- Character Grounding and Re-Identification in Story of Videos and Text Descriptions. `Spotlight` [[paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500528.pdf) [[code]](https://github.com/yj-yu/CiSIN/)
- Identity-Aware Multi-Sentence Video Description. [[paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660358.pdf) 

### CVPR-2020

**Image Captioning**

- Context-Aware Group Captioning via Self-Attention and Contrastive Features [[paper]](https://arxiv.org/abs/2004.03708)  
  Zhuowan Li, Quan Tran, Long Mai, Zhe Lin, Alan L. Yuille
- More Grounded Image Captioning by Distilling Image-Text Matching Model [[paper]](https://arxiv.org/abs/2004.00390v1) [[code]](https://github.com/YuanEZhou/Grounded-Image-Captioning)  
  Yuanen Zhou, Meng Wang, Daqing Liu, Zhenzhen Hu, Hanwang Zhang
- Show, Edit and Tell: A Framework for Editing Image Captions [[paper]](https://arxiv.org/abs/2003.03107) [[code]](https://github.com/fawazsammani/show-edit-tell)  
  Fawaz Sammani, Luke Melas-Kyriazi
- Say As You Wish: Fine-Grained Control of Image Caption Generation With Abstract Scene Graphs [[paper]](https://arxiv.org/abs/2003.00387) [[code]](https://github.com/cshizhe/asg2cap)  
  Shizhe Chen, Qin Jin, Peng Wang, Qi Wu
- Normalized and Geometry-Aware Self-Attention Network for Image Captioning [[paper]](https://arxiv.org/abs/2003.08897)  
  Longteng Guo, Jing Liu, Xinxin Zhu, Peng Yao, Shichen Lu, Hanqing Lu
- Meshed-Memory Transformer for Image Captioning [[paper]](https://arxiv.org/abs/1912.08226) [[code]](https://github.com/aimagelab/meshed-memory-transformer)  
  Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara
- X-Linear Attention Networks for Image Captioning [[paper]](https://arxiv.org/abs/2003.14080) [[code]](https://github.com/JDAI-CV/image-captioning)  
  Yingwei Pan, Ting Yao, Yehao Li, Tao Mei
- Transform and Tell: Entity-Aware News Image Captioning [[paper]](https://arxiv.org/abs/2004.08070) [[code]](https://github.com/alasdairtran/transform-and-tell) [[website]](https://transform-and-tell.ml/)  
  Alasdair Tran, Alexander Mathews, Lexing Xie

**Video Captioning**

- Object Relational Graph With Teacher-Recommended Learning for Video Captioning [[paper]](https://arxiv.org/abs/2002.11566)  
  Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, Zheng-Jun Zha

- Spatio-Temporal Graph for Video Captioning With Knowledge Distillation [[paper]](https://arxiv.org/abs/2003.13942?context=cs) [[code]](https://github.com/StanfordVL/STGraph)  
  Boxiao Pan, Haoye Cai, De-An Huang, Kuan-Hui Lee, Adrien Gaidon, Ehsan Adeli, Juan Carlos Niebles
- Better Captioning With Sequence-Level Exploration [[paper]](https://arxiv.org/abs/2003.03749)  
  Jia Chen, Qin Jin
- Syntax-Aware Action Targeting for Video Captioning [[code]](https://github.com/SydCaption/SAAT)   
  Qi Zheng, Chaoyue Wang, Dacheng Tao   

### ACL-2020

**Image Captioning**

- Clue: Cross-modal Coherence Modeling for Caption Generation [[paper]](https://arxiv.org/abs/2005.00908)  
  Malihe Alikhani, Piyush Sharma, Shengjie Li, Radu Soricut and Matthew Stone
- Improving Image Captioning Evaluation by Considering Inter References Variance [[paper]](https://www.aclweb.org/anthology/2020.acl-main.93.pdf)  
  Yanzhi Yi, Hangyu Deng and Jinglu Hu
- Improving Image Captioning with Better Use of Caption [[paper]](https://www.aclweb.org/anthology/2020.acl-main.664.pdf) [[code]](https://github.com/Gitsamshi/WeakVRD-Captioning)  
  Zhan Shi, Xu Zhou, Xipeng Qiu and Xiaodan Zhu

**Video Captioning** 

- MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning [[paper]](https://arxiv.org/abs/2005.05402) [[code]](https://github.com/jayleicn/recurrent-transformer)  
  Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara Berg and Mohit Bansal

### AAAI-2020

**Image Captioning**

- **Unified VLP**: Unified Vision-Language Pre-Training for Image Captioning and VQA  [[paper]](https://arxiv.org/abs/1909.11059)  
  *Luowei Zhou (University of Michigan); Hamid Palangi (Microsoft Research); Lei Zhang (Microsoft); Houdong Hu  (Microsoft AI and Research); Jason Corso (University of Michigan); Jianfeng Gao (Microsoft Research)*  
- **OffPG**: Reinforcing an Image Caption Generator using Off-line Human Feedback  [[paper]](https://arxiv.org/abs/1911.09753)  
  *Paul Hongsuck Seo (POSTECH); Piyush Sharma (Google Research); Tomer Levinboim (Google); Bohyung Han(Seoul National University); Radu Soricut (Google)*  
- **MemCap**: Memorizing Style Knowledge for Image Captioning  [[paper]](https://wuxinxiao.github.io/assets/papers/2020/MemCap.pdf)  
  *Wentian Zhao (Beijing Institute of Technology); Xinxiao Wu (Beijing Institute of Technology); Xiaoxun Zhang(Alibaba Group)*  
- **C-R Reasoning**: Joint Commonsense and Relation Reasoning for Image and Video Captioning  [[paper]](https://wuxinxiao.github.io/assets/papers/2020/C-R_reasoning.pdf)  
  *Jingyi Hou (Beijing Institute of Technology); Xinxiao Wu (Beijing Institute of Technology); Xiaoxun Zhang (AlibabaGroup); Yayun Qi (Beijing Institute of Technology); Yunde Jia (Beijing Institute of Technology); Jiebo Luo (University of Rochester)*
- **MHTN**: Learning Long- and Short-Term User Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized Image Caption  [[paper]](https://weizhangltt.github.io/paper/zhang-aaai20.pdf)  
  *Wei Zhang (East China Normal University); Yue Ying (East China Normal University); Pan Lu (The University of California, Los Angeles); Hongyuan Zha (GEORGIA TECH)*  
- Show, Recall, and Tell: Image Captioning with Recall Mechanism [[paper]](https://arxiv.org/abs/2001.05876)  
  *Li WANG (MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China); Zechen BAI(Institute of Software, Chinese Academy of Science, China); Yonghua Zhang (Bytedance); Hongtao Lu (Shanghai Jiao Tong University)*   
- Interactive Dual Generative Adversarial Networks for Image Captioning  
  *Junhao Liu (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences); Kai Wang (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences); Chunpu Xu (Huazhong University of Science and Technology); Zhou Zhao (Zhejiang University); Ruifeng Xu (Harbin Institute of Technology (Shenzhen)); Ying Shen (Peking University Shenzhen Graduate School); Min Yang ( Chinese Academy of Sciences)*  

- **FDM-net**: Feature Deformation Meta-Networks in Image Captioning of Novel Objects [[paper]](https://www.aaai.org/Papers/AAAI/2020GB/AAAI-CaoT.4566.pdf)  
  Tingjia Cao (Fudan University); Ke Han (Fudan University); Xiaomei Wang (Fudan University); Lin Ma (Tencent AI Lab); Yanwei Fu (Fudan University); Yu-Gang Jiang (Fudan University); Xiangyang Xue (Fudan University)

**Video Captioning** 

- An Efficient Framework for Dense Video Captioning  
  Maitreya Suin (Indian Institute of Technology Madras)*; Rajagopalan Ambasamudram (Indian Institute of Technology Madras)

### ACMMM-2019
**Image Captioning**
- Aligning Linguistic Words and Visual Semantic Units for Image Captioning   
- Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards   
- MUCH: Mutual Coupling Enhancement of Scene Recognition and Dense Captioning   
- Generating Captions for Images of Ancient Artworks   

**Video Captioning**
- Hierarchical Global-Local Temporal Modeling for Video Captioning [[peper]](https://dl.acm.org/doi/pdf/10.1145/3343031.3351072)   
- Attention-based Densely Connected LSTM for Video Captioning [[paper]](https://dl.acm.org/doi/pdf/10.1145/3343031.3350932)  
- Critic-based Attention Network for Event-based Video Captioning [[paper]](https://dl.acm.org/doi/pdf/10.1145/3343031.3351037)  
- Watch It Twice: Video Captioning with a Refocused Video Encoder [[paper]](https://dl.acm.org/doi/pdf/10.1145/3343031.3351060)   


### ACL-2019

- Informative Image Captioning with External Sources of Information  [[paper]](https://www.aclweb.org/anthology/P19-1650.pdf)  
  *Sanqiang Zhao, Piyush Sharma, Tomer Levinboim and Radu Soricut*
- Dense Procedure Captioning in Narrated Instructional Videos  [[paper]](https://www.aclweb.org/anthology/P19-1641.pdf)  
  *Botian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen, Zhendong Niu and Ming Zhou*
- Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning  [[paper]](https://www.aclweb.org/anthology/P19-1652.pdf)  
  *Zhihao Fan, Zhongyu Wei, Siyuan Wang and Xuanjing Huang*
- Generating Question Relevant Captions to Aid Visual Question Answering  [[paper]](https://www.aclweb.org/anthology/P19-1348.pdf)  
  *Jialin Wu, Zeyuan Hu and Raymond Mooney*
- Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning  [[paper]](https://www.aclweb.org/anthology/P19-1652.pdf)  
  *Zhihao Fan, Zhongyu Wei, Siyuan Wang and Xuanjing Huang*

### NeurIPS-2019

**Image Captioning**

- **AAT**: Adaptively Aligned Image Captioning via Adaptive Attention Time  [[paper]](http://papers.nips.cc/paper/by-source-2019-4799) [[code]](https://github.com/husthuaan/AAT)  
  *Lun Huang, Wenmin Wang, Yaxian Xia, Jie Chen*  
- **ObjRel Transf**: Image Captioning: Transforming Objects into Words  [[paper]](http://papers.nips.cc/paper/by-source-2019-5963) [[code]](https://github.com/yahoo/object_relation_transformer)  
  *Simao Herdade, Armin Kappeler, Kofi Boakye, Joao Soares*  
- **VSSI-cap**: Variational Structured Semantic Inference for Diverse Image Captioning  [[paper]](http://papers.nips.cc/paper/by-source-2019-1113)  
  *Fuhai Chen, Rongrong Ji, Jiayi Ji, Xiaoshuai Sun, Baochang Zhang, Xuri Ge, Yongjian Wu, Feiyue Huang*  


### ICCV-2019

**Video Captioning**  

- **VATEX**: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.pdf) [[challenge]](https://vatex.org/main/index.html)   
  *Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, William Yang Wang*  
  `ICCV 2019 Oral`
- **POS+CG**: Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Controllable_Video_Captioning_With_POS_Sequence_Guidance_Based_on_Gated_ICCV_2019_paper.pdf)  
  *Bairui Wang, Lin Ma, Wei Zhang, Wenhao Jiang, Jingwen Wang, Wei Liu*
- **POS**: Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Joint_Syntax_Representation_Learning_and_Visual_Cue_Translation_for_Video_ICCV_2019_paper.pdf)  
  *Jingyi Hou, Xinxiao Wu, Wentian Zhao, Jiebo Luo, Yunde Jia*  
**Image Captioning**   

- **DUDA**: Robust Change Captioning  
  *Dong Huk Park, Trevor Darrell, Anna Rohrbach*  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Robust_Change_Captioning_ICCV_2019_paper.pdf)    
  `ICCV 2019 Oral`
- **AoANet**: Attention on Attention for Image Captioning  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.pdf)  
  *Lun Huang, Wenmin Wang, Jie Chen, Xiao-Yong Wei*   
  `ICCV 2019 Oral`
- **MaBi-LSTMs**: Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Ge_Exploring_Overall_Contextual_Information_for_Image_Captioning_in_Human-Like_Cognitive_ICCV_2019_paper.pdf)   
  *Hongwei Ge, Zehang Yan, Kai Zhang, Mingde Zhao, Liang Sun*  
- **Align2Ground**: Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Datta_Align2Ground_Weakly_Supervised_Phrase_Grounding_Guided_by_Image-Caption_Alignment_ICCV_2019_paper.pdf)  
  Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja, Devi Parikh, Ajay Divakaran*    
- **GCN-LSTM+HIP**: Hierarchy Parsing for Image Captioning [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Yao_Hierarchy_Parsing_for_Image_Captioning_ICCV_2019_paper.pdf)  
  *Ting Yao, Yingwei Pan, Yehao Li, Tao Mei*    
- **IR+Tdiv**: Generating Diverse and Descriptive Image Captions Using Visual Paraphrases   [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Generating_Diverse_and_Descriptive_Image_Captions_Using_Visual_Paraphrases_ICCV_2019_paper.pdf)  
  *Lixin Liu, Jiajun Tang, Xiaojun Wan, Zongming Guo*    
- **CNM+SGAE**: Learning to Collocate Neural Modules for Image Captioning  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Learning_to_Collocate_Neural_Modules_for_Image_Captioning_ICCV_2019_paper.pdf)  
  *Xu Yang, Hanwang Zhang, Jianfei Cai*     
- **Seq-CVAE**: Sequential Latent Spaces for Modeling the Intention During Diverse Image Captioning  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Aneja_Sequential_Latent_Spaces_for_Modeling_the_Intention_During_Diverse_Image_ICCV_2019_paper.pdf)   
  Jyoti Aneja, Harsh Agrawal, Dhruv Batra, Alexander Schwing
- Towards Unsupervised Image Captioning With Shared Multimodal Embeddings  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Laina_Towards_Unsupervised_Image_Captioning_With_Shared_Multimodal_Embeddings_ICCV_2019_paper.pdf)   
  *Iro Laina, Christian Rupprecht, Nassir Navab*    
- Human Attention in Image Captioning: Dataset and Analysis  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/He_Human_Attention_in_Image_Captioning_Dataset_and_Analysis_ICCV_2019_paper.pdf)  
  *Sen He, Hamed R. Tavakoli, Ali Borji, Nicolas Pugeault*   
- **RDN**: Reflective Decoding Network for Image Captioning  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Ke_Reflective_Decoding_Network_for_Image_Captioning_ICCV_2019_paper.pdf)  
  *Lei Ke, Wenjie Pei, Ruiyu Li, Xiaoyong Shen, Yu-Wing Tai*   
- **PSST**: Joint Optimization for Cooperative Image Captioning  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Vered_Joint_Optimization_for_Cooperative_Image_Captioning_ICCV_2019_paper.pdf)   
  *Gilad Vered, Gal Oren, Yuval Atzmon, Gal Chechik*   
- **MUTAN**: Watch, Listen and Tell: Multi-Modal Weakly Supervised Dense Event Captioning  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Rahman_Watch_Listen_and_Tell_Multi-Modal_Weakly_Supervised_Dense_Event_Captioning_ICCV_2019_paper.pdf)  
  *Tanzila Rahman, Bicheng Xu, Leonid Sigal*   
- **ETA**: Entangled Transformer for Image Captioning   [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.pdf)  
  *Guang Li, Linchao Zhu, Ping Liu, Yi Yang*   
- **nocaps**: novel object captioning at scale  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Agrawal_nocaps_novel_object_captioning_at_scale_ICCV_2019_paper.pdf)  
  *Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson*   
- **Cap2Det**: Learning to Amplify Weak Caption Supervision for Object Detection  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Cap2Det_Learning_to_Amplify_Weak_Caption_Supervision_for_Object_Detection_ICCV_2019_paper.pdf)  
  *Keren Ye, Mingda Zhang, Adriana Kovashka, Wei Li, Danfeng Qin, Jesse Berent*   
- **Graph-Align**: Unpaired Image Captioning via Scene Graph Alignments  [paper](http://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Unpaired_Image_Captioning_via_Scene_Graph_Alignments_ICCV_2019_paper.pdf)  
  *Jiuxiang* Gu, Shafiq Joty, Jianfei Cai, Handong Zhao, Xu Yang, Gang Wang   
- : Learning to Caption Images Through a Lifetime by Asking Questions  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Learning_to_Caption_Images_Through_a_Lifetime_by_Asking_Questions_ICCV_2019_paper.pdf)  
  *Tingke Shen, Amlan Kar, Sanja Fidler*   

### CVPR-2019

**Image Captioning**  

- **SGAE**: Auto-Encoding Scene Graphs for Image Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Auto-Encoding_Scene_Graphs_for_Image_Captioning_CVPR_2019_paper.pdf) [[code]](https://github.com/fengyang0317/unsupervised_captioning)   
  *XU YANG (Nanyang Technological University); Kaihua Tang (Nanyang Technological University); Hanwang Zhang (Nanyang Technological University); Jianfei Cai (Nanyang Technological University)*  
  `CVPR 2019 Oral` 
- **POS**: Fast, Diverse and Accurate Image Captioning Guided by Part-Of-Speech  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Deshpande_Fast_Diverse_and_Accurate_Image_Captioning_Guided_by_Part-Of-Speech_CVPR_2019_paper.pdf)   
  *Aditya Deshpande (University of Illinois at UC); Jyoti Aneja (University of Illinois, Urbana-Champaign); Liwei Wang (Tencent AI Lab); Alexander Schwing (UIUC); David Forsyth (Univeristy of Illinois at Urbana-Champaign)*  
  `CVPR 2019 Oral`
- Unsupervised Image Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Feng_Unsupervised_Image_Captioning_CVPR_2019_paper.pdf) [[code]](https://github.com/fengyang0317/unsupervised_captioning)   
  *Yang Feng (University of Rochester); Lin Ma (Tencent AI Lab); Wei Liu (Tencent); Jiebo Luo (U. Rochester)*  
- Adversarial Attack to Image Captioning via Structured Output Learning With Latent Variables  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Exact_Adversarial_Attack_to_Image_Captioning_via_Structured_Output_Learning_CVPR_2019_paper.pdf)  
  *Yan Xu (UESTC); Baoyuan Wu (Tencent AI Lab); Fumin Shen (UESTC); Yanbo Fan (Tencent AI Lab); Yong Zhang (Tencent AI Lab); Heng Tao Shen (University of Electronic Science and Technology of China (UESTC)); Wei Liu (Tencent)*   
- Describing like Humans: On Diversity in Image Captioning [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Describing_Like_Humans_On_Diversity_in_Image_Captioning_CVPR_2019_paper.pdf)   
  *Qingzhong Wang (Department of Computer Science, City University of Hong Kong); Antoni Chan (City University of Hong Kong, Hong, Kong)*  
- **MSCap**: Multi-Style Image Captioning With Unpaired Stylized Text  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_MSCap_Multi-Style_Image_Captioning_With_Unpaired_Stylized_Text_CVPR_2019_paper.pdf)   
  *Longteng Guo ( Institute of Automation, Chinese Academy of Sciences); Jing Liu (National Lab of Pattern Recognition, Institute of Automation,Chinese Academy of Sciences); Peng Yao (University of Science and Technology Beijing); Jiangwei Li (Huawei); Hanqing Lu (NLPR, Institute of Automation, CAS)*  
- **CapSal**: Leveraging Captioning to Boost Semantics for Salient Object Detection  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_CapSal_Leveraging_Captioning_to_Boost_Semantics_for_Salient_Object_Detection_CVPR_2019_paper.pdf)  [[code]](https://github.com/zhangludl/code-and-dataset-for-CapSal)  
  *Lu Zhang (Dalian University of Technology); Huchuan Lu (Dalian University of Technology); Zhe Lin (Adobe Research); Jianming Zhang (Adobe Research); You He (Naval Aviation University)*    
- Context and Attribute Grounded Dense Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yin_Context_and_Attribute_Grounded_Dense_Captioning_CVPR_2019_paper.pdf)  
  *Guojun Yin (University of Science and Technology of China); Lu Sheng (The Chinese University of Hong Kong); Bin Liu (University of Science and Technology of China); Nenghai Yu (University of Science and Technology of China); Xiaogang Wang (Chinese University of Hong Kong, Hong Kong); Jing Shao (Sensetime)*  
- Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Dense_Relational_Captioning_Triple-Stream_Networks_for_Relationship-Based_Captioning_CVPR_2019_paper.pdf)   
  *Dong-Jin Kim (KAIST); Jinsoo Choi (KAIST); Tae-Hyun Oh (MIT CSAIL); In So Kweon (KAIST)*  
- **Show, Control and Tell**: A Framework for Generating Controllable and Grounded Captions  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Cornia_Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_CVPR_2019_paper.pdf)  
  *Marcella Cornia (University of Modena and Reggio Emilia); Lorenzo Baraldi (University of Modena and Reggio Emilia); Rita Cucchiara (Universita Di Modena E Reggio Emilia)*  
- Self-Critical N-step Training for Image Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Self-Critical_N-Step_Training_for_Image_Captioning_CVPR_2019_paper.pdf)  
  *Junlong Gao (Peking University Shenzhen Graduate School); Shiqi Wang (CityU); Shanshe Wang (Peking University); Siwei Ma (Peking University, China); Wen Gao (PKU)*  
- Look Back and Predict Forward in Image Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Qin_Look_Back_and_Predict_Forward_in_Image_Captioning_CVPR_2019_paper.pdf)  
  *Yu Qin (Shanghai Jiao Tong University); Jiajun Du (Shanghai Jiao Tong University); Hongtao Lu (Shanghai Jiao Tong University); Yonghua Zhang (Bytedance)*  
- Intention Oriented Image Captions with Guiding Objects  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Intention_Oriented_Image_Captions_With_Guiding_Objects_CVPR_2019_paper.pdf)   
  *Yue Zheng (Tsinghua University); Ya-Li Li (THU); Shengjin Wang (Tsinghua University)*  
- Adversarial Semantic Alignment for Improved Image Captions  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Dognin_Adversarial_Semantic_Alignment_for_Improved_Image_Captions_CVPR_2019_paper.pdf)  
  *Pierre Dognin (IBM); Igor Melnyk (IBM); Youssef Mroueh (IBM Research); Jarret Ross (IBM); Tom Sercu (IBM Research AI)*  
- Good News, Everyone! Context driven entity-aware captioning for news images  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Biten_Good_News_Everyone_Context_Driven_Entity-Aware_Captioning_for_News_Images_CVPR_2019_paper.pdf)  [[code]](https://github.com/furkanbiten/GoodNews)  
  *Ali Furkan Biten (Computer Vision Center); Lluis Gomez (Universitat Autónoma de Barcelona); Marçal Rusiñol (Computer Vision Center, UAB); Dimosthenis Karatzas (Computer Vision Centre)*  
- Pointing Novel Objects in Image Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Pointing_Novel_Objects_in_Image_Captioning_CVPR_2019_paper.pdf)   
  *Yehao Li (Sun Yat-Sen University); Ting Yao (JD AI Research); Yingwei Pan (JD AI Research); Hongyang Chao (Sun Yat-sen University); Tao Mei (AI Research of JD.com)*  
- Engaging Image Captioning via Personality  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Shuster_Engaging_Image_Captioning_via_Personality_CVPR_2019_paper.pdf)  
  *Kurt Shuster (Facebook); Samuel Humeau (Facebook); Hexiang Hu (USC); Antoine Bordes (Facebook); Jason Weston (FAIR)*  
- Intention Oriented Image Captions With Guiding Objects  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Intention_Oriented_Image_Captions_With_Guiding_Objects_CVPR_2019_paper.pdf)  
  *Yue Zheng, Yali Li, Shengjin Wang*   
- Exact Adversarial Attack to Image Captioning via Structured Output Learning With Latent Variables  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Exact_Adversarial_Attack_to_Image_Captioning_via_Structured_Output_Learning_CVPR_2019_paper.pdf)  
  *Yan Xu, Baoyuan Wu, Fumin Shen, Yanbo Fan, Yong Zhang, Heng Tao Shen, Wei Liu*    

**Video Captioning**

-  **SDVC**: Streamlined Dense Video Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Mun_Streamlined_Dense_Video_Captioning_CVPR_2019_paper.pdf)  
   *Jonghwan Mun (POSTECH); Linjie Yang (ByteDance AI Lab); Zhou Ren (Snap Inc.); Ning Xu (Snap); Bohyung Han (Seoul National University)*  
   `CVPR 2019 Oral`
-  **GVD**: Grounded Video Description  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Grounded_Video_Description_CVPR_2019_paper.pdf)  
   *Luowei Zhou (University of Michigan); Yannis Kalantidis (Facebook Research); Xinlei Chen (Facebook AI Research); Jason J Corso (University of Michigan); Marcus Rohrbach (Facebook AI Research)*  
   `CVPR 2019 Oral`
-  **HybridDis**: Adversarial Inference for Multi-Sentence Video Description  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Park_Adversarial_Inference_for_Multi-Sentence_Video_Description_CVPR_2019_paper.pdf)  
   *Jae Sung Park (UC Berkeley); Marcus Rohrbach (Facebook AI Research); Trevor Darrell (UC Berkeley); Anna Rohrbach (UC Berkeley)*	  
   `CVPR 2019 Oral`
-  **OA-BTG**: Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Object-Aware_Aggregation_With_Bidirectional_Temporal_Graph_for_Video_Captioning_CVPR_2019_paper.pdf)  
   *Junchao Zhang (Peking University); Yuxin Peng (Peking University)*  
-  **MARN**: Memory-Attended Recurrent Network for Video Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Pei_Memory-Attended_Recurrent_Network_for_Video_Captioning_CVPR_2019_paper.pdf)  
   *Wenjie Pei (Tencent); Jiyuan Zhang (Tencent YouTu); Xiangrong Wang (Delft University of Technology); Lei Ke (Tencent); Xiaoyong Shen (Tencent); Yu-Wing Tai (Tencent)*  
-  **GRU-EVE**: Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Aafaq_Spatio-Temporal_Dynamics_and_Semantic_Attribute_Enriched_Visual_Encoding_for_Video_CVPR_2019_paper.pdf)  
   *Nayyer Aafaq (The University of Western Australia); Naveed Akhtar (The University of Western Australia); Wei Liu (University of Western Australia); Syed Zulqarnain Gilani (The University of Western Australia); Ajmal Mian (University of Western Australia)*  

### AAAI-2019

**Image Captioning**  

- Improving Image Captioning with Conditional Generative Adversarial Nets  [[paper]](https://arxiv.org/pdf/1805.07112.pdf)  
  *CHEN CHEN (Tencent); SHUAI MU (Tencent); WANPENG XIAO (Tencent); ZEXIONG YE (Tencent); LIESI WU (Tencent); QI JU (Tencent)*   
  `AAAI 2019 Oral`
- **PAGNet**: Connecting Language to Images: A Progressive Attention-Guided Network for Simultaneous Image Captioning and Language Grounding  [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4916)  
  *Lingyun Song (Xi'an JiaoTong University); Jun Liu (Xi'an Jiaotong Univerisity); Buyue Qian (Xi'an Jiaotong University); Yihe Chen (University of Toronto)*  
  `AAAI 2019 Oral`
- Meta Learning for Image Captioning  [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4883)  
  *Nannan Li (Wuhan University); Zhenzhong Chen (WHU); Shan Liu (Tencent America)*  
- **DA**: Deliberate Residual based Attention Network for Image Captioning  [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4845/4718) 
  *Lianli Gao (The University of Electronic Science and Technology of China); kaixuan fan (University of Electronic Science and Technology of China); Jingkuan Song (UESTC); Xianglong Liu (Beihang University); Xing Xu (University of Electronic Science and Technology of China); Heng Tao Shen (University of Electronic Science and Technology of China (UESTC))*  
- **HAN**: Hierarchical Attention Network for Image Captioning  [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4924)  
  *Weixuan Wang (School of Electronic and Information Engineering, Sun Yat-sen University);Zhihong Chen (School of Electronic and Information Engineering, Sun Yat-sen University); Haifeng Hu (School of Electronic and Information Engineering, Sun Yat-sen University)*   
- **COCG**: Learning Object Context for Dense Captioning  [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4886)  
  *Xiangyang Li (Institute of Computing Technology, Chinese Academy of Sciences); Shuqiang Jiang (ICT, China Academy of Science); Jungong Han (Lancaster University)*  


**Video Captioning**  

- **TAMoE**: Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning  [[code]](https://github.com/eric-xw/Zero-Shot-Video-Captioning) [[paper]](https://arxiv.org/pdf/1811.02765.pdf)  
  *Xin Wang (University of California, Santa Barbara); Jiawei Wu (University of California, Santa Barbara); Da Zhang (UC Santa Barbara); Yu Su (OSU); William Wang (UC Santa Barbara)*  
  `AAAI 2019 Oral`
- **TDConvED**: Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning   [[paper]](https://arxiv.org/pdf/1905.01077v1.pdf)  
  *Jingwen Chen (Sun Yat-set University); Yingwei Pan (JD AI Research); Yehao Li (Sun Yat-Sen University); Ting Yao (JD AI Research); Hongyang Chao (Sun Yat-sen University); Tao Mei (AI Research of JD.com)*  
  `AAAI 2019 Oral`
- **FCVC-CF&IA**: Fully Convolutional Video Captioning with Coarse-to-Fine and Inherited Attention  [[paper]](https://aaai.org/ojs/index.php/AAAI/article/view/4839)  
  *Kuncheng Fang (Fudan University); Lian Zhou (Fudan University); Cheng Jin (Fudan University); Yuejie Zhang (Fudan University); Kangnian Weng (Shanghai University of Finance and Economics); Tao Zhang (Shanghai University of Finance and Economics); Weiguo Fan (University of Iowa)*   
- **MGSA**: Motion Guided Spatial Attention for Video Captioning  [[paper]](http://yugangjiang.info/publication/19AAAI-vidcaptioning.pdf)  
  *Shaoxiang Chen (Fudan University); Yu-Gang Jiang (Fudan University)*  
",['forence'],0,,0.69,0,,,,,,19,,,
89636006,MDEwOlJlcG9zaXRvcnk4OTYzNjAwNg==,OpenTPU,UCSBarchlab/OpenTPU,0,UCSBarchlab,https://github.com/UCSBarchlab/OpenTPU,A open source reimplementation of Google's Tensor Processing Unit (TPU).,0,2017-04-27 20:06:28+00:00,2025-03-02 15:42:02+00:00,2017-12-06 22:46:16+00:00,,14823,410,410,Python,1,1,1,1,0,0,71,0,0,7,bsd-3-clause,1,0,0,public,71,7,410,master,1,1,"# UCSB ArchLab OpenTPU Project 

OpenTPU is an open-source re-implementation of Google's Tensor Processing Unit (TPU) by the UC Santa Barbara ArchLab.

The TPU is Google's custom ASIC for accelerating the inference phase of neural network computations.

Our design is based on details from Google's paper titled ""In-Datacentre Performance Analysis of a Tensor Processing Unit"" (https://arxiv.org/abs/1704.04760), which is to appear at ISCA2017. However, no formal spec, interface, or ISA has yet been published for the TPU.

#### The OpenTPU is powered by PyRTL (http://ucsbarchlab.github.io/PyRTL/).

## Requirements

- Python 3
- PyRTL version >= 0.8.5
- numpy

Both PyRTL and numpy can be installed with pip; e.g., `pip install pyrtl`.

## How to Run

To run the simple matrix multiply test in both the hardware and functional simulators:

Make sure MATSIZE is set to 8 in config.py, then

```
python3 assembler.py simplemult.a
python3 runtpu.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy
python3 sim.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy
```

To run the Boston housing data regression test in both the hardware and functional simulators:

Make sure MATSIZE is set to 16 in config.py, then
```
python3 assembler.py boston.a
python3 runtpu.py boston.out boston_inputs.npy boston_weights.npy
python3 sim.py boston.out boston_inputs.npy boston_weights.npy
```


### Hardware Simulation
The executable hardware spec can be run using PyRTL's simulation features by running `runtpu.py`. The simulation expects as inputs a binary program and numpy array files containing the initial host memory and the weights.

Be aware that the size of the hardware Matrix Multiply unit is parametrizable --- double check `config.py` to make sure MATSIZE is what you expect.

### Functional Simulation
sim.py implements the functional simulator of OpenTPU. It reads in three cmd args: the assembly program, the host memory file, and the weights file. Due to the different quantization mechnisms between high-level applications (written in tensorflow) and OpenTPU, the simulator runs in two modes: 32b float mode and 8b int mode. The downsampling/quantization mechanism is consistent with the HW implementation of OpenTPU. It generates two sets of outputs, one set being 32b-float typed, the other 8b-int typed.

Example usage:

    python sim.py boston.out boston_input.npy boston_weights.npy

Numpy matrices (.npy files) can be generated by calling `numpy.save` on a numpy array.

checker.py implementes a simple checking function to verify the results from HW, simulator and applications. It checkes the 32b-float application results against 32b-float simulator results and then checks the 8b-int simulator results against 8b-int HW results.

Example usage:

    python checker.py


## FAQs:

### How big/efficient/fast is OpenTPU?
As of the alpha release, we do not have hard synthesis figures for the full 256x256 OpenTPU.

### What can OpenTPU do?
The hardware prototype can currently handle matrix multiplies and activations for ReLU and sigmoid --- i.e., the inference phase of many neural network computations.

### What features are missing?
Convolution, pooling, programmable normalization.

### Does your design follow that of the TPU?
We used high-level design details from the TPU paper to guide our design when possible. Thus, the major components of the chip are the same --- matrix multiply unit, unified buffer, activation unit, accumulator, weight FIFO, etc. Beyond that, the implementations may have many differences.

### Does OpenTPU support all the same instructions as TPU?
No. Currently, OpenTPU supports the RHM, WHM, RW, MMC, ACT, NOP, and HLT instructions (see ISA section for details). The purpose, definition, and specification of other TPU instructions is absent from the published paper. Some instructions will likely be added to OpenTPU as we continue development (such as SYNC), but the final ISA will likely feature many differences without a published spec from Google to work off of.

### Is OpenTPU binary compatible with the TPU?
No. There is no publicly available interface or spec for TPU.

### I'd like to do some analysis/extensions of OpenTPU, but I need Verilog. Do you have a Verilog version?
PyRTL can can output structural Verilog for the design, using the `OutputToVerilog` function.

### I have suggestions, criticisms, and/or would like to contribute.
That's not a question, but please get in touch! Email Deeksha (deeksha@cs.ucsb.edu) or Joseph (jmcmahan@cs.ucsb.edu).

### I'm a Distinguished Hardware Engineer at Google and the Lead Architect of the TPU. I see many inefficiencies in your implementation.
Hi Norm! Tim welcomes you to Santa Barbara to talk about all things TPU :)


## Software details

### ISA

- RHM src, dst, N
Read Host Memory.
Read _N_ vectors from host memory beginning at address _src_ and save them in the UB (unified buffer) beginning at address _dst_.
- WHM src, dst, N
Write Host Memory.
Write _N_ vectors from the UB beginning at address _src_ to host memory beginning at address _dst_.
- RW addr
Read Weights.
Load the weights tile from the weights DRAM at address _addr_ into the on-chip FIFO.
- MMC.{OS} src, dst, N
Matrix Multiply/Convolution.
Perform a matrix multiply operation on the _N_ vectors beginning at UB address _src_, storing the result in the accumulator buffers beginning at address _dst_. If the _O_ (overwrite) flag is specified, overwrite the contents of the accumulator buffers at the destination addresses; default behavior is to add to the value there and store the new sum. If the _S_ (switch) flag is specified, switch to using the next tile of weights, which must have already been pre-loaded. The first `MMC` instruction in a program should always use the _S_ flag.
- ACT.{RQ} src, dst, N
Activate.
Perform activation on _N_ vectors in the accumulator buffers starting at address _src_, storing the results in the UB beginning at address _dst_. Activation function is specified with a flag: _R_ for ReLU and _Q_ for sigmoid. With no flag, values are passed through without activation. Normalization is programmable at synthesis-time, but not at run-time; by default, after activation the upper 24 bits are dropped from each value, producing an 8-bit integer.
- NOP
No op. Do nothing for one cycle.
- HLT
Halt. Stop simulation.


### Writing a Program
OpenTPU uses no dynamic scheduling; all execution is fully determinstic* and the hardware relies on the compiler to correctly schedule operations and pad NOPs to handle delays. This OpenTPU release does \
not support ""repeat"" flags on instructions, so many NOPs are required to ensure correct execution.

*DRAM is a source of non-deterministic latency, discussed in the Memory Controller section of Microarchitecture.

### Generating Data
__Application__

1. Simple one hot 2-layer NN

gen_one_hot.py generates 8b-int typed random squre matrix as training data and vector as label, example usage:
    
    python gen_one_hot.py --path simple_train --shape 8 8 --range -5 5
    python gen_one_hot.py --path simple_train_label --shape 8 1 --range 0 2

simple_nn.py trains a simple 2-layer nn on the given train/label dataset and writes the weights into a file, example usage (run gen_one_hot example first to generate the files):
    
    python simple_nn.py --path simple_train.npy --label simple_train_label.npy

After running the above command, two files are generated: simple_nn_weight_dram.npy is the 8b-int typed weight dram that the OpenTPU operates on, simple_nn_gt is the pickled ground truth 32b-float resulits and weights. To run with OpenTPU, a test file must also be generated, example usage:

    python gen_one_hot.py --path simple_test --shape 100 8 --range 1, 9

After which simple_test.npy will be generated and it should be used as the host memory by OpenTPU.

We also provide simple_nn.a -- the assembly program for this simple nn.

2. Tensorflow DNN regression

Although applications written in any high-level nn framework can be used, here we use tensorflow as an example.

tf_nn.py trains a MLP regressor on the Boston Housing Dataset (https://archive.ics.uci.edu/ml/datasets/housing). Example usage:
    
    python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output
    python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output --raw

After running the above command, four files are generated: gt32.npy holds the ground truth prediction values, boston_input.npy holds the input test cases which is used as the host memeory for OpenTPU, boston_output.npy holds all the intermediate output values, and boston_weights.npy holds the weight matrices which are used as the weight dram for OpenTPU.

Adding --raw to the command generates 32b-float typed files instead of 8b ints.


### Latencies
The following gives the hardware execution latency for each instruction on OpenTPU:

- RHM - _M_ cycles for reading _M_ vectors
- WHM - _M_ cycles for writing _M_ vectors
- RW - _N*N_/64 cycles for _N_x_N_ MM Array for DRAM transfer, and up to 3 additional cycles to propagate through the FIFO
- MMC - _L+2N_ cycles, for _N_x_N_ MM Array and _L_ vectors multiplied in the instruction
- ACT - _L+1_ cycles, for _L_ vectors activated in the instruction


## Microarchitecture

### Matrix Multiply (MM) Unit
The core of the compute of the OpenTPU is the parametrizable array of 8-bit Multiply-Accumulate Units (MACs), each consisting of an 8-bit integer multiplier and an integer adder of between 16 and 32 bits\
*. Each MAC has two buffers storing 8-bit weights (the second buffer allows weight programming to happen in parallel). Input vectors enter the array from the left, with values advancing one unit to the r\
ight each cycle. Each unit multiplies the input value by the active weight, adds it to the value from the unit above, and passes the result to the unit below. Input vectors are fed diagonally so that val\
ues align correctly as partial sums flow down the array.

*The multipliers produce 16-bit outputs; as values move down the columns of the array, each add produces 1 extra bit. Width is capped at 32, creating the potential for uncaught overflow.


### Accumulator Buffers
Result vectors from the MM Array are written to a software-specified address in a set of accumulator buffers. Instructions indicate whether values should be added into the value already at the address or\
 overwrite it. MM instructions read from the Unified Buffer (UB) and write to the accumulator buffers; activate instructions read from the accumulator buffers and write to the UB.


### Weight FIFO
At scale (256x256 MACs), a full matrix of weights (a ""tile"") is 64KB; to avoid stalls while weights are moved from off-chip weight DRAM, a 4-entry FIFO is used to buffer tiles. It is assumed the connecti\
on to the weight DRAM is a standard DDR interface moving data in 64-byte chunks (memory controllers are currently emulated with no simulated delay, so one chunk arrives each cycle). When an MM instructio\
n carries the ""switch"" flag, each MAC switches the active weight buffer as first vector of the instruction propagates through the array. Once it reaches the end of the first row, the FIFO begins feeding \
new weight values into the free buffers of the array. New weight values are passed down through the array each cycle until each row reaches its destination.


### Systolic Setup
Vectors are read all at once from the Unified Buffer, but must be fed diagonally into the MM Array. This is accomplished with a set of sequential buffers in a lower triangular configuration. The top valu\
e reaches the matrix immediately, the second after one cycle, the third after two, etc., so that each value reaches a MAC at the same time as the corresponding partial sum from the same source vector.


### Memory Controllers
Currently, memory controllers are emulated and have no delay. The connection to Host Memory is currently the size of one vector. The connection to the Weight DRAM uses a standard width of 64 bytes.

Because the emulated controllers can return a new value each cycle, the OpenTPU hardware simulation currently has no non-detministic delay. With a more accurate DRAM interface that may encounter dynamic \
delays, programs would need to either take care to schedule for the worst-case memory delay, or make use of another instruction to ensure memory operations complete before the values are required*.

*We note that the TPU ""SYNC"" instruction may fulfill this purpose, but is currently unimplemented on OpenTPU.


### Configuration
Unified Buffer size, Accumulator Buffer size, and the size of the MM Array can all be specified in config.py. However, the MM Array must always be square, and vectors/weights are always composed of 8-bit integers.
 
","['jemcmahan13', 'Weil0ng', 'dcoffill', 'masoug', 'deekshadangwal', 'trigish', 'gtzimpragos']",1,,0.83,0,,,,,,33,,,
934994891,R_kgDON7rjyw,MLGym,facebookresearch/MLGym,0,facebookresearch,https://github.com/facebookresearch/MLGym,MLGym A New Framework and Benchmark for Advancing AI Research Agents,0,2025-02-18 18:26:47+00:00,2025-03-06 16:12:41+00:00,2025-02-25 00:26:50+00:00,,9829,407,407,Python,1,1,1,0,0,0,36,0,0,3,other,1,0,0,public,36,3,407,main,1,1,"<p align=""center"">
    <img src=""./assets/logos/mlgym_logo.png"" height=""300"" width=""600"" alt=""MLGym Logo"">
</p>

<p align=""center"">
  <a href=""https://creativecommons.org/licenses/by-nc/4.0/""><img src=""https://img.shields.io/badge/License-CC_BY--NC_4.0-lightgrey.svg"" /></a>
  <!-- Someone else has pypi package with the same name -->
  <!-- <a href=""https://pepy.tech/project/mlgym""><img src=""https://static.pepy.tech/personalized-badge/minihack?period=total&units=international_system&left_color=black&right_color=red&left_text=Downloads"" /></a> -->
  <!-- <a href=""https://github.com/facebookresearch/minihack/actions/workflows/test_and_deploy.yml""><img src=""https://github.com/facebookresearch/minihack/actions/workflows/test_and_deploy.yml/badge.svg?branch=main"" /></a> -->
  <a href=""https://arxiv.org/abs/2502.14499""><img src=""https://img.shields.io/badge/arXiv-2502.14499-b31b1b.svg""/></a>
 </p>

## Table of contents

* [Introduction](#introduction)
* [Installation](#installation)
* [Quick Start](#quick-start)
* [Trajectory Visualizer](#trajectory-visualizer)
* [Contributions and Maintenance](#contributions-and-maintenance)
* [License](#license)

## Introduction

This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. <span style=""font-variant:small-caps;"">MLGym</span>-Bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task.
![image info](./assets/figs/mlgym.png)

> [!WARNING]
> Meta <span style=""font-variant:small-caps;"">MLGym</span> is currently an experimental framework intended for benchmarking AI Research Agents. It is under heavy development. Please expect major changes to the design.
>
> The primary goal of <span style=""font-variant:small-caps;"">MLGym</span> is to expand the selection of AI research tasks for benchmarking the LLM Agents and implementing RL algorithms to train LLMs in a research environment.
> `main` branch will always contain the latest stable release and all breaking changes will be announced in the [release notes](./CHANGELOG.md).

## Installation

1. Clone and install dependencies

    ```bash
    git clone git@github.com:facebookresearch/MLGym.git
    cd MLGym
    conda create -y -n mlgym python=3.11
    conda activate mlgym
    pip install -e .
    ```

2. Create a `.env` file in the MLGym directory (`MLGym/.env`) to save all the environment variables including API keys.

    ```bash
    # Env variables
    MLGYM_CONFIG_ROOT=""<path_to_MLGYM_root>/configs""
    MLGYM_TASK_CONFIG_DIR=""<path_to_MLGYM_root>/configs/tasks""
    MLGYM_WORKSPACE_PATH=""<path_to_MLGYM_root>/workspace""
    MLGYM_ENV_TIMEOUT=10000
    MLGYM_ACTION_SHORT_TIMEOUT=60
    MLGYM_ACTION_LONG_TIMEOUT=10000
    MLGYM_MODEL_MAX_RETRIES=3

    # API keys
    OPENAI_API_KEY=""""
    ANTHROPIC_API_KEY=""""
    ```

3. You can use either Docker or Podman to run tasks inside a container. Podman is the recommended way to run containers on macOS.

4. Follow the instructions [here](https://docs.docker.com/desktop/) to install docker. Select the appropriate installation command based on your OS.

5. If you are working on a Linux machine, please install the `nvidia-container-runtime`. This is required to start docker containers with GPU support.

    ```bash
    sudo dnf install -y nvidia-container-toolkit
    ```

6. **Please skip to step 9 if you don't want to use Podman**.

7. For Linux:  
    a. Follow the instructions [here](https://podman.io/get-started) to install Podman.  
    b. Start podman socket. The last command should return a running podman socket:

    ```bash
    systemctl --user enable podman.socket
    systemctl --user start podman.socket
    systemctl --user status podman.socket 
    ```

    c. Redirect docker host to podman by exporting docker host env variable in bashrc or current session:

    ```bash
    export DOCKER_HOST=unix:///run/user/$UID/podman/podman.sock
    ```

8. For MacOS:  
    a. If you use Homebrew package manager, install Podman with `brew install podman`. Otherwise, follow the instructions [here](https://podman.io/get-started).  
    b. Start the podman machine and set the docker host env variable:
    ```bash
    podman machine init
    podman machine start
    export DOCKER_HOST=unix://$(podman machine inspect --format '{{.ConnectionInfo.PodmanSocket.Path}}')
    ```

9. Pull the container image:

    ```bash
    docker pull aigym/mlgym-agent:latest
    ```

    or  
    ```bash
    podman pull aigym/mlgym-agent:latest
    ```

10. Test launching a docker/podman container with GPU support

    ```bash
    docker run -it --gpus all --name test aigym/mlgym-agent /bin/bash
    ls -la
    exit
    ```

11. Check that GPUs are available in the docker container using `nvidia-smi`.

### Troubleshooting

If you get Nvidia CDI spec errors on linux (eg. `Error: setting up CDI devices: unresolvable CDI devices nvidia.com/gpu=all`), run these additional commands.

```bash
sudo mkdir /etc/cdi
sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
sudo touch /etc/containers/nodocker
```

## Quick Start

### Docker

```bash
python run.py \
  --container_type docker \
  --task_config_path tasks/battleOfSexes.yaml \
  --model litellm:claude-3-5-sonnet-20240620 \
  --per_instance_cost_limit 4.00 \
  --agent_config_path configs/agents/default.yaml \
  --temp 1 \
  --gpus 0 \
  --max_steps 50 \
  --aliases_file ./docker/aliases.sh
```

### Podman

```bash
python run.py \
  --container_type podman \
  --task_config_path tasks/battleOfSexes.yaml \
  --model litellm:claude-3-5-sonnet-20240620 \
  --per_instance_cost_limit 4.00 \
  --agent_config_path configs/agents/default.yaml \
  --temp 1 \
  --gpus 0 \
  --max_steps 50 \
  --aliases_file ./docker/aliases.sh
```

To see a full list of flags, please run `python run.py --help`.

> [!NOTE]
> A detailed documentation for all parts of the <span style=""font-variant:small-caps;"">MLGym</span> framework is under construction. Please stay tuned!

## Trajectory Visualizer

<span style=""font-variant:small-caps;"">MLGym</span> provides a Web UI to inspect the agent trajectories.

```bash
streamlit run demo/trajectory_visualizer.py -- --trajectory_dir <absolute_path_to_trajectories>

# An example
streamlit run demo/trajectory_visualizer.py -- --trajectory_dir $HOME/Projects/MLGym/trajectories/mlgym_bench_v0
```

To run the demo for <span style=""font-variant:small-caps;"">MLGym</span>, use the following command:

```bash
streamlit run demo/demo.py
```

## Contributions and Maintenance

<span style=""font-variant:small-caps;"">MLGym</span> was built and is maintained by [GenAI at Meta](https://ai.meta.com/) and [UCSB NLP](http://nlp.cs.ucsb.edu/). We welcome contributions to <span style=""font-variant:small-caps;"">MLGym</span>. If you are interested in contributing, please see [this document](./CONTRIBUTING.md). Our maintenance plan can be found [here](./MAINTENANCE.md).

## Citation

If you find this work helpful, please consider citing us using the following:

```tex
@misc{nathani2025mlgymnewframeworkbenchmark,
      title={MLGym: A New Framework and Benchmark for Advancing AI Research Agents}, 
      author={Deepak Nathani and Lovish Madaan and Nicholas Roberts and Nikolay Bashlykov and Ajay Menon and Vincent Moens and Amar Budhiraja and Despoina Magka and Vladislav Vorotilov and Gaurav Chaurasia and Dieuwke Hupkes and Ricardo Silveira Cabral and Tatiana Shavrina and Jakob Foerster and Yoram Bachrach and William Yang Wang and Roberta Raileanu},
      year={2025},
      eprint={2502.14499},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.14499}, 
}
```

## License

The majority of this code is licensed under CC-BY-NC 4.0 (Attribution-NonCommercial 4.0 International) license. However portions of the project are available under separate license terms: [SWE-Agent](https://github.com/SWE-agent/SWE-agent?tab=MIT-1-ov-file) and [Modded-NanoGPT](https://github.com/KellerJordan/modded-nanogpt?tab=MIT-1-ov-file) are released under MIT license; [Gymnax](https://github.com/RobertTLange/gymnax?tab=Apache-2.0-1-ov-file) and [Gymnax-blines](https://github.com/RobertTLange/gymnax-blines?tab=Apache-2.0-1-ov-file) are released under Apache 2.0 License.
","['deepakn97', 'ollmer', 'Rybolos', 'rraileanu', 'CharlesCNorton', 'NasonZ']",1,,0.69,0,"# Code of Conduct

Facebook has adopted a Code of Conduct that we expect project participants to adhere to.
Please read the [full text](https://code.fb.com/codeofconduct/)
so that you can understand what actions will and will not be tolerated.
","# Contributing to <span style=""font-variant:small-caps;"">MLGym</span>

We want to make contributing to this project as easy and transparent as
possible.

## Pull Requests

We actively welcome your pull requests.

1. Fork the repo and create your branch from `main`.
2. If you've added code that should be tested, add tests.
3. If you've changed APIs, update the documentation.
4. Make sure your code lints.
5. If you haven't already, complete the Contributor License Agreement (""CLA"").

## Contributor License Agreement (""CLA"")

In order to accept your pull request, we need you to submit a CLA. You only need
to do this once to work on any of Facebook's open source projects.

Complete your CLA here: <https://code.facebook.com/cla>

## Issues

We use GitHub issues to track public bugs. Please ensure your description is
clear and has sufficient instructions to be able to reproduce the issue.

Facebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe
disclosure of security bugs. In those cases, please go through the process
outlined on that page and do not file a public issue.

## License

By contributing to <span style=""font-variant:small-caps;"">MLGym</span>, you agree that your contributions will be licensed
under the LICENSE file in the root directory of this source tree.
",,Directory exists,,8,,,
215886167,MDEwOlJlcG9zaXRvcnkyMTU4ODYxNjc=,karonte,ucsb-seclab/karonte,0,ucsb-seclab,https://github.com/ucsb-seclab/karonte,Karonte is a static analysis tool to detect multi-binary vulnerabilities in embedded firmware,0,2019-10-17 21:10:07+00:00,2025-03-05 04:52:43+00:00,2021-09-18 17:15:47+00:00,,1242,401,401,Python,1,1,1,1,0,0,61,0,0,10,bsd-2-clause,1,0,0,public,61,10,401,master,1,1,"# Karonte
[![License](https://img.shields.io/github/license/angr/angr.svg)](https://github.com/angr/angr/blob/master/LICENSE)


Karonte is a static analysis tool to detect multi-binary vulnerabilities in embedded firmware.

The `master` branch provides the latest version of Karonte, ported to python3. For the original implementation and experiments presented in our paper, please checkout the `IEEE-SP-20` branch and have a look at our [docker container](https://hub.docker.com/r/badnack/karonte).


## Overview

<img src=""overview.png"" width=""60%"">

## Research paper

We present our approach and the findings of this work in the following research paper:

**KARONTE: Detecting Insecure Multi-binary Interactions in Embedded Firmware** 
[[PDF]](https://www.badnack.it/static/papers/University/karonte.pdf)  
Nilo Redini, Aravind Machiry, Ruoyu Wang, Chad Spensky, Andrea Continella, Yan Shoshitaishvili, Christopher Kruegel, Giovanni Vigna.  
*In Proceedings of the IEEE Symposium on Security & Privacy (S&P), May 2020*

If you use *Karonte* in a scientific publication, we would appreciate citations using this **Bibtex** entry:
``` tex
@inproceedings{redini_karonte_20,
 author    = {Nilo Redini and Aravind Machiry and Ruoyu Wang and Chad Spensky and Andrea Continella and Yan Shoshitaishvili and Christopher Kruegel and Giovanni Vigna},
 booktitle = {In Proceedings of the IEEE Symposium on Security & Privacy (S&P)},
 month     = {May},
 title     = {KARONTE: Detecting Insecure Multi-binary Interactions in Embedded Firmware},
 year      = {2020}
}
```

## Repository Structure

There are four main directories:
- **tool**: Karonte python files
- **firmware**: Karonte firmware dataset
- **configs**: configuration files to analyze the firmware samples in the dataset
- **eval**: scripts to run the various evaluations on Karonte
- **karonte-viz**: script to visualize the results produced by Karonte

## Run Karonte

To  run karonte, from the root directory, just run
> **SYNOPSIS**
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; python **tool/karonte.py** JSON_CONFIG_FILE [LOG_NAME]
>
> **DESCRIPTION**
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runs karonte on the firmware sample represented by the JSON_CONFIG_FILE, and  save the results in LOG_NAME
>
> **EXAMPLE**
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;python tool/karonte.py config/NETGEAR/r_7800.json
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It runs karonte on the R7800 NETGEAR firmware

By default, results are saved in **/tmp/** with the suffix **Karonte.txt**.

To inspect the generated alerts, just run:
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; python **tool/pretty_print.py** LOG_NAME

## Dataset
You can obtain the dataset that we used to evaluate Karonte at this [link](https://drive.google.com/file/d/1-VOf-tEpu4LIgyDyZr7bBZCDK-K2DHaj/view?usp=sharing).

","['badnack', 'conand']",1,,0.69,0,,,,,,22,,,
100426946,MDEwOlJlcG9zaXRvcnkxMDA0MjY5NDY=,BootStomp,ucsb-seclab/BootStomp,0,ucsb-seclab,https://github.com/ucsb-seclab/BootStomp,BootStomp: a bootloader vulnerability finder,0,2017-08-15 23:18:16+00:00,2025-03-01 15:54:48+00:00,2022-01-10 09:29:30+00:00,https://seclab.cs.ucsb.edu/academic/publishing/#bootstomp-security-bootloaders-mobile-devices-2017,3758,391,391,Python,1,1,1,1,0,0,69,0,0,2,bsd-2-clause,1,0,0,public,69,2,391,master,1,1,"BootStomp
===================

[![License](https://img.shields.io/github/license/angr/angr.svg)](https://github.com/ucsb-seclab/BootStomp/blob/master/LICENSE)

BootStomp is a boot-loader bug finder. It looks for two different class of bugs: memory corruption and state storage vulnerabilities. For more info please refer to the BootStomp paper at https://seclab.cs.ucsb.edu/academic/publishing/#bootstomp-security-bootloaders-mobile-devices-2017 

To run BootStomp's analyses, please read the following instructions. Note that BootStomp works with boot-loaders compiled for ARM architectures (32 and 64 bits both) and that results might slightly vary depending on angr and Z3's versions. This is because of the time angr takes to analyze basic blocks and to Z3's expression concretization results.


----------

Directory structure
--
* **analysis**: Contains analysis results (Ex: IDA idbs etc)  of boot images of different devices.
* **tools**: Contains tools that can be used to work with various images.

Pre-requisites
--

* angr (http://angr.io/)
>$ pip install angr

* IDA PRO (https://www.hex-rays.com/products/ida/)
* IDA Decompiler (https://www.hex-rays.com/products/decompiler/)

How to run it
--
## Run BootStomp using docker
The easiest way to use BootStomp is to run it in a docker container.
The folder `docker` contains an appropriate `Dockerfile`.
These are the commands to use it.
```bash
cd docker
# build the docker image
docker build -t bootstomp .
# run the docker image (if you need, use proper options to have persistent changes or shared files)
docker run -it bootstomp

# now you are inside a docker container
cd BootStomp
# run BootStomp's taint analysis on one of the examples
# this will take about 30 minutes
python taint_analysis/bootloadertaint.py config/config.huawei
# the last line of the output will be something like:
# INFO    | 2017-10-14 01:54:10,617 | _CoreTaint | Results in /tmp/BootloaderTaint_fastboot.img_.out

# you can then ""pretty print"" the results using:
python taint_analysis/result_pretty_print.py /tmp/BootloaderTaint_fastboot.img_.out
```
The output should be something like this:
```
...
17)
===================== Start Info path =====================
Dereference address at: 0x5319cL
Reason: at location 0x5319cL a tainted variable is dereferenced and used as address.
...
Tainted Path 
----------------
0x52f3cL -> 0x52f78L -> 0x52f8cL -> 0x52fb8L -> 0x52fc8L -> 0x52fecL -> 0x53000L -> 0x53014L -> 0x5301cL -> 0x53030L -> 0x53044L -> 0x53050L -> 0x5305cL -> 0x53068L
===================== End Info path =====================
# Total sinks related alerts: 5
# Total loop related alerts: 8
# Total dereference related alerts: 4
```

## Run BootStomp manually
### Automatic detection of taint sources and sinks

1. Load the boot-loader binary in IDA (we used v6.95). Depending on the CPU architecture of the phone it has been extracted from, 32 bit or 64 bit IDA is needed. 
2. From the menu-bar, run File => Script file => `find_taint.py`
3. Output will appear in the file `taint_source_sink.txt` under the same directory as the boot-loader itself.

### Configuration file
Create a JSON configuration file for the boot-loader binary (see examples in `config/`), where:

* **bootloader**: boot-loader file path
* **info_path**: boot-loader source/sink info file path  (i.e., taint_source_sink.txt )
* **arch**: architecture's number of bits (available options are 32 and 64)
* **enable_thumb**: consider thumb mode (when needed) during the analysis 
* **start_with_thumb**: starts the analysis with thumb mode enabled  
* **exit_on_dec_error**: stop the analysis if some instructions cannot be decoded
* **unlock_addr**: unlocking function address. This field is necessary only for finding insecure state storage vulnerabilities.

### Finding memory corruption vulnerabilities
Run

 > python bootloadertaint.py config-file-path
 
 Results will be stored in `/tmp/BootloaderTaint_[boot-loader].out`, where `[boot-loader]` is the name of the analyzed boot-loader. Note that paths involving loops might appear more than once.

### Finding insecure state storage vulnerability
Run
 > python unlock_checker.py config-file-path

 Results will be stored in `/tmp/UnlockChecker_[boot-loader].out`, where `[boot-loader]` is the name of the analyzed boot-loader. Note that paths involving loops might appear more than once.

### Checking results
To check BootStomp results, use the script `result_pretty_print.py`, as follows:
 > python result_pretty_print.py results_file

#### [Exploit for CVE-2017-2729](https://github.com/ucsb-seclab/BootStomp/tree/master/tools/huawei_tools#oeminfo_exploitpy)

Other references
-------------
* [Kernel and lk source for MediaTek MT65x2](https://github.com/ariafan/MT65x2_kernel_lk)
* [MediaTek details: Partitions and Preloader](https://sturmflut.github.io/mediatek/2015/07/04/mediatek-details-partitions-and-preloader)
* [Reverse Engineering Android's Aboot](http://newandroidbook.com/Articles/aboot.html)
* [(L)ittle (K)ernel based Android bootloader](https://www.codeaurora.org/blogs/little-kernel-based-android-bootloader)
* [Little Kernel Boot Loader Overview by Qualcomm](https://developer.qualcomm.com/qfile/28821/lm80-p0436-1_little_kernel_boot_loader_overview.pdf)
* [android: arm: bootloader: how (L)ittle (K)ernel loads boot.img](https://chengyihe.wordpress.com/2015/09/22/android-arm-bootloader-how-little-kernel-loads-boot-img)
* [BootUnlocker for Nexus Devices](https://github.com/osm0sis/boot-unlocker/blob/wiki/HowItWorks.md)
* [Verifying Boot](https://source.android.com/security/verifiedboot/verified-boot.html)
* [Freeing my tablet (Android hacking, SW and HW)](https://www.thanassis.space/android.html)
* [How to lock the samsung download mode using an undocumented feature of aboot](https://ge0n0sis.github.io/posts/2016/05/how-to-lock-the-samsung-download-mode-using-an-undocumented-feature-of-aboot/)
* [BIOS and Secure Boot Attacks Uncovered](http://www.intelsecurity.com/resources/pr-bios-secure-boot-attacks-uncovered.pdf)
* [Apple IOS Security](https://www.apple.com/business/docs/iOS_Security_Guide.pdf)
* [Debugging HTC phone boot-laoders](http://archive.hack.lu/2013/hacklu2013_hbootdbg.pdf)
* [Debugger for HBOOT](https://github.com/sogeti-esec-lab/hbootdbg)
* [Analysing HBOOT](http://tjworld.net/wiki/android/htc/vision/hbootanalysis)

 
","['badnack', 'Machiry', 'antoniobianchi333', 'balika011']",1,,0.77,0,,,,,,36,,,
200720411,MDEwOlJlcG9zaXRvcnkyMDA3MjA0MTE=,TimeSeries,zhykoties/TimeSeries,0,zhykoties,https://github.com/zhykoties/TimeSeries,Implementation of deep learning models for time series in PyTorch.,0,2019-08-05 19:59:24+00:00,2025-02-26 07:04:58+00:00,2020-04-03 21:10:07+00:00,,1344,385,385,Python,1,1,1,1,1,0,99,0,0,2,apache-2.0,1,0,0,public,99,2,385,master,1,,"# List of Implementations:
Currently, the reimplementation of the DeepAR paper(DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks https://arxiv.org/abs/1704.04110) is available in PyTorch. More papers will be coming soon.

## Authors:
* **Yunkai Zhang**(<yunkai_zhang@ucsb.edu>) - *University of California, Santa Barbara* 

* **Qiao Jiang** - *Brown University*

* **Xueying Ma** - *Columbia University*
* Acknowledgement: Professor Xifeng Yan's group at UC Santa Barbara. Part of the work was done at WeWork.

## To run:


1. Install all dependencies listed in requirements.txt. Note that the model has only been tested in the versions shown in the text file.

1. Download the dataset and preprocess the data:
  
   ```bash
   python preprocess_elect.py
   ```
1. Start training:
  
   ```bash
   python train.py
   ```
   
   - If you want to perform ancestral sampling,
   
        ```bash
        python train.py --sampling
        ```
   - If you do not want to do normalization during evaluation,
              
   
        ```bash
        python train.py --relative-metrics
        ```
1. Evaluate a set of saved model weights:
        
   ```bash
   python evaluate.py
   ```
1. Perform hyperparameter search:
        
   ```bash
    python search_params.py
   ```

## Results

​        The model is evaluated on the electricity dataset, which contains the electricity consumption of 370 households from 2011 to 2014. Under hourly frequency, we use the first week of September, 2014 as the test set and all time steps prior to that as the train set. Following the experiment design in DeepAR, the window size is chosen to be 192, where the last 24 is the forecasting horizon. History (number of time steps since the beginning of each household), month of the year, day of the week, and hour of the day are used as time covariates. Notice that some households started at different times, so we only use windows that contain non-missing values.

​        Under Gaussian likelihood, we use the Adam optimizer with early stopping to train the model for 20 epoches. The same set of hyperparameters is used as outlined in the paper. Weights with the best ND value is selected, where __ND = 0.06349__, RMSE = 0.452, rou90 = 0.034 and rou50 = 0.063.

​        Sample results on electricity. The top 10 plots are sampled from the test set with the highest 10% ND values, whereas the bottom 10 plots are sampled from the rest of the test set.

![Sample results on electricity. The top 10 plots are sampled from the test set with the highest 10% ND values, whereas the bottom 10 plots are sampled from the rest of the test set.](./experiments/base_model/figures/best_ND.png)

",['zhykoties'],1,,0.63,0,,,,,,14,,,
100532855,MDEwOlJlcG9zaXRvcnkxMDA1MzI4NTU=,difuze,ucsb-seclab/difuze,0,ucsb-seclab,https://github.com/ucsb-seclab/difuze,Fuzzer for Linux Kernel Drivers,0,2017-08-16 21:09:21+00:00,2025-02-24 02:17:09+00:00,2022-04-30 02:33:52+00:00,,2651,376,376,C++,1,1,1,1,0,0,84,0,0,7,bsd-2-clause,1,0,0,public,84,7,376,master,1,1,"difuze: Fuzzer for Linux Kernel Drivers
===================

[![License](https://img.shields.io/github/license/angr/angr.svg)](https://github.com/ucsb-seclab/difuze/blob/master/LICENSE)

This repo contains all the sources (including setup scripts), you need to get `difuze` up and running.
### Tested on
Ubuntu >= 14.04.5 LTS

### 0. Running difuze from Docker
Refer the [readme](https://github.com/ucsb-seclab/difuze/blob/master/DOCKER.md)

As explained in our [paper](https://acmccs.github.io/papers/p2123-corinaA.pdf), There are two main components of `difuze`: **Interface Recovery** and **Fuzzing Engine**

## 1. Interface Recovery
The Interface recovery mechanism is based on LLVM analysis passes. Every step of interface recovery are written as  individual passes. Follow the below instructions on how to get the *Interface Recovery* up and running.

### 1.1 Setup
This step takes care of installing LLVM and `c2xml`:

First, make sure that you have libxml (required for c2xml):
```
sudo apt-get install libxml2-dev
sudo pip install lxml
```

Next, We have created a single script, which downloads and builds all the required tools.
```
cd helper_scripts
python setup_difuze.py --help
usage: setup_difuze.py [-h] [-b TARGET_BRANCH] [-o OUTPUT_FOLDER]

optional arguments:
  -h, --help        show this help message and exit
  -b TARGET_BRANCH  Branch (i.e. version) of the LLVM to setup. Default:
                    release_38 e.g., release_38
  -o OUTPUT_FOLDER  Folder where everything needs to be setup.

```
Example:
```
python setup_difuze.py -o difuze_deps
```
To complete the setup you also need modifications to your local `PATH` environment variable. The setup script will give you exact changes you need to do.
### 1.2 Building
This depends on the successful completion of [Setup](#11-setup).
We have a single script that builds everything, you are welcome.
```
cd InterfaceHandlers
./build.sh
```
### 1.3 Running
This depends on the successful completion of [Build](#12-building).
To run the Interface Recovery components on kernel drivers, we need to first the drivers into llvm bitcode.
#### 1.3.1 Building kernel
First, we need to have a buildable kernel. Which means you should be able to compile the kernel using regular build setup. i.e., `make`.
We first capture the output of `make` command, from this output we extract the exact compilation command.
##### 1.3.1.1 Generating output of `make`
###### Option 1: Using Bear (RECOMMENDED)
1. Install [Bear](https://github.com/Machiry/Bear)
2. Run make using Bear:
    ```
    bear make <all the options to make>
    ```
    Example: `bear make -j8`

This will generate a file `compile_commands.json` in the current directory.

###### Option 2
Just pass `V=1` and redirect the output to the file.
Example:
```
make V=1 O=out ARCH=arm64 > makeout.txt 2>&1
```
**NOTE: DO NOT USE MULTIPLE PROCESSES** i.e., `-j`. Running in multi-processing mode will mess up the output file as multiple process try to write to the output file.

That's it. Next, in the following step our script takes the generated `makeout.txt` and run the Interface Recovery on all the recognized drivers.

#### 1.3.2 Running Interface Recovery analysis
All the various steps of Interface Recovery are wrapped in a single script `helper_scripts/run_all.py`
How to run:
```
cd helper_scripts
python run_all.py --help

usage: run_all.py [-h] [-l LLVM_BC_OUT] [-a CHIPSET_NUM] [-m MAKEOUT]
                  [-c COMPJSON] [-g COMPILER_NAME] [-n ARCH_NUM] [-o OUT]
                  [-k KERNEL_SRC_DIR] [-isclang] [-clangp CLANG_PATH]
                  [-llvmlinkp LLVMLINK_PATH] [-skb] [-skl] [-skp] [-skP]
                  [-ske] [-skI] [-ski] [-skv] [-skd] [-f IOCTL_FINDER_OUT]

optional arguments:
  -h, --help            show this help message and exit
  -l LLVM_BC_OUT        Destination directory where all the generated bitcode
                        files should be stored.
  -a CHIPSET_NUM        Chipset number. Valid chipset numbers are:
                        1(mediatek)|2(qualcomm)|3(huawei)|4(samsung)
  -m MAKEOUT            Path to the makeout.txt file.
  -c COMPJSON           Path to the compile_commands_json generated by Bear.
  -g COMPILER_NAME      Name of the compiler used in the makeout.txt, This is
                        needed to filter out compilation commands. Ex: aarch64
                        -linux-android-gcc
  -n ARCH_NUM           Destination architecture, 32 bit (1) or 64 bit (2).
  -o OUT                Path to the out folder. This is the folder, which
                        could be used as output directory during compiling
                        some kernels.
  -k KERNEL_SRC_DIR     Base directory of the kernel sources.
  -isclang              flag to indicate that clang was used to built the
                        kernel
  -clangp CLANG_PATH    Absolute path to the clang binary (if not provided,
                        the one available in the path will be used)
  -llvmlinkp LLVMLINK_PATH
                        Absolute path to the llvm-link binary (if not
                        provided, the one available in the path will be used)
  -skb                  Skip LLVM Build (default: not skipped).
  -skl                  Skip Dr Linker (default: not skipped).
  -skp                  Skip Parsing Headers (default: not skipped).
  -skP                  Skip Generating Preprocessed files (default: not
                        skipped).
  -ske                  Skip Entry point identification (default: not
                        skipped).
  -skI                  Skip Generate Includes (default: not skipped).
  -ski                  Skip IoctlCmdParser run (default: not skipped).
  -skv                  Skip V4L2 ioctl processing (default: not skipped).
  -skd                  Skip Device name finder (default: not skipped).
  -f IOCTL_FINDER_OUT   Path to the output folder where the ioctl command
                        finder output should be stored.


```
The script builds, links and runs Interface Recovery on all the recognized drivers, as such it might take **considerable time(45 min-90 min)**. 

The above script performs following tasks in a multiprocessor mode to make use of all CPU cores:
##### 1.3.2.1 LLVM Build 
* Enabled by default.

All the bitcode files generated will be placed in the folder provided to the argument `-l`.
This step takes considerable time, depending on the number of cores you have. 
So, if you had already done this step, You can skip this step by passing `-skb`. 
##### 1.3.2.2 Linking all driver bitcode files in s consolidated bitcode file.
* Enabled by default

This performs linking, it goes through all the bitcode files and identifies the related bitcode files that need to be linked and links them (using `llvm-link`) in to a consolidated bitcode file (which will be stored along side corresponding bitcode file).

Similar to the above step, you can skip this step by passing `-skl`.
##### 1.3.2.3 Parsing headers to identify entry function fields.
* Enabled by default.

This step looks for the entry point declarations in the header files and stores their configuration in the file: `hdr_file_config.txt` under LLVM build directory.

To skip: `-skp`
##### 1.3.2.4 Identify entry points in all the consolidated bitcode files.
* Enabled by default

This step identifies all the entry points across all the driver consolidated bitcode files.
The output will be stored in file: `entry_point_out.txt` under LLVM build directory.

Example of contents in the file `entry_point_out.txt`:
```
IOCTL:msm_lsm_ioctl:/home/difuze/kernels/pixel/msm/sound/soc/msm/qdsp6v2/msm-lsm-client.c:msm_lsm_ioctl.txt:/home/difuze/pixel/llvm_out/sound/soc/msm/qdsp6v2/llvm_link_final/final_to_check.bc
IOCTL:msm_pcm_ioctl:/home/difuze/kernels/pixel/msm/sound/soc/msm/qdsp6v2/msm-pcm-lpa-v2.c:msm_pcm_ioctl.txt:/home/difuze/pixel/llvm_out/sound/soc/msm/qdsp6v2/llvm_link_final/final_to_check.bc

```
To skip: `-ske`
##### 1.3.2.5 Run Ioctl Cmd Finder on all the identified entry points.
* Enabled by default.

This step will run the main Interface Recovery component (`IoctlCmdParser`) on all the entry points in the file `entry_point_out.txt`. The output for each entry point will be stored in the folder provided for option `-f`.

To skip: `-ski`

### 1.4 Example:
Now, we will show an example from the point where you have kernel sources to the point of getting Interface Recovery results.

We have uploaded a mediatek kernel [33.2.A.3.123.tar.bz2](https://drive.google.com/file/d/0B4XwT5D6qkNmLXdNTk93MjU3SWM/view?usp=sharing&resourcekey=0-cb1ceDme5Fi2xugS7nYl7w). 
First download and extract the above file.

Lets say you extracted the above file in a folder called: `~/mediatek_kernel`

#### 1.4.1 Building
Install [Bear](https://github.com/Machiry/Bear) and follow the below steps:
```
cd ~/mediatek_kernel
source ./env.sh
cd kernel-3.18
# the following step may not be needed depending on the kernel
mkdir out
make O=out ARCH=arm64 tubads_defconfig
# generating compile_commands.json
bear make -j8 O=out ARCH=arm64
```
#### 1.4.2 Running Interface Recovery
```
cd <repo_path>/helper_scripts

python run_all.py -l ~/mediatek_kernel/llvm_bitcode_out -a 1 -c ~/mediatek_kernel/kernel-3.18/compile_commands.json -n 2 -o ~/mediatek_kernel/kernel-3.18/out -k ~/mediatek_kernel/kernel-3.18 -f ~/mediatek_kernel/ioctl_finder_out
```
The above command takes quite **some time (30 min - 1hr)**.

#### 1.4.3 Understanding the output
First, all the analysis results will be in the folder: **`~/mediatek_kernel/ioctl_finder_out` (argument given to the option `-f`)**, for each entry point a `.txt` file will be created, which contains all the information about the recovered interface.

If you are interested in information about just the interface and don't care about anything else, We recommend you use the [`parse_interface_output.py`](https://github.com/ucsb-seclab/difuze/blob/master/helper_scripts/parse_interface_output.py) script. This script converts the crazy output of Interface Recovery pass into nice json files with a clean and consistent format.

```
cd <repo_path>/helper_scripts
python parse_interface_output.py <ioctl_finder_out_dir> <output_directory_for_json_files>
```
Here `<ioctl_finder_out_dir>` should be same as the folder you provided to the `-f` option and `<output_directory_for_json_files>` is the folder where the json files should be created.

You can use the corresponding json files for the interface recovery of the corresponding ioctl.

#### 1.4.4 Things to note:
##### 1.4.4.1 Value for option `-g` (only if you use makeout.txt)
To provide value for option `-g` you need to know the name of the `*-gcc` binary used to compile the kernel.
An easy way to know this would be to `grep` for `gcc` in `makeout.txt` and you will see compiler commands from which you can know the `*-gcc` binary name.

For our example above, if you do `grep gcc makeout.txt` for the example build, you will see lot of lines like below:
```
aarch64-linux-android-gcc -Wp,-MD,fs/jbd2/.transaction.o.d  -nostdinc -isystem ...
```
So, the value for `-g` should be `aarch64-linux-android-gcc`. 

If the kernel to be built is 32-bit then the binary most likely will be `arm-eabi-gcc`

For Qualcomm (or msm) chipsets, you may see `*gcc-wrapper.py` instead of `*.gcc`, in which case you should provide the `*gcc-wrapper.py`.

##### 1.4.4.2 Value for option `-a`
Depeding on the chipset type, you need to provide corresponding number.

##### 1.4.4.3 Value for option `-o`
This is the path of the folder provided to the option `O=` for `make` command during kernel build.

Not all kernels need a separate out path. You may build kernel by not providing an option `O`, in which case you SHOULD NOT provide value for that option while running `run_all.py`.


### Kernels built using clang
For kernels built using `clang`, in addition to the above options please specify the following options (assuming you used `compile_commands.json`):
```
-isclang -clangp <PATH_TO_THE_CLANG_USED_TO_BUILD_THE_KERNEL> -llvmlinkp <PATH_TO_THE_LLVM_LINK (will be in the same folder as clang)>
```
### 1.5 Post Processing
Before we can begin fuzzing we need to process the output a bit with our very much research quality (sorry) parsers.

These are found [here](helper_scripts/post_processing). The main script to run will be `run_all.py`:
```
$ python run_all.py --help
usage: run_all.py [-h] -f F -o O [-n {manual,auto,hybrid}] [-m M]

run_all options

optional arguments:
  -h, --help            show this help message and exit
  -f F                  Filename of the ioctl analysis output OR the entire
                        output directory created by the system
  -o O                  Output directory to store the results. If this
                        directory does not exist it will be created
  -n {manual,auto,hybrid}
                        Specify devname options. You can choose manual
                        (specify every name manually), auto (skip anything that
                        we don't identify a name for), or hybrid (if we
                        detected a name, we use it, else we ask the user)
  -m M                  Enable multi-device output most ioctls only have one
                        applicable device node, but some may have multiple. (0
                        to disable)
```
You'll want to pass `-f` the output directory of the ioctl analysis e.g. `~/mediatek_kernel/ioctl_finder_out`.

`-o` Is where you where to store the post-processed results. These will be easily digestible XML files (jpits).

`-n` Specifies the system to what degree you want to rely on our device name recovery.
If you don't want to do any work/name hunting, you can specify `auto`.
This of course comes at the cost of skipping any device for which we don't recover a name. If you want to be paranoid and not trust any of our recovery efforts (totally reasonable) you can use the `manual` option to name every single device yourself.
`hybrid` then is a combination of both -- we will name the device for you when we can, and fall back to you when we've failed.

`-m` Sometimes ioctls can correspond to more than one device (this is common with v4l2/subdev ioctls for example). Support for this in enabled by default, but it requires user interaction to specify the numberof devices for each device. If this is too annoying for you, you can disable the prompt by passing `-m 0` (we will assume a single device for each ioctl).

After running, you should have, in your out folder, a folder for each ioctl.

## 2 Fuzzing

### 2.1 Mango Fuzz
MangoFuzz is our simple prototype fuzzer and is based off of Peach (specifically [MozPeach](https://github.com/MozillaSecurity/peach)).

It's not a particularly sophisticated fuzzer but it does find bugs.
It was also built to be easily expandable.
There are 2 components to this fuzzer, the fuzz engine and the executor.
The executor can be found [here](MangoFuzz/executor), and the fuzz engine can be found [here](MangoFuzz/fuzzer).

#### 2.1.1 Executor
The executor runs on the phone, listening for data that the fuzz engine will send to it.

Simply compile it for your phones architecture, `adb push` it on to the phone, and execute with the port you want it to listen on!

#### 2.1.2 Fuzz Engine
Interfacing with MangoFuzz is fairly simple. You'll want an `Engine` object and a `Parser` object, which you'll feed your engine into.
From here, you parse jpits with your Parser, and then run the Engine. Easy!
We've provided some simple run scripts to get you started.

To run against specific drivers you can use `runner.py` on one of the ioctl folders in the output directory (created by our post processing scripts).

e.g. `./runner.py -f honor8/out/chb -num 1000`. This tells MangoFuzz to run for 1000 iterations against all ioctl command value pairs pertaining to the `chb` ioctl/driver.

If instead we want to run against an entire device (phone), you can use `dev_runner.py`. e.g. `./dev_runner.py -f honor8/out -num 100`.
This will continue looping over the driver files, randomly switching between them for 100 iterations each.

Note that before the fuzz engine can communicate with the phone, you'll need to use ADB to set up port forwarding e.g. `adb forward tcp:2022 tcp:2022`
","['Machiry', 'jcorina', 'zardus', 'teesec']",1,,0.69,0,,,,,,31,,,
169170694,MDEwOlJlcG9zaXRvcnkxNjkxNzA2OTQ=,papers,LyleMi/papers,0,LyleMi,https://github.com/LyleMi/papers,"Academic papers and articles that I read related to web hacking, fuzzing, etc. / 阅读过的Web安全方向、模糊测试方向的一些论文与阅读笔记",0,2019-02-05 00:25:05+00:00,2025-03-02 06:47:57+00:00,2024-01-26 00:23:35+00:00,,1703415,370,370,Python,1,1,1,1,0,1,30,0,0,0,,1,0,0,public,30,0,370,master,1,,,['LyleMi'],0,,0.67,0,,,,,,11,,,
594721524,R_kgDOI3K69A,ML_DL_CV_with_pytorch,Michael-Jetson/ML_DL_CV_with_pytorch,0,Michael-Jetson,https://github.com/Michael-Jetson/ML_DL_CV_with_pytorch,一个计算机视觉、机器学习与深度学习相关的项目，看课程的笔记还有自己做的程序,0,2023-01-29 12:51:24+00:00,2025-03-04 05:46:29+00:00,2025-02-27 16:44:44+00:00,,2793471,362,362,Python,1,1,1,1,0,0,25,0,0,1,,1,0,0,public,25,1,362,main,1,,"# 概述

这是我们进行机器学习和深度学习（计算机视觉方向）的一个仓库，用来保存笔记、代码和其他文件，方便记录学习进程

文档结构如下

```shell
.
├── AutoDriving
├── CUDA
├── ComputerGraphic
├── ComputerVision
├── BEV
├── MultiModal
├── git_img
├── Tools
├── img_cls
├── pytorch_image_classification
├── GAMES101Notes
└── scripts
```

- AutoDriving：自动驾驶方面的笔记，目前主要是记录Autoware软件用法
- CUDA：CUDA并行计算笔记
- ComputerGraph：计算机图形学笔记，主要是参考Games101，未更完
- ComputerVision：一个很综合的深度学习与计算机视觉笔记，主体是学习EECS498并且大佬开源笔记上进行修改的，主要参考密歇根大学EECS498（CS231N课程的扩充版）
- BEV：更新自动驾驶BEV感知方面的内容
- MultiModel：参考CMU的MMML课程，没有更新完
- git_img是git笔记中图片保存的位置
- Tools：工具性软件的笔记，目前主要是更新CUDA和TensorRT的内容
- pytorch_image_classification是一个基于pytorch的图像分类数据集
- img_cls是一个使用不同网络进行图像分类的简单项目，个人感觉这是一个很标准的小项目
- GAMES101Notes是UCSB闫令琪主讲的现代计算机图形学入门课程的笔记

参考书：周志华《机器学习》，李航《统计学习方法》

参考资料：

- 密歇根大学公开课EECS498（主讲人也是是CS231n的主讲人Justin博士）
- [理解和利用CNN的内部表征 TechBeat 周博磊](https://www.techbeat.net/talk-info?id=180)
- [点云上的深度学习及其在三维场景理解中的应用](https://www.techbeat.net/talk-info?id=254)
- 其他公开课

## 2023.8.8更新情况

目前主要是更新计算机视觉和深度学习的相关笔记，在另一位大佬的开源上进行增加我们自己的思考和感悟

我们在这个笔记上投入了大量精力，希望这个笔记可以帮助到其他人，后面我们会加入大量相关的内容

笔记在同名文件夹下，格式为markdown，可使用typora打开，记得在里面使用LaTeX进行渲染，否则个别公式无法正常显示

此外还有git的笔记（整合了B站几个教程，几个同学一起做的），笔记中的图片在git_img文件夹下，还有另一个同学的Python自学笔记

# 2024.6月更新及后期计划

- 学习NeRF相关并且记录笔记（约3-5万字）
- 学习3DGS相关并且记录笔记（约1-2万字）
- 学习CS236（斯坦福最新的深度生成模型课程），并且记录笔记约4-6万字
- 对EEECS498的内容进行精简和优化，删除原始笔记中的部分内容（如删除numpy的代码，更新为pytorch代码等等）
","['Michael-Jetson', 'gaozhu99', 'yulongC', 'keedled', 'Wangben1019', 'introspectDaily']",0,,0.57,0,,,,,,4,,,
92881927,MDEwOlJlcG9zaXRvcnk5Mjg4MTkyNw==,dr_checker,ucsb-seclab/dr_checker,0,ucsb-seclab,https://github.com/ucsb-seclab/dr_checker,DR.CHECKER : A Soundy Vulnerability Detection Tool for Linux Kernel Drivers,0,2017-05-30 22:34:02+00:00,2025-02-18 06:51:24+00:00,2022-04-30 02:34:23+00:00,,1024,334,334,C++,1,1,1,1,1,0,71,0,0,12,bsd-2-clause,1,0,0,public,71,12,334,speedy,1,1,"DR.CHECKER : A Soundy Vulnerability Detection Tool for Linux Kernel Drivers
===================

[![License](https://img.shields.io/github/license/angr/angr.svg)](https://github.com/ucsb-seclab/dr_checker/blob/master/LICENSE)

![warning](https://raw.githubusercontent.com/ucsb-seclab/dr_checker/speedy/images/warning.png)

This repo contains all the sources, including setup scripts. 
Now with an Amazing UI to view the warnings along with corresponding source files.
### Tested on
Ubuntu >= 14.04.5 LTS
### Announcements
**16 Feb 2018**:
* DR.CHECKER has been dockerized. Refer [Docker Usage](https://github.com/ucsb-seclab/dr_checker/blob/speedy/docs/docker.md) on how to use it.

## [Frequently Asked Questions](https://github.com/ucsb-seclab/dr_checker/blob/master/docs/faq.md)

## 0. Using Dockerized Setup (Recommended)
Refer the [Docker Usage](https://github.com/ucsb-seclab/dr_checker/blob/speedy/docs/docker.md) document for details on how to use DR.CHECKER in a pre-built docker container.

## 1. Setup
Our implementation is based on LLVM, specifically LLVM 3.8. We also need tools like `c2xml` to parse headers.

First, make sure that you have cmake (used by setup/build scripts) and libxml (required for c2xml):
```
sudo apt-get install cmake libxml2-dev
```

Next, We have created a single script, which downloads and builds all the required tools.
```
cd helper_scripts
python setup_drchecker.py --help
usage: setup_drchecker.py [-h] [-b TARGET_BRANCH] [-o OUTPUT_FOLDER]

optional arguments:
  -h, --help        show this help message and exit
  -b TARGET_BRANCH  Branch (i.e. version) of the LLVM to setup. Default:
                    release_38 e.g., release_38
  -o OUTPUT_FOLDER  Folder where everything needs to be setup.

```
Example:
```
python setup_drchecker.py -o drchecker_deps
```
To complete the setup you also need modifications to your local `PATH` environment variable. The setup script will give you exact changes you need to do.
## 2. Building
This depends on the successful completion of [Setup](#markdown-header-setup).
We have a single script that builds everything, you are welcome.
```
cd llvm_analysis
./build.sh
```
## 3. Running
This depends on the successful completion of [Build](#markdown-header-building).
To run DR.CHECKER on kernel drivers, we need to first convert them into llvm bitcode.
### 3.1 Building kernel
First, we need to have a buildable kernel. Which means you should be able to compile the kernel using regular build setup. i.e., `make`.
We first capture the output of `make` command, from this output we extract the exact compilation command.
#### 3.1.1 Generating output of `make` (or `makeout.txt`)

Just pass `V=1` and redirect the output to the file.
Example:
```
make V=1 O=out ARCH=arm64 > makeout.txt 2>&1
```
NOTE: DO NOT USE MULTIPLE PROCESSES i.e., `-j`. Running in multi-processing mode will mess up the output file as multiple process try to write to the output file.

That's it. DR.CHECKER will take care from here.
### 3.2 Running DR.CHECKER analysis
There are several steps to run DR.CHECKER analysis, all these steps are wrapped in a single script `helper_scripts/runner_scripts/run_all.py`
How to run:
```
python run_all.py --help
usage: run_all.py [-h] [-l LLVM_BC_OUT] [-a CHIPSET_NUM] [-m MAKEOUT] [-g COMPILER_NAME] [-n ARCH_NUM] [-o OUT] [-k KERNEL_SRC_DIR] [-skb] [-skl] [-skp] [-ske] [-ski] [-f SOUNDY_ANALYSIS_OUT]

optional arguments:
  -h, --help            show this help message and exit
  -l LLVM_BC_OUT        Destination directory where all the generated bitcode files should be stored.
  -a CHIPSET_NUM        Chipset number. Valid chipset numbers are:
                        1(mediatek)|2(qualcomm)|3(huawei)|4(samsung)
  -m MAKEOUT            Path to the makeout.txt file.
  -g COMPILER_NAME      Name of the compiler used in the makeout.txt, This is
                        needed to filter out compilation commands. Ex: aarch64-linux-android-gcc
  -n ARCH_NUM           Destination architecture, 32 bit (1) or 64 bit (2).
  -o OUT                Path to the out folder. This is the folder, which
                        could be used as output directory during compiling
                        some kernels. (Note: Not all kernels needs a separate out folder)
  -k KERNEL_SRC_DIR     Base directory of the kernel sources.
  -skb                  Skip LLVM Build (default: not skipped).
  -skl                  Skip Dr Linker (default: not skipped).
  -skp                  Skip Parsing Headers (default: not skipped).
  -ske                  Skip Entry point identification (default: not
                        skipped).
  -ski                  Skip Soundy Analysis (default: not skipped).
  -f SOUNDY_ANALYSIS_OUT    Path to the output folder where the soundy analysis output should be stored.

```
The script builds, links and runs DR.CHECKER on all the drivers, as such might take **considerable time(45 min-90 min)**. If you want to run DR.CHECKER manually on individual drivers, refer [standalone](https://github.com/ucsb-seclab/dr_checker/tree/master/docs/standalone.md)

The above script performs following tasks in a multiprocessor mode to make use of all CPU cores:
#### 3.2.1. LLVM Build 
* Enabled by default.

All the bitcode files generated will be placed in the folder provided to the argument `-l`.
This step takes considerable time, depending on the number of cores you have. 
So, if you had already done this step, You can skip this step by passing `-skb`. 
#### 3.2.2. Linking all driver bitcode files in s consolidated bitcode file.
* Enabled by default

This performs linking, it goes through all the bitcode files and identifies the related bitcode files that need to be linked and links them (using `llvm-link`) in to a consolidated bitcode file (which will be stored along side corresponding bitcode file).

Similar to the above step, you can skip this step by passing `-skl`.
#### 3.2.3.Parsing headers to identify entry function fields.
* Enabled by default.

This step looks for the entry point declarations in the header files and stores their configuration in the file: `hdr_file_config.txt` under LLVM build directory.

To skip: `-skp`
#### 3.2.4.Identify entry points in all the consolidated bitcode files.
* Enabled by default

This step identifies all the entry points across all the driver consolidated bitcode files.
The output will be stored in file: `entry_point_out.txt` under LLVM build directory.

Example of contents in the file `entry_point_out.txt`:
```
FileRead:hidraw_read:/home/drchecker/33.2.A.3.123/llvm_bc_out/drivers/hid/llvm_link_final/final_to_check.bc
FileWrite:hidraw_write:/home/drchecker/33.2.A.3.123/llvm_bc_out/drivers/hid/llvm_link_final/final_to_check.bc
IOCTL:hidraw_ioctl:/home/drchecker/33.2.A.3.123/llvm_bc_out/drivers/hid/llvm_link_final/final_to_check.bc
```
To skip: `-ske`
#### 3.2.5.Run Soundy Analysis on all the identified entry points.
* Enabled by default.

This step will run DR.CHECKER on all the entry points in the file `entry_point_out.txt`. The output for each entry point will be stored in the folder provided for option `-f`.

To skip: `-ski`
#### 3.2.6 Example:
Now, we will show an example from the point where you have kernel sources to the point of getting vulnerability warnings.

We have uploaded a mediatek kernel [33.2.A.3.123.tar.bz2](https://drive.google.com/file/d/0B4XwT5D6qkNmLXdNTk93MjU3SWM/view?usp=sharing&resourcekey=0-cb1ceDme5Fi2xugS7nYl7w). 
First download and extract the above file.

Lets say you extracted the above file in a folder called: `~/mediatek_kernel`

##### 3.2.6.1 Building
```
cd ~/mediatek_kernel
source ./env.sh
cd kernel-3.18
# the following step may not be needed depending on the kernel
mkdir out
make O=out ARCH=arm64 tubads_defconfig
# this following command copies all the compilation commands to makeout.txt
make V=1 -j8 O=out ARCH=arm64 > makeout.txt 2>&1
```
##### 3.2.6.2 Running DR.CHECKER
```
cd <repo_path>/helper_scripts/runner_scripts

python run_all.py -l ~/mediatek_kernel/llvm_bitcode_out -a 1 -m ~/mediatek_kernel/kernel-3.18/makeout.txt -g aarch64-linux-android-gcc -n 2 -o ~/mediatek_kernel/kernel-3.18/out -k ~/mediatek_kernel/kernel-3.18 -f ~/mediatek_kernel/dr_checker_out
```
The above command takes quite **some time (30 min - 1hr)**.
##### 3.2.6.3 Understanding the output
First, all the analysis results will be in the folder: **`~/mediatek_kernel/dr_checker_out` (argument given to the option `-f`)**, for each entry point a `.json` file will be created which contains all the warnings in JSON format. These `json` files contain warnings organized by contexts. 

Second, The folder **`~/mediatek_kernel/dr_checker_out/instr_warnings` (w.r.t argument given to the option `-f`)** contains warnings organized by instruction location.

These warnings could be analyzed using our [Visualizer](https://github.com/ucsb-seclab/dr_checker/tree/speedy/visualizer).

Finally, a summary of all the warnings for each entry point organized by the type will be written to the output CSV file: **`~/mediatek_kernel/dr_checker_out/warnings_stats.csv` (w.r.t argument given to the option `-f`)**.

#### 3.2.7 Things to note:
##### 3.2.7.1 Value for option `-g`
To provide value for option `-g` you need to know the name of the `*-gcc` binary used to compile the kernel.
An easy way to know this would be to `grep` for `gcc` in `makeout.txt` and you will see compiler commands from which you can know the `*-gcc` binary name.

For our example above, if you do `grep gcc makeout.txt` for the example build, you will see lot of lines like below:
```
aarch64-linux-android-gcc -Wp,-MD,fs/jbd2/.transaction.o.d  -nostdinc -isystem ...
```
So, the value for `-g` should be `aarch64-linux-android-gcc`. 

If the kernel to be built is 32-bit then the binary most likely will be `arm-eabi-gcc`

##### 3.2.7.2 Value for option `-a`
Depeding on the chipset type, you need to provide corresponding number.

##### 3.2.7.3 Value for option `-o`
This is the path of the folder provided to the option `O=` for `make` command during kernel build.

Not all kernels need a separate out path. You may build kernel by not providing an option `O`, in which case you SHOULD NOT provide value for that option while running `run_all.py`.

### 3.3 Visualizing DR.CHECKER results :snowflake:
We provide a web-based UI to view all the warnings. Please refer [Visualization](https://github.com/ucsb-seclab/dr_checker/tree/speedy/visualizer).

### 3.6 Disabling Vulnerability checkers
You can disable one or more vulnerability checkers by uncommenting the corresponding `#define DISABLE_*` lines in [BugDetectorDriver.cpp](https://github.com/ucsb-seclab/dr_checker/blob/speedy/llvm_analysis/MainAnalysisPasses/SoundyAliasAnalysis/src/bug_detectors/BugDetectorDriver.cpp#L19)

### 3.5 Post-processing DR.CHECKER results
To your liking, we also provide a script to post-process the results. [Check it out](https://github.com/ucsb-seclab/dr_checker/blob/master/docs/postprocessing.md).

Have fun!!

## 4. Contact
* Slack: [JOIN SLACK CHANNEL](https://join.slack.com/t/driverchecking/shared_invite/enQtNDI3NDgyOTI4MTM1LTk3NTc4OWIxZTQyZjljNzkzMGM4YjczZjI4OTY0ODgyMmY1MWZmM2ZhZGM2OWY0NTMwOWQ5MmZjNzExMTQwODQ)
* Aravind Machiry (machiry@cs.ucsb.edu)
","['Machiry', 'Phat3', 'jwilk', 'idl3r', 'deadly-platypus', 'tnballo']",1,,0.7,0,,,,,,18,,,
23082724,MDEwOlJlcG9zaXRvcnkyMzA4MjcyNA==,ictf-framework,shellphish/ictf-framework,0,shellphish,https://github.com/shellphish/ictf-framework,"The iCTF Framework, presented by Shellphish!",0,2014-08-18 18:58:00+00:00,2025-02-22 22:24:49+00:00,2023-02-15 16:07:59+00:00,,22187,332,332,Python,1,1,1,1,0,0,88,0,0,21,other,1,0,0,public,88,21,332,master,1,1,"# The iCTF Framework 3.0

This is the framework that [Shellphish](http://www.shellphish.net) uses to host the [iCTF](http://ictf.cs.ucsb.edu).

The iCTF Framework is described in a [paper](https://www.usenix.org/conference/3gse14/summit-program/presentation/vigna) presented at the Usenix 3GSE workshop in 2014. 

We released this in the hope that it allows educators and trainers to host their own A/D CTFs. 
This framework is free for commercial use, but the support that we can provide is limited.

We are planning to release more technical documentation regarding each components in the future; as for now you can find instruction on how to create a game [here](https://github.com/shellphish/ictf-framework/wiki/running-a-class-ctf). 

If you have questions, please send an email to ctf-admin@lists.cs.ucsb.edu. 

**DISCLAIMER**: This framework is still a work in progress and this release have to be considered a **BETA** version. New pull requests and new issues are welcome :)

## TODOs and known issues

- The codebase needs to be cleaned from old pieces of unused code.
- Finish to port every component to python 3.
- Finish to document the various components.
- Extend the framework to support multiple cloud providers other than AWS.
- The CTF cannot be run for more than 12 hours because the credentials we use to login to the docker registry will expire after such time and we currently don't have a way to renew them when the game is running.

## Database

This is the central database that tracks the state of the game. 
It runs on the Database VM and exposes a RESTful API.  
Note that this database should not be directly accessed by the teams, which instead should go through the team services component.   

## Gamebot

The Gamebot is the component responsible for advancing the competition. 
The competition is divided into ticks. 
At the beginning of each tick, the gamebot decides which scripts need to be executed by the scriptbot (e.g., scripts to set flags, retrieve flags, or test services) and writes the schedule in the central database. 
Then, it extracts from the database the data about the previous tick (e.g., flag submitted and the status of service checks) and computes the points to be assigned to each team. 
The new scores are stored in the database, so that they can be displayed by the dashboard component. 

## Scriptbot

The scriptbot is responsible for the execution of the scripts scheduled by the gamebot. 
The scriptbot extracts the scripts scheduled for execution from the central database, and then runs them. 
For example, the scripts retrieve flags that have been set in the previous tick, or check if the services are up and functional. 

## Router

The router component is responsible for routing the traffic between the teams in the competition. 
The component implements an OpenVPN service. Each team is given a VM that acts as the router for the team.
The traffic among teams needs to be anonymized to prevent teams from distinguishing scriptbot-generate traffic from team traffic.

## Creating a CTF competition

For more information visit our wiki page about [running a class CTF](https://github.com/shellphish/ictf-framework/wiki/running-a-class-ctf)

","['Phat3', 'Lukas-Dresel', 'sidsenkumar11', 'degrigis', 'dependabot[bot]', 'robmcl4', 'adamdoupe', 'giovannivigna']",1,,0.77,0,,,,,,47,,,
504471454,R_kgDOHhGfng,awesome-gnn-systems,ch-wan/awesome-gnn-systems,0,ch-wan,https://github.com/ch-wan/awesome-gnn-systems,A list of awesome GNN systems.,0,2022-06-17 09:25:36+00:00,2025-03-04 06:17:04+00:00,2025-03-03 01:02:11+00:00,,644,302,302,Python,1,1,1,1,1,0,27,0,0,0,,1,0,0,public,27,0,302,main,1,,"# Awesome Graph Neural Network Systems [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A list of awesome systems for graph neural network (GNN). If you have any comment, please create an [issue](https://github.com/chwan1016/awesome-gnn-systems/issues) or [pull request](https://github.com/chwan1016/awesome-gnn-systems/pulls).
## Contents
- [Open Source Libraries](#open-source-libraries)
- [Papers](#papers)
  - [Survey Papers](#survey-papers)
  - [GNN Libraries](#gnn-libraries)
  - [GNN Kernels](#gnn-kernels)
  - [GNN Compilers](#gnn-compilers)
  - [Distributed GNN Training Systems](#distributed-gnn-training-systems)
  - [Training Systems for Scaling Graphs](#training-systems-for-scaling-graphs)
  - [Quantized GNNs](#quantized-gnns)
  - [GNN Dataloaders](#gnn-dataloaders)
  - [GNN Training Accelerators](#gnn-training-accelerators)
  - [GNN Inference Accelerators](#gnn-inference-accelerators)
- [Contribute](#contribute)
## Open Source Libraries
- [PyG: Graph Neural Network Library for PyTorch](https://github.com/rusty1s/pytorch_geometric) ![GitHub stars](https://img.shields.io/github/stars/rusty1s/pytorch_geometric.svg?logo=github&label=Stars)
- [DGL: Python Package Built to Ease Deep Learning on Graph](https://github.com/dmlc/dgl) ![GitHub stars](https://img.shields.io/github/stars/dmlc/dgl.svg?logo=github&label=Stars)
- [Graph Nets: Build Graph Nets in Tensorflow](https://github.com/deepmind/graph_nets) ![GitHub stars](https://img.shields.io/github/stars/deepmind/graph_nets.svg?logo=github&label=Stars)
- [Euler: A Distributed Graph Deep Learning Framework](https://github.com/alibaba/euler) ![GitHub stars](https://img.shields.io/github/stars/alibaba/euler.svg?logo=github&label=Stars)
- [StellarGraph: Machine Learning on Graphs](https://github.com/stellargraph/stellargraph) ![GitHub stars](https://img.shields.io/github/stars/stellargraph/stellargraph.svg?logo=github&label=Stars)
- [Spektral: Graph Neural Networks with Keras and Tensorflow 2](https://github.com/danielegrattarola/spektral) ![GitHub stars](https://img.shields.io/github/stars/danielegrattarola/spektral.svg?logo=github&label=Stars)
- [PGL: An Efficient and Flexible Graph Learning Framework Based on PaddlePaddle](https://github.com/PaddlePaddle/PGL) ![GitHub stars](https://img.shields.io/github/stars/PaddlePaddle/PGL.svg?logo=github&label=Stars)
- [CogDL: An Extensive Toolkit for Deep Learning on Graphs](https://github.com/THUDM/cogdl) ![GitHub stars](https://img.shields.io/github/stars/THUDM/cogdl.svg?logo=github&label=Stars)
- [DIG: A Turnkey Library for Diving into Graph Deep Learning Research](https://github.com/divelab/DIG) ![GitHub stars](https://img.shields.io/github/stars/divelab/DIG.svg?logo=github&label=Stars)
- [Jraph: A Graph Neural Network Library in Jax](https://github.com/deepmind/jraph) ![GitHub stars](https://img.shields.io/github/stars/deepmind/jraph.svg?logo=github&label=Stars)
- [Graph-Learn: An Industrial Graph Neural Network Framework](https://github.com/alibaba/graph-learn) ![GitHub stars](https://img.shields.io/github/stars/alibaba/graph-learn.svg?logo=github&label=Stars)
- [DeepGNN: a Framework for Training Machine Learning Models on Large Scale Graph Data](https://github.com/microsoft/DeepGNN) ![GitHub stars](https://img.shields.io/github/stars/microsoft/DeepGNN.svg?logo=github&label=Stars)
## Papers
### Survey Papers
| Venue | Title | Affiliation | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;Source&nbsp;&nbsp; |
| :---: | :---: | :---------: | :---: | :----: |
|CSUR 2024|Distributed Graph Neural Network Training: A Survey|BUPT| [[paper]](https://dl.acm.org/doi/10.1145/3648358)![Scholar citations](https://img.shields.io/badge/Citations-61-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|Proceedings of the IEEE 2023|A Comprehensive Survey on Distributed Training of Graph Neural Networks|Chinese Academy of Sciences| [[paper]](https://ieeexplore.ieee.org/abstract/document/10348966)![Scholar citations](https://img.shields.io/badge/Citations-28-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|arXiv 2023|A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware|UCLA| [[paper]](https://arxiv.org/abs/2306.14052)![Scholar citations](https://img.shields.io/badge/Citations-31-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|arXiv 2022|Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis|ETHZ| [[paper]](https://arxiv.org/abs/2205.09702)![Scholar citations](https://img.shields.io/badge/Citations-54-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|CSUR 2022|Computing Graph Neural Networks: A Survey from Algorithms to Accelerators|UPC| [[paper]](https://dl.acm.org/doi/10.1145/3477141)![Scholar citations](https://img.shields.io/badge/Citations-287-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
### GNN Libraries
| Venue | Title | Affiliation | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;Source&nbsp;&nbsp; |
| :---: | :---: | :---------: | :---: | :----: |
|JMLR 2021|DIG: A Turnkey Library for Diving into Graph Deep Learning Research|TAMU| [[paper]](https://arxiv.org/abs/2103.12608)![Scholar citations](https://img.shields.io/badge/Citations-107-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/divelab/DIG)![GitHub stars](https://img.shields.io/github/stars/divelab/DIG.svg?logo=github&label=Stars)|
|arXiv 2021|CogDL: A Toolkit for Deep Learning on Graphs|THU| [[paper]](https://arxiv.org/abs/2103.00959)![Scholar citations](https://img.shields.io/badge/Citations-2-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/THUDM/cogdl)![GitHub stars](https://img.shields.io/github/stars/THUDM/cogdl.svg?logo=github&label=Stars)|
|CIM 2021|Graph Neural Networks in TensorFlow and Keras with Spektral|Università della Svizzera italiana| [[paper]](https://arxiv.org/abs/2006.12138)![Scholar citations](https://img.shields.io/badge/Citations-329-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/danielegrattarola/spektral)![GitHub stars](https://img.shields.io/github/stars/danielegrattarola/spektral.svg?logo=github&label=Stars)|
|arXiv 2019|Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks|AWS| [[paper]](https://arxiv.org/abs/1909.01315)![Scholar citations](https://img.shields.io/badge/Citations-1.4k-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/dmlc/dgl)![GitHub stars](https://img.shields.io/github/stars/dmlc/dgl.svg?logo=github&label=Stars)|
|VLDB 2019|AliGraph: A Comprehensive Graph Neural Network Platform|Alibaba| [[paper]](https://dl.acm.org/doi/pdf/10.14778/3352063.3352127)![Scholar citations](https://img.shields.io/badge/Citations-349-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/alibaba/graph-learn)![GitHub stars](https://img.shields.io/github/stars/alibaba/graph-learn.svg?logo=github&label=Stars)|
|arXiv 2019|Fast Graph Representation Learning with PyTorch Geometric|TU Dortmund University| [[paper]](https://arxiv.org/abs/1903.02428)![Scholar citations](https://img.shields.io/badge/Citations-5.4k-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/rusty1s/pytorch_geometric)![GitHub stars](https://img.shields.io/github/stars/rusty1s/pytorch_geometric.svg?logo=github&label=Stars)|
|arXiv 2018|Relational Inductive Biases, Deep Learning, and Graph Networks|DeepMind| [[paper]](https://arxiv.org/abs/1806.01261)![Scholar citations](https://img.shields.io/badge/Citations-4.0k-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/deepmind/graph_nets)![GitHub stars](https://img.shields.io/github/stars/deepmind/graph_nets.svg?logo=github&label=Stars)|
### GNN Kernels
| Venue | Title | Affiliation | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;Source&nbsp;&nbsp; |
| :---: | :---: | :---------: | :---: | :----: |
|MLSys 2022|Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective|THU| [[paper]](https://proceedings.mlsys.org/paper/2022/file/9a1158154dfa42caddbd0694a4e9bdc8-Paper.pdf)![Scholar citations](https://img.shields.io/badge/Citations-56-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/dgSPARSE/dgNN)![GitHub stars](https://img.shields.io/github/stars/dgSPARSE/dgNN.svg?logo=github&label=Stars)|
|HPDC 2022|TLPGNN: A Lightweight Two-Level Parallelism Paradigm for Graph Neural Network Computation on GPU|GW| [[paper]](https://dl.acm.org/doi/abs/10.1145/3502181.3531467)![Scholar citations](https://img.shields.io/badge/Citations-28-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|IPDPS 2021|FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Networks|Indiana University Bloomington| [[paper]](https://arxiv.org/abs/2011.06391)![Scholar citations](https://img.shields.io/badge/Citations-53-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/HipGraph/FusedMM)![GitHub stars](https://img.shields.io/github/stars/HipGraph/FusedMM.svg?logo=github&label=Stars)|
|SC 2020|GE-SpMM: General-purpose Sparse Matrix-Matrix Multiplication on GPUs for Graph Neural Networks|THU| [[paper]](https://arxiv.org/abs/2007.03179)![Scholar citations](https://img.shields.io/badge/Citations-138-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/hgyhungry/ge-spmm)![GitHub stars](https://img.shields.io/github/stars/hgyhungry/ge-spmm.svg?logo=github&label=Stars)|
|ICCAD 2020|fuseGNN: Accelerating Graph Convolutional Neural Network Training on GPGPU|UCSB| [[paper]](https://seal.ece.ucsb.edu/sites/default/files/publications/fusegcn_camera_ready_.pdf)![Scholar citations](https://img.shields.io/badge/Citations-24-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/apuaaChen/gcnLib)![GitHub stars](https://img.shields.io/github/stars/apuaaChen/gcnLib.svg?logo=github&label=Stars)|
|IPDPS 2020|PCGCN: Partition-Centric Processing for Accelerating Graph Convolutional Network|PKU| [[paper]](https://ieeexplore.ieee.org/document/9139807)![Scholar citations](https://img.shields.io/badge/Citations-51-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
### GNN Compilers
| Venue | Title | Affiliation | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;Source&nbsp;&nbsp; |
| :---: | :---: | :---------: | :---: | :----: |
|MLSys 2022|Graphiler: Optimizing Graph Neural Networks with Message Passing Data Flow Graph|ShanghaiTech| [[paper]](https://proceedings.mlsys.org/paper/2022/file/a87ff679a2f3e71d9181a67b7542122c-Paper.pdf)![Scholar citations](https://img.shields.io/badge/Citations-26-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/xiezhq-hermann/graphiler)![GitHub stars](https://img.shields.io/github/stars/xiezhq-hermann/graphiler.svg?logo=github&label=Stars)|
|EuroSys 2021|Seastar: Vertex-Centric Programming for Graph Neural Networks|CUHK| [[paper]](http://www.cse.cuhk.edu.hk/~jcheng/papers/seastar_eurosys21.pdf)![Scholar citations](https://img.shields.io/badge/Citations-60-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|SC 2020|FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems|Cornell| [[paper]](https://arxiv.org/pdf/2008.11359.pdf)![Scholar citations](https://img.shields.io/badge/Citations-102-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/dglai/FeatGraph)![GitHub stars](https://img.shields.io/github/stars/dglai/FeatGraph.svg?logo=github&label=Stars)|
### Distributed GNN Training Systems
| Venue | Title | Affiliation | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;Source&nbsp;&nbsp; |
| :---: | :---: | :---------: | :---: | :----: |
|arXiv 2023|Communication-Free Distributed GNN Training with Vertex Cut|Stanford| [[paper]](https://arxiv.org/pdf/2308.03209.pdf)![Scholar citations](https://img.shields.io/badge/Citations-4-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|arXiv 2023|GNNPipe: Accelerating Distributed Full-Graph GNN Training with Pipelined Model Parallelism|Purdue| [[paper]](https://arxiv.org/pdf/2308.10087.pdf)![Scholar citations](https://img.shields.io/badge/Citations-1-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|OSDI 2023|MGG: Accelerating Graph Neural Networks with Fine-Grained Intra-Kernel Communication-Computation Pipelining on Multi-GPU Platforms|UCSB| [[paper]](https://www.usenix.org/system/files/osdi23-wang-yuke.pdf)![Scholar citations](https://img.shields.io/badge/Citations-28-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/YukeWang96/MGG_OSDI23)![GitHub stars](https://img.shields.io/github/stars/YukeWang96/MGG_OSDI23.svg?logo=github&label=Stars)|
|VLDB 2022|Sancus: Staleness-Aware Communication-Avoiding Full-Graph Decentralized Training in Large-Scale Graph Neural Networks|HKUST| [[paper]](https://www.vldb.org/pvldb/vol15/p1937-peng.pdf)![Scholar citations](https://img.shields.io/badge/Citations-75-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/chenzhao/light-dist-gnn)![GitHub stars](https://img.shields.io/github/stars/chenzhao/light-dist-gnn.svg?logo=github&label=Stars)|
|MLSys 2022|BNS-GCN: Efficient Full-Graph Training of Graph Convolutional Networks with Partition-Parallelism and Random Boundary Node Sampling|Rice, UIUC| [[paper]](https://proceedings.mlsys.org/paper/2022/file/d1fe173d08e959397adf34b1d77e88d7-Paper.pdf)![Scholar citations](https://img.shields.io/badge/Citations-86-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/RICE-EIC/BNS-GCN)![GitHub stars](https://img.shields.io/github/stars/RICE-EIC/BNS-GCN.svg?logo=github&label=Stars)|
|MLSys 2022|Sequential Aggregation and Rematerialization: Distributed Full-batch Training of Graph Neural Networks on Large Graphs|Intel| [[paper]](https://arxiv.org/abs/2111.06483)![Scholar citations](https://img.shields.io/badge/Citations-26-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/IntelLabs/SAR)![GitHub stars](https://img.shields.io/github/stars/IntelLabs/SAR.svg?logo=github&label=Stars)|
|WWW 2022|PaSca: A Graph Neural Architecture Search System under the Scalable Paradigm|PKU| [[paper]](https://dl.acm.org/doi/abs/10.1145/3485447.3511986)![Scholar citations](https://img.shields.io/badge/Citations-59-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|ICLR 2022|PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication|Rice| [[paper]](https://openreview.net/pdf?id=kSwqMH0zn1F)![Scholar citations](https://img.shields.io/badge/Citations-82-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/RICE-EIC/PipeGCN)![GitHub stars](https://img.shields.io/github/stars/RICE-EIC/PipeGCN.svg?logo=github&label=Stars)|
|ICLR 2022|Learn Locally, Correct Globally: A Distributed Algorithm for Training Graph Neural Networks|PSU| [[paper]](https://openreview.net/pdf?id=FndDxSz3LxQ)![Scholar citations](https://img.shields.io/badge/Citations-41-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/MortezaRamezani/llcg)![GitHub stars](https://img.shields.io/github/stars/MortezaRamezani/llcg.svg?logo=github&label=Stars)|
|arXiv 2021|Distributed Hybrid CPU and GPU training for Graph Neural Networks on Billion-Scale Graphs|AWS| [[paper]](https://arxiv.org/pdf/2112.15345.pdf)![Scholar citations](https://img.shields.io/badge/Citations-44-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|SC 2021|DistGNN: Scalable Distributed Training for Large-Scale Graph Neural Networks|Intel| [[paper]](https://dl.acm.org/doi/pdf/10.1145/3458817.3480856)![Scholar citations](https://img.shields.io/badge/Citations-139-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/dmlc/dgl/pull/3024)|
|SC 2021|Efficient Scaling of Dynamic Graph Neural Networks|IBM| [[paper]](https://dl.acm.org/doi/pdf/10.1145/3458817.3480858)![Scholar citations](https://img.shields.io/badge/Citations-30-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|CLUSTER 2021|2PGraph: Accelerating GNN Training over Large Graphs on GPU Clusters|NUDT| [[paper]](https://ieeexplore.ieee.org/abstract/document/9556026)![Scholar citations](https://img.shields.io/badge/Citations-18-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|OSDI 2021|$P^3$: Distributed Deep Graph Learning at Scale|MSR| [[paper]](https://www.usenix.org/system/files/osdi21-gandhi.pdf)![Scholar citations](https://img.shields.io/badge/Citations-177-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|OSDI 2021|Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads|UCLA| [[paper]](http://web.cs.ucla.edu/~harryxu/papers/dorylus-osdi21.pdf)![Scholar citations](https://img.shields.io/badge/Citations-166-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/uclasystem/dorylus)![GitHub stars](https://img.shields.io/github/stars/uclasystem/dorylus.svg?logo=github&label=Stars)|
|arXiv 2021|GIST: Distributed Training for Large-Scale Graph Convolutional Networks|Rice| [[paper]](https://arxiv.org/abs/2102.10424)![Scholar citations](https://img.shields.io/badge/Citations-15-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|EuroSys 2021|FlexGraph: A Flexible and Efficient Distributed Framework for GNN Training|Alibaba| [[paper]](https://dl.acm.org/doi/pdf/10.1145/3447786.3456229)![Scholar citations](https://img.shields.io/badge/Citations-72-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|EuroSys 2021|DGCL: An Efficient Communication Library for Distributed GNN Training|CUHK| [[paper]](https://dl.acm.org/doi/abs/10.1145/3447786.3456233)![Scholar citations](https://img.shields.io/badge/Citations-104-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/czkkkkkk/ragdoll)![GitHub stars](https://img.shields.io/github/stars/czkkkkkk/ragdoll.svg?logo=github&label=Stars)|
|SC 2020|Reducing Communication in Graph Neural Network Training|UC Berkeley| [[paper]](https://arxiv.org/pdf/2005.03300.pdf)![Scholar citations](https://img.shields.io/badge/Citations-131-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/PASSIONLab/CAGNET)![GitHub stars](https://img.shields.io/github/stars/PASSIONLab/CAGNET.svg?logo=github&label=Stars)|
|VLDB 2020|G$^3$: When Graph Neural Networks Meet Parallel Graph Processing Systems on GPUs|NUS| [[paper]](http://www.vldb.org/pvldb/vol13/p2813-liu.pdf)![Scholar citations](https://img.shields.io/badge/Citations-56-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/Xtra-Computing/G3)![GitHub stars](https://img.shields.io/github/stars/Xtra-Computing/G3.svg?logo=github&label=Stars)|
|IA3 2020|DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs|AWS| [[paper]](https://arxiv.org/pdf/2010.05337.pdf)![Scholar citations](https://img.shields.io/badge/Citations-155-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/dmlc/dgl/tree/master/python/dgl/distributed)|
|MLSys 2020|Improving the Accuracy, Scalability, and Performance of Graph Neural Networks with Roc|Stanford| [[paper]](https://proceedings.mlsys.org/paper/2020/file/fe9fc289c3ff0af142b6d3bead98a923-Paper.pdf)![Scholar citations](https://img.shields.io/badge/Citations-278-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/jiazhihao/ROC)![GitHub stars](https://img.shields.io/github/stars/jiazhihao/ROC.svg?logo=github&label=Stars)|
|arXiv 2020|AGL: A Scalable System for Industrial-purpose Graph Machine Learning|Ant Financial Services Group| [[paper]](https://arxiv.org/abs/2003.02454)![Scholar citations](https://img.shields.io/badge/Citations-141-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|ATC 2019|NeuGraph: Parallel Deep Neural Network Computation on Large Graphs|PKU| [[paper]](https://www.usenix.org/system/files/atc19-ma_0.pdf)![Scholar citations](https://img.shields.io/badge/Citations-293-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
### Training Systems for Scaling Graphs
| Venue | Title | Affiliation | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;Source&nbsp;&nbsp; |
| :---: | :---: | :---------: | :---: | :----: |
|DaMoN 2024|In situ neighborhood sampling for large-scale GNN training|Boston University| [[paper]](https://dl.acm.org/doi/10.1145/3662010.3663443)![Scholar citations](https://img.shields.io/badge/Citations-0-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/CASP-Systems-BU/damon24-gnn-in-situ-sampling/)|
|HPCA 2024|BeaconGNN: Large-Scale GNN Acceleration with Out-of-Order Streaming In-Storage Computing|UCLA| [[paper]](https://ieeexplore.ieee.org/abstract/document/10476427)![Scholar citations](https://img.shields.io/badge/Citations-3-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|EuroSys 2023|MariusGNN: Resource-Efficient Out-of-Core Training of Graph Neural Networks|UW–Madison| [[paper]](https://arxiv.org/abs/2202.02365)![Scholar citations](https://img.shields.io/badge/Citations-44-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/marius-team/marius)![GitHub stars](https://img.shields.io/github/stars/marius-team/marius.svg?logo=github&label=Stars)|
|VLDB 2022|ByteGNN: Efficient Graph Neural Network Training at Large Scale|ByteDance| [[paper]](https://dl.acm.org/doi/abs/10.14778/3514061.3514069)![Scholar citations](https://img.shields.io/badge/Citations-88-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|VLDB 2022|Ginex: SSD-enabled Billion-scale Graph Neural Network Training on a Single Machine via Provably Optimal In-memory Caching|Seoul National University| [[paper]](https://dl.acm.org/doi/10.14778/3551793.3551819)![Scholar citations](https://img.shields.io/badge/Citations-16-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/SNU-ARC/Ginex)![GitHub stars](https://img.shields.io/github/stars/SNU-ARC/Ginex.svg?logo=github&label=Stars)|
|ISCA 2022|SmartSAGE: Training Large-scale Graph Neural Networks using In-Storage Processing Architectures|KAIST| [[paper]](https://dl.acm.org/doi/10.1145/3470496.3527391)![Scholar citations](https://img.shields.io/badge/Citations-47-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|ICML 2022|GraphFM: Improving Large-Scale GNN Training via Feature Momentum|TAMU| [[paper]](https://arxiv.org/abs/2206.07161)![Scholar citations](https://img.shields.io/badge/Citations-44-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/divelab/DIG/tree/dig-stable/dig/lsgraph)|
|ICML 2021|GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings|TU Dortmund University| [[paper]](https://arxiv.org/abs/2106.05609)![Scholar citations](https://img.shields.io/badge/Citations-187-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/rusty1s/pyg_autoscale)![GitHub stars](https://img.shields.io/github/stars/rusty1s/pyg_autoscale.svg?logo=github&label=Stars)|
|OSDI 2021|GNNAdvisor: An Adaptive and Efficient Runtime System for GNN Acceleration on GPUs|UCSB| [[paper]](https://www.usenix.org/system/files/osdi21-wang-yuke.pdf)![Scholar citations](https://img.shields.io/badge/Citations-175-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/YukeWang96/OSDI21_AE)![GitHub stars](https://img.shields.io/github/stars/YukeWang96/OSDI21_AE.svg?logo=github&label=Stars)|
### Quantized GNNs
| Venue | Title | Affiliation | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;Source&nbsp;&nbsp; |
| :---: | :---: | :---------: | :---: | :----: |
|Neurocomputing 2022|EPQuant: A Graph Neural Network Compression Approach Based on Product Quantization|ZJU| [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0925231222008293)![Scholar citations](https://img.shields.io/badge/Citations-16-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/Lyun-Huang/EPQuant)![GitHub stars](https://img.shields.io/github/stars/Lyun-Huang/EPQuant.svg?logo=github&label=Stars)|
|ICLR 2022|EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression|Rice| [[paper]](https://openreview.net/pdf?id=vkaMaq95_rX)![Scholar citations](https://img.shields.io/badge/Citations-64-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/warai-0toko/Exact)![GitHub stars](https://img.shields.io/github/stars/warai-0toko/Exact.svg?logo=github&label=Stars)|
|PPoPP 2022|QGTC: Accelerating Quantized Graph Neural Networks via GPU Tensor Core|UCSB| [[paper]](https://arxiv.org/abs/2111.09547)![Scholar citations](https://img.shields.io/badge/Citations-51-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/YukeWang96/PPoPP22_QGTC)![GitHub stars](https://img.shields.io/github/stars/YukeWang96/PPoPP22_QGTC.svg?logo=github&label=Stars)|
|CVPR 2021|Binary Graph Neural Networks|ICL| [[paper]](https://arxiv.org/abs/2012.15823)![Scholar citations](https://img.shields.io/badge/Citations-58-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/mbahri/binary_gnn)![GitHub stars](https://img.shields.io/github/stars/mbahri/binary_gnn.svg?logo=github&label=Stars)|
|CVPR 2021|Bi-GCN: Binary Graph Convolutional Network|Beihang University| [[paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Bi-GCN_Binary_Graph_Convolutional_Network_CVPR_2021_paper.html)![Scholar citations](https://img.shields.io/badge/Citations-60-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/bywmm/Bi-GCN)![GitHub stars](https://img.shields.io/github/stars/bywmm/Bi-GCN.svg?logo=github&label=Stars)|
|EuroMLSys 2021|Learned Low Precision Graph Neural Networks|Cambridge| [[paper]](https://arxiv.org/abs/2009.09232)![Scholar citations](https://img.shields.io/badge/Citations-39-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|World Wide Web 2021|Binarized Graph Neural Network|UTS| [[paper]](https://arxiv.org/pdf/2004.11147.pdf)![Scholar citations](https://img.shields.io/badge/Citations-34-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|ICLR 2021|Degree-Quant: Quantization-Aware Training for Graph Neural Networks|Cambridge| [[paper]](https://arxiv.org/abs/2008.05000)![Scholar citations](https://img.shields.io/badge/Citations-208-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/camlsys/degree-quant)![GitHub stars](https://img.shields.io/github/stars/camlsys/degree-quant.svg?logo=github&label=Stars)|
|ICTAI 2020|SGQuant: Squeezing the Last Bit on Graph Neural Networks with Specialized Quantization|UCSB| [[paper]](https://ieeexplore.ieee.org/abstract/document/9288186/)![Scholar citations](https://img.shields.io/badge/Citations-55-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/YukeWang96/SGQuant)![GitHub stars](https://img.shields.io/github/stars/YukeWang96/SGQuant.svg?logo=github&label=Stars)|
### GNN Dataloaders
| Venue | Title | Affiliation | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;Source&nbsp;&nbsp; |
| :---: | :---: | :---------: | :---: | :----: |
|NSDI 2023|BGL: GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing|ByteDance| [[paper]](https://arxiv.org/abs/2112.08541)![Scholar citations](https://img.shields.io/badge/Citations-84-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|MLSys 2022|Accelerating Training and Inference of Graph Neural Networks with Fast Sampling and Pipelining|MIT| [[paper]](https://proceedings.mlsys.org/paper/2022/file/35f4a8d465e6e1edc05f3d8ab658c551-Paper.pdf)![Scholar citations](https://img.shields.io/badge/Citations-64-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/MITIBMxGraph/SALIENT)![GitHub stars](https://img.shields.io/github/stars/MITIBMxGraph/SALIENT.svg?logo=github&label=Stars)|
|EuroSys 2022|GNNLab: A Factored System for Sample-based GNN Training over GPUs|SJTU| [[paper]](https://dl.acm.org/doi/abs/10.1145/3492321.3519557)![Scholar citations](https://img.shields.io/badge/Citations-90-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/SJTU-IPADS/gnnlab)![GitHub stars](https://img.shields.io/github/stars/SJTU-IPADS/gnnlab.svg?logo=github&label=Stars)|
|KDD 2021|Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs|UCLA| [[paper]](https://arxiv.org/pdf/2106.06150.pdf)![Scholar citations](https://img.shields.io/badge/Citations-38-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|PPoPP 2021|Understanding and Bridging the Gaps in Current GNN Performance Optimizations|THU| [[paper]](https://dl.acm.org/doi/pdf/10.1145/3437801.3441585)![Scholar citations](https://img.shields.io/badge/Citations-93-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/xxcclong/GNN-Computing)![GitHub stars](https://img.shields.io/github/stars/xxcclong/GNN-Computing.svg?logo=github&label=Stars)|
|VLDB 2021|Large Graph Convolutional Network Training with GPU-Oriented Data Communication Architecture|UIUC| [[paper]](https://arxiv.org/abs/2103.03330)![Scholar citations](https://img.shields.io/badge/Citations-77-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/K-Wu/pytorch-direct_dgl)![GitHub stars](https://img.shields.io/github/stars/K-Wu/pytorch-direct_dgl.svg?logo=github&label=Stars)|
|TPDS 2021|Efficient Data Loader for Fast Sampling-Based GNN Training on Large Graphs|USTC| [[paper]](https://gnnsys.github.io/papers/GNNSys21_paper_8.pdf)![Scholar citations](https://img.shields.io/badge/Citations-38-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/zhiqi-0/PaGraph)![GitHub stars](https://img.shields.io/github/stars/zhiqi-0/PaGraph.svg?logo=github&label=Stars)|
|SoCC 2020|PaGraph: Scaling GNN Training on Large Graphs via Computation-aware Caching|USTC| [[paper]](https://dl.acm.org/doi/pdf/10.1145/3419111.3421281)![Scholar citations](https://img.shields.io/badge/Citations-191-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/zhiqi-0/PaGraph)![GitHub stars](https://img.shields.io/github/stars/zhiqi-0/PaGraph.svg?logo=github&label=Stars)|
|arXiv 2019|TigerGraph: A Native MPP Graph Database|UCSD| [[paper]](https://arxiv.org/pdf/1901.08248.pdf)![Scholar citations](https://img.shields.io/badge/Citations-84-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
### GNN Training Accelerators
| Venue | Title | Affiliation | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;Source&nbsp;&nbsp; |
| :---: | :---: | :---------: | :---: | :----: |
|ISCA 2022|Graphite: Optimizing Graph Neural Networks on CPUs Through Cooperative Software-Hardware Techniques|UIUC| [[paper]](http://iacoma.cs.uiuc.edu/iacoma-papers/isca22.pdf)![Scholar citations](https://img.shields.io/badge/Citations-33-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|ISCA 2022|Hyperscale FPGA-as-a-service architecture for large-scale distributed graph neural network|Alibaba| [[paper]](https://dl.acm.org/doi/abs/10.1145/3470496.3527439)![Scholar citations](https://img.shields.io/badge/Citations-29-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|arXiv 2021|GCNear: A Hybrid Architecture for Efficient GCN Training with Near-Memory Processing|PKU| [[paper]](https://arxiv.org/abs/2111.00680)![Scholar citations](https://img.shields.io/badge/Citations-11-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|DATE 2021|ReGraphX: NoC-enabled 3D Heterogeneous ReRAM Architecture for Training Graph Neural Networks|WSU| [[paper]](https://arxiv.org/abs/2102.07959)![Scholar citations](https://img.shields.io/badge/Citations-36-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|TCAD 2021|Rubik: A Hierarchical Architecture for Efficient Graph Learning|Chinese Academy of Sciences| [[paper]](https://arxiv.org/pdf/2009.12495.pdf)![Scholar citations](https://img.shields.io/badge/Citations-13-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|FPGA 2020|GraphACT: Accelerating GCN Training on CPU-FPGA Heterogeneous Platforms|USC| [[paper]](https://dl.acm.org/doi/pdf/10.1145/3373087.3375312)![Scholar citations](https://img.shields.io/badge/Citations-167-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/GraphSAINT/GraphACT)![GitHub stars](https://img.shields.io/github/stars/GraphSAINT/GraphACT.svg?logo=github&label=Stars)|
### GNN Inference Accelerators
| Venue | Title | Affiliation | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;Source&nbsp;&nbsp; |
| :---: | :---: | :---------: | :---: | :----: |
|JAIHC 2022|DRGN: a dynamically reconfigurable accelerator for graph neural networks|XJTU| [[paper]](https://link.springer.com/article/10.1007/s12652-022-04402-x)![Scholar citations](https://img.shields.io/badge/Citations-3-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|DAC 2022|GNNIE: GNN Inference Engine with Load-balancing and Graph-specific Caching|UMN| [[paper]](https://arxiv.org/abs/2105.10554)![Scholar citations](https://img.shields.io/badge/Citations-21-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|IPDPS 2022|Understanding the Design Space of Sparse/Dense Multiphase Dataflows for Mapping Graph Neural Networks on Spatial Accelerators|GaTech| [[paper]](https://arxiv.org/abs/2103.07977)![Scholar citations](https://img.shields.io/badge/Citations-0-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/stonne-simulator/omega)![GitHub stars](https://img.shields.io/github/stars/stonne-simulator/omega.svg?logo=github&label=Stars)|
|arXiv 2022|FlowGNN: A Dataflow Architecture for Universal Graph Neural Network Inference via Multi-Queue Streaming|GaTech| [[paper]](https://arxiv.org/abs/2204.13103)![Scholar citations](https://img.shields.io/badge/Citations-13-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|CICC 2022|StreamGCN: Accelerating Graph Convolutional Networks with Streaming Processing|UCLA| [[paper]](https://web.cs.ucla.edu/~atefehsz/publication/StreamGCN-CICC22.pdf)![Scholar citations](https://img.shields.io/badge/Citations-6-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|HPCA 2022|Accelerating Graph Convolutional Networks Using Crossbar-based Processing-In-Memory Architectures|HUST| [[paper]](https://ieeexplore.ieee.org/document/9773267)![Scholar citations](https://img.shields.io/badge/Citations-50-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|HPCA 2022|GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm and Accelerator Co-Design|Rice, PNNL| [[paper]](https://arxiv.org/abs/2112.11594)![Scholar citations](https://img.shields.io/badge/Citations-64-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)| [[code]](https://github.com/RICE-EIC/GCoD)![GitHub stars](https://img.shields.io/github/stars/RICE-EIC/GCoD.svg?logo=github&label=Stars)|
|arXiv 2022|GenGNN: A Generic FPGA Framework for Graph Neural Network Acceleration|GaTech| [[paper]](https://arxiv.org/abs/2201.08475)![Scholar citations](https://img.shields.io/badge/Citations-17-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|DAC 2021|DyGNN: Algorithm and Architecture Support of vertex Dynamic Pruning for Graph Neural Networks|Hunan University| [[paper]](https://ieeexplore.ieee.org/document/9586298)![Scholar citations](https://img.shields.io/badge/Citations-37-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|DAC 2021|BlockGNN: Towards Efficient GNN Acceleration Using Block-Circulant Weight Matrices|PKU| [[paper]](https://arxiv.org/abs/2104.06214)![Scholar citations](https://img.shields.io/badge/Citations-40-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|DAC 2021|TARe: Task-Adaptive in-situ ReRAM Computing for Graph Learning|Chinese Academy of Sciences| [[paper]](https://ieeexplore.ieee.org/abstract/document/9586193)![Scholar citations](https://img.shields.io/badge/Citations-15-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|ICCAD 2021|G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency|Rice| [[paper]](https://arxiv.org/abs/2109.08983)![Scholar citations](https://img.shields.io/badge/Citations-30-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|MICRO 2021|I-GCN: A Graph Convolutional Network Accelerator with Runtime Locality Enhancement through Islandization|PNNL| [[paper]](https://dl.acm.org/doi/pdf/10.1145/3466752.3480113)![Scholar citations](https://img.shields.io/badge/Citations-125-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|arXiv 2021|ZIPPER: Exploiting Tile- and Operator-level Parallelism for General and Scalable Graph Neural Network Acceleration|SJTU| [[paper]](https://arxiv.org/abs/2107.08709)![Scholar citations](https://img.shields.io/badge/Citations-7-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|TComp 2021|EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph Neural Networks|Chinese Academy of Sciences| [[paper]](https://arxiv.org/abs/1909.00155)![Scholar citations](https://img.shields.io/badge/Citations-189-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|HPCA 2021|GCNAX: A Flexible and Energy-efficient Accelerator for Graph Convolutional Neural Networks|GWU| [[paper]](https://ieeexplore.ieee.org/abstract/document/9407104)![Scholar citations](https://img.shields.io/badge/Citations-148-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|APA 2020|GNN-PIM: A Processing-in-Memory Architecture for Graph Neural Networks|PKU| [[paper]](http://115.27.240.201/docs/20200915165942122459.pdf)![Scholar citations](https://img.shields.io/badge/Citations-25-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|ASAP 2020|Hardware Acceleration of Large Scale GCN Inference|USC| [[paper]](https://ieeexplore.ieee.org/document/9153263)![Scholar citations](https://img.shields.io/badge/Citations-88-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|DAC 2020|Hardware Acceleration of Graph Neural Networks|UIUC| [[paper]](http://rakeshk.web.engr.illinois.edu/dac20.pdf)![Scholar citations](https://img.shields.io/badge/Citations-145-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|MICRO 2020|AWB-GCN: A Graph Convolutional Network Accelerator with Runtime Workload Rebalancing|PNNL| [[paper]](https://ieeexplore.ieee.org/abstract/document/9252000)![Scholar citations](https://img.shields.io/badge/Citations-305-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|arXiv 2020|GRIP: A Graph Neural Network Accelerator Architecture|Stanford| [[paper]](https://arxiv.org/pdf/2007.13828.pdf)![Scholar citations](https://img.shields.io/badge/Citations-102-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
|HPCA 2020|HyGCN: A GCN Accelerator with Hybrid Architecture|UCSB| [[paper]](https://arxiv.org/pdf/2001.02514.pdf)![Scholar citations](https://img.shields.io/badge/Citations-363-_.svg?logo=google-scholar&labelColor=4f4f4f&color=3388ee)||
## Contribute

We welcome contributions to [this repository](https://github.com/chwan1016/awesome-gnn-systems). To add new papers to this list, please update JSON files under `./res/papers/`. Our bots will update the paper list in `README.md` automatically. The citations of newly added papers will be updated within one day.
","['ch-wan', 'WWWzq-01', 'AmadeusChan']",0,,0.6,0,,,,,,7,,,
185863347,MDEwOlJlcG9zaXRvcnkxODU4NjMzNDc=,just-the-class,kevinlin1/just-the-class,0,kevinlin1,https://github.com/kevinlin1/just-the-class,"A modern, highly customizable, responsive Jekyll template for course websites.",0,2019-05-09 20:00:42+00:00,2025-03-01 09:32:06+00:00,2025-01-27 18:30:17+00:00,https://kevinl.info/just-the-class/,171,302,302,SCSS,1,0,1,0,1,1,110,0,0,0,mit,1,1,0,public,110,0,302,main,1,,"---
layout: home
title: Just the Class
nav_exclude: true
permalink: /:path/
seo:
  type: Course
  name: Just the Class
---

# Just the Class

Just the Class is a GitHub Pages template developed for the purpose of quickly deploying course websites. In addition to serving plain web pages and files, it provides a boilerplate for:

- [announcements](announcements.md),
- a [course calendar](calendar.md),
- a [staff](staff.md) page,
- and a weekly [schedule](schedule.md).

Just the Class is a template that extends the popular [Just the Docs](https://github.com/just-the-docs/just-the-docs) theme, which provides a robust and thoroughly-tested foundation for your website. Just the Docs include features such as:

- automatic [navigation structure](https://just-the-docs.github.io/just-the-docs/docs/navigation-structure/),
- instant, full-text [search](https://just-the-docs.github.io/just-the-docs/docs/search/) and page indexing,
- and a set of [UI components](https://just-the-docs.github.io/just-the-docs/docs/ui-components) and authoring [utilities](https://just-the-docs.github.io/just-the-docs/docs/utilities).

## Getting Started

Getting started with Just the Class is simple.

1. Create a [new repository based on Just the Class](https://github.com/kevinlin1/just-the-class/generate).
1. Update `_config.yml` and `README.md` with your course information. [Be sure to update the url and baseurl](https://mademistakes.com/mastering-jekyll/site-url-baseurl/).
1. Configure a [publishing source for GitHub Pages](https://help.github.com/en/articles/configuring-a-publishing-source-for-github-pages). Your course website is now live!
1. Edit and create `.md` [Markdown files](https://guides.github.com/features/mastering-markdown/) to add more content pages.

Just the Class has been used by instructors at Stanford University ([CS 161](https://stanford-cs161.github.io/winter2021/)), UC Berkeley ([Data 100](https://ds100.org/fa21/)), UC Santa Barbara ([CSW8](https://ucsb-csw8.github.io/s22/)), Northeastern University ([CS4530/5500](https://neu-se.github.io/CS4530-CS5500-Spring-2021/)), and Carnegie Mellon University ([17-450/17-950](https://cmu-crafting-software.github.io/)). Share your course website and find more examples in the [show and tell discussion](https://github.com/kevinlin1/just-the-class/discussions/categories/show-and-tell)!

### Local development environment

Just the Class requires no special Jekyll plugins and can run on GitHub Pages' standard Jekyll compiler. To setup a local development environment, clone your template repository and follow the GitHub Docs on [Testing your GitHub Pages site locally with Jekyll](https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/testing-your-github-pages-site-locally-with-jekyll).
","['kevinlin1', 'mattxwang', 'ykharitonova', 'addcninblue', 'jesse-wei', 'akindofyoga']",0,,0.84,0,,,,"blank_issues_enabled: false
contact_links:
  - name: Ask a question
    url: https://github.com/kevinlin1/just-the-class/discussions
    about: Ask questions and discuss with other community members
",,7,,,
12723124,MDEwOlJlcG9zaXRvcnkxMjcyMzEyNA==,versor,wolftype/versor,0,wolftype,https://github.com/wolftype/versor,Versor Geometric Algebra Library,0,2013-09-10 07:30:59+00:00,2025-02-27 14:58:16+00:00,2024-11-11 20:01:18+00:00,wolftype.github.io/versor/devel/html/,2462,301,301,C++,1,1,1,1,0,0,48,0,0,12,,1,0,0,public,48,12,301,devel,1,,"CSS: scripts/style.css


<script type=""text/javascript""
  src=""https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"">
</script>


<img src = ""http://versor.mat.ucsb.edu/resources/images/cshape.gif"" width = 80>
<img src = ""http://versor.mat.ucsb.edu/resources/images/twine_0.gif"" width = 100>
<img src = ""http://versor.mat.ucsb.edu/resources/images/twist_04_bw.gif"" width = 400>
<img src = ""http://versor.mat.ucsb.edu/resources/images/lox_0.jpg"" width = 100>

Versor (libvsr)
===

A (fast) Generic C++ library for Geometric Algebras, including Euclidean, Projective, Conformal, Spacetime (etc).
---
### Currently tested on Linux and Mac OS X ###

[Homepage (versor.mat.ucsb.edu)](http://versor.mat.ucsb.edu)

[Documentation](http://wolftype.github.io/versor/devel/html/)

[Combinatorics Cheat Sheet](http://versor.mat.ucsb.edu/masters_appendix.pdf)

[Bibtex (for referencing this work in your paper)](http://versor.mat.ucsb.edu/bibtex.txt)


**Versor** is a C++ Library for **Geometric Algebra**, sometimes called **Clifford Algebra**, a system for encoding geometric concepts numerically.

The headers generate optimized code at compile-time through template metaprogramming.  The core of the library
is under 150kb, and supports arbitrary dimensions and metrics (limited by your compiler...).

The library can be used as a math-only, or as an application with built-in graphics.  Both OpenGL and OpeGLES draw routines are supported.


Lead Developer: Pablo Colapinto
`gmail: wolftype`


## CONTENTS: ##

* [Recent Changes](#recentchanges)
* [Compilation](#compilation)
* [Introduction](#introduction)
* [Generators](#generators)
* [What the Point is](#whatthepointis)
* [Basics](#basics)
* [Gui](#gui)
* [Operators](#operators)
* [Elements](#elements)

Recent Changes
---

Some major revisions have been enacted that change the API.

* namespaces are used to distinguish between algebras:
	* **`vsr::nga::`** is the namespace for generic n-dimensional functions
	* **`vsr::cga::`** is the namespace for 3D Conformal Geometric Algebra
		* For example **`vsr::nga::Round::`** calls the generic implementation of n-D round elements whereas **`vsr::cga::Round::`** calls the 3D CGA specification
* all static function structs (**`Round::`**, **`Flat::`**, **`Tangent::`**) are written in full (as opposed to **`Ro::`**, **`Fl::`** and **`Ta::`**).  We note these are Capitalized, since earlier versions of the devel branch used lowercase namespaces.
* **`Construct::`** is now used to construct useful objects within a particular namespace
	* example: **`Construct::point(0,1,0)`** constructs a point at coordinate `(0,1,0)`
* header folders **`/detail /space /form /draw`** and **`/util`** organize the various files



## LINKS: ##

[Mailing List](http://lists.create.ucsb.edu/mailman/listinfo/versor) | [AlloSphere Research Group](http://www.allosphere.ucsb.edu/)

## PUBS: ##

See also my page at [Academia.edu](https://ucsb.academia.edu/PabloColapinto)

My disseration, [Articulating Space](http://versor.mat.ucsb.edu/ArticulatingSpace.pdf), introduces geometric algebra and explores applications to parametric design. My [Master's Thesis](http://wolftype.com/versor/colapinto_masters_final_02.pdf) before it, serves as an introduction as well.

At C++ Now in Aspen I gave [a Presentation on Metaprogramming Implementation Details](https://www.youtube.com/watch?v=W4p-e-g37tg)




> As long as algebra and geometry have been separated, their progress have been slow and their uses limited; but when these two sciences have been united, they have lent each mutual forces, and have marched together towards perfection. 
>
> -Joseph Louis Lagrange


> No attention should be paid to the fact that algebra and geometry are different in appearance.
>
> -Omar Khayyám


> L’algèbre n’est qu’une géométrie écrite; la géométrie n’est qu’une algèbre figurée.
>
> -Sophie Germain


> If you want to see, learn how to act
>
> -Heinz von Foerster

## OTHER NICE SOFTWARE: ##
* [Cinderella](http://www.cinderella.de/tiki-index.php) Standalone GA interface
* [GAViewer](http://www.science.uva.nl/ga/viewer/content_viewer.html) Standalone GA interface
* [Gaigen](http://staff.science.uva.nl/~fontijne/g25.html) An Implementation Generator
* [CluCalc/CluViz](http://www.clucalc.info/) Standalone and Library
* [Gaalop](http://www.gaalop.de/) A precompiler to optimize CluScripts
* [Gaalet](http://sourceforge.net/apps/trac/gaalet/) An expression template library whose backend is somewhat similar to Versor's
* [versor.js](http://www.github.com/weshoke/versor.js) A javascript port of this library


COMPILATION
---

For this version you need C++11 support, which is now common (gcc 4.7 or higher or clang 3.2 or higher) and for graphics support you'll want glew.
See the [Troubleshooting](#TROUBLESHOOTING) section below for instructions on installing glew.

	git clone https://github.com/wolftype/versor.git

You'll need to initialize the submodules to build any graphics examples:

	cd versor
	git submodule update --init --recursive

To build library

	./build.sh

To build library and examples:

	./build.sh --examples

To build library without graphics:

	./build.sh --math

To compile and run programs in scratch/projects:

	./run.sh scratch/projects/<folder>/<filename>

Note that not all projects in the scratch folder will compile as I am in the process of updating them in this development branch.

You can add your own file to the list of targets by adding it to /examples or scratch/projects/<yourname>/<your_cpp_file> and re-running `./build.sh`.

INSTALLATION
---
Hmmm, haven't added install options yet!  Will do so soon I promise.

Use Cases
---

1. A math library:
---

	#include <vsr/vsr.h>

	using Vec = vsr::euclidean_vector<3,float>;  		//<-- A 3 dimensional euclidean vector defined over floats
	using Biv = vsr::euclidean_bivector<3,float>;   //<-- A 3 dimensional bivector or ""directed area element""

	int main(){

		auto v = Vec(1,2,3);		  			              //<-- A 3D vector at coordinates 1,2,3;

		v.rotate( Biv::xy * .25 ).print();		        //<-- Rotate the vector in the xy plane and print result

		return 0;
	};


2. A stand-alone application (with window and gui)
---
  (for now, please see examples/xEmptyProject.cpp for an example)


While fully enabling arbitrary metric spaces, *Versor* has a lot of built-in functionality for specifically working with Conformal Geometric Algebra of 3D space, which is THE way to model all Euclidean transformations:

	Point p = Round::point(0,0,0);

	auto tp = p.translate(x, y, z);

	auto rp = p.rotate( Biv::xy * theta)


TROUBLESHOOTING
---

* You may need to install Glew on newer macs.  Best way to do this is with brew:

    brew install glew

* You'll need C++11 support on your compiler (See makefile notes below). For C++11 you'll want clang 3.2 (mac) or above or gcc 4.7 or above (linux).
* On Linux: you may need to sudo apt-get install libxmu-dev libxi-dev
* Alternatively an earlier version of Versor is available at [github.com/wolftype/versor_1.0.git](https://github.com/wolftype/versor_1.0.git)
This older version runs just as fast, but is strictly 3D CGA (i.e. R4,1 metric) since I generated headers ahead of time.


INTRODUCTION
---

_Versor_ provides operations and draw routines for Euclidean and Conformal Geometric Algebras,
a relatively new spatial computing model used by physicists, engineers, and artists. _Versor_ is designed to make graphical
experimentation of geometric algebra within a C++ environment easier. You can use this library to draw geometrical things, explore
spherical and hyperbolic spaces, transformations, design robots, etc.
I am using it for my PhD on bio-inspired engineering.

I first developed _Versor_ while reading ""Geometric Algebra for Computer Science"" by Leo Dorst, Daniel Fontijne, and Stephen Mann.
It's a fantastic book and if you're reading this you should also consider reading that.


License
---
This software is licensed under the  FreeBSD (2-clause) open source license (see copyrights/COPYRIGHT).  This is a permissive and compatible open source license -- you are pretty much free to do what you want with the source code.  In addition, you are strongly encouraged to email me to let me know how it is being used, so that we can all learn more.

Built to aid in my modelling of organic forms, the initial development was partially funded by the Olivia Long Converse Fellowship for Botanic research, courtesy of the Graduate Division at the University of California in Santa Barbara.  It has also received support from the Robert W. Deutsch Foundation.

---

One quick word: clifford algebras and the spatial relationships they embody can often feel abstract and daunting.  But it's a twisty, boosty ride, full of weird discoveries.  You're bound to make some, so have fun!

---


#### BACKGROUND
The homogenous 5D CGA model used here was initially proposed by David Hestenes, Hongbo Li, and Alan Rockwood in 2001, and given full form and weight through the excellent
and careful work of Leo Dorst, Joan and Anthony Lasenby, and Eduardo Bayro-Corrochano, and others.  These researchers' writings have helped me quite a bit.  CGA is particular breed of _Clifford Algebras_ (also known as Geometric Algebras),
which operate upon combinatoric hypercomplex vector spaces that emerged from William Clifford's attempt to fuse Hamilton's quaternions with Grassmans' extension algebras.  Thus
_transformations_ were married with a system of _abstraction_.  For more information, take a look at the [links](#links) to the sites at the bottom of
this page.  For instance, for practical applications in robotics and ""Geometric Cybernetics"", see Eduardo Bayro-Corrochano's work.  For some
very helpful algorithms in rigid body dynamics and gravitational physics see the variety of publications by Joan and Anthony Lasenby.  To get at the beginning of it all, read David Hestenes' _New Foundations for Classical Mechanics_.



#### SPEED
Typical matrix operation libraries have templated inlined functions for Vector and Matrix multiplication.  Versor
is similar, but on steroids, where _vectors_ and sparse _matrices_ of various sizes are all just called _multivectors_ and represent geometric
elements beyond just xyz directions and transformation matrices. Circles, lines, spheres, planes, points are all algebraic elements, as are
operators that spin, twist, dilate, and bend those variables.  Both these elements and operators are _multivectors_ which multiply together in many many many different ways.


#### What's new?

Versor compiles much faster than before, and without any silly predetermined list
of allowable operations or types.  Most notably, arbitrary metrics are now possible.  For example,
the xRoots.cpp example calculates all the Euclidean 4D reflections of a couple of point groups
(F4 and D4, namely). So you can hypercube and polychoron away (8D cubes no problem!).  Number of dimensions
allowed are somewhat limited by your compiler infrastructure -- let me know if you have a need that is not being met!

As for CGA, all the Pnt, Vec, Dll notation remains as before, but i've started adding utility functions
since it helps people out.


	auto pa = Round::point( 1,0,0 );
	auto pb = Round::point( 0,1,0 );
	auto pc = Round::point(-1,0,0 );
	auto circle = pa ^ pb ^ pc;

	Draw(c);

#### How does it work?

If you like functional template metaprogramming, take a look at the code
and please let me know what you think.  If you don't, then I wouldn't . . .
But if you have ideas or questions please do not hesitate to contact me.

WHAT THE POINT IS
---

GA combines many other maths (matrix, tensor, vector, and lie algebras). It is **holistic**. CGA uses a particular mapping (a conformal one) of 3D Euclidean space to a
4D sphere. Operations on that hypersphere are then projected back down to 3D. That how it works in a nutshell.

A fuller treatment of this question (er, the question of why we do this) can be found in my [Master's thesis on the subject](http://wolftype.com/versor/colapinto_masters_final_02.pdf).  But basically,
Geometic Algebra offers a particular richness of spatial expression.  Imagine needing glasses and not knowing you needed glasses.  Then, when you do get glasses, the world changes
unexpectedly.  GA is like glasses for the inside of your brain.  _Conformal_ Geometric Algebra, especially the 5D variety enlisted here, are like x-ray glasses.  One
point of clarification that occurs are **disambiguations** of previously collapsed concepts.

For instance, the main disambiguation, is that between a _Point_ in space and a _Vector_ in space.
A Point has no magnitude, but a Vector does.  A Point has no direction, but a Vector does. Points are _null_ Vectors.  We can make them
by writing

	Vec( 1,0,0 ).null();

* Points are null Vectors
* Points square to 0
* The dot (inner) product of two Points returns their squared distance
* The wedge (outer) product of two Points returns a Point Pair

More on that last point later . . . there are various binary operators defined (mainly three).  We can introduce one right now, which is the **dot** or **inner** product.
In mathematics, the inner product of two points `pa` and `pb` is written \\(p_{a} \rfloor p_{b}\\).  In _Versor_ we use the `<=` operator:

	Point pa = Vec(1,0,0).null();
	Point pb = Vec(-1,0,0).null();
	Scalar squaredDist = ( pa <= pb ) * -2;

which in this case would return a Scalar value of `4`.  The `-2` is there since the inner product really returns **half the negative** squared distance.
We can extract the Scalar into a c++ double like so:

	double squaredDist = ( pa <= pb )[0] * -2;

Points thought of as Spheres (really, Dual Spheres, more on _Duality_ later): they are Spheres of zero radius.  As such they are a type of _Round_ element.  We can also build points this way:

	Round::null( 1,0,0 );

or you can pass in another element

	Round::null( Vec(1,0,0) );

or use the built-in method

	Point pa = Vec(1,0,0).null();

Points can also be made with the macro `PT`

	Point pa = PT(1,0,0);

which is just ""syntactic sugar"" for `Vec(1,0,0).null()`

Speaking of Spheres, we can also make spheres with a radius this way:

	DualSphere dls = Round::dls( Vec( 1,0,0 ).null(), 1 );

or

	DualSphere dls = Round::dls( Vec( 1,0,0 ), 1 );

or, specifying the radius first and then the coordinate:

	DualSphere dls = Round::dls( 1 /* <--radius */ , 1,0,0 )

all of which give a dual sphere of radius 1 at coordinate 1,0,0;


BASICS
---

_Versor_ is named after the one of the basic category of elements of geometric algebra.
A **versor** is a type of **multivector** which can be used to compose geometric transformations,
namely reflections, translations, rotations, twists, dilations, and transversions (special conformal transformations).

More on all of those transformations later.

In Versor, a `Vector` (or `Vec`) is a typical Euclidean 3D element.  It can be built in the normal way:

	Vec v(1,2,3);

Some built-in Vectors exist:

	Vec::x x; //<-- X Direction Unit Vector Vec(1,0,0)
	Vec::y y; //<-- Y Direction Unit Vector Vec(0,1,0)
	Vec::z z; //<-- Z Direction Unit Vector Vec(0,0,1)

A `Vector` can be spun around using a `Rotor`, which is exactly like a quaternion.  However, whereas quaternions are often built by specifying an axis
and an angle, rotors are built by specifying the **plane** of rotation.  Eventually this will make much more sense to you: in general **planes** are what we
will be using to transform things.  For instance, a reflection is a reflection in a plane.  As we will see, planes can become **hyperplanes** which will allow for more extraordinary transformations.

The first completely new element to introduce is the `Bivector`, which is the plane we will use to generate our `Rotor`.  Bivectors represent **directed areas** and are __dual__
to the cross product: the cross product of two vectors in typical vector algebra returns a vector normal to the plane they define.  So it is not completely new,
but just sort of new.

Bivectors are also just three elements long, and are built the same way Vectors are.

	Biv b(1,2,3);

Some built-in Bivectors exist:

	Biv::xy xy; //<-- XY Counterclockwise Unit Area Biv(1,0,0)
	Biv::xz xz; //<-- XZ Counterclockwise Unit Area Biv(0,1,0)
	Biv::yz yz; //<-- YZ Counterclockwise Unit Area Biv(0,0,1)

While it is perfectly valid to write `Vector`, `Bivector` and `Rotor`, you'll notice I've truncated them to their three letter nicknames, `Vec` and `Rot`.
That's up to you: Both long-name and nick-name versions are valid in libvsr (they are typedef'ed to each other).

	Biv b = Biv::xy;
	double theta = PIOVERTWO;
	Vec v1 = Vec::x.rot( b * theta )

You can also generate rotors using `Gen::rot( <some bivector> )`  In fact, all transformations can be generated this way, and then later applied to arbitrary elements.
For instance, `Motors` can be generated which translate and rotate an element at the same time.  This is also called a _twist_.

	Motor m = Gen::mot(<some dual line>); 	//<-- Makes A Twisting Motor around Some Dual Line
	Point p = Vec(0,0,0).null().sp(m);		//<-- Applies above motor to a Point

You'll notice there are _dual_ versions of elements: as in a `DualLine` (or `Dll` for short).  That's because in the real world of abstract geometry, there are usually
two ways of defining an element.  For instance, we can build a _direct_ `Line` on the Y-axis by wedging two points together, along with infinity:

	Line lin = Vec(0,0,0).null() ^ Vec(0,1,0).null() ^ Inf(1);

Or we can define a line by the bivector plane that it is normal to, and a support vector that determines how far away the line is from the origin.  To convert the above
line into its dual representation, we just call the dual() method:

	Dll dll = lin.dual();

For those who are interested, this dual representation is isomorphic to the Plücker coordinates, which are used in screw theory to twist things around.  Here, too, we can use
dual lines to generate transformations which twist things around them.


Gui
---

The examples/*.cpp files include bindings to the GLV framework for windowing and user interface controls. This provides the minumum necessary glue to get started quickly building your own GA based graphics applications.

The interface has a built in gui, mouse info, and keyboard info stored.

    //... a member of your App
	Circle circle;
    //...in App::setup()
	objectController.attach(&circle);


Putting the above code inside your application will enable you to click and modify geometric elements by hitting the ""T"", ""R"" and ""S"" keys (for translate, rotate, and scale)
Hit any other key to deselect all elements.


[**BUILT-IN INTERFACE**]

|                                 |                                             |
Key                                 | Response
------------------------------      | ------------------------------------------
`~`                                 | Toggle full screen.
`SHIFT` + `Mouse` or `Arrow Keys`   | Translate the camera  in x and z directions.
`CTRL`+ `Mouse` or `Arrow Keys` 	| Rotate the camera
`ALT` +`Arrow Keys`              	| Rotate the model view around.
`T`                                 | Translate an Element
`R`                                 | Rotate an Element
`S`                                 | Scale an Element
`Tab`                               | Switch from navigation mode (default) to object manipulation mode
Any other key						| Release all Elements



OPERATORS
---

The elements of the algebra are geometric entities (circles, planes, spheres, etc) and operators (rotations, translations, twists, etc) which
act on the elements of the algebra.  All are known as _multivectors_ since they are more than just your typical vectors.

Multivector elements are most often combined using three overloaded binary operators:

The **Geometric** Product of elements `A` and `B`:

	A * B

multiplies two multivector elements together.  This is most useful when multiplying one by the inverse of another (see `!` operator, below).

The **Outer** Product of elements `A` and `B`:

	A ^ B

""wedges"" two multivectors together.  Its from Grassman's algebra of extensions, and can be thought of as a way of creating higher dimensions from smaller ones.
For instance, wedging two `Vectors` (directed magnitudes) together returns a `Bivector` (a directed Area).  Wedging two `Points` together returns a `PointPair`.
Wedging three `Points` together returns a `Circle`.

The **Inner** Product of elements `A` and `B`:

	A<=B


There is also a **Commutator** product (differential)

	A%B

And a few overloaded operations, including,

The Inverse:

	!A

returns \\(A^{-1}\\)

The Reverse:

	~A

returns \\(\tilde{A}\\)


And finally, since I ran out of overloadable operators, some basic methods

	A.conj()

which returns \\(\bar{A}\\)

	A.inv()

which returns \\(\hat{A}\\)

In summary:

| Versor     | Math                                              |                                          Description                                         |   |   |
| ---------- | ------------------------------------------------- | :------------------------------------------------------------------------------------------: | - |
| `A * B`    | \\(AB\\)                                          | Multiplies two elements together (and, in the case of A * !B finds ratios between elements). |
| `A ^ B`    | \\(A \wedge B\\)                                  |             Wedges two elements together (builds up higher dimensional elements).            |
| `A <= B`   | \\(A \rfloor B\\) or \\(\boldsymbol{a} \cdot B\\) |             Contracts A out of B (returns the part of B ""least like A"", sort of).            |
| `A % B`    | \\(A \times B\\)                                  |                         Commutator, equal to \\(\frac{1}{2}(AB-BA)\\)                        |
| `!A`       | \\(A^{-1}\\)                                      |                                       The Inverse of A.                                      |
| `~A`       | \\(\tilde{A}\\)                                   |                                       The Reverse of A.                                      |
| `A.conj()` | \\(\bar{A}\\)                                     |                                         Conjugation.                                         |
| `A.inv()`  | \\(\hat{A}\\)                                     |                                          Involution.                                         |


ELEMENTS
---

To make the process of writing code faster, all elements of the algebra are represented by types 3 letters long.
Alternatively, you can also use the long-form name.

[**BASIC ELEMENTS**]
Type  |  Long Form   		     |     Descrription                                                        |
----- | ---------------- | :---------------------------------------------------------: |
_Euclidean_             ||
`Sca` | `Scalar`         |                         A real number
`Vec` | `Vector`         |  A Directed Magnitude, or 3D Vector, typical cartesian stuff
`Biv` | `Bivector`       | A Directed Area. Use them to make Rotors: `Gen::Rot( Biv b )`
`Tri` | `Trivector`      |                   A Directed Volume Element

_Round_                 ||
`Pnt` | `Point`          |           A Null Vector: `Pnt a = Vec(1,0,0).null()`
`Par` | `PointPair`      |    A 0-Sphere (Sphere on a Line): `Par par = Pnt a ^ Pnt b`
`Cir` | `Circle`         |         A 1-Sphere: `Cir cir = Pnt a ^ Pnt b ^ Pnt c`
`Sph` | `Sphere`         |     A 2-Sphere: `Sph sph = Pnt a ^ Pnt b ^ Pnt c ^ Pnt d`
`Dls` | `DualSphere`     |            Typedef'ed as a point: `typedef Pnt Dls`

_Flat_                  ||
`Lin` | `Line`           |        A Direct Line: e.g. `Lin lin = Par par ^ Inf(1)`
`Dll` | `DualLine`       |            A Dual Line: e.g. `Dll dll = lin.dual()`
`Pln` | `Plane`          |       A Direct Plane: e.g. `Pln pln = Cir cir ^ Inf(1)`
`Dlp` | `DualPlane`      |                A Dual Plane: e.g. `Dlp dlp = `
`Flp` | `FlatPoint`      |

_Versors_               ||
`Rot` | `Rotor`          |            Spins an Element (as a Quaternion would)
`Trs` | `Translator`     |                     Translates an Element
`Dil` | `Dilator`        |                       Dilates an Element
`Mot` | `Motor`          |                Twists an Element along an axis
`Trv` | `Transversor`    |                        Bends an Element about the Origin
`Bst` | `Booster`        |                        Bends an Element around an ""Orbit""

_Abstract_              ||
`Mnk` | `MinkowskiPlane` |
`Pss` | `Pseudoscalar`   |
`Inf` | `Infinity`       |


There are others as well (for instance, affine planes, lines, and points) but the above are more than sufficient to start with.
There are also built in macros, for instance

`EP`  			| Sphere At the Origin.
`EM`  			| Imaginary Sphere at the Origin.
`PT(x,y,z)`  	| A null Point at x,y,z

`EP` and `EM` can be invoked instead of `Inf` to work in non-Euclidean metrics ( Spherical and Hyperbolic, respectively)

Many Euclidean elements can be drawn by invoking Draw::Render(<element>).  Some can't (yet) either because it wasn't obvious
how to draw them (e.g the scalar) or because I just didn't figure out how to do it or because I forgot or was lazy.  If you
want something to be drawable, let me know and I'll add it in.  Or try adding it in yourself and send a pull request via github.

All elements can be dualized by invoking their `dual()` method

All elements can be reflected over spinors with the `sp(<spinor>)` method

All elements can be reflected over versors with the `re(<versor>)` method

The versors are constructed by the geometric entities, typically by using the `Gen::` routines.  Operators can also be acted on by operators -- you can rotate a translation, or twist a boost.



COMMON CONFORMAL FUNCTIONS
---

`vsr_generic_op.h` and `vsr_cga3D_op.h` contain the bulk of the functions for generating elements from other elements.  Some guidelines:

* `Gen::` methods **generate** or otherwise operate on versors
* `roond::` methods create or otherwise operate on **Round** elements (Points, Point Pairs, Circles, Spheres)
* `Flat::` methods create or otherwise operate on **Flat** elements (Lines, Dual Lines, Planes, Dual Planes, or Flat Points)
* `Tangent::` methods create or otherwise operate on **Tangent** elements (Tangent Vectors, Tangent Bivectors, Tangent Trivectors)


GENERATORS
---

|       |                                     |
Returns | Function                            | Description
------- | ----------------------------------- | -------------------------------------------------
Rot     | Gen::rot( const Biv& b );           | //<-- Generate a Rotor from a Bivector
Trs     | Gen::trs( const Drv& v);            | //<-- Generate a Translator from a Direction Vector
Mot     | Gen::mot( const Dll& d);            | //<-- Generate a Motor from a Dual Line
Dil     | Gen::dil( const Pnt& p, double amt );| //<-- Generate a Dilator from a Point and an amount
Trv     | Gen::trv( cont Tnv& v);             | //<-- Generate a Transveror from a Tangent Vector
Bst     | Gen::bst( const Par& p);            | //<-- Generate a Booster from a Point Pair


REFLECTIONS
---

In addition to the above ""even"" spinors, we can also reflect.  Reflections (in a sphere, circle, or point pair, or over a line or plane ) can be calculated by writing

	Pnt p = PT(1,0,0);
	Pnt r = p.re( CXY(1) ); //Reflection of a point in a circle
	r = r / r[3]; 			    //Renormalization of a point

The re() method calculates `v.re(C)` as `C*v.inv()*~C` where inv() is an involution. With a versor `C` and an element `v` you might also try `C * v * !C`.  Inversion in a circle or a sphere may change the
weight of the element (for a Point at x, it will change it by x^2)


LINKS
---

* [Some Video Demos of Versor](http://vimeo.com/wolftype)
* [The Good Book: _Geometric Algebra for Computer Science_](http://www.geometricalgebra.net/)
* [GA Bookmarks on Delicious](http://www.delicious.com/tag/geometricalgebra)
* [GA Google Group](https://groups.google.com/forum/?fromgroups#!forum/geometric_algebra)
* [David Hestenes' Geometric Calculus Page](http://geocalc.clas.asu.edu/)
* [University of Amsterdam Intelligent Systems Lab](http://www.science.uva.nl/research/isla/)
* [Eduardo Bayro-Corrochano's Robotics Lab](http://www.gdl.cinvestav.mx/~edb/)
* [Cambridge University Geometric Algebra Research Group](http://www.mrao.cam.ac.uk/~clifford/)
* [Cognitive Systems at Christian-Albrechts-Universität zu Kiel](http://www.mrao.cam.ac.uk/~clifford/)

PAPERS
---

* 2011 [Versor: Spatial Computing With Conformal Geometric Algebra](http://wolftype.com/versor/colapinto_masters_final_02.pdf)
* 2012 [Boosted Surfaces: Synthesis of Meshes using Point Pair Generators in the Conformal Model](http://versor.mat.ucsb.edu/Boosted_Surfaces_submission_0113.pdf)
","['wolftype', 'bollu', 'tingelst', 'DrewRWx', 'wkoszek']",1,,0.72,0,,,,,,29,,,
173046012,MDEwOlJlcG9zaXRvcnkxNzMwNDYwMTI=,NLP-Resources,jia-zh/NLP-Resources,0,jia-zh,https://github.com/jia-zh/NLP-Resources,A useful list of NLP(Natural Language Processing) resources,0,2019-02-28 05:21:00+00:00,2025-03-04 06:49:29+00:00,2020-07-07 03:11:42+00:00,,129858,300,300,,1,1,1,1,0,0,76,0,0,0,apache-2.0,1,0,0,public,76,0,300,master,1,,"# NLP Resources
[![](https://img.shields.io/badge/update-anytime-success.svg)](https://github.com/jia-zh/NLP-Resources)
  
A useful list of NLP(Natural Language Processing) resources 
  
自然语言处理的相关资源列表，持续更新
  
  
## Contents
- [NLP Toolkits 自然语言工具包](#nlp-toolkits-自然语言工具包)
  - [Toolkits](#toolkits)
  - [Small Tools](#small-tools)
- [NLP Corpus 自然语言处理语料库](#nlp-corpus-自然语言处理语料库)
  - [Corpus Collection](#corpus-collection)
  - [Corpus Construction](#corpus-construction)
- [Learning Materials 学习资料](#learning-materials-学习资料)
  - [深度学习框架](#dl-f)
  - [ML Resources 机器学习书籍与资料](#ml-books-resources)
  - [NLP Resources NLP书籍与资料](#nlp-books-resources)
  - [Blogs and Courses 博客和课程](#blogs-courses)
- [NLP Technology 自然语言处理相关技术](#nlp-technology-自然语言处理相关技术)
  - [Bert Model](#bert-model)
  - [Text Modeling and Analysis](#text-modeling-and-analysis)
  - [Text Similarity](#text-similarity)
  - [Text Disambiguation](#text-disambiguation)
  - [Information Extraction](#information-extraction)
  - [Text Generation](#text-generation)
  - [Sequence Labeling](#sequence-labeling)
  - [Reading Comprehension](#reading-comprehension)
  - [QA System](#qa-system)
  - [Knowledge Graph](#knowledge-graph)
  - [Relation Extraction](#relation-extraction)
- [NLP Organizations 学术组织](#nlp-organizations-学术组织)
  - [中国大陆地区高校/研究所](#china-school)
  - [中国大陆地区企业](#china-company)
  - [中国香港/澳门/台湾地区](#china-hmt)
  - [新加坡/日本/以色列/澳大利亚](#east-asia)
  - [北美地区](#north-america)
  - [欧洲地区](#europe)
- [Reference](#reference)
  
  
## NLP-Toolkits 自然语言工具包

<a name=""toolkits""></a>
  
- Toolkits

  - [CoreNLP](https://github.com/stanfordnlp/CoreNLP)： a set of natural language analysis tools written in **Java**，by Stanford

  - [NLTK](http://www.nltk.org/)：a **Python** Natural Language Toolkit includes corpora, lexical resources and text processing libraries

  - [gensim](https://radimrehurek.com/gensim/)：[Github](https://github.com/RaRe-Technologies/gensim)，a **Python** library for topic modelling, document indexing and similarity retrieval with large corpora

  - [LTP](https://github.com/HIT-SCIR/ltp)：[语言技术平台](http://ltp.ai/)，中文NLP工具，支持**Java & Python**，by 哈工大

  - [jieba](https://github.com/fxsjy/jieba)：结巴中文分词，做最好的 **Python** 中文分词组件，现已覆盖几乎所有的语言和系统
  
  - [fast jieba](https://github.com/deepcs233/jieba_fast)：使用cpython重写了jieba分词库中计算DAG和HMM中的vitrebi函数，速度得到大幅提升。

  - [NLPIR](https://github.com/NLPIR-team/NLPIR)：**Java** 分词组件，by 中科院/北理工， [PyNLPIR](https://github.com/tsroten/pynlpir/blob/develop/docs/tutorial.rst) for **Python**

  - [HanLP](https://github.com/hankcs/HanLP)：中文NLP模型与算法工具包，支持**Java & Python**，by 上海林原信息科技有限公司

  - [THULAC](http://thulac.thunlp.org/)：高效的中文词法分析工具包，支持**C++ & Java & Python**，by 清华

  - [pkuseg](https://github.com/lancopku/pkuseg-python)：多领域中文分词工具包，支持细分领域分词，支持**Python**，by 北大

  - [FudanNLP](https://github.com/FudanNLP/fnlp)：中文NLP工具包、机器学习算法和数据集，支持**Java**，by 复旦

  - [Apache OpenNLP](https://github.com/FudanNLP/fnlp)：支持常见的NLP任务，比如分词、断句、词性标注、命名实体抽取、组块分析、解析和指代消解，支持**Java**，[官网](https://opennlp.apache.org/)

  - [SnowNLP](https://github.com/isnowfy/snownlp) 中文分词、词性标注、情感分析、文本分类（NB）、拼音转换（Trie树）、简繁转换（Trie树）、关键词提取（TextRank）、摘要提取（TextRank算法）、tf、idf、Tokenization、文本相似（BM25）
      
  - [Ansj Seg](https://github.com/NLPchina/ansj_seg) Ansj中文分词，支持**Java**


<a name=""small-tools""></a>
  
- Small Tools

  - [Chinese Cixing](https://github.com/liuhuanyong/ChineseCixing) 针对中文词语的笔画拆解，偏旁查询，拼音转换接口
    
  - [Chai Zi](https://github.com/kfcd/chaizi) 含开发词典可用以提供字旁和部件查询的拆字字典数据库
    
  - [Python Pinyin](https://github.com/mozillazg/python-pinyin) 将汉字转为拼音。可以用于汉字注音、排序、检索(Russian translation) 
    
  - [Nstools](https://github.com/skydark/nstools/tree/master/zhtools) 中文繁简体互转
  
  - [Query Correction](https://github.com/liuhuanyong/QueryCorrection) 基于用户词表，采用拼音相似度与编辑距离进行查询纠错

  - [Matplotlib 可视化最有价值的 50 个图表](http://mob.dataguru.cn/mportal.php?mod=view&aid=14479)

## NLP Corpus 自然语言处理语料库

<a name=""corpus-collection""></a>
  
- Corpus Collection

  - [中文 Wikipedia Dump](https://dumps.wikimedia.org/zhwiki/)
  
  - [NLP语料集合](https://github.com/SimmerChan/corpus) 自然语言处理，知识图谱相关语料。按照Task细分
  
  - [人民日报199801标注语料](https://pan.baidu.com/s/10_CQck5mKsKyfpA08slyFA)
  
  - [Sogou Labs](http://www.sogou.com/labs/resource/list_pingce.php) 互联网词库、中文词语搭配库、全网新闻数据（2012）、搜狐新闻数据（2012）、互联网语料库、链接关系库等
       
  - [中文聊天语料](https://github.com/codemayq/chaotbot_corpus_Chinese) chatterbot、豆瓣多轮、PTT八卦语料、青云语料、电视剧对白语料、贴吧论坛回帖语料、微博语料、小黄鸡语料
         
  - [领域中文词库](https://github.com/thunlp/THUOCL) IT、财经、成语、地名、历史名人、诗词、医学、饮食、法律、汽车、动物
         
  - [汉语词库](http://www.hankcs.com/nlp/corpus/tens-of-millions-of-giant-chinese-word-library-share.html) 各种类型词库如人名库、金融专业相关词、政府机关团体机构大全等

  - [中文依存语料库](http://www.hankcs.com/nlp/corpus/chinese-treebank.html) 第二届自然语言处理与中文计算会议（NLP&CC 2013）的技术评测中文树库语料
       
  - [微信公众号语料库](https://github.com/nonamestreet/weixin_public_corpus) 网络抓取的微信公众号的文章，包括微信公众号名字、微信公众号ID、题目和正文
       
  - [中文谣言微博数据](https://github.com/thunlp/Chinese_Rumor_Dataset) 从新浪微博不实信息举报平台抓取的中文谣言数据
  
  - [Tencent AI Lab Embedding Corpus](https://ai.tencent.com/ailab/nlp/embedding.html) A corpus on continuous distributed representations of Chinese words and phrases
  
  - [Word2vec Slim](https://github.com/eyaler/word2vec-slim) word2vec Google News model slimmed down to 300k English words
    
  - [Chinese Word2vec Model](https://github.com/to-shimo/chinese-word2vec)
     
  - [Chinese Word Vectors](https://github.com/Embedding/Chinese-Word-Vectors)
  
  - [NLP Chinese Corpus](https://github.com/brightmart/nlp_chinese_corpus) 维基百科中文词条、新闻语料、百科问答、社区问答、翻译语料
       
  - [中文诗歌古典文集数据库](https://github.com/chinese-poetry/chinese-poetry)

  - [Chinese RC Dataset](https://github.com/ymcui/Chinese-RC-Dataset) A Chinese Reading Comprehension Dataset
  
  - [Chinese Word Ordering Errors Detection and Correction Corpus](http://nlg.csie.ntu.edu.tw/nlpresource/woe_corpus/)
  
  - [中文文本分类数据集THUCNews](http://thuctc.thunlp.org/#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86THUCNews) 根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档
   
  - [公司名语料库](https://github.com/wainshine/Company-Names-Corpus) 公司名语料库、机构名语料库、公司简称、品牌词等
  
  - [中文人名语料库](https://github.com/wainshine/Chinese-Names-Corpus) 中文常见人名、中文古代人名、日文人名、翻译人名、中文姓氏、中文称呼、成语词典
     
  - [中文简称词库](https://github.com/zhangyics/Chinese-abbreviation-dataset)
    
  - [Chinese Xinhua](https://github.com/pwxcoo/chinese-xinhua) 中华新华字典数据库，包括歇后语，成语，词语，汉字
         
  - [对联数据集](https://github.com/wb14123/couplet-dataset)
  
  - [无忧无虑中学语文网](http://jyc.5156edu.com/) 常见中文词语工具，包括近义词、反义词、汉字拼音转换、简繁转换等
  
  - [EmotionLexicon](https://github.com/songkaisong/EmotionLexicon) 细粒度情感词典、网络词汇、否定词典、停用词典
  
  - [Chinese_Dictionary](https://github.com/guotong1988/chinese_dictionary) 同义词表、反义词表、否定词表
  
  - [Synonyms](https://github.com/huyingxi/Synonyms) 中文近义词工具包

  - [Chinese NLP Corpus](https://github.com/liuhuanyong/ChineseNLPCorpus) 中文自然语言处理的语料集合，包括语义词、领域共识、历时语料库、评测语料库等

  - [Chinese-Xinhua](https://github.com/pwxcoo/chinese-xinhua) 中华新华字典数据库。包括歇后语，成语，词语，汉字。

  - [CEC-Corpus](https://github.com/shijiebei2009/CEC-Corpus) 中文突发事件语料库（Chinese Emergency Corpus）
      
  - [NLP太难了系列](https://github.com/fighting41love/hardNLU)

<a name=""corpus-construction""></a>
  
- Corpus Construction
  
  - [Opencc Python](https://github.com/yichen0831/opencc-python) Python简繁转换
  
  - [Pinyin Python](https://github.com/mozillazg/python-pinyin) 汉字拼音转换工具（Python 版）
  
  - [Python模拟登陆](https://github.com/CriseLYJ/awesome-python-login-model) Python模拟登陆一些大型网站

  - [Baidu Baike Spider](https://github.com/jia-zh/Baidu-Baike-Spider) 基于Python的百度百科词条爬取
  
  - [Sina Weibo Spider](https://github.com/jia-zh/Sina-Weibo-Spider) 基于Java的新浪微博采集
  
  - [Sougou Words Collector](https://github.com/liuhuanyong/SougouWordsCollector) 搜狗输入法词库抓取与格式转换
  
  - [Baike Knowledge Schema](https://github.com/liuhuanyong/BaikeKnowledgeSchema) 面向百度百科与互动百科的概念分类体系抓取脚本
  
  - [Baike Info Extraction](https://github.com/liuhuanyong/BaikeInfoExtraction) 基于互动百科、百度百科、搜狗百科的词条infobox结构化信息抽取，百科知识的融合
  
  - [Baidu Index Spyder](https://github.com/liuhuanyong/BaiduIndexSpyder) 基于关键词的历时百度搜索指数自动采集
  
  - [Ali Index Spyder](https://github.com/liuhuanyong/AliIndexSpyder) 阿里商品指数抓取，包括淘宝采购指数、淘宝供应指数、1688供应指数

  - [新闻搜索引擎新闻爬取](https://github.com/jia-zh/News-Spider) 基于Scrapy框架的新闻搜索引擎爬虫，支持百度新闻、搜狗新闻、新浪新闻、360新闻和新华社搜索新闻。

  - [通用新闻类网站分布式爬虫](https://github.com/liubo0621/distributed-spider) 可提取新闻标题、时间、作者、正文等信息

## Learning Materials 学习资料
  
<a name=""dl-f""></a>

- 深度学习框架

  - Keras [官方文档](https://github.com/keras-team/keras)，[中文文档](https://keras-cn.readthedocs.io/en/latest/)，[例子](https://github.com/keras-team/keras/tree/master/examples)
  
  - Tensorflow [官方文档](https://github.com/tensorflow/tensorflow)，[中文文档](http://www.tensorfly.cn/)，[中文教程](https://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial)，[例子](https://github.com/aymericdamien/TensorFlow-Examples)， [Tensorflow Cookbook](https://github.com/taki0112/Tensorflow-Cookbook)
  
  - Pytorch [官方文档](https://github.com/pytorch/pytorch)，[中文文档](https://pytorch.apachecn.org/#/)，[例子](https://github.com/yunjey/pytorch-tutorial)，[资源](https://github.com/bharathgs/Awesome-pytorch-list)，[PyTorch实战指南](https://zhuanlan.zhihu.com/p/29024978)，[Awesome Pytorch List](https://github.com/bharathgs/Awesome-pytorch-list)，[pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial)

  - [如何用flask部署pytorch模型](https://zhuanlan.zhihu.com/p/35879835) 可延伸到其他深度学习模型的REST API部署
  
<a name=""ml-books-resources""></a>

- ML Resources 机器学习书籍与资料

  - [《统计学习方法》](https://github.com/jia-zh/NLP-Resources/blob/master/books/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_%E6%9D%8E%E8%88%AA.pdf) ML经典书籍，值得反复读，从公式推导到定理证明逻辑严谨，by 李航 

  - [《机器学习》](https://github.com/jia-zh/NLP-Resources/blob/master/books/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%91%A8%E5%BF%97%E5%8D%8E.pdf) 俗称西瓜书，机器学习入门必备，by 周志华 

  - [《深度学习（中文版）》](https://github.com/jia-zh/NLP-Resources/blob/master/books/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E4%B8%AD%E6%96%87%E7%89%88%20.pdf) 是一本皆在帮助学生和从业人员进入机器学习领域的教科书，以开源的形式免费在网络上提供[Github](https://github.com/exacity/deeplearningbook-chinese)，由学界领军人物 Ian Goodfellow、Yoshua Bengio 和 Aaron Courville 合力打造  
  
  - [Machine Learning](https://github.com/wepe/MachineLearning) 一些常见的机器学习算法的实现代码

  - [Deep Learning 500 Questions](https://github.com/scutan90/DeepLearning-500-questions) 以问答形式对常用的概率知识、线性代数、机器学习、深度学习、计算机视觉等热点问题进行阐述

  - [神经网络与深度学习](https://nndl.github.io/) 介绍神经网络与深度学习中的基础知识、主要模型（卷积神经网络、递归神经网络等）以及在计算机视觉、自然语言处理等领域的应用，by 邱锡鹏
  
  - 吴恩达老师课程学习笔记：[机器学习笔记](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)、[深度学习笔记](https://github.com/fengdu78/deeplearning_ai_books)

  - [Machine Learning Yearning](https://github.com/xiaqunfeng/machine-learning-yearning) 吴恩达老师根据自己多年实践经验整理出来的一本机器学习、深度学习实践经验宝典，重点不在于机器学习算法理论基础，而在于实践中使机器学习算法的实战经验

<a name=""nlp-books-resources""></a>
  
- NLP Resources NLP书籍与资料

  - [《数学之美》](https://github.com/jia-zh/NLP-Resources/blob/master/books/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E_%E5%90%B4%E5%86%9B.pdf) 生动形象，没有太多公式，属于科普性质，可以说是NLP入门必备，by 吴军
  
  - [《统计自然语言处理》](https://github.com/jia-zh/NLP-Resources/blob/master/books/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E7%89%88_%20%E5%AE%97%E6%88%90%E5%BA%86.pdf) 全面介绍了统计自然语言处理的基本概念、理论方法和最新研究进展，by 宗成庆
  
  - [《Neural Network Methods for Natural Language Proces》](https://github.com/jia-zh/NLP-Resources/blob/master/books/Neural%20Network%20Methods%20for%20Natural%20Language%20Proces_Yoav%20Goldberg.pdf) 利用神经网络的方法来进行自然语言处理任务，by Yoav Goldberg

  - [中文信息发展报告](https://cips-upload.bj.bcebos.com/cips2016.pdf) 中国中文信息学会2016年12月 发布的中文NLP总览报告，涵盖了中文和英文NLP主要的技术方向
    
  - [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) by Dan Jurafsky and James H. Martin
  
  - [Deep Learning for Natural Language Processing](http://nlp.fudan.edu.cn/xpqiu/slides/20160618_DL4NLP@CityU.pdf) 深度学习在自然语言处理中的应用，by 邱锡鹏

  - [NLP入门推荐书目（2019版）](https://zhuanlan.zhihu.com/p/58874484) 刘知远老师推荐NLP入门推荐参考书目
  
  - [nlp](https://github.com/duoergun0729/nlp) 一本开源的NLP入门书籍

<a name=""blogs-courses""></a>
  
- Blogs and Courses 博客和课程

  - NLP 中文博客：[52NLP](http://www.52nlp.cn/)、[码农场](http://www.hankcs.com/)、[剑指汉语自然语言处理](https://blog.csdn.net/FontThrone/column/info/16265)
  
  - NLP 英文博客：[Natural Language Processing Blog](https://nlpers.blogspot.com/)、[Language Log](http://languagelog.ldc.upenn.edu/nll/)、[Jay Alammar](http://jalammar.github.io/)

  - AI 博客：[Google AI Blog](https://ai.googleblog.com/)

  - Stanford NLP Courses：[Stanford CS224n Home](http://web.stanford.edu/class/cs224n/)、[Stanford CS224d 2015年NLP课程](https://www.youtube.com/playlist?list=PLmImxx8Char8dxWB9LRqdpCTmewaml96q)、[Stanford CS224d 2016年NLP课程](https://www.youtube.com/playlist?list=PLmImxx8Char9Ig0ZHSyTqGsdhb9weEGam)、[Natural Language Processing with Deep Learning (Winter 2017)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)
      
  - Oxford NLP Courses：[Oxford CS Deep NLP 2017](https://github.com/oxford-cs-deepnlp-2017)

  - Stanford ML/DL Courses：[CS 229 Machine Learning](https://stanford.edu/~shervine/teaching/cs-229/)、[CS 230 Deep Learning](https://stanford.edu/~shervine/teaching/cs-230/)
  
  - [Gt NLP Class CS 4650 and 7650](https://github.com/jacobeisenstein/gt-nlp-class)

  - [NLP Course](https://github.com/yandexdataschool/nlp_course) YSDA course in Natural Language Processing
      
## NLP Technology 自然语言处理相关技术

- [NLP Progress](https://github.com/yuquanle/NLP-progress) Repository to track the progress in NLP, including the datasets and the current state-of-the-art for the most common NLP tasks.

  - [关于NLP和机器学习之文本处理的你需要知道的一切](https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247495751&idx=1&sn=f2fd87880418e994f5503c426185e9a0&chksm=e9e1f9ccde9670dad7a7cbc7cbd557fbf589c241a391af9a7afbb43899ee1bb1f2a91bf9f026&mpshare=1&scene=1&srcid=#rd) 自然语言处理和机器学习中常见的文本预处理方法

<a name=""bert-model""></a>

- Bert Model

  - [从Word Embedding到Bert模型--自然语言处理预训练技术发展史](https://www.jiqizhixin.com/articles/2018-12-10-8) by 张俊林

  - [BERT](https://github.com/google-research/bert) TensorFlow code and pre-trained models for [BERT](https://arxiv.org/abs/1810.04805)
  
  - [Awesome Bert](https://github.com/Jiakui/awesome-bert) bert nlp papers、applications and github resources

  - [Awesome Bert NLP](https://github.com/cedrickchee/awesome-bert-nlp) A curated list of NLP resources focused on BERT, attention mechanism, Transformer networks, and transfer learning.

  - [The Illustrated BERT, ELMo, and co](https://jalammar.github.io/illustrated-bert/) The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)
  
  - [Bert as Service](https://github.com/hanxiao/bert-as-service) Using BERT model as a sentence encoding service, i.e. mapping a variable-length sentence to a fixed-length vector
  
  - [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT) A PyTorch implementation of Google AI's BERT model provided with Google's pre-trained models, examples and utilities.
  
  - [BERT Classification Tutorial](https://github.com/Socialbird-AILab/BERT-Classification-Tutorial)

  - [BERT Utils](https://github.com/terrifyzhao/bert-utils) BERT生成句向量，BERT做文本分类、文本相似度计算

  - [BERT BiLSTM CRF NER](https://github.com/macanv/BERT-BiLSTM-CRF-NER) Tensorflow solution of NER task Using BiLSTM-CRF model with Google BERT Fine-tuning

  - [BERT Chinese NER](https://github.com/ProHiryu/bert-chinese-ner) 使用预训练语言模型BERT做中文NER

  - [Bert时代的创新：Bert在NLP各领域的应用进展](https://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&mid=2247505634&idx=1&sn=34e163b399a3fcb9a899cb72aeef1bab&chksm=e99ee51bdee96c0d17695cfd83e5dbcc8467f3d3358c87461416c69d13473c775afbb29405ea&scene=0&xtrack=1&key=bf2c15454a31549f7efad466960381ace4531a8648d45603d0e2893798103f134bf9d3f013fdeb0c21f6adcb6ce9237c839319e061b05c6eb654d3b144f86baa247573af3a6987c1cb066363866b1002&ascene=1&uin=MTMzMDA4OTIwNA%3D%3D&devicetype=Windows+10&version=62060833&lang=zh_CN&pass_ticket=7qlKhPwlxXpE4z4zXH0cuqMVe4ktrt44xeFP0NC9tq3S7IKqcvXwX37pVvqrioUV) by 张俊林

<a name=""text-modeling-and-analysis""></a>

- Text Modeling and Analysis
  
  - [Self Attention Mechanism](https://github.com/roomylee/self-attentive-emb-tf) Simple Tensorflow Implementation of ""[A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)"" (ICLR 2017) 
  
  - [Encoder Decoder](https://github.com/jacoxu/encoder_decoder) Four styles of encoder decoder model by Python, Theano, Keras and Seq2Seq
  
  - [Seq2seq](https://github.com/farizrahman4u/seq2seq) Sequence to Sequence Learning with Keras
  
  - [Keras Language Modeling](https://github.com/codekansas/keras-language-modeling) Some code for doing language modeling with Keras, in particular for question-answering tasks
  
  - [CNN for Sentence Classification in Keras](https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras) Simple Keras Implementation of ""[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)"" (EMNLP 2014) 
  
  - [CNN for Classification](https://github.com/Shijihao/CNN_for_classification) Simple Pytorch Implementation of ""[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)"" (EMNLP 2014) 
  
  - [Awesome NLP Sentiment Analysis](https://github.com/haiker2011/awesome-nlp-sentiment-analysis) 情感分析领域相关的数据集、论文、开源实现
  
<a name=""text-similarity""></a>

- Text Similarity
  
  - [Cilin and Hownet](https://github.com/yaleimeng/Final_word_Similarity) 综合了同义词词林扩展版与知网（Hownet）的词语相似度计算方法
  
  - [Similarity Compute](https://github.com/liuhuanyong/SentenceSimilarity) 基于同义词词林，知网，指纹，字词向量，向量空间模型的句子相似度计算
  
  - [Siamese Sentence Similarity](https://github.com/liuhuanyong/SiameseSentenceSimilarity) 基于Siamese bilstm模型的相似句子判定模型，提供训练数据集和测试数据集

  - [SentenceSim](https://github.com/fssqawj/SentenceSim)，中文短文句相似度计算方法，包括基于知网、Onehot、word2vec、哈工大SDP及多个算法的融合以及LSTM算法
     
<a name=""text-disambiguation""></a>

- Text Disambiguation
  
  - [Word MultiSense Disambiguation](https://github.com/liuhuanyong/WordMultiSenseDisambiguation) 基于百科知识库的中文词语多义项获取与特定句子词语语义消歧
  
<a name=""information-extraction""></a>

- Information Extraction
    
  - [Open IE Papers](https://github.com/NPCai/Open-IE-Papers) This list containts OpenIE and ORE papers and resources
  
  - [关系抽取(分类)总结](http://shomy.top/2018/02/28/relation-extraction/) 对近几年(到2017)一些关系抽取/分类(Relation Extraction)的部分文献的一个简单总结

  - [LM-LSTM-CRF](https://github.com/jia-zh/NLP-Resourcesss/blob/master/Information%20Extraction.md) PyTorch Implementation of ""[Empower Sequence Labeling with Task-Aware Neural Language Model](http://arxiv.org/abs/1709.04109)"" (AAAI 2018)
  
  - [Named Entity Realtion Extraction](https://github.com/twjiang/NamedEntity_realtion_extraction) 基于句法分析的命名实体关系抽取程序
  
  - [Pytorch Relation Extraction](https://github.com/ShomyLiu/pytorch-relation-extraction) Pytorch 复现 [PCNN + MIL (Zeng 2015)](http://aclweb.org/anthology/D/D15/D15-1203.pdf) 与 [PCNN + ATT (Lin 2016)](http://nlp.csai.tsinghua.edu.cn/~lyk/publications/acl2016_nre.pdf)
  
  - [Zh NER TF](https://github.com/fighting41love/zh-NER-TF) A very simple BiLSTM-CRF model for Chinese Named Entity Recognition 中文命名实体识别 (TensorFlow)
  
  - [BERT BiLSTM CRF NER](https://github.com/macanv/BERT-BiLSTM-CRF-NER) Tensorflow solution of NER task Using BiLSTM-CRF model with Google BERT Fine-tuning
  
  - [Event Triples Extraction](https://github.com/liuhuanyong/EventTriplesExtraction) 基于依存句法与语义角色标注的事件三元组抽取
  
  - [Important Event Extractor](https://github.com/liuhuanyong/ImportantEventExtractor) 针对某一事件话题下的新闻报道集合，通过使用docrank算法，对新闻报道进行重要性识别，并通过新闻报道时间挑选出时间线上重要新闻
  
  - [Text Grapher](https://github.com/liuhuanyong/TextGrapher) 对文档进行关键信息提取，进行结构化，并组织成图谱组织形式，形成对文章语义信息的图谱化展示
  
  - [从零开始构建知识图谱](https://zhuanlan.zhihu.com/c_1018901137012928512) 知识图谱构建的知乎专栏
  
  - [Text Info Exp](https://github.com/Roshanson/TextInfoExp) TF-IDF、文本分类、聚类、词向量、情感识别、关系抽取等

<a name=""text-generation""></a>

- Text Generation
  
  - [Texar](https://github.com/asyml/texar) Toolkit for Text Generation and Beyond
  
  - [Awesome Text Generation](https://github.com/ChenChengKuan/awesome-text-generation) A curated list of recent models of text generation and application
  
  - [Ehud Reiter's Blog](https://ehudreiter.com/) 博客对NLG技术、评价与应用进行了深入的探讨

  - [Talk Latent](https://github.com/harvardnlp/Talk-Latent/blob/master/main.pdf) slides of ""Controlling Text Generation"" by Alexander Rush

<a name=""sequence-labeling""></a>

- Sequence Labeling
    
  - [Kashgari](https://github.com/BrikerMan/Kashgari) Simple and powerful NLP framework, build your state-of-art model in 5 minutes for named entity recognition (NER), part-of-speech tagging (PoS) and text classification tasks

<a name=""reading-comprehension""></a>

- Reading Comprehension
  
  - [CMRC 2017](https://github.com/ymcui/cmrc2017) The First Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2017)
  
  - [CMRC 2018](https://github.com/ymcui/cmrc2018) The Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC2018)
  
  - [Neural Reading Comprehension and Beyond](https://stacks.stanford.edu/file/druid:gd576xb1833/thesis-augmented.pdf) Danqi Chen（陈丹琦）博士毕业论文

  - [教机器学习阅读](http://rsarxiv.github.io/2016/06/18/%E6%95%99%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%98%85%E8%AF%BB/) 机器阅读理解的综述文章，系统地总结和对比了相关paper
    
<a name=""qa-system""></a>

- QA System
  
  - [AnyQ](https://github.com/baidu/AnyQ) FAQ-based Question Answering System by Baidu

  - [基于知识库的问答：seq2seq模型实践](https://zhuanlan.zhihu.com/p/34585912) 基于知识库的问答具体实现，[Github地址](https://github.com/wavewangyue/kbqa)
  
<a name=""knowledge-graph""></a>

- Knowledge Graph
  
  - [知识图谱技术与应用指南](https://www.jiqizhixin.com/articles/2018-06-20-4) 这是一份通俗易懂的知识图谱技术与应用指南

  - [Slides About Knowledge Graph](https://pan.baidu.com/s/1zS8Yye88bk7yAbQnyBdr0Q) 收集整理的知识图谱相关的Slides，[百度网盘链接](https://pan.baidu.com/s/1zS8Yye88bk7yAbQnyBdr0Q) 
百度网盘提取码：z5yb
  - [Agriculture Knowledge Graph](https://github.com/cjm1044642385/Agriculture_KnowledgeGraph) 农业领域的信息检索，命名实体识别，关系抽取，分类树构建，数据挖掘
  
  - [Person Relation Knowledge Graph](https://github.com/liuhuanyong/PersonRelationKnowledgeGraph) 中文人物关系知识图谱项目，包括中文人物关系图谱构建、基于知识库的数据回标、基于远程监督与bootstrapping方法的人物关系抽取、基于知识图谱的知识问答等应用

  - [Awesome Knowledge Graph](https://github.com/husthuke/awesome-knowledge-graph) 整理知识图谱相关学习资料，提供系统化的知识图谱学习路径。

  - [知识库构建前沿：自动和半自动知识提取](http://blog.sina.com.cn/s/blog_4caedc7a0102ewpj.html) by 微软亚洲研究院主管研究员 史树明

  - [详解自下而上构建知识图谱全过程](http://www.4u4v.net/xiang-jie-zi-xia-er-shang-gou-jian-zhi-shi-tu-pu-quan-guo-cheng.html?from=timeline) 自底向上构建知识图谱的全过程

  - [中文知识图谱API与工具、科研机构与算法框架](https://blog.csdn.net/sinat_26917383/article/details/66473253) 中国国内中文知识图谱API、工具、科研机构与算法框架的总结

  - [知识图谱的价值和应用](http://www.woshipm.com/it/1088237.html) 从**产品经理的角度**分析知识图谱的价值和应用
  
  - [军事知识图谱及问答](https://github.com/liuhuanyong/QAonMilitaryKG) 基于mongodb存储的军事领域知识图谱问答项目，包括飞行器、太空装备等8大类，100余小类，共计5800项的军事武器知识库，
    
<a name=""relation-extraction""></a>

- Relation Extraction

  - [关系抽取(分类)总结](http://shomy.top/2018/02/28/relation-extraction/) 关系抽取的解释、数据集介绍及2013-2017经典工作的总结
        
## NLP Organizations 学术组织
***排名不分先后，收集不全，欢迎完善***

- [ACL Anthology](https://aclanthology.info/)

- [NLP Conference Calender](http://cs.rochester.edu/~omidb/nlpcalendar/)
  
<a name=""china-school""></a>
  
- 中国大陆地区高校/研究所

  - [中科院计算所自然语言处理研究组](http://nlp.ict.ac.cn/index_zh.php)

  - [中科院自动化所自然语言处理研究组](http://nlp.ict.ac.cn/index_zh.php)

  - [中科院软件所中文信息处理实验室](http://www.icip.org.cn/zh/homepage/)

  - [清华大学自然语言处理与人文计算实验室](http://nlp.csai.tsinghua.edu.cn/site2/index.php/zh) ，   [实验室Github](https://github.com/thunlp)
  
  - [智能技术与系统国家重点实验室信息检索课题组](http://www.thuir.cn/)

  - [北京大学计算语言学教育部重点实验室](http://klcl.pku.edu.cn/)
  
  - [北京大学计算语言所](http://icl.pku.edu.cn/)

  - [北京大学计算机科学技术研究所语言计算与互联网挖掘研究室](http://59.108.48.12/lcwm/index.php?title=%E9%A6%96%E9%A1%B5)

  - [哈工大社会计算与信息检索研究中心](http://ir.hit.edu.cn/)

  - [哈工大智能技术与自然语言处理实验室](http://insun.hit.edu.cn/main.htm)
  
  - [哈工大机器智能与翻译研究室](http://mitlab.hit.edu.cn/)

  - [复旦大学自然语言处理研究组](http://nlp.fudan.edu.cn/)

  - [苏州大学自然语言处理组](http://nlp.suda.edu.cn/)

  - [苏州大学人类语言技术研究所](http://hlt.suda.edu.cn/)

  - [南京大学自然语言处理研究组](http://nlp.nju.edu.cn/homepage/)

  - [东北大学自然语言处理实验室](http://www.nlplab.com/)

  - [厦门大学自然语言处理实验室](http://nlp.xmu.edu.cn/)

  - [郑州大学自然语言处理实验室](http://nlp.zzu.edu.cn/)
  
<a name=""china-company""></a>
  
- 中国大陆地区企业

  - [微软亚洲研究院自然语言计算组](https://www.microsoft.com/en-us/research/group/natural-language-computing/)

  - [华为诺亚方舟实验室](http://www.noahlab.com.hk/)
  
  - [Tencent AI Lab NLP Center](https://ai.tencent.com/ailab/nlp/)
  
  - [百度自然语言处理部](https://nlp.baidu.com/)
  
  - [头条人工智能实验室（Toutiao AI Lab）](http://lab.toutiao.com/)
  
<a name=""china-hmt""></a>
  
- 中国香港/澳门/台湾地区
  
  - [CUHK Text Mining Group](http://www1.se.cuhk.edu.hk/~textmine/)（香港中文大学文本挖掘组）

  - [PolyU Social Media Mining Group](http://www4.comp.polyu.edu.hk/~cswjli/Group.html)（香港理工大学社交媒体挖掘组）

  - [HKUST Human Language Technology Center](http://www.cse.ust.hk/~hltc/)（香港科技大学人类语言技术中心）
  
  - [NLP<sup>2</sup>CT @ University of Macau](http://nlp2ct.cis.umac.mo/index.html)（澳门大学自然语言处理与中葡机器翻译实验室）

  - [National Taiwan University NLP Lab](http://nlg.csie.ntu.edu.tw/)（台湾大学自然语言处理实验室）

<a name=""east-asia""></a>
  
- 新加坡/日本/以色列/澳大利亚

  - [NUS Natural Language Processing Group](http://www.comp.nus.edu.sg/~nlp/index.html)（新加坡国立大学自然语言处理组）

  - [NLP and Big Data Research Group in the ISTD pillar at the Singapore University of Technology and Design](http://www.statnlp.org/) （新加坡科技设计大学自然语言处理和大数据研究组） 
  
  - [NLP Research Group at the Nanyang Technological University](https://ntunlpsg.github.io/)（南洋理工大学自然语言处理组）
  
  - [Advanced Translation Technology Laboratory at National Institute of Information and Communications Technology](http://att-astrec.nict.go.jp/en/)（日本情报通讯研究所高级翻译技术实验室）
  
  - [Nakayama Laboratory at University of Tokyo](http://www.nlab.ci.i.u-tokyo.ac.jp/index-e.html) （东京大学中山实验室） 
  
  - [Natural Language Processing Lab at Bar-Ilan University](http://u.cs.biu.ac.il/~nlp/)  （以色列巴伊兰大学自然语言处理实验室）

  - [The University of Melbourne NLP Group](http://hum.csse.unimelb.edu.au/nlp-group/)（澳大利亚墨尔本大学自然语言处理组）

<a name=""north-america""></a>
  
- 北美地区

  - [Natural Language Processing - Research at Google](https://research.google.com/pubs/NaturalLanguageProcessing.html) （Google自然语言处理组）

  - [The Redmond-based Natural Language Processing group](http://research.microsoft.com/en-us/groups/nlp/) （微软自然语言处理组）

  - [Facebook AI Research (FAIR)](https://research.fb.com) （Facebook AI 研究部）

  - [IBM Thomas J. Watson Research Center](http://researchweb.watson.ibm.com/labs/watson/index.shtml)（IBM Thomas J. Watson研究中心）

  - [The Stanford Natural Language Processing Group](http://nlp.stanford.edu/) （斯坦福大学自然语言处理组）
  
  - [The Berkeley Natural Language Processing Group](http://nlp.cs.berkeley.edu/index.shtml)（伯克利加州大学自然语言处理组）
  
  - [Natural Language Processing research at Columbia University](http://www1.cs.columbia.edu/nlp/index.cgi)（哥伦比亚大学自然语言处理组）
  
  - [Graham Neubig's lab at the Language Technologies Instititute of Carnegie Mellon University](http://www.cs.cmu.edu/~neulab/) （卡内基梅隆大学语言技术研究所Graham Neubig实验室）
  
  - [RPI Blender Lab](http://nlp.cs.rpi.edu/)（伦斯勒理工学院Blender Lab）
  
  - [UC Santa Barbara Natural Language Processing Group](http://nlp.cs.ucsb.edu/)（加州大学圣巴巴拉分校自然语言处理组）
  
  - [The Natural Language Group at the USC Information Sciences Institute](http://nlg.isi.edu/) （南加利福尼亚大学信息科学研究所自然语言处理组）
  
  - [Natural Language Processing @USC](https://cl.usc.edu/) （南加利福尼亚大学自然语言处理组）
  
  - [Natural Language Processing Group at University of Notre Dame](http://nlp.nd.edu/) （圣母大学自然语言处理组）
  
  - [Artificial Intelligence Research Group at Harvard](http://www.eecs.harvard.edu/ai/) （哈佛大学人工智能研究组）
  
  - [The Harvard natural-language processing group](http://nlp.seas.harvard.edu/) （哈佛大学自然语言处理组）
  
  - [Computational Linguistics and Information Processing at Maryland](https://wiki.umiacs.umd.edu/clip/index.php/Main_Page) （马里兰大学计算语言学和信息处理实验室）
  
  - [Language and Speech Processing at Johns Hopkins University](http://www.clsp.jhu.edu/about-clsp/)（约翰斯·霍普金斯大学语言语音处理实验室）
  
  - [Human Language Technology Center of Excellence at Johns Hopkins University](http://hltcoe.jhu.edu/)（约翰斯·霍普金斯大学人类语言技术卓越中心）
  
  - [Machine Translation Group at The Johns Hopkins University](http://www.statmt.org/jhu/)（约翰斯·霍普金斯大学机器翻译组）
  
  - [Machine Translation Research at Rochester](https://www.cs.rochester.edu/~gildea/mt/)（罗切斯特大学机器翻译组）
  
  - [NLP @ University of Illinois at Urbana-Champaign](http://nlp.cs.illinois.edu/)（伊利诺伊大学厄巴纳-香槟分校自然语言处理组）
  
  - [UIC Natural Language Processing Laboratory](http://nlp.cs.uic.edu/)（伊利诺伊大学芝加哥分校自然语言处理组）
  
  - [Human Language Technology Research Institute at The University of Texas at Dallas](http://www.hlt.utdallas.edu/)（德克萨斯大学达拉斯分校人类语言技术研究所
  
  - [Natural Language Processing Group at MIT CSAIL](http://nlp.csail.mit.edu/)（麻省理工学院自然语言处理组）
  
  - [Natural Language Processing Group at Texas A&M University](http://nlp.cs.tamu.edu/)（德克萨斯A&M大学自然语言处理组）
  
  - [The Natural Language Processing Group at Northeastern University](https://nlp.ccis.northeastern.edu/)（东北大学自然语言处理组）
  
  - [Cornell NLP group](https://confluence.cornell.edu/display/NLP/Home/)（康奈尔大学自然语言处理组）
  
  - [Natural Language Processing group at University Of Washington](https://www.cs.washington.edu/research/nlp)（华盛顿大学自然语言处理组）
  
  - [Natural Language Processing Research Group at University of Utah](https://www.cs.utah.edu/nlp/)（犹他大学自然语言处理组）
  
  - [Natural Language Processing and Information Retrieval group at University of Pittsburgh](http://www.isp.pitt.edu/research/nlp-info-retrieval-group)（匹兹堡大学自然语言处理和信息检索小组）
  
  - [Brown Laboratory for Linguistic Information Processing (BLLIP)](http://bllip.cs.brown.edu/)（布朗大学布朗语言信息处理实验室）
  
  - [Natural Language Processing (NLP) group at University of British Columbia](https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/)（不列颠哥伦比亚大学自然语言处理组）
  
<a name=""europe""></a>
  
- 欧洲地区
  
  - [Natural Language and Information Processing Research Group at University of Cambridge](http://www.cl.cam.ac.uk/research/nl/)（英国剑桥大学自然语言和信息处理组）
  
  - [The Computational Linguistics Group at Oxford University](http://www.clg.ox.ac.uk/)（英国牛津大学计算语言学组）
  
  - [Human Language Technology and Pattern Recognition Group at the RWTH Aachen](https://www-i6.informatik.rwth-aachen.de/)（德国亚琛工业大学人类语言技术与模式识别组）
  
  - [The Natural Language Processing Group at the University of Edinburgh (EdinburghNLP)](http://edinburghnlp.inf.ed.ac.uk/)（英国爱丁堡大学自然语言处理研究组）
  
  - [Statistical Machine Translation Group at the University of Edinburgh](http://www.statmt.org/ued/?n=Public.HomePage)（英国爱丁堡大学统计机器翻译组）
  
  - [Natural Language Processing Research Group at The University of Sheffield](http://nlp.shef.ac.uk/)（英国谢菲尔德大学自然语言处理研究组）
  
  - [Speech Research Group at University of Cambridge](http://mi.eng.cam.ac.uk/Main/Speech/)（英国剑桥大学语音研究组）
  
  - [Statistical Machine Translation Group at the University of Cambridge](http://divf.eng.cam.ac.uk/smt)（英国剑桥大学统计机器翻译组）
  
  - [Computational Linguistics group at Uppsala University](http://www.lingfil.uu.se/forskning/datorlingvistik/?languageId=1)（瑞典乌普萨拉大学计算语言学组）

  - [The Center for Information and Language Processing at University of Munich](http://www.cis.uni-muenchen.de/ueber_uns/)（德国慕尼黑大学信息与语言处理中心）

  - [National Centre for Language Technology at Dublin City University](http://www.nclt.dcu.ie/)（爱尔兰都柏林城市大学国家语言技术中心）

  - [The National Centre for Text Mining (NaCTeM) at University of Manchester](http://nactem.ac.uk/)（英国曼彻斯特大学国家文本挖掘中心）

  - [The Information and Language Processing Systems group at the University of Amsterdam](http://ilps.science.uva.nl/)（荷兰阿姆斯特丹大学信息与语言处理系统组）

  - [Institute of Formal and Applied Linguistics at Charles University](http://ufal.mff.cuni.cz/)（捷克查理大学语言学应用与规范研究所）

  - [DFKI Language Technology Lab](https://www.dfki.de/lt/)（德国人工智能研究中心自然语言处理组）

  - [IXA in University of the Basque Country](http://ixa.eus/)（西班牙巴斯克大学自然语言处理组）

  - [Statistical Natural Language Processing Group at the Institute for Computational Linguistics at Heidelberg University](http://www.cl.uni-heidelberg.de/statnlpgroup/)（德国海德堡大学计算语言学研究所统计自然语言处理组）

  - [NLP Research at the University of Helsinki](https://blogs.helsinki.fi/language-technology/)（芬兰赫尔辛基大学自然语言处理组）
    
    
    
## Reference
有部分资源来自于以下工作，在此表示感谢：
  
- [FunNLP](https://github.com/fighting41love/funNLP)  

- [Awesome Chinese NLP](https://github.com/crownpku/Awesome-Chinese-NLP)  

- [国内外自然语言处理(NLP)研究组](https://blog.csdn.net/wangxinginnlp/article/details/44890553) 

同时，对相关工作的作者一并表示感谢！

另外，相关资料的使用范围、授权请参考原始发布源（如果有的话），如有侵权，请联系我删除。

",['jia-zh'],0,,0.6,0,,,,,,10,,,
642121754,R_kgDOJkYAGg,Logic-LLM,teacherpeterpan/Logic-LLM,0,teacherpeterpan,https://github.com/teacherpeterpan/Logic-LLM,"The project page for ""LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning""",0,2023-05-17 21:47:31+00:00,2025-03-05 13:06:23+00:00,2024-06-13 20:31:59+00:00,,8685,294,294,C,1,1,1,1,0,0,52,0,0,8,mit,1,0,0,public,52,8,294,main,1,,"# Logic-LM
Data and Codes for [""LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning""](https://arxiv.org/abs/2305.12295) (Findings of EMNLP 2023). 

Authors: **Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang**. 

[NLP Group](http://nlp.cs.ucsb.edu/), University of California, Santa Barbara

## Introduction

Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, **Logic-LM**, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. 

![The general framework of Logic-LM](./framework.png)

First, install all the required packages:

```bash
pip install -r requirements.txt
```

## Datasets

The datasets we used are preprocessed and stored in the `./data` folder. We evaluate on the following datasets:

- [ProntoQA](https://github.com/asaparov/prontoqa): Deductive resoning dataset. We use the 5-hop subset of the *fictional characters* version, consisting of 500 testing examples. 
- [ProofWriter](https://allenai.org/data/proofwriter): Deductive resoning dataset. We use the depth-5 subset of the OWA version. To reduce overall experimentation costs, we randomly sample 600 examples in the test set and ensure a balanced label distribution.
- [FOLIO](https://github.com/Yale-LILY/FOLIO): First-Order Logic reasoning dataset. We use the entire FOLIO test set for evaluation, consisting of 204 examples.
- [LogicalDeduction](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/logical_deduction): Constraint Satisfaction Problems (CSPs). We use the full test set consisting of 300 examples.
- [AR-LSAT](https://github.com/zhongwanjun/AR-LSAT): Analytical Reasoning (AR) problems, containing all analytical logic reasoning questions from the Law School Admission Test from 1991 to 2016. We use the test set which has 230 multiple-choice questions. 

## Baselines

To replicate the **Standard-LM (Direct)** and the **Chain-of-Thought (CoT)** baselines, please run the following commands:

```bash
cd ./baselines
python gpt3_baseline.py \
    --api_key ""Your OpenAI API Key"" \
    --model_name ""Model Name [text-davinci-003 | gpt-4]"" \
    --dataset_name ""Dataset Name [ProntoQA | ProofWriter | FOLIO | LogicalDeduction ｜ AR-LSAT]"" \
    --split dev \
    --mode ""Baseline [Direct | CoT]"" \
    --max_new_tokens ""16 for Direct; 1024 for CoT"" \
```

The results will be saved in `./baselines/results`. To evaluate the results, please run the following commands:

```bash
python evaluate.py \
    --dataset_name ""Dataset Name [ProntoQA | ProofWriter | FOLIO | LogicalDeduction ｜ AR-LSAT]"" \
    --model_name ""Model Name [text-davinci-003 | gpt-4]"" \
    --split dev \
    --mode ""Baseline [Direct | CoT]"" \
```

## Logic Program Generation

To generate logic programs for logical reasoning problems in each dataset, at the root directory, run the following commands:

```bash
python models/logic_program.py \
    --api_key ""Your OpenAI API Key"" \
    --dataset_name ""Dataset Name [ProntoQA | ProofWriter | FOLIO | LogicalDeduction ｜ AR-LSAT]"" \
    --split dev \
    --model_name ""Model Name [text-davinci-003 | gpt-4]"" \
    --max_new_tokens 1024 \
```

The generated logic programs will be saved in `outputs/logic_programs`. You can also reuse the logic programs we generated in `./outputs/logic_programs`.

## Logic Inference with Symbolic Solver

After generating logic programs, we can perform inference with symbolic solvers. At the root directory, run the following commands:

```bash
DATASET=""Dataset Name [ProntoQA | ProofWriter | FOLIO | LogicalDeduction ｜ AR-LSAT]""
SPLIT=""Dataset Split [dev | test]""
MODEL=""The logic programs are generated by which model? [text-davinci-003 | gpt-4]""
BACKUP=""The random backup answer (random) or CoT-Logic collabration mode (LLM)""

python models/logic_inference.py \
    --model_name ${MODEL} \
    --dataset_name ${DATASET} \
    --split ${SPLIT} \
    --backup_strategy ${BACKUP} \
    --backup_LLM_result_path ./baselines/results/CoT_${DATASET}_${SPLIT}_${MODEL}.json
```

The logic reasoning results will be saved in `outputs/logic_inferences`. 

Backup Strategies:
- `random`: If the generated logic program cannot be executed by the symbolic solver, we will use random guess as the prediction.
- `LLM`: If the generated logic program cannot be executed by the symbolic solver, we will back up to using CoT to generate the prediction. To run this mode, you need to have the corresponding baseline LLM results stored in `./baselines/results`. To make the inference more efficient, the model will just load the baseline LLM results and use them as the prediction if the symbolic solver fails.

## Evaluation

To evaluate the logic reasoning results, please run the following commands:

```bash
python models/evaluation.py \
    --dataset_name ""Dataset Name [ProntoQA | ProofWriter | FOLIO | LogicalDeduction]"" \
    --model_name ""The logic programs are generated by which model? [text-davinci-003 | gpt-4]"" \
    --split dev \
    --backup ""The basic mode (random) or CoT-Logic collabration mode (LLM)""
```

## Self-Refinement

After generating the logic programs without self-refinement, run the following commands for self-refinement:

```bash
DATASET=""Dataset Name [ProntoQA | ProofWriter | FOLIO | LogicalDeduction ｜ AR-LSAT]""
SPLIT=""Dataset Split [dev | test]""
MODEL=""The logic programs are generated by which model? [text-davinci-003 | gpt-4]""
BACKUP=""The random backup answer (random) or CoT-Logic collabration mode (LLM)""

python models/self_refinement.py \
    --model_name ${MODEL} \
    --dataset_name ${DATASET} \
    --split ${SPLIT} \
    --backup_strategy ${BACKUP} \
    --backup_LLM_result_path ./baselines/results/CoT_${DATASET}_${SPLIT}_${MODEL}.json
    --api_key ""Your OpenAI API Key"" \
    --maximum_rounds 3 \
```

The self-refinement results will be saved in `outputs/logic_inferences`. 

## Reference
Please cite the paper in the following format if you use this dataset during your research.

```
@inproceedings{PanLogicLM23,
  author       = {Liangming Pan and
                  Alon Albalak and
                  Xinyi Wang and
                  William Yang Wang},
  title        = {{Logic-LM:} Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning},
  booktitle    = {Findings of the 2023 Conference on Empirical Methods in Natural Language Processing (Findings of EMNLP)},
  address      = {Singapore},
  year         = {2023},
  month        = {Dec},
  url          = {https://arxiv.org/abs/2305.12295}
}
```

## Credit
The codes for the SMT solver are modified from [SatLM](https://github.com/xiye17/sat-lm). 

## Q&A
If you encounter any problem, please either directly contact the [Liangming Pan](liangmingpan@ucsb.edu) or leave an issue in the github repo.
",['teacherpeterpan'],1,,0.77,0,,,,,,7,,,
34755344,MDEwOlJlcG9zaXRvcnkzNDc1NTM0NA==,pentest-bookmarks,rutkai/pentest-bookmarks,0,rutkai,https://github.com/rutkai/pentest-bookmarks,A collection of penetration testing related sites,0,2015-04-28 20:53:48+00:00,2025-02-21 15:47:49+00:00,2020-09-30 20:11:27+00:00,,196,279,279,,1,1,0,0,0,0,83,1,0,1,,1,0,0,public,83,1,279,master,1,,,['rutkai'],0,,0.62,0,,,,,,17,,,
99738996,MDEwOlJlcG9zaXRvcnk5OTczODk5Ng==,awesome-digital-humanities,dh-tech/awesome-digital-humanities,0,dh-tech,https://github.com/dh-tech/awesome-digital-humanities,Software for humanities scholars using quantitative or computational methods.,0,2017-08-08 21:48:57+00:00,2025-03-04 04:55:57+00:00,2024-11-18 18:51:53+00:00,https://dh-tech.github.io/awesome-digital-humanities/,220,268,268,HTML,1,1,1,1,1,0,24,0,0,1,cc0-1.0,1,0,0,public,24,1,268,main,1,1,"# Awesome Digital Humanities [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

[<img src=""android-chrome-512x512.png"" align=""right"" width=""100"">](https://github.com/dh-tech/awesome-digital-humanities/)

> Software for humanities scholars using quantitative or computational methods.

This is a curated list of tools, resources, and services supporting the Digital Humanities. [Contributions](CONTRIBUTING.md) are welcome!

## Contents

- [Bibliography and Sources Management](#bibliography-and-sources-management)
- [Corpus linguistics](#corpus-linguistics)
- [Data Collection](#data-collection)
- [Data Analysis](#data-analysis)
- [Data Extraction and Conversion](#data-extraction-and-conversion)
- [Data Annotation](#data-annotation)
- [DH Centers](#dh-centers)
- [Document Management and Processing](#document-management-and-processing)
- [Journals](#journals)
- [Organizations and Research Infrastructures](#organizations-and-research-infrastructures)
- [Other Resources](#other-resources)
- [Platforms](#platforms)
- [Publishing](#publishing)
- [Tool Building and Rapid Prototyping](#tool-building-and-rapid-prototyping)
- [Twitter](#twitter)
- [User Guides and Training Materials](#user-guides-and-training-materials)
- [Visualization](#visualization)

## Bibliography and Sources Management

- [JabRef](https://www.jabref.org/) - Open source bibliography reference manager.
- [Tropy](https://tropy.org/) - Research Photo Management.
- [Zotero](https://www.zotero.org/) - Free, easy-to-use tool to help you collect, organize, cite, and share research.

## Corpus linguistics

- [AntConc](https://www.laurenceanthony.net/software/antconc/) - A freeware corpus analysis toolkit for concordancing and text analysis.
- [CorpusExplorer v2.0](http://www.CorpusExplorer.de) - Software for corpus linguists and text/data mining enthusiasts. The CorpusExplorer combines over 45 interactive visualizations under an user-friendly interface. Routine tasks such as text acquisition, cleaning or tagging are completely automated. The simple interface supports the use in university teaching and leads the users/students to fast and substantial results. The CorpusExplorer is open for many standards (XML, CSV, JSON, R, etc.) and also offers its own software development kit (SDK), which allows you to integrate all functions into your own programs.
- [TXM](https://txm.gitpages.huma-num.fr/textometrie/en/) - The project brings together open-source Textometry software developments to set up a modular platform called TXM, in synergy with existing corpus technologies (Unicode, XML, TEI, NLP tools, CQP, R).

## Data Collection

- [Data Commons](https://datacommons.org/) - Data Commons aggregates data from a [wide range of sources](https://docs.datacommons.org/datasets/) into a unified database to make it more accessible and useful.
- [OpenArchive](https://open-archive.org/) - Making it easy to store, share, and amplify your mobile media while protecting your identity.
- [Open EU Data Portal](https://data.europa.eu/euodp/en/data/) - European Union open data.
- [Social Feed Manager](https://gwu-libraries.github.io/sfm-ui/) - Open source software that harvests social media data and web resources from Twitter, Tumblr, Flickr, and Sina Weibo.
- [Trafilatura](https://trafilatura.readthedocs.io/) - Open source software to gather text and metadata on the Web: Crawling, scraping, extraction, output in multiple formats. Usable with Python, R and on the command-line.
- [Transkribus](https://transkribus.eu/) - Transcribe. Collaborate. Share and benefit from cutting edge research in Handwritten Text Recognition!
- [Textgrid](https://textgrid.de/) - Open source tools and services support humanistic scholars during the entire process of research, especially in digital scholarly editing.
- [webrecorder.io](https://webrecorder.io/) - Web archiving service anyone can use for free to save web pages.

## Data Analysis

- [Allmaps](https://allmaps.org/) - Curating, georeferencing and exploring for IIIF maps.
- [Breve](http://hdlab.stanford.edu/breve/) - Visualize and edit tabular data.
- [Data Pen](http://hdlab.stanford.edu/data-pen/) - Framework for humanities researchers to access, explore, and manipulate multidimensional historical data.
- [DocFetcher](http://docfetcher.sourceforge.net/) - Open Source desktop search application.
- [Leipzig Corpus Miner (LCM)](https://ilcm.informatik.uni-leipzig.de/) - The LCM project pursues the development of an integrated research environment for the analysis of structured and unstructured data in a 'Software as a Service' architecture (SaaS). The research environment addresses requirements for the quantitative evaluation of large amounts of text data (e.g. 3 million news articles) using text mining methods and requirements for the reproducibility of data-driven research designs in the social sciences and the digital humanities.
- [Lexos](http://lexos.wheatoncollege.edu) - Online tool for text analysis.
- [Mallet](https://mimno.github.io/Mallet/) - Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.
- [Open Semantic Search](https://www.opensemanticsearch.org/) - Free Software for your own Search Engine, Explorer for Discovery of large document collections, Media Monitoring, Text Analytics, Document Analysis & Text Mining platform based on Apache Solr or Elasticsearch open-source enterprise-search and Open Standards for Linked Data, Semantic Web & Linked Open Data integration.
- [Stylo](https://github.com/computationalstylistics/stylo) - R package for stylometric analyses.
- [TinEye](https://tineye.com/) - Search by image or image section and find where that image appears online.
- [Voyant](https://voyant-tools.org/) - Reading and analysis environment for digital texts.
- [YouTube Metadata](https://mattw.io/youtube-metadata/) - It grabs singular details about a video and its uploader, playlist and its creator, or channel.

## Data Extraction and Conversion

- [ImageMagick](https://imagemagick.org/) - Image conversion tool.
- [MuPDF](https://mupdf.com/) - PDF viewer and converter.
- [OCRmyPDF](https://github.com/jbarlow83/OCRmyPDF) - OCR toolkit.
- [Poppler](https://poppler.freedesktop.org/) - PDF toolkit.
- [QPDF](http://qpdf.sourceforge.net/) - PDF toolkit.

## Data Annotation

- [Annotation Studio](https://www.annotationstudio.org/) - Suite of tools for collaborative web-based annotation, developed by MIT's HyperStudio. 
- [CATMA](https://catma.de/) - Computer Assisted Text Markup and Analysis.
- [Glycerine](https://glycerine.io/) - Provides a suite of IIIF image annotation tools and end-to-end workflows for researchers, curators and students to collaborate on projects across repositories and publish research ouputs.
- [Recogito](https://recogito.pelagios.org/) - Semantic Annotation for images and texts.

## DH Centers

- [ACDH-DH](https://www.oeaw.ac.at/acdh/) - Austrian Centre for Digital Humanities.
- [DHCenter UNIL-EPFL](https://dhcenter-unil-epfl.com/) - Founded in 2018, the dhCenter UNIL-EPFL is an interdisciplinary research platform.
- [Digital Humanities Bern](https://www.dh.unibe.ch/) - It explores different topics, in the context of digital text and image analysis, digital edition, and reflection on the impact of digital methods on the humanities.
- [Digital Humanities Lab - Universität Basel](https://dhlab.philhist.unibas.ch/en/) - The Digital Humanities is an interdisciplinary institution of the University of Basel.
- [HDLab](http://hdlab.stanford.edu/) - Humanities + Design a Research Lab at Stanford University.
- [Humanités numériques Unversity of Geneva](https://www.unige.ch/lettres/humanites-numeriques/) - The aim of the Chair is to teach the use of digital technology to all human sciences, according to the methods and issues specific to the Humanities.
- [Ladhul dhCenter](http://www.dhlausanne.ch/) - Laboratoire de cultures et humanités digitales de l'Université de Lausanne.
- [Luxembourg Centre for Contemporary and Digital History](https://www.c2dh.uni.lu/) - Research on new digital methods and tools for historical research and teaching.
- [Roy Rosenzweig Center for History and New Media](https://rrchnm.org/) - Creators of Zotero and other amazing open-source software tools for historians.

## Document Management and Processing

- [Giles Ecosystem](https://diging.atlassian.net/wiki/spaces/GECO/overview) - The Giles Ecosystem is a distributed system based on Apache Kafka that allows users to upload documents for text and image extraction. It automatically performs OCR on uploaded images and extracts images and embedded texts from pdf files. The Giles Ecosystem can be easily scaled to accommodate higher workloads.

## Journals

- [Current Research in Digital History](http://crdh.rrchnm.org/) - Annual open-access, peer-reviewed publication of the Roy Rosenzweig Center for History and New Media at George Mason University.

## Organizations and Research Infrastructures

- [ACH](http://ach.org/) - Association for Computers and the Humanities (ACH) is a major professional society for the digital humanities. We support and disseminate research and cultivate a vibrant professional community through conferences, publications, and outreach activities.
- [ADHO](http://adho.org/) - The Alliance of Digital Humanities Organizations (ADHO) promotes and supports digital research and teaching across all arts and humanities disciplines, acting as a community-based advisory force, and supporting excellence in research, publication, collaboration and training.
- [CHAIN](https://mith.umd.edu/chain/) - Coalition of Humanities and Arts Infrastructures and Networks.
- [CHCI](https://chcinetwork.org/) - Consortium of Humanities Centers and Institutes. Currently it has a membership of more than 250 organizations in the Americas, Europe, Africa, Asia, and Pacific Rim. Our members include humanities centers at small, medium, and large colleges and universities, community colleges, independent scholarly societies, research libraries, and other institutes of advanced study.
- [centerNet](http://dhcenternet.org/) - An international network of digital humanities centers.
- [CLARIN](https://www.clarin.eu/) - CLARIN stands for ""Common Language Resources and Technology Infrastructure"". It is a research infrastructure that was initiated from the vision that all digital language resources and tools from all over Europe and beyond are accessible through a single sign-on online environment for the support of researchers in the humanities and social sciences.
- [DARIAH](https://www.dariah.eu/) - The Digital Research Infrastructure for the Arts and Humanities (DARIAH) aims to enhance and support digitally-enabled research and teaching across the arts and humanities. DARIAH is a network of people, expertise, information, knowledge, content, methods, tools and technologies from its member countries. It develops, maintains and operates an infrastructure in support of ICT-based research practices and sustains researchers in using them to build, analyse and interpret digital resources.
- [DHCH](https://dh-ch.ch/) - Interdisciplinary research in the digital humanities in Switzerland.
- [Digital History Network Switzerland](https://www.digitalhistorynetwork.ch/) - Network of digital historians in academia and GLAM institutions in Switzerland with a fairly active mailing list.
- [Digital Humanities Now](http://digitalhumanitiesnow.org/) - Digital Humanities Now is an experimental, edited publication that highlights and distributes informally published digital humanities scholarship and resources from the open web.
- [European Holocaust Research Infrastructure (EHRI)](https://www.ehri-project.eu/) - The EHRI Portal enables online access to information about Holocaust sources, no matter where they are located. It also promotes innovative tools that advance the digital transformation of Holocaust research.
- [European Research Infrastructure for Heritage Science (E-RIHS)](http://www.e-rihs.eu/) - It supports research on heritage interpretation, preservation, documentation and management.
- [GO::DH](http://www.globaloutlookdh.org/) - The purpose of Global Outlook::Digital Humanities (GO::DH) is to help break down barriers that hinder communication and collaboration among researchers and students of the Digital Arts, Humanities, and Cultural Heritage sectors in high, mid, and low income economies.

## Other Resources

- [Awesome Digital History](https://github.com/maehr/awesome-digital-history) - A curated list of awesome things related to digital history.
- [Awesome IIIF](https://github.com/IIIF/awesome-iiif) - A curated list of awesome resources related to the International Image Interoperability Framework (IIIF).
- [Awesome OCR](https://github.com/kba/awesome-ocr) - This list contains links to great software tools and libraries and literature related to Optical Character Recognition (OCR).
- [Awesome Scientific Writing](https://github.com/writing-resources/awesome-scientific-writing) - A curated list of awesome tools, demos and resources to go beyond LaTeX.
- [Awesome Web Archiving](https://github.com/iipc/awesome-web-archiving) - Resources to archive the web.
- [Carolina Digital Humanities Initiative Tools Page](http://digitalhumanities.unc.edu/resources/tools/) - It provides a range of platforms, plug-ins, readings, and other items that might be of use for DH researchers.
- [DH Toychest](http://dhresourcesforprojectbuilding.pbworks.com/w/page/69244319/Digital%20Humanities%20Tools) - Guides, tools, and other resources for practical work in the digital humanities by researchers, teachers, and students. Curated by [Alan Liu](http://liu.english.ucsb.edu/), University of California, Santa Barbara.
- [Digital Textuality Resource Pages](http://digitaltextuality.pbworks.com/w/page/68178062/Digital%20Textuality%20Resource%20Pages) - Inspired by Alan Liu's ToyChest, Kimberly Knight and her students at U. Texas (Dallas) keep in this repository a list of tools for text production, visualization, still image work, sound work, and video and animation; includes some student reviews of tools.
- [Duke University's DH Tools catalog](https://digitalhumanities.duke.edu/tools) - This list includes tools that Duke supports and tools that have been used by Duke digital projects. Some of the tools are made specifically for DH and others that can be re-purposed quite effectively for Humanities research.
- [FID Romanistik](https://www.fid-romanistik.de/forschungsdaten/suche-nach-forschungsdaten/fid-internetressourcen/tools/) - Curated list of tools and resources for digital humanists in German.
- [GLAMS Workbench](https://glam-workbench.net/) - A collection of tools, tutorials, examples, and hacks to help you work with data from galleries, libraries, archives, and museums (the GLAM sector). The primary focus is Australia and New Zealand, but new collections are being added all the time.
- [Social Sciences & Humanities Open Marketplace](https://marketplace.sshopencloud.eu/) - A discovery portal which pools and contextualises resources for Social Sciences and Humanities research communities: tools, services, training materials, datasets, publications and workflows.
- [TAPoR 3](http://tapor.ca/home) - TAPoR is a gateway to the tools used in sophisticated text analysis and retrieval. It was redesigned in order to integrate the DiRT (Digital Research Tools) directory.
- [Taxonomy of Digital Research Activities in the Humanities (TaDiRAH)](https://tadirah.info/) - This taxonomy has been developed for use by community-driven sites and projects that aim to structure information relevant to digital humanities and make it more easily discoverable. The taxonomy is expected to be particularly useful to endeavors aiming to collect information on digital humanities tools, methods, projects, or readings.

## Platforms

- [DHSlack](https://github.com/amandavisconti/DHslack/blob/master/CodeOfConduct.md) - Slack channels for digital humanities scholars.
- [HSS Digital](https://hssonline.org/page/digitalprojects) - Digital scholarship in the history of science initiative.
- [Perspectives on History](https://www.historians.org/community-careers/digital-history-resources/) - The newsmagazine of the American Historical Association.
- [wethink.hypotheses.org](https://wethink.hypotheses.org/) - Collaborative Digital History.
- [Wikipedia](https://en.wikipedia.org/wiki/Digital_history) - Digital history is the use of digital media to further historical analysis, presentation, and research.

## Publishing

- [CollectionBuilder](https://collectionbuilder.github.io/) - Open source tool for creating digital collection and exhibition websites based on metadata and powered by modern static web technology.
- [Manifold](https://manifoldapp.org/) - Scholarly publishing and collaborative and social reading platform.
- [Omeka](https://omeka.org/) - Open-source web publishing platform for sharing digital collections and creating media-rich online exhibits.
- [Wax](https://minicomp.github.io/wax/) - Jekyll based framework for minimal exhibitions with IIIF.

## Tool Building and Rapid Prototyping

- [JupyterHub Workspace](https://github.com/maltevogl/jupyterworkspace) - The JupyterHub Workspace aims to be a collaborative programming and code-sharing platform. It provides access to browser-based Jupyter Notebooks, which integrate code with explanatory text, and are already used as a new publishing form. Data can be shared using the Nextcloud backend. A single sign-on mechanism simplifies access. By sharing useful code snippets among users, a growing examples collection further lowers the entrance barrier to programming for new DH members.

## Twitter

- [Digital History by @moritzmaehr](https://twitter.com/moritzmaehr/lists/digital-history) - List of Digital History people by Moritz Mährg.
- [Digital Humanities by @GrandjeanMartin](https://twitter.com/GrandjeanMartin/lists/digital-humanities) - List of Digital Humanities people by Martin Grandjean.
- [Digital Humanities by @normanlippert](https://twitter.com/normanlippert/lists/digital-humanities) - List of Digital Humanities people by Norman Lippert.
- [Digital Humanities by @wpippich](https://twitter.com/i/lists/907641704949436416) - List of Digital Humanities people by Waltraud von Pippich.
- [Digital Humanities Women by @amandafrench](https://twitter.com/i/lists/81258446) - List of Digital Humanities women by Amanda French.

## User Guides and Training Materials

- [#dariahTeach](https://teach.dariah.eu/) - An open-source, multilingual, community-driven platform for high-quality teaching and training materials for the digital arts and humanities.
- [DARIAH-CAMPUS](https://campus.dariah.eu/) - A discovery framework and hosting platform for DARIAH learning resources.
- [DH Tools for Beginners](https://medium.com/dh-tools-for-beginners) - A collection of tutorials about DH tools aiming at digital humanities researchers.
- [Digital Editions Course](https://www.prisms.digital/training/) - The course covers the the whole process of creating a digital edition, from selecting a text right through to publication.
- [Digital Humanities Literacy Guidebook](https://cmu-lib.github.io/dhlg/topics/) - Overview on the field of Digital History and Digital Humanities.
- [Digital Humanities Theories and Practice](https://ltagliaferri.github.io/dh-rutgers-2022/) - Balancing practical guidance on tools and methodologies with modes for entering into the research of participants, the course will foster experimentation with and critical exploration of digital scholarship coupled with humanistic inquiry.
- [forText](https://fortext.net) - Collection of german tutorials for the interpretation and visualization of literature.
- [Framework for Information Literacy for Higher Education](http://www.ala.org/acrl/standards/ilframework) - How to teach digital literacy.
- [Humanities Data Analysis](https://www.humanitiesdataanalysis.org/) - A practical guide to data-intensive humanities research using the Python programming language.
- [Intro Cultural Analytics](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html) - Analyze cultural artifacts with Python.
- [Introduction to Digital Humanities (DH101)](https://asandersgarcia.humspace.ucla.edu/courses/dh101f18/) - Collection of resources/online coursebook based on the *Introduction to Digital Humanities (DH101)* course at [UCLA](http://www.ucla.edu/).
- [Jupyter Notebooks for Digital Humanities](https://github.com/quinnanya/dh-jupyter/blob/master/README.md) - A diverse range of Jupyter notebooks, comprising research materials, course content, Python tutorials, and specific analysis tools.
- [Missing Semester](https://missing.csail.mit.edu/) - Useful tools that are not taught in class.
- [New Languages for NLP](https://new-languages-for-nlp.github.io/course-materials/intro.html) - Learn how to annotate linguistic data and train statistical language models using cutting-edge natural language processing (NLP) tools.
- [PARTHENOS Training](https://training.parthenos-project.eu/) -  The PARTHENOS cluster of humanities research infrastructure projects has devised a series of training modules and resources for those who want to learn more about research infrastructures in the Digital Humanities. Contains material about research infrastructures, research data, existing digital collections of use to researchers, guides to ontologies and a catalogue of webinars and training material.
- [The Programming Historian](https://programminghistorian.org/) - Novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate research and teaching.

## Visualization

- [Bertin.js](https://github.com/neocarto/bertin) - JavaScript library for visualizing geospatial data and make thematic maps for the web.
- [DARIAH-DE Geo-Browser](https://geobrowser.de.dariah.eu/) - Create visualizations with geotagges data.
- [Gephi](https://gephi.org/) - Gephi is the leading visualization and exploration software for all kinds of graphs and networks.
- [Khartis](https://www.sciencespo.fr/cartographie/khartis/en/) - A tool for easy creation of thematic maps in 3 steps from CSV data.
- [Palladio](https://hdlab.stanford.edu/palladio/) - Visualize complex historical data with ease.
- [RAWGraphs](https://rawgraphs.io/) - Open source, web-based tool for the visualization of complex data.
- [StorylineJS](http://storyline.knightlab.com/) - Tell the story behind the numbers.
- [StorymapJS](https://storymap.knightlab.com/) - Maps that tell stories.
- [TimelineJS](https://timeline.knightlab.com/) - Easy-to-make, beautiful timelines.
- [Vistorian.online](https://vistorian.github.io/) - Interactive Visualizations for Dynamic and Multivariate Networks.
","['diegosiqueir4', 'maehr', 'jdamerow', 'schildwaechter', 'padraic7a', 'maltevogl', 'adbar', 'dmj', 'alhuber1502', 'notesjor', 'jwyg', 'julsraemy', 'stark1tty', 'robcast', 'rscircus', 'fungunga', 'yangli0516']",0,,0.76,0,"# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
 advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
 address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
 professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at moritz.maehr@gmail.com. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq
","# Contribution Guidelines

Please note that this project is released with a
[Contributor Code of Conduct](CODE_OF_CONDUCT.md). By participating in this
project you agree to abide by its terms.

---

## We welcome

- **Additions**: restricted to addition of one new entry per pull-request.
- **Removals**: restricted to removal of one obsolete entry per pull-request.
- **Edits**: you may correct the descriptions if it can be improved.

---

  If you are proficient with GitHub, follow the instructions below. Otherwise open an [issue](https://github.com/dh-tech/awesome-digital-humanities/issues/new/choose).

---

## Criteria for accepting a pull-request

*Contributors, make sure that*:

- a **short pitch** is included in the pull-request description,
- the table of contents has been updated (if a section is added / removed),
- the contents are sorted **alphabetically**,
- The addition you proposed is NOT part of [everything that did not make it into the list](https://github.com/maehr/awesome-digital-history/wiki).

Thank you for your suggestions!

*Maintainers, make sure that*:

- the above criteria are followed,
- the tests pass on the CI,
- in case of addition or removal, make an assessment of
  awesomeness of the entry.

## Updating your PR

A lot of times, making a PR adhere to the standards above can be difficult.
If the maintainers notice anything that we'd like changed, we'll ask you to
edit your PR before we merge it. There's no need to open a new PR, just edit
the existing one. If you're not sure how to do that,
[here is a guide](https://github.com/RichardLitt/knowledge/blob/master/github/amending-a-commit-guide.md)
on the different ways you can update your PR so that we can merge it.

## Appendix: running lint tests

To run tests locally:

    # using ruby
    gem install awesome_bot
    awesome_bot README.md
    # using node.js
    npm install -g awesome-lint
    awesome-lint README.md
","# Security Policy

## Reporting a Vulnerability

Please use the [GitHub Issue Tracker](https://github.com/maehr/awesome-digital-history/issues) to report vulnerabilities.
",Directory exists,,38,,,
17123539,MDEwOlJlcG9zaXRvcnkxNzEyMzUzOQ==,PyRTL,UCSBarchlab/PyRTL,0,UCSBarchlab,https://github.com/UCSBarchlab/PyRTL,"A collection of classes providing simple hardware specification, simulation, tracing, and testing suitable for teaching and research.  Simplicity, usability, clarity, and extensibility are the overarching goals, rather than performance or optimization.",0,2014-02-24 02:58:06+00:00,2025-03-06 16:33:53+00:00,2025-03-06 16:33:48+00:00,http://ucsbarchlab.github.io/PyRTL,4727,267,267,Python,1,0,1,0,1,1,81,0,0,26,bsd-3-clause,1,0,0,public,81,26,267,development,1,1,"<!-- This README is also published on PyPI. GitHub relative links, like
[link](docs/README.md) do not work on PyPI, so don't use them here. -->

PyRTL
=====

[![PyPI version](https://badge.fury.io/py/pyrtl.svg)](http://badge.fury.io/py/pyrtl)
[![Build Status](https://github.com/UCSBarchlab/PyRTL/actions/workflows/python-test.yml/badge.svg)](https://github.com/UCSBarchlab/PyRTL/actions/workflows/python-test.yml)
[![Code Coverage](https://codecov.io/github/UCSBarchlab/PyRTL/coverage.svg?branch=development)](https://codecov.io/github/UCSBarchlab/PyRTL?branch=development)
[![Documentation Status](https://readthedocs.org/projects/pyrtl/badge/?version=latest)](http://pyrtl.readthedocs.org/en/latest/?badge=latest)
[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/UCSBarchlab/PyRTL/development?filepath=%2Fipynb-examples%2F)

PyRTL provides a collection of classes for Pythonic [register-transfer
level](https://en.wikipedia.org/wiki/Register-transfer_level) design,
simulation, tracing, and testing suitable for teaching and research.
Simplicity, usability, clarity, and extensibility rather than performance or
optimization is the overarching goal. Features include:

* Elaboration-through-execution, meaning all of Python can be used including
  introspection
* Design, instantiate, and simulate all in one file and without leaving Python
* Export to, or import from, common HDLs (BLIF-in, Verilog-out currently
  supported)
* Examine execution with waveforms on the terminal or export to `.vcd` as
  projects scale
* Elaboration, synthesis, and basic optimizations all included
* Small and well-defined internal core structure means writing new transforms
  is easier
* Batteries included means many useful components are already available and
  more are coming every week

What README would be complete without a screenshot? Below you can see the
waveform rendered right on the terminal for a small state machine written in
PyRTL.

![Command-line waveform for PyRTL state machine](https://github.com/UCSBarchlab/PyRTL/blob/development/docs/screenshots/pyrtl-statemachine.png?raw=true ""PyRTL State Machine Screenshot"")

### Tutorials and Documentation

* For users, more info and demo code is available on the [PyRTL project web
  page](http://ucsbarchlab.github.io/PyRTL/).
* Try the examples in the `examples/` directory. You can also [try the examples
  on
  MyBinder](https://mybinder.org/v2/gh/UCSBarchlab/PyRTL/development?filepath=%2Fipynb-examples%2F).
* Full reference documentation is available at https://pyrtl.readthedocs.io/

### Package Contents

If you are just getting started with PyRTL it is suggested that you start with
the `examples/` first to get a sense of the ""thinking with PyRTLs"" required to
design hardware in this way. If you are looking for a deeper understanding,
dive into the code for the object `Block`. It is the core data structure at the
heart of PyRTL and defines its semantics at a high level -- everything is
converted to or from the small, simple set of primitives defined there.

The package contains the following files and directories:
* **`pyrtl/`**  The src directory for the module
* **`pyrtl/rtllib/`** Finished PyRTL libraries which are hopefully both useful
  and documented
* **`examples/`** A set of hardware design examples that show the main idea
  behind pyrtl
* **`tests/`** A set of unit tests for PyRTL which you can run with `pytest`
* **`docs/`** Location of the sphinx documentation

Testing requires the Python packages `tox` and `pytest`. Once installed a
complete test of the system should be possible with the simple command `tox`
and nothing more.

### Contributing to PyRTL

*Picking a first project*

* One of the earliest things you should submit is a unit test that hits some
  [uncovered lines of code in
  PyRTL](https://codecov.io/github/UCSBarchlab/PyRTL?branch=development). For
  example, pick a `PyrtlError` that is not covered and add a unit test in
  `tests/` that will hit it.
* After you have that down check in the [PyRTL
  Issues](https://github.com/UCSBarchlab/PyRTL/issues) list for a feature that
  is marked as ""beginner friendly"".
* Once you have that down, ask for access to the PyRTL-research repo where we
  keep a list of more advanced features and designs that could use more help!

*Coding style*

* All major functionality should have unit tests covering and documenting their
  use
* All public functions and methods should have useful docstrings
* All code needs to conform to
  [PEP8](https://www.python.org/dev/peps/pep-0008/) conventions
* No new root-level dependencies on external libs, import locally if required
  for special functions

*Workflow*

* A useful reference for working with Git is this [Git
  tutorial](https://www.atlassian.com/git/tutorials/)
* A useful Git Fork workflow for working on this repo is [found
  here](http://blog.scottlowe.org/2015/01/27/using-fork-branch-git-workflow/)
* The `development` branch is the primary stable working branch (everyone is
  invited to submit pull requests)
* Bugs and minor enhancements tracked directly through the [issue
  tracker](https://github.com/UCSBarchlab/PyRTL/issues)
* When posting a bug please post a small chunk of code that captures the bug,
  e.g. [Issue #56](https://github.com/UCSBarchlab/PyRTL/issues/56)
* When pushing a fix to a bug or enhancement please reference issue number in
  commit message, e.g. [Fix to Issue
  #56](https://github.com/UCSBarchlab/PyRTL/commit/1d5730db168a9e4490c580cb930075715468047a)

*Documentation*

* All important functionality should have an executable example in `examples/`
* All classes should have a block comment with high level description of the
  class
* All functions should follow the following (Sphinx parsable) docstring format:
  ```python
  """"""One Line Summary (< 80 chars) of the function, followed by period.

  :param param_name : Description of this parameter.
  :param param_name : Longer parameter descriptions take up a newline with four
      leading spaces like this.
  :return: Description of function's return value.

  A long description of what this function does. Talk about what the user
  should expect from this function and also what the users needs to do to use
  the function (this part is optional).

  """"""

  # Developer Notes (Optional):
  #
  # These would be anything that the user does not need to know in order to use
  # the functions.
  # These notes can include internal workings of the function, the logic behind
  # it, or how to extend it.
  ```
* Sphinx parses [Python type
  annotations](https://docs.python.org/3/library/typing.html), so put type
  information into annotations instead of docstrings.
* The Sphinx-generated documentation is published to
  https://pyrtl.readthedocs.io/
* PyRTL's Sphinx build process is documented in
  [`docs/README.md`](https://github.com/UCSBarchlab/PyRTL/blob/development/docs/README.md).
* PyRTL's release process is documented in
  [`docs/release/README.md`](https://github.com/UCSBarchlab/PyRTL/blob/development/docs/release/README.md).

### Using PyRTL

We love to hear from users about their projects, and if there are issues we
will try our best to push fixes quickly. You can read more about how we have
been using it in our research at UCSB both in simulation and on FPGAs in [our
PyRTL paper at FPL](http://www.cs.ucsb.edu/~sherwood/pubs/FPL-17-pyrtl.pdf).

### Related Projects

It is always important to point out that PyRTL builds on the ideas of several
other related projects as we all share the common goal of trying to make
hardware design a better experience! You can read more about those
relationships on our [PyRTL project web
page](http://ucsbarchlab.github.io/PyRTL/).
","['timsherwood', 'Gamrix', 'mdko', 'fdxmw', 'nvandervoort', 'kmahorker', 'Bharath-shashi', 'shannon1006', 'dvicory', 'abe-k', 'jemcmahan13', 'trevorfrese', 'masoug', 'deekshadangwal', 'dependabot[bot]', 'dkupsh', 'sjvrijn', 'gaborszita', 'iamburitto', 'vaaniarora', 'DivyaJanaswamy', 'jolting', 'tirkarthi', 'vegaluisjose', 'nbp-lbl', 'PrajwalVandana', 'ryoon', 'sarthikg', 'SwiftWinds', 'TonyKorol1', 'VarunRao21', 'LinChai']",1,,0.73,0,,,,,,26,,,
179299245,MDEwOlJlcG9zaXRvcnkxNzkyOTkyNDU=,Hands-on-Design-Patterns-and-Best-Practices-with-Julia,PacktPublishing/Hands-on-Design-Patterns-and-Best-Practices-with-Julia,0,PacktPublishing,https://github.com/PacktPublishing/Hands-on-Design-Patterns-and-Best-Practices-with-Julia,"Hands-On Design Patterns with Julia, published by Packt",0,2019-04-03 13:45:53+00:00,2025-01-29 12:08:14+00:00,2025-01-20 21:29:51+00:00,,27900,265,265,Julia,1,0,1,1,0,0,53,0,0,2,mit,1,0,0,public,53,2,265,master,1,1,"


# Hands-On Design Patterns and Best Practices with Julia 

<a href=""https://www.packtpub.com/en-us/product/hands-on-design-patterns-and-best-practices-with-julia-9781838646615""><img src=""https://content.packt.com/_/image/original/B14109/cover_image_large.jpg"" alt=""Hands-On Design Patterns and Best Practices with Julia"" height=""256px"" align=""right""></a>

This is the code repository for [Hands-On Design Patterns and Best Practices with Julia ](https://www.packtpub.com/en-us/product/hands-on-design-patterns-and-best-practices-with-julia-9781838646615), published by Packt.

**Proven solutions to common problems in software design for Julia 1.x**

## What is this book about?
Design patterns are fundamental techniques for developing reusable and maintainable code. They provide a set of proven solutions that allow developers to solve problems in software development quickly. This book will demonstrate how to leverage design patterns with real-world applications.


This book covers the following exciting features:
* Master the Julia language features that are key to developing large-scale software applications 
* Discover design patterns to improve overall application architecture and design 
* Develop reusable programs that are modular, extendable, performant, and easy to maintain 
* Weigh up the pros and cons of using different design patterns for use cases 
* Explore methods for transitioning from object-oriented programming to using equivalent or more advanced Julia techniques

If you feel this book is for you, get your [copy](https://www.amazon.com/dp/183864881X) today!

<a href=""https://www.packtpub.com/?utm_source=github&utm_medium=banner&utm_campaign=GitHubBanner""><img src=""https://raw.githubusercontent.com/PacktPublishing/GitHub/master/GitHub.png"" 
alt=""https://www.packtpub.com/"" border=""5"" /></a>

## Instructions and Navigations
All of the code is organized into folders. For example, Chapter02.

The code will look like the following:
```
abstract type Formatter end
struct IntegerFormatter <: Formatter end
struct FloatFormatter <: Formatter end
```

**Following is what you need for this book:**
This book is for beginner to intermediate-level Julia programmers who want to enhance their skills in designing and developing large-scale applications.

With the following software and hardware list you can run all code files present in the book (Chapter 1-12).
### Software and Hardware List
| Chapter | Software required | OS required |
| -------- | ------------------------------------ | ----------------------------------- |
| 1-12 | Julia Version 1.3.0 or above | Windows, Mac OS X, and Linux (Any) |

## Code in Action

Click on the following link to see the Code in Action:

[YouTube](https://www.youtube.com/playlist?list=PLeLcvrwLe184DZW3gaIBXoAu0xHBt46SP)

## Errata

Page numbers below refer to the printed version. E-copy page numbers are slightly different.

* Page 39 and 49:
Before running the using Calculator command, he/she can first go to the Pkg mode (by pressing the ] key) and then run the dev Calculator command.  For example:
 ```
        (@v1.4) pkg> dev Calculator

        Path `/Users/tomkwong/.julia/dev/Calculator` exists and looks like the correct package. Using existing path.

          Resolving package versions...

           Updating `~/.julia/environments/v1.4/Project.toml`

          [17fd2872] + Calculator v0.1.0 [`~/.julia/dev/Calculator`]

           Updating `~/.julia/environments/v1.4/Manifest.toml`

          [17fd2872] + Calculator v0.1.0 [`~/.julia/dev/Calculator`]
```

* Page 54:
The `subtypetree` function does not work when there is cycle in the type hierarchy. It is generally not a problem with the exception that the `Any` type is a subtype of itself. So, running `subtypetree(Any)` would get into an infinite loop. Please see [Chapter02/subtypetree2.jl](Chapter02/subtypetree2.jl) for a more robust version.

* Page 67:
The hierarchy of numerical types in the diagram erroneously lists the concrete type `Bright` as a subtype of `Integer`; there is no such type that ships with Julia. For completeness, the diagram should have included the concrete type `BigInt` among the subtypes of the abstract type `Signed`.

* Page 110:
The signature of the first `explode` function under Using type parameters section should take `<: Any` rather than just `Any`.
```
function explode(things::AbstractVector{<:Any})
```

Because every type is a subtype of `Any`, it is more idiomatic to omit the parametric part and use with a simpler syntax:
```
function explode(things::AbstractVector)
```

* Page 219:
The `vendor_id` of the `TripPayment` has the wrong type. It should be `Int` rather than `String`.

* Page 249:
The second image on page number 249 is incorrect. The correct image is as follows:
<img src=""https://user-images.githubusercontent.com/1159782/93007143-19430980-f51a-11ea-9982-5fb58fc5ed01.png"">

* Page 318, 352, 494:
These pages contain typos where ""accessor"" was misspelled as ""assessor"".

### Related products
* Julia 1.0 High Performance  [[Packt]](https://www.packtpub.com/application-development/julia-10-high-performance?utm_source=github&utm_medium=repository&utm_campaign=9781788298117) [[Amazon]](https://www.amazon.com/dp/1785880918)

* Julia Programming Projects  [[Packt]](https://www.packtpub.com/big-data-and-business-intelligence/julia-programming-projects?utm_source=github&utm_medium=repository&utm_campaign=9781788292740) [[Amazon]](https://www.amazon.com/dp/178829274X)

## Get to Know the Author
**Tom Kwong** is an experienced software engineer with over 30 years of industry programming experience. He spent the majority of his career in the financial services and tech industries. He currenlty works at Meta and has worked on Privacy Aware Infra and Ads Measurement infra products.

His expertise includes software architecture, design, and the development of trading/risk systems. Since 2017, he has uncovered the Julia language and has worked on several open source packages, including SASLib.jl. He currently works at Western Asset Management Company, a prestige asset management company that specializes in fixed income investment services. He has MS degree in computer science from the University of California, Santa Barbara (1993), and he holds the Chartered Financial Analyst® designation during his tenure at Western Asset.

## Other books by the authors
### Suggestions and Feedback
[Click here](https://docs.google.com/forms/d/e/1FAIpQLSdy7dATC6QmEL81FIUuymZ0Wy9vH1jHkvpY57OiMeKGqib_Ow/viewform) if you have any feedback or suggestions.
### Download a free PDF

 <i>If you have already purchased a print or Kindle version of this book, you can get a DRM-free PDF version at no cost.<br>Simply click on the link to claim your free PDF.</i>
<p align=""center""> <a href=""https://packt.link/free-ebook/9781838648817"">https://packt.link/free-ebook/9781838648817 </a> </p>
","['tk3369', 'ketankamble', 'Packt-ITService', 'packtutkarshr', 'gwr69', 'tikshas', 'packt-pradeeps']",1,,0.65,0,,,,,,20,,,
221356137,MDEwOlJlcG9zaXRvcnkyMjEzNTYxMzc=,DeepAR-pytorch,husnejahan/DeepAR-pytorch,0,husnejahan,https://github.com/husnejahan/DeepAR-pytorch,,0,2019-11-13 02:33:33+00:00,2025-03-04 13:55:08+00:00,2021-05-03 11:07:55+00:00,,1339,258,258,Python,1,1,1,1,0,0,56,0,0,3,apache-2.0,1,0,0,public,56,3,258,master,1,,"# Original Authors:

Yunkai Zhang(yunkai_zhang@ucsb.edu) - University of California, Santa Barbara

Qiao Jiang - Brown University

Xueying Ma - Columbia University

Acknowledgement: Professor Xifeng Yan's group at UC Santa Barbara. Part of the work was done at WeWork.

https://github.com/zhykoties/TimeSeries


# List of Implementations:
Currently, the reimplementation of the DeepAR paper(DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks https://arxiv.org/abs/1704.04110) is available in PyTorch. More papers will be coming soon.



Traffic: https://archive.ics.uci.edu/ml/datasets/PEMS-SF

Electricity: https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014

Parts: https://robjhyndman.com/expsmooth/expsmooth_data.zip


## To run:


1. Install all dependencies listed in requirements.txt. Note that the model has only been tested in the versions shown in the text file.

1. Download the dataset and preprocess the data:
  
   ```bash
   python preprocess_elect.py
   ```
1. Start training:
  
   ```bash
   python train.py
   ```
   
   - If you want to perform ancestral sampling,
   
        ```bash
        python train.py --sampling
        ```
   - If you do not want to do normalization during evaluation,
              
   
        ```bash
        python train.py --relative-metrics
        ```
1. Evaluate a set of saved model weights:
        
   ```bash
   python evaluate.py
   ```
1. Perform hyperparameter search:
        
   ```bash
    python search_params.py
   ```

## Results

​	The model is evaluated on the electricity dataset, which contains the electricity consumption of 370 households from 2011 to 2014. Under hourly frequency, we use the first week of September, 2014 as the test set and all time steps prior to that as the train set. Following the experiment design in DeepAR, the window size is chosen to be 192, where the last 24 is the forecasting horizon. History (number of time steps since the beginning of each household), month of the year, day of the week, and hour of the day are used as time covariates. Notice that some households started at different times, so we only use windows that contain non-missing values.

​	Under Gaussian likelihood, we use the Adam optimizer with early stopping to train the model for 20 epoches. The same set of hyperparameters is used as outlined in the paper. Weights with the best ND value is selected, where __ND = 0.06349__, RMSE = 0.452, rou90 = 0.034 and rou50 = 0.063.

​	Sample results on electricity. The top 10 plots are sampled from the test set with the highest 10% ND values, whereas the bottom 10 plots are sampled from the rest of the test set.

![Sample results on electricity. The top 10 plots are sampled from the test set with the highest 10% ND values, whereas the bottom 10 plots are sampled from the rest of the test set.](./experiments/base_model/figures/best_ND.png)

",['husnejahan'],1,,0.73,0,,,,,,6,,,
79816786,MDEwOlJlcG9zaXRvcnk3OTgxNjc4Ng==,stancon_talks,stan-dev/stancon_talks,0,stan-dev,https://github.com/stan-dev/stancon_talks,Materials from Stan conferences,0,2017-01-23 15:24:12+00:00,2025-02-17 21:25:24+00:00,2020-10-22 07:09:22+00:00,https://mc-stan.org/,698774,254,254,HTML,1,1,1,1,0,0,83,1,0,1,,1,0,0,public,83,1,254,master,1,1,"<a href=""http://mc-stan.org"">
<img src=""https://raw.githubusercontent.com/stan-dev/logos/master/logo.png"" width=150 alt=""Stan Logo""/>
</a>

# Materials from StanCon

This repository contains materials from StanCon and links to StanCon content hosted elsewhere. Unless otherwise noted, the text in this repository is distributed under the [CC BY 4.0 License](https://creativecommons.org/licenses/by/4.0/legalcode) and code is distributed under the [New BSD License](https://opensource.org/licenses/BSD-3-Clause). Copyright to the authors.

### Contents:

**2020 Virtual Conference**

* [Videos](https://www.youtube.com/playlist?list=PLCrWEzJgSUqzI3goQEAKkDsHg72inmqbe)

**2019 Cambridge (UK)**

* [Abstracts](https://mc-stan.org/events/stancon2019Cambridge/abstracts.html)
* [Videos](https://mc-stan.org/events/stancon2019Cambridge/#recorded-talks)

**2018 Helsinki**

* [Contributed talks](#2018-helsinki-peer-reviewed-contributed-talks)
* [Invited talks](#2018-helsinki-invited-talks) 
* [Tutorials](#2018-helsinki-tutorials)

**2018 Asilomar**

* [Contributed talks](#2018-peer-reviewed-contributed-talks)
* [Invited talks](#2018-invited-talks) 

**2017 NYC**

* [Contributed talks](#2017-peer-reviewed-contributed-talks) 

<br>
  
## StanCon 2018 | August 29-31, Aalto University, Helsinki

StanCon’s version of conference proceedings is a collection of contributed talks based on interactive notebooks. Every submission is peer reviewed by at least two reviewers. The reviewers are members of the [Stan Conference Organizing Committee](https://mc-stan.org/events/stancon2018Helsinki/#committee) and the [Stan Developmemt Team](https://mc-stan.org/about/team/). This repository contains all of the accepted notebooks as well as any supplementary materials required for building the notebooks. The slides presented at the conference are also included.

### 2018 Helsinki Peer reviewed contributed talks


**_Solving ODEs in the wild: Scalable pharmacometrics with Stan_** 

* Authors: Sebastian Weber (Novartis)

Pharmacometric modeling involves nonlinear hierarchical models, which
are most naturally expressed as forced ordinary differential equations
(ODEs). These class of models lead to a number of challenges which
complicate a practical modeling work-flow in Stan mostly due to long
model execution times. This contribution demonstrates at the example
of the drug Warfarin how forced ODEs can be written efficiently in
Stan leading to a doubling in model evaluation speed for the presented
example. Finally, it is demonstrated how the new `map_rect` facility
in Stan can be used to make models scalable to large data sets leading
to substantial speedups in model evaluation time and most importantly
this enables to *scale* Stan's performance as needed.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465996.svg)](https://doi.org/10.5281/zenodo.1465996)

Links: 

* [Video](https://www.youtube.com/watch?v=wcpjZC9AV84&t=1h44m33s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/weber/stancon18-master) 

<br>

**_Analysis of repeated measures data in RStan_** 

* Authors: Marco Munda (Pharmalex)

We illustrate the analysis of repeated measures data in the Bayesian framework using RStan.
In addition to the modelling itself, we further show how to make inference on the primary effect based on a probability of success, and how to predict the longitudinal profile of a future patient, two difficult (if not impossible) tasks from a frequentist perspective.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465988.svg)](https://doi.org/10.5281/zenodo.1465988)

Links: 

* Video not available
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/weber/stancon18-master) 

<br>

**_Define custom response distributions with brms_** 

* Authors: Paul Bürkner (University of Münster)

The **brms** package (Bürkner, 2017a, 2017b) implements Bayesian regression models using the probabilistic programming language **Stan** (Carpenter et al., 2016) behind the scenes. It has grown to be one of the most flexible R packages when it comes to multilevel regression modelling. A wide range of response distributions are supported, allowing users to fit -- among others -- linear, robust linear, count data, survival, response times, ordinal, zero-inflated, hurdle, and even self-defined mixture models all in a multilevel context. Predictor terms can be specified with a simple yet powerful formula syntax. Thanks to **Stan**, even very complex models tend to converge well in a reasonable amount of time. While **brms** comes with a lot of built-in response distributions (usually called *families* in R), there is still a long list of distributions which are not natively supported. The present notebook will explain how to specify such *custom families* in **brms**. By doing that, users can benefit from the modeling flexibility and post-processing options of **brms** even when applying self-defined response distributions.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465958.svg)](https://doi.org/10.5281/zenodo.1465958)

Links: 

* [Video](https://www.youtube.com/watch?v=FoaxA7sJi7w&t=2h13m03s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/buerkner/buerkner_notebook.Rmd) 

<br>


**_Are shots predictive of soccer results?_** 

* Authors: Leonardo Egidi, Francesco Pauli, Nicola Torelli (University of Trieste)

Predicting the outcome of a soccer match is the subject of much debate, and several models based on different assumptions 
have been proposed for modeling the numbers of goals scored by two competing teams. In this case study we adopt a different perspective and propose a Bayesian hierarchical model consisting of three nested multiple outcomes: 
**number of scores**, **number of shots on target** and **number of total shots**. We model the number of scores and the number of shots on target with  <span style=""color:red"">two binomial 
distributions</span> respectively, whereas the total shots follow a negative binomial distributon. Our dataset consists of nine seasons of the English Premier League (EPL): eight seasons---from 2008/2009 to 2015/2016, 3040 matches---represent the train set, whereas the nineth season, 2016/2017 (with the remaining 380 matches), is our test set, used for out-of sample prediction. 

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465963.svg)](https://doi.org/10.5281/zenodo.1465963)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=4h15m39s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/egidi/egidi) 

<br>

**_Flexible models of holiday lift_** 

* Authors: Alex Braylan, Dan Marthaler (Revionics)

We develop a flexible, portable, and interpretable Bayesian model of cyclical holiday effects on time series. Our model uses five parameters for each possible holiday that capture the general shape, magnitude, and peak location offset of each holiday effect. Choice of priors prevents the model from overfitting while still achieving considerable flexibility. We experiment on simulated and real data from Google Trends and demonstrate the model's performance on held-out data.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465943.svg)](https://doi.org/10.5281/zenodo.1465943)

Links: 

* [Video](https://www.youtube.com/watch?v=FoaxA7sJi7w&t=18m15s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/braylan) 

<br>

**_Artificial turf advantage and predictive accuracy in Dutch football_** 

* Authors: Gertjan Verhoeven 

This submission uses Stan to learn about the so-called artificial turf advantage in Dutch football. Two model variants are used to model match outcomes. One of the models is the model from Milad Kharratzadeh presented at Stancon 2017, the other is new to Stan (a dynamic Skellam model). I use out-of-sample forecasts together with the Ranked Probability Score (a proper scoring rule) to learn whether including the artificial turf advantage increases predictive accuracy. Bookmakers odds are used as a benchmark to check the quality of our forecasts.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465994.svg)](https://doi.org/10.5281/zenodo.1465994)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=4h40m34s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/verhoeven/artificial_turf_predictive-master) 

<br>

**_ODE model of gene regulation_** 

* Authors: Martin Modrák (Czech Academy of Sciences)

In this notebook we fit time series of gene expression data with a 
non-linear ODE-based model. Splines are used to model noisily observed 
regulator expression. The ODE is not solved explicitly, it is instead 
transformed to a definite integral and integrated via the trapezoid 
rule. Some interesting reparametrizations are introduced to make the 
model well identified.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465978.svg)](https://doi.org/10.5281/zenodo.1465978)

Links: 

* [Video](https://www.youtube.com/watch?v=wcpjZC9AV84&t=1h24m10s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/modrak/genexpi-stan) 

<br>

**_Predicting New York City school enrollment_** 

* Authors: Jonathan Auerbach, Timothy Jones, Robin Winstanley (Columbia University)

We propose a Bayesian hierarchical Age-Period-Cohort model to predict elementary 
school enrollment in New York City. We demonstrate this model using student 
enrollment data for grades K-5 in each Census Tract of Brooklyn's 20th School 
District over the 2001-02 to 2010-11 school years. Specifically, our model 
disaggregates enrollment into grade (age), year (period), and cohort effects so 
that each can be interpreted and extrapolated over the 2011-12 to 2017-18 school 
years. We find this approach ideal for incorporating spatial information 
indicative of the socioeconomic forces that determine school enrollment in New 
York City.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465936.svg)](https://doi.org/10.5281/zenodo.1465936)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=4h02m50s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/auerbach_jones_winstanley/auerbach_jones_winstanley_notebook.Rmd) 

<br>

**_Analyzing brain taxonomy trees_** 

* Authors: Chris Hammill, Jason Lerch

This talk and notebook introduce the idea of analyzing brain anatomy as a taxonomy of structures, defined by containment, using Stan. This taxonomy imposes dependence between model coefficients for structures and their enclosing structure. The slides and notebook compare this approach to other hierarchical modelling strategies using the mouse brain for illustration.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465967.svg)](https://doi.org/10.5281/zenodo.1465967)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=2h18m48s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/hammill) 

<br>


**_Getting more out of Stan: some ideas from the Haskell bindings_** 

* Authors: Thomas A. Nielsen (Tweag I/O), Dominic Steinitz (Tweag I/O), Henrik Nilsson (University of Nottingham)

We present draft bindings to Stan in Haskell, a purely functional programming language. Unlike in most bindings, our models are encoded as a data type the host language. We show how this can be used to widen the range of computations that can be done based on the Stan model definition. For instance, predictions, posterior predictive checks and residual calculations can be done based on a single model definition.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465992.svg)](https://doi.org/10.5281/zenodo.1465992)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=5h05m28s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/nielsen) 

<br>

**_Dose-finding clinical trial designs in Stan with trialr_** 

* Authors: Kristian Brock (University of Birmingham)

My notebook illustrates two different methods for conducting dose-finding clinical trials.
The first, CRM, escalates dose according to toxicity outcomes only, assuming implicitly that higher doses are more likely to benefit the patient.
The second, EffTox, escalates dose according to efficacy and toxicity outcomes, thus addressing the potential that higher may not always mean better.
These models are implemented in the trialr R-package using Stan.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465952.svg)](https://doi.org/10.5281/zenodo.1465952)

Links: 

* [Video](https://www.youtube.com/watch?v=FoaxA7sJi7w&t=2h32m49s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/brock) 

<br>

**_""The implementation of a model of choice: the (truncated) linear ballistic accumulator._** 

* Authors: Bruno Nicenboim (University of Potsdam)

It is very common in cognitive science and psychology to use experimental tasks that involve making a fast choice among a restricted number of alternatives.  In this notebook, I focus on one influential and relatively simple model that belongs to the class of sequential-sampling models: the linear ballistic accumulator with a drift rate drawn from a normal distribution (restricted to positive values) (S. D. Brown and Heathcote 2008; Heathcote and Love 2012). First, I discuss the motivation for fitting this model using the Stroop task (Stroop 1935) as a case study. Then, I discuss the challenges in the implementation of the model in (R)Stan (Stan Development Team 2017), which might also apply to other hierarchical models with complex likelihood functions. Finally, I show some results that exemplify how the linear ballistic accumulator can be used for examining individual differences.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465990.svg)](https://doi.org/10.5281/zenodo.1465990)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=1h46m05s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/nicenboim) 

<br>

**_Hierarchical Ornstein-Uhlenbeck type t-processes_** 

* Authors: Ville Laitinen, Leo Lahti (University of Turku)

This work investigates probabilistic time series models that are motivated by applications in statistical ecology. In particular, we investigate variants of the mean-reverting and stochastic Ornstein-Uhlenbeck (OU) process. We provide a hierarchical extension for joint analysis of multiple (short) time series, validate the model, and analyze its performance with simulations. The works extends the recent Stan implementation of the OU process (A 2018), where parameter estimates of a Student-t type OU process are obtained based on a single (long) time series. We have added a level of hierarchy, which allows joint inference of the model parameters across multiple time series.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1471578.svg)](https://doi.org/10.5281/zenodo.1471578)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=2h05m53s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/laitinen) 

<br>

**_GPU optimized math routines in the Stan math library_** 

* Rok &#268;e&#353;novar, Davor Sluga, Jure Dem&#353;ar, Steve Bronder, Erik &#352;trumbelj

The Stan Math library's Hamilton Monte Carlo (HMC) sampler has computationally expensive draws while usually searching the target distribution more efficiently than alternative MCMC methods with fewer iterations. The bottleneck within draws makes Stan a prime candidate for GPU optimizations within samples. This project implements GPU optimizations for the Cholesky decomposition and it's derivative in the Stan Math library [@stanmath2015]. This work is the first known open source implementation of the Cholesky decomposition with a GPU in an HMC setting. Furthermore, the GPU kernels use OpenCL which allows the use of these methods across any brand of GPU. While results show that GPU optimizations are not optimal for small $N\times M$ matrices, large matrices can see speedups of 7.8x while retaining the same precision as models run purely on a CPU.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1471578.svg)](https://doi.org/10.5281/zenodo.1471578)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=5h46m00s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/Bronder) 

<br>
 
**_Relating Disparate Measures of Coagulapathy Using Unorthodox Data: A Hybrid
  Mechanistic-Statistical Approach_** 

* Authors: Arya A. Pourzanjani, Tie Bo Wu, Benjamin B. Bales, Linda R. Petzold (University
  of California, Santa Barbara)
  
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1489142.svg)](https://doi.org/10.5281/zenodo.1489142)

Links: 

* [Video](https://www.youtube.com/watch?v=wcpjZC9AV84&t=1h02m58s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/arya) 

 
<br>
 
**_Modeling the Effects of Nutrition with Mixed-Effect Bayesian Network_** 

* Authors: Jari Turkia (University of Eastern Finland)

This work proposes Mixed-Effect Bayesian Network (MEBN) as a method for modeling the effects of nutrition. It allows identifying both typical and personal correlations between nutrients and their bodily responses. Predicting a personal network of nutritional reactions would allow interesting applications at personal diets and in understanding this complex system. Brief theory of MEBN is first given, followed by the implementation in R and Stan. A real life dataset from a nutritional study (Sysdimet) is then analyzed with this method and the results are visualized with a responsive JavaScript-visualization.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1489146.svg)](https://doi.org/10.5281/zenodo.1489146)

Links: 

* [Video](https://www.youtube.com/watch?v=FoaxA7sJi7w&t=39m11s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/Turkia) 

<br>
 
**_Using counterfactual queries to improve models for decision-support_** 

* Authors: Sundin, Iiris(1) Peter Schulam(2) Eero Siivola(1) Aki Vehtari(1) Suchi Saria(2) Samuel Kaski(1)
1 Department of Computer Science, Aalto University, Espoo, Finland
2 Department of Computer Science, Johns Hopkins University

In this extended abstract, we generalize active learning to tasks where a human has to choose which action a to take for a target after observing its covariates x ̃ and predicted outcomes p( ̃y|x, a  ̃ ).
An example case is personalized medicine and the decision of which treatment to give to a pa-
tient. We show that standard active learning, which is not aware of the final task, would be
very inefficient, and we introduce a new problem of decision-making-aware active learning. We for-
mulate the problem as finding the query with the highest information gain for the specific decision-
making task, assuming a rational decision-maker. The problem can be solved particularly efficiently
assuming an expert able to answer queries about counterfactuals. We demonstrate the effective-
ness of the proposed method in a binary outcome decision-making task using simulated data, and in a continuous-valued outcome task on the medical dataset IHDP with synthetic treatment outcomes. The outcomes are predicted using Gaussian processes.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1489150.svg)](https://doi.org/10.5281/zenodo.1489150)

Links: 

* [Video](https://www.youtube.com/watch?v=FoaxA7sJi7w&t=0m00s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/Sundin.pdf) 


### 2018 Helsinki Invited talks 

**_Hierarchical modelling of galaxy clusters for cosmology_**
    
* Presenter: Maggie Lieu (European Space Agency)
* [Video](https://youtu.be/wcpjZC9AV84?t=890)
* [Slides](2018-helsinki/Invited-Talks/lieu.pdf) 


**_Bad data, big models & statistical methods for studying evolution_**
    
* Presenter: Richard McElreath (Max Planck Institute for Evolutionary Anthropology)
* [Video](https://youtu.be/FoaxA7sJi7w?t=3581)
* [Slides](2018-helsinki/Invited-Talks/mcelreath.pdf) 

**_Identifying the effect of public holidays on daily demand for gas_**
    
* Presenter: Sarah Heaps (Newcastle University)
* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=585s)
* [Slides](2018-helsinki/Invited-Talks/heaps.pdf) 


**_Esther Williams in the Harold Holt Memorial Swimming Pool_**
    
* Presenter: Daniel Simpson (University of Toronto)
* [Video](https://youtu.be/pKZLJPrZLhU?t=26305)
* [Slides](2018-helsinki/Invited-Talks/simpson.pdf) 

### 2018 Helsinki tutorials

**_Basics of Bayesian inference and Stan_**

* Instructors: Jonah Gabry & Lauren Kennedy
* [Video part 1](https://www.youtube.com/watch?v=ZRpo41l02KQ&t=18s&index=6&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Video part 2](https://www.youtube.com/watch?v=6cc4N1vT8pk&t=2s&index=7&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Slides and code](https://github.com/jgabry/stancon2018helsinki_intro)

**_Hierarchical models_**

* Instructor: Ben Goodrich 
* [Video part 1](https://www.youtube.com/watch?v=DzaQiJG3RpA&t=4s&index=8&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Video part 2](https://www.youtube.com/watch?v=DPnLb5EaCkA&t=0s&index=9&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Slides](http://mc-stan.org/workshops/stancon2018_hierarchical)

**_Stan C++ development: adding a new function to Stan_**

* Instructors: Bob Carpenter, Sean Talts, and Mitzi Morris. 
* [Video part 1](https://www.youtube.com/watch?v=tIzxfFxCb2Y&t=4s&index=13&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Video part 2](https://www.youtube.com/watch?v=pHatMUlK6nk&t=1s&index=5&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)

**_Ordinary differential equation (ODE) models in Stan_**

* Instructor: Daniel Lee 
* [Video](https://www.youtube.com/watch?v=hJ34_xJhYeY&t=217s&index=11&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)

**_Productization of Stan_**

Instructor: Eric Novik 
Productization panel discussion: Markus Ojala (Smartly), Tom Nielsen (Tweag.io), Anna Kircher (Lendable), Eric Novik (Generable)
* [Video](https://www.youtube.com/watch?v=4vfilYZ-F3A&t=54s&index=12&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)

**_Model assessment and selection_**

* Instructor: Aki Vehtari 
* [Video](https://www.youtube.com/watch?v=hpr8pxqkCH8&t=1329s&index=10&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Slides and demos](https://avehtari.github.io/modelselection/)

<br>
  
## StanCon 2018 | January 10-12, Asilomar, California  

StanCon’s version of conference proceedings is a collection of contributed talks based on interactive notebooks. Every submission is peer reviewed by at least two reviewers. The reviewers are members of the Stan Conference Organizing Committee and the [Stan Developmemt Team](https://mc-stan.org/about/team/). This repository contains all of the accepted notebooks as well as any supplementary materials required for building the notebooks. The slides presented at the conference are also included.

### 2018 Peer reviewed contributed talks

**_Does the New York City Police Department rely on quotas?_** 

* Authors: Jonathan Auerbach (Columbia University)

This submission investigates whether the New York City Police Department (NYPD) uses productivity targets or quotas to manage officers in contravention of New York State Law. The analysis is presented in three parts. First, the NYPD's employee evaluation system is introduced, and the criticism that it constitutes a quota is summarized. Secondly, a publically available dataset of traffic tickets issued by NYPD officers in 2014 and 2015 is described. Finally, a generative model to describe how officers write traffic tickets is proposed. The fitted model is consistent with the criticism that police officers substantially alter their ticket writing to coincide with departmental targets. The submission concludes by discussing the implication of these findings and offering directions for further research.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284317.svg)](https://doi.org/10.5281/zenodo.1284317)

Links: 

  - [Video](https://youtu.be/5qojKAiirqI)
  - [Notebook, code, slides](2018/Contributed-Talks/01_auerbach) 
  - <a href=""https://github.com/jauerbach""> github.com/jauerbach</a>

  
<br>  

**_Diagnosing Alzheimer’s the Bayesian way_** 

* Authors: Arya A. Pourzanjani, Benjamin B. Bales, Linda R. Petzold, Michael Harrington (UC Santa Barbara)

Alzheimer's Disease is one the most debilitating diseases, but how do we diagnose it accurately? Researchers have been trying to answer this question by building generative models to describe how patient biomarkers, such as MRI scans, psychological tests, and lab tests relate over time to the underlying brain deterioration that's present in Alzheimer's Disease. In this notebook we show how we translated these models to the Bayesian framework in Stan and how this allowed for several model improvements that can ultimately improve our understanding of Alzheimer's and help physicians in diagnosis. In particular, we describe how we hierarchically model patient disease trajectories to obtain stable estimates for patients who lack data. We describe how fitting in Stan yields uncertainties on these disease trajectories, and why that is important for weighing the pros and cons of risky treatment. Lastly, we describe a new method for Bayesian modeling of these monotonic disease trajectories in Stan using I-Splines.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284330.svg)](https://doi.org/10.5281/zenodo.1284330)

Links: 

  - [Video](https://youtu.be/j_JIfNiO9TA)
  - [Notebook, code, slides](2018/Contributed-Talks/02_pourzanjani)
  - [https://github.com/pourzanj/Stancon2018\_Alzheimers](https://github.com/pourzanj/Stancon2018_Alzheimers)
  - <a href=""https://aryastats.com/""> aryastats.com</a>

<br>  

**_Joint longitudinal and time-to-event models via Stan_** 

* Authors: Sam Brilleman, Michael Crowther, Margarita Moreno-Betancur, Jacqueline Buros Novik, Rory Wolfe (Monash University, Columbia University)

The joint modelling of longitudinal and time-to-event data has received much attention in the biostatistical literature in recent years. In this notebook (and talk), we describe the implementation of a shared parameter joint model for longitudinal and time-to-event data in Stan. The methods described in the
notebook are a simplified version of those underpinning the `stan_jm` modeling function that has recently been contributed to the [**rstanarm**](http://mc-stan.org/rstanarm) R package.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284334.svg)](https://doi.org/10.5281/zenodo.1284334)

Links: 

  - [Video](https://youtu.be/8r-Ipt885FA)
  - [Notebook, code, slides](2018/Contributed-Talks/03_brilleman) 
  - [github.com/sambrilleman/2018-StanCon-Notebook](https://github.com/sambrilleman/2018-StanCon-Notebook)
  - <a href=""http://www.sambrilleman.com/""> sambrilleman.com</a>


<br>

**_A tutorial on Hidden Markov Models using Stan_** 

* Authors: Luis Damiano, Brian Peterson, Michael Weylandt 

We implement a standard Hidden Markov Model (HMM) and the Input-Output Hidden Markov Model for unsupervised learning of time series dynamics in Stan. We begin by reviewing three commonly-used algorithms for inference and parameter estimation, as well as a number of computational techniques and modeling strategies that make full Bayesian inference practical. For both models, we demonstrate the effectiveness of our proposed approach in simulations. Finally, we give an example of embedding a HMM within a larger model using an example from the econometrics literature.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284341.svg)](https://doi.org/10.5281/zenodo.1284341)

Links: 

  - [Video](https://youtu.be/oe9PAEI97oI)
  - [Notebook, code, slides](2018/Contributed-Talks/04_damiano) 
  - <a href=""https://github.com/luisdamiano/stancon18""> github.com/luisdamiano/stancon18</a>

  
<br>

**_Student Ornstein-Uhlenbeck models served three ways (with applications for population dynamics data)_** 

* Authors: Aaron Goodman (Stanford University)

Ornstein-Uhlenbeck (OU) processes are a mean reverting process and is used to model dynamics in biology, physics, and finance. I fit an extension of the OU process that is driven by a Lévy process with Student's t-marginals rather than Brownian motion with Gaussian marginals, which allows for heavy-tailed increments. I implement four formulations of the Student-t OU-type model in Stan and compare the sampling performance on both real and simulated population dynamic data. 

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284346.svg)](https://doi.org/10.5281/zenodo.1284346)

Links: 

  - Video (coming soon)
  - [Notebook, code, slides](2018/Contributed-Talks/05_goodman) 
  - [github.com/aaronjg/outype\_t\_process\_stan](https://github.com/aaronjg/outype_t_process_stan)
  - <a href=""https://web.stanford.edu/~aaronjg/""> web.stanford.edu/~aaronjg</a>

  
<br>  

**_SlicStan: a blockless Stan-like language_** 

* Authors: Maria I. Gorinova, Andrew D. Gordon, Charles Sutton (University of Edinburgh) 

We present SlicStan — a probabilistic programming language that compiles to Stan and uses static analysis techniques to allow for more abstract and flexible models. SlicStan is novel in two ways: (1) it allows variable declarations and statements to be automatically shredded into different components needed for efficient Hamiltonian Monte Carlo inference, and (2) it introduces more flexible user-defined functions that allow for new model parameters to be declared as local variables. This work demonstrates that efficient automatic inference can be the result of the machine learning and programming languages communities joint efforts.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284348.svg)](https://doi.org/10.5281/zenodo.1284348)

Links: 

  - [Video](https://youtu.be/WTqnehdFNbo)
  - [Notebook, code, slides](2018/Contributed-Talks/06_gorinova) 
  - [github.com/mgorinova/SlicStan-Paper](https://github.com/mgorinova/SlicStan-Paper)
  - <a href=""http://homepages.inf.ed.ac.uk/s1207807/""> homepages.inf.ed.ac.uk/s1207807</a>, <a href=""https://www.microsoft.com/en-us/research/people/adg/""> microsoft.com/en-us/research/people/adg</a>, <a href=""http://homepages.inf.ed.ac.uk/csutton/""> http://homepages.inf.ed.ac.uk/csutton</a> 


<br>

**_idealstan: an R package for ideal point modeling with Stan_** 

* Authors: Robert Kubinec (University of Virginia)

Item-response theory (IRT) ideal-point scaling/dimension reduction methods that incorporate additional response categories and missing/censored values, including absences and abstentions, for roll call voting data (or any other kind of binary or ordinal item-response theory data). Full and approximate Bayesian inference is done via Stan.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284361.svg)](https://doi.org/10.5281/zenodo.1284361)

Links: 

  - [Video](https://youtu.be/0ZjrLOosXwk)
  - [Notebook, code, slides](2018/Contributed-Talks/07_kubinec) 
  - [https://CRAN.R-project.org/package=idealstan](https://CRAN.R-project.org/package=idealstan)


<br>  

**_Computing steady states with Stan’s nonlinear algebraic solver_** 

* Authors: Charles C. Margossian (Metrum, Columbia University)

Stan’s numerical algebraic solver can be used to solve systems of nonlinear algebraic equations with no closed form solutions. One of its key applications in scientific and engineering fields is the computation of equilibrium states (equivalently steady states). This case study illustrates the use of the algebraic solver by applying it to a problem in pharmacometrics. In particular, I show the algebraic system we solve can be quite complex and embed, for instance, numerical solutions to ordinary differential equations. The code in R and Stan are provided, and a Bayesian model is fitted to simulated data. 

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284375.svg)](https://doi.org/10.5281/zenodo.1284375)

Links: 

  - [Video](https://youtu.be/JhwZIX5ryw0) 
  - [Notebook, code, slides](2018/Contributed-Talks/08_margossian) 
  - [github.com/charlesm93](https://github.com/charlesm93)


<br>

**_Bayesian estimation of mechanical elastic constants_** 

* Authors: Ben Bales, Brent Goodlet, Tresa Pollock, Linda Petzold (UC Santa Barbara)

This outlines a Bayesian approach to resonance ultrasound spectroscopy (RUS), a technique for estimating elastic constants of a material from a sample's measured resonance modes. The notebook includes an example of how to take advantage of custom automatic differentiation in specialized Stan models (either for numerical or efficiency reasons).

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285265.svg)](https://doi.org/10.5281/zenodo.1285265)

Links: 

  - [Video](https://youtu.be/vOoZBTpN8n4)
  - [Notebook, code, slides](2018/Contributed-Talks/09_bales) 
  - [github.com/bbbales2/stancon_2018](https://github.com/bbbales2/stancon_2018)


<br>

**_Aggregate random coefficients logit — a generative approach_** 

* Authors: Jim Savage (Lendable Marketplace), Shoshana Vasserman (Harvard University).

This notebook illustrates how to fit aggregate random coefficient logit models in Stan, using Bayesian techniques. It’s far easier to learn and implement than the standard BLP algorithm, and has the benefits of being robust to mismeasurement of market shares, and giving limited-sample posterior uncertainty of all parameters (and demand shocks). This comes at the cost of modeling firms’ price-setting process, including how unobserved product-market demand shocks affect prices.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285268.svg)](https://doi.org/10.5281/zenodo.1285268)

Links: 

  - [Video](https://youtu.be/LDOhRIRRe8M)
  - [Notebook, code, slides](2018/Contributed-Talks/10_savage) 
  - [github.com/khakieconomics](https://github.com/khakieconomics), [github.com/shoshievass](https://github.com/shoshievass)


<br>

**_The threshold test: Testing for racial bias in vehicle searches by police_** 

* Authors: Camelia Simoiu, Sam Corbett-Davies, Sharad Goel, Emma Pierson (Stanford University)

We develop a new statistical test to detect bias in decision making — the threshold test—that mitigates the problem of infra-marginality by jointly estimating decision thresholds and risk distributions.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285270.svg)](https://doi.org/10.5281/zenodo.1285270)

Links: 

  - [Video](https://youtu.be/vEO-rjAqGW8)
  - [Notebook, code, slides](2018/Contributed-Talks/11_simoiu) 
  - [github.com/camioux/stancon2018](https://github.com/camioux/stancon2018) 
  - [web.stanford.edu/~csimoiu](http://web.stanford.edu/~csimoiu/), [samcorbettdavies.com](https://samcorbettdavies.com/), [cs.stanford.edu/~emmap1](https://cs.stanford.edu/~emmap1/), [5harad.com](https://5harad.com/)


<br>

**_Assessing the safety of Rosiglitazone for the treatment of type II diabetes_** 

* Authors: Konstantinos Vamvourellis, K. Kalogeropoulos, L. Phillips (London School of Economics and Political Science) 

A Bayesian paradigm for making drug approval decisions. Case study in the treatment of Diabetes (Type 2).

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285274.svg)](https://doi.org/10.5281/zenodo.1285274)

Links: 

  - [Video](https://youtu.be/Gt73VNaZLXA)
  - [Notebook, code, slides](2018/Contributed-Talks/12_vamvourellis) 
  - [github.com/bayesways/case\_studies\_R/tree/master/stancon18](https://github.com/bayesways/case_studies_R/tree/master/stancon18)
  - [personal.lse.ac.uk/vamourel/](http://personal.lse.ac.uk/vamourel/)


<br>

**_Causal inference with the g-formula in Stan_** 

* Authors: Leah Comment (Harvard University)

The potential outcomes framework often uses one or more parametric outcome models to learn about underlying causal processes. In Stan, parameter estimation using observed data takes place in the model block, while simulation-based estimation of causal parameters using the g-formula can be done separately with generated quantities. Bayesian estimation allows for data-driven sensitivity analysis regarding the assumption of no unmeasured confounding. This presentation shows some simple causal models, then outlines a basic sensitivity analysis using prior information derived from an external data source.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285276.svg)](https://doi.org/10.5281/zenodo.1285276)

Links: 

  - [Video](https://youtu.be/W3gnbG0v4IE)
  - [Notebook, code, slides](2018/Contributed-Talks/13_comment) 
  - [https://github.com/lcomm/stancon2018](https://github.com/lcomm/stancon2018)
  - [scholar.harvard.edu/leahcomment](https://scholar.harvard.edu/leahcomment/)

<br>

**_Bayesian estimation of ETAS models with Rstan_** 

* Authors: Fausto Fabian Crespo Fernandez (Universidad San Francisco de Quito)

Earthquake modeling with Stan. Applied to seismic recurrence in Ecuador in 2016.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285278.svg)](https://doi.org/10.5281/zenodo.1285278)

Links: 

  - [Video](https://youtu.be/hTswMCRzltQ)
  - [Notebook, code, slides](2018/Contributed-Talks/14_crespo) 
  - [linkedin.com/in/phd-student-fausto-fabian-crespo-fernandez](https://www.linkedin.com/in/phd-student-fausto-fabian-crespo-fernandez-2b457a71/)


<br>

### 2018 Invited talks 

**_Predictive information criteria in hierarchical Bayesian models for clustered data_**
    
* Presenters: Sophia Rabe-Hesketh, Daniel Furr (UC Berkeley)
* [Video](https://youtu.be/FiSw6adfZcY)
* [Slides and code](2018/Invited-Talks/RabeHesketh_Furr) 
* [gse.berkeley.edu/people/sophia-rabe-hesketh](https://gse.berkeley.edu/people/sophia-rabe-hesketh), [github.com/danielcfurr](https://github.com/danielcfurr)

**_ScalaStan_** 

* Presenter: Joe Wingbermuehle (Cibo Technologies)
* [Video](https://youtu.be/OtggQJI4J7U)
* [Slides](2018/Invited-Talks/Wingbermuehle.pdf) 
* <a href=""https://github.com/cibotech/ScalaStan""> github.com/cibotech/ScalaStan</a>

**_Stan applications in physics: Testing quantum mechanics and modeling neutrino masses_**

* Presenter: Talia Weiss (MIT)
* [Slides](2018/Invited-Talks/Weiss.pdf) 
* [https://www.linkedin.com/in/talia-weiss-184753139](https://www.linkedin.com/in/talia-weiss-184753139/)


**_Forecasting at scale: How and why we developed Prophet for forecasting at Facebook_**

* Presenters: Sean Taylor, Ben Letham (Facebook)
* [Video](https://youtu.be/E8z3LObimok)
* [research.fb.com/facebook-at-stancon-2018](https://research.fb.com/facebook-at-stancon-2018/)
* [facebook.github.io/prophet](https://facebook.github.io/prophet/)


**_Stan applications in human genetics: Prioritizing genetic mutations that protect individuals from human disease_**

* Presenter: Manuel Rivas (Stanford University)
* [Video](https://youtu.be/S6FzGHPPxV4) 
* [Slides](2018/Invited-Talks/Rivas.pdf)
* [med.stanford.edu/rivaslab](http://med.stanford.edu/rivaslab.html)

**_Statistics using geometry to show uncertainties and integrate graph information_**

* Presenter: Susan Holmes (Stanford University)
* [Video]( https://youtu.be/W8TxxN8UdDQ)
* [Slides](2018/Invited-Talks/Holmes.pdf)
* [statweb.stanford.edu/~susan](http://statweb.stanford.edu/~susan/)

  
**_A brief history of Stan_** 

* Presenter: Daniel Lee (Generable)
* [Video](https://youtu.be/xJTZKawa-bM)
* [Slides](2018/Invited-Talks/Lee.pdf)
* [github.com/syclik](https://github.com/syclik)


**_Model assessment, model selection and inference after model selection_** 

* Presenter: Aki Vehtari (Aalto University)
* [Video](https://youtu.be/FUROJM3u5HQ)
* [Notebook, code, slides](https://github.com/avehtari/modelselection_tutorial) 
* [users.aalto.fi/~ave/](https://users.aalto.fi/~ave/)
  
  
**_Spatial models in Stan: intrinsic auto-regressive models for areal data_** 

* Presenter: Mitzi Morris (Columbia University)
* [Video](https://youtu.be/bwLkumivtjU)
* [Slides](2018/Invited-Talks/Morris.pdf)
* [Case study](http://mc-stan.org/users/documentation/case-studies/icar_stan.html)
* [github.com/mitzimorris](https://github.com/mitzimorris)

**_Some problems I'd like to solve in Stan, and what we'll need to do to get there_** 

* Presenter: Andrew Gelman (Columbia University) 
* [Video](https://youtu.be/uDB_NF_i5Ps)

<br>

## StanCon 2017 | January 21, Columbia University, New York

StanCon’s version of conference proceedings is a collection of contributed talks based on interactive notebooks. Every submission is peer reviewed by at least two reviewers. The reviewers are members of the Stan Conference Organizing Committee and the [Stan Developmemt Team](https://mc-stan.org/about/team/). This repository contains all of the accepted notebooks as well as any supplementary materials required for building the notebooks. The slides presented at the conference are also included.

### 2017 Peer reviewed contributed talks

**_Twelve Cities: Does lowering speed limits save pedestrian lives?_**      

* Authors: Jonathan Auerbach, Rob Trangucci (Columbia University)

We investigate whether American cities can expect to achieve a meaningful reduction in pedestrian deaths by lowering the posted speed limit. We find some evidence that a lower speed limit does in fact reduce fatality rates, and our estimated causal effect is similar to the traditional before-after analysis espoused by policy analysts. Nevertheless, we conclude that adjusting the posted speed limit in urban environments does not correspond with a reliable reduction in pedestrian fatalities.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1283490.svg)](https://doi.org/10.5281/zenodo.1283490)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=1h58m49s)
  - [Notebook and materials](2017/Contributed-Talks/01_auerbach)
  - [Slides](2017/Contributed-Talks/slides/01_auerbach_stancon_slides.pdf)
  - https://github.com/jauerbach


<br> 

**_Hierarchical Bayesian Modeling of the English Premier League_**    

* Authors: Milad Kharratzadeh (Columbia University)

In this case study, we provide a hierarchical Bayesian model for the English Premier League in the season of 2015/2016. The league consists of 20 teams and each two teams play two games with each other (home and away games). So, in total, there are 38 weeks, and 380 games. We model the score difference (home team goals − away team goals) in each match. The main parameters of the model are the teams’ abilities which is assumed to vary over the course of the 38 weeks. The initial abilities are determined by performance in the previous season plus some variation.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1283496.svg)](https://doi.org/10.5281/zenodo.1283496)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=2h12m12s)
  - [Notebook and materials](2017/Contributed-Talks/02_kharratzadeh); 
  - [Slides](2017/Contributed-Talks/slides/02_kharratzadeh_stancon_slides.pdf)
  - http://www.columbia.edu/~mk3971/


<br>

**_Advertising Attribution Modeling in the Movie Industry_** 

* Authors: Victor Lei, Nathan Sanders, Abigail Dawson (Legendary Entertainment)

We present a Bayesian method for inferring advertising platform effectiveness as applied to the movie industry, and show some possibilities for drawing inferences by analyzing model parameters at different levels of the hierarchy. In addition, we show some common ways to check model efficacy, and possibilities for comparing between different models.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284248.svg)](https://doi.org/10.5281/zenodo.1284248)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=2h25m43s)
  - [Notebook and materials](2017/Contributed-Talks/03_lei)
  - [Slides](2017/Contributed-Talks/slides/03_lei_stancon_slides.pdf)
  - https://github.com/foo-bar-baz-qux


<br>

**_hBayesDM: Hierarchical Bayesian modeling of decision-making tasks_** 

* Authors: Woo-Young Ahn, Nate Haines, Lei Zhang (Ohio State University)

hBayesDM (hierarchical Bayesian modeling of Decision-Making tasks) is a user-friendly R package that offers hierarchical Bayesian analysis of various computational models on an array of decision-making tasks. 

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284262.svg)](https://doi.org/10.5281/zenodo.1284262)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=2h40m20s)
  - [Notebook and materials](2017/Contributed-Talks/04_ahn)
  - [Slides](2017/Contributed-Talks/slides/04_ahn_stancon_slides.pdf)
  - https://ccs-lab.github.io


<br>

**_Differential Equation Based Models in Stan_** 

* Authors: Charles Margossian, Bill Gillespie (Metrum Research Group)

Differential equations can help us model sophisticated processes in biology, physics, and many other fields. Over the past year, the Stan team has developed many tools to tackle models based on differential equations.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284264.svg)](https://doi.org/10.5281/zenodo.1284264)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=2h53m26s)
  - [Notebook and materials](2017/Contributed-Talks/05_margossian)
  - [Slides](2017/Contributed-Talks/slides/05_margossian_stancon_slides.pdf)
  - http://metrumrg.com/


<br>

**_How to Test IRT Models Using Simulated Data_**

* Authors: Teddy Groves (Football Radar)

This notebook explains how to code some IRT models using Stan and test whether they can recover input parameters when given simulated data.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284275.svg)](https://doi.org/10.5281/zenodo.1284275)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=6h3m16s)
  - [Notebook and materials](2017/Contributed-Talks/06_groves)
  - [Slides](2017/Contributed-Talks/slides/06_groves_stancon_slides.html)
  - https://kent.academia.edu/TeddyGroves


<br>

**_Models of Retrieval in Sentence Comprehension_** 

* Authors: Bruno Nicenboim, Shravan Vasishth (University of Potsdam)

This work presents an evaluation of two well-known models of retrieval processes in sentence comprehension, the activation-based model and the direct-access model. We implemented these models in a Bayesian hierarchical framework and showed that some aspects of the data can be explained better by the direct access model. Specifically, the activation-based cannot predict that, on average, incorrect retrievals would be faster than correct ones. More generally, our work leverages the capabilities of Stan to provide a powerful framework for flexibly developing computational models of competing theories of retrieval, and demonstrates how these models’ predictions can be compared in a Bayesian setting.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284286.svg)](https://doi.org/10.5281/zenodo.1284286)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=6h18m1s)
  - [Notebook and materials](2017/Contributed-Talks/07_nicenboim) 
  - [Slides](2017/Contributed-Talks/slides/07_nicenboim_stancon_slides.pdf)
  - http://www.ling.uni-potsdam.de/~nicenboim/
  

<br>

**_Hierarchical Gaussian Processes in Stan_** 

* Authors: Rob Trangucci (Columbia University)

Stan’s library has been expanded with functions that facilitate adding Gaussian processes (GPs) to Stan models. I will share the best practices for coding GPs in Stan, and demonstrate how GPs can be added as one component of a larger model.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284293.svg)](https://doi.org/10.5281/zenodo.1284293)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=6h31m27s)
  - [Notebook and materials](2017/Contributed-Talks/08_trangucci)
  - [Slides](2017/Contributed-Talks/slides/08_trangucci_stancon_slides.pdf)
  - https://github.com/rtrangucci
  

<br>

**_Modeling the Rate of Public Mass Shootings with Gaussian Processes_** 

* Authors: Nathan Sanders, Victor Lei (Legendary Entertainment)

We have used Stan to develop a new model for the annualized rate of public mass shootings in the United States based on a Gaussian process with a time-varying mean function. This design yields a predictive model with the full non-parametric flexibility of a Gaussian process, while retaining the direct interpretability of a parametric model for long-term evolution of the mass shooting rate. We apply this model to the Mother Jones database of public mass shootings and explore the posterior consequences of different prior choices and of correlations between hyperparameters. We reach conclusions about the long term evolution of the rate of public mass shootings in the United States and short-term periods deviating from this trend.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284299.svg)](https://doi.org/10.5281/zenodo.1284299)

Links:

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=6h45m55s)  
  - [Notebook and materials](2017/Contributed-Talks/09_sanders)
  - [Slides](2017/Contributed-Talks/slides/09_sanders_stancon_slides.pdf)
  - https://github.com/nesanders



<br>
","['jgabry', 'lauken13', 'unrealmcg', 'avehtari', 'bnicenboim', 'youngahn']",0,,0.63,0,,,,,,41,,,
37153277,MDEwOlJlcG9zaXRvcnkzNzE1MzI3Nw==,Blockchain-Developer-Resources,ChristopherA/Blockchain-Developer-Resources,0,ChristopherA,https://github.com/ChristopherA/Blockchain-Developer-Resources,List of opininated links to resources useful to blockchain and bitcoin developers,0,2015-06-09 19:24:35+00:00,2025-02-13 13:28:15+00:00,2017-07-23 17:50:37+00:00,,14,249,249,,1,1,1,1,0,0,50,0,0,0,cc0-1.0,1,0,0,public,50,0,249,master,1,,"# Blockchain-Developer-Resources

A list of opininated links to resources useful to blockchain and bitcoin developers.

---

## Bitcoin

### Non-Technical Introductions

#### Non-Technical Videos (Very Intro)
  * [Bitcoin explained and made simple](https://www.youtube.com/watch?v=s4g1XFU8Gto) (3m:24s) by [The Guardian](http://www.theguardian.com/)
  * [The Essence of How Bitcoin Works (Non-Technical)](https://www.youtube.com/watch?v=t5JGQXCTe3c) (5m:24s) by [Curious Inventor](http://Patreon.com/CuriousInventor)
  
#### Non-Technical Videos (More Detail)
  * [Blockchain University — Bitcoin & the Blockchain: An Introduction](https://www.youtube.com/watch?v=ZUoXUW9zVMs) (26m:16s) by [@ChristopherA](https://twitter.com/ChristopherA) 
  * [Bitcoin 101 - What is Bitcoin?](https://www.youtube.com/watch?v=Bhe61JaNFLU) (22m:32s) by [James D'Angelo WBN](https://www.youtube.com/channel/UCgo7FCCPuylVk4luP3JAgVw)
 
#### Non-Technical Videos (Intermediate & Related Topics)
  * [Blockchain University — Bitcoin Keys Addresses and Wallets](https://www.youtube.com/watch?v=Ic76iSnCb_0) (1h:06m:43s) by [@ChristopherA](https://twitter.com/ChristopherA)
  * [Bitcoin Sidechains](https://www.youtube.com/watch?v=6sXNVIaNL2Y) (5m:54s) by [Diginomics](https://diginomics.com/)

#### Non-Technical Articles
  * [What is Bitcoin?](http://www.coindesk.com/information/what-is-bitcoin/) by CoinDesk
  * [What is Bitcoin?](http://money.cnn.com/infographic/technology/what-is-bitcoin/) by CNN Money
  * [The Impact of the Blockchain Goes Beyond Financial Services](https://hbr.org/2016/05/the-impact-of-the-blockchain-goes-beyond-financial-services) by Don an Alex Tapscott

### Technical Introductions

#### Technical Videos
  * [How Bitcoin Works in 5 Minutes (Technical)](https://www.youtube.com/watch?v=l9jOJk30eQs) (5m:25s)  by [Curious Inventor](http://Patreon.com/CuriousInventor)
  * [How Bitcoin Works Under the Hood](https://www.youtube.com/watch?v=Lx9zgZCMqXE) (22m:24s) by [Curious Inventor](http://Patreon.com/CuriousInventor)
  * [Mechanics of Bitcoin](https://www.youtube.com/watch?v=t3hJsFpPmXs) (1h:19m:49s) by [Princeton 
Bitcoin and Cryptocurrency Technologies Online Course](https://www.coursera.org/course/bitcointech)
  * [Programming Bitcoin Youtube Channel](https://www.youtube.com/programmingbitcoin) by Murray
  * [Coding Multi-Signature Addresses](https://www.youtube.com/watch?v=zIbUSaZBJgU) by D'Angelo

#### Technical Books
  * [Mastering Bitcoin](https://github.com/aantonop/bitcoinbook) by Andreas M. Antonopoulos LLC

#### Technical Articles
* [Elliptic Curve Digital Signature Algorithm and its Applications in Bitcoin (http://cs.ucsb.edu/~koc/ecc/project/2015Projects/Malvik+Witzoee.pdf) by Arnt Gunnar Malvik and Bendik Witzoee
* [Programming Bitcoin Transaction Scripts](https://docs.google.com/document/d/1D_gi_7Sf9sOyAHG25cMpOO4xtLq3iJUtjRwcZXFLv1E/edit) by Kofler
* [Developer’s Introduction to Bitcoin](http://bitcoinmagazine.com/9249/developers-introduction-bitcoin/) by Buterin
* [How Bitcoin Works Under the Hood](http://www.imponderablethings.com/2013/07/how-bitcoin-works-under-hood.html) by Driscoll
* [Bitcoins the hard way: Using the raw Bitcoin protocol](http://www.righto.com/2014/02/bitcoins-hard-way-using-raw-bitcoin.html) by Shirriff
* [Bitcoin mining the hard way: the algorithms, protocols, and bytes](http://www.righto.com/2014/02/bitcoin-mining-hard-way-algorithms.html) by Shirriff

#### Tutorials
* [Cryptographic Currencies Crash Course (C4)](http://www2016.net/proceedings/companion/p1021.pdf) by Aljosha Judmayer and Edgar Weipp
* [The libbitcoin Tutorial](http://libbitcoin.dyne.org/doc/) by Taaki
* [How to Parse the Bitcoin Blockchain](http://codesuppository.blogspot.com/2014/01/how-to-parse-bitcoin-blockchain.html) by Ratcliff
* [Signing Offline Transactions](https://gist.github.com/jashmenn/9811205) by Maxwell
* [2 of 2 escrow example](https://gist.github.com/jashmenn/9811198) by Maxwell
* [2 of 3 multisig example](https://gist.github.com/jashmenn/9811185) by Andresen
* [How to decrypt messages in the blockchain from btcmsg](https://gist.github.com/ripper234/1625828) by ripper234


## Sidechains

Sidechains are new blockchains, but are backed by Bitcoin rather than being an altcoin. Using ""two-way pegging"" these sidechains provide a method for developers to make changes and play around with blockchain rules in a separate blockchain, while keeping these coins linked to Bitcoin.

### Introduction
  * [Ask Dr. Bitcoin: What are Side-Chains](http://siliconangle.com/blog/2014/04/21/bitcoin-sidechains/)
  * [Introduction To Sidechains and Blockchain 2.0](https://www.deepdotweb.com/2014/06/26/sidechains-blockchain-2-0/)
  * [Sidechains, Treechains, the TL;DR](http://blog.greenaddress.it/2014/06/13/sidechains-treechains-the-tldr/) by Sanders
  * [Side Chains: The How, The Challenges and the Potential](http://bitcoinmagazine.com/12349/side-chains-challenges-potential/) by Buterin
  * [Sidechain Technical Feasibility Discussion](https://bitcointalk.org/index.php?topic=566704.0;all) (discussion)
  * [Alternative Chains / merged mining](https://en.bitcoin.it/wiki/Alternative_Chains) 

### Concepts
  * The Initial Two-Way Pegging [proposal](http://sourceforge.net/p/bitcoin/mailman/message/32108143/) by Adam Back
  * Blockstream Whitepaper [Enabling Blockchain Innovations with Pegged Sidechains](https://www.blockstream.com/sidechains.pdf)
  * [Tree-chains preliminary summary](http://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg04388.html) by Todd

### Tutorials
  * [Sidechains: Alpha Sidechain Tutorial](http://blog.cryptoiq.ca/?p=395)

### Projects
  * [Sidechain Elements Project](https://github.com/ElementsProject/elementsproject.github.io/) - Open source github project investigating and experiment with various bitcoin sidechain concepts

## Altcoins

### Introduction
  * [A Treatise on Altcoins](https://download.wpsoftware.net/bitcoin/alts.pdf) 
  
### Tutorials
* [How To Clone Scrypt Based Altcoins for Fun and Profit](http://devtome.com/doku.php?id=scrypt_altcoin_cloning_guide&rev=1391981820) by shakezula 

### Projects
* [CoinGecko](https://www.coingecko.com/en) has charts for a number of altcoins, but more importantly, attempts to measure developer and user community activity.

## Javascript & Bitcoin

At Blockchain University we use a number of Javascript based examples to teach the more technical details of Bitcoin. You are not required to have an in-depth knowledge of Javascript, but learning some basics is very useful.

### Setting Up on Mac

If using a Mac, you'll need some basic knowledge how to use the Terminal and the Mac's command line interface, and you'll need to install brew, node and git. A basic tutorial on how to do this is at https://github.com/ChristopherA/intro-mac-command-line

You can also use this script which sets up your Mac automatically, but the above teaches you how do to do it manually https://github.com/blockchainu/prepare-osx-for-blockchain-webdev

### Introduction to Javascript

Javascript is in both server (node) and client (browser) development. Some basics of Javascript are common to both. Here are some resources for learning about Javascript that are generally applicable to both platform.

  * [Learn Javascript](https://www.gitbook.com/book/gitbookio/javascript/details) - a free online book with interactive exercises.

### Introduction to Node & Javascript

Server-based Javascript typically uses Node. These Javascript learning resources are node specific:

  * I like the command-line based [nodeschool.io](http://nodeschool.io) tutorials, as they require you to both use the command line and to create real working code. These are the basic interactive tutorials, but there are many more available.
   * Learn javascripting basics `npm install -g javascripting`
   * Learn Node basics: `npm install -g learnyounode`
   * Learn git: `npm install -g git-it`

### Online courses on Javascript

Most of these courses teach general Javascript, but tend to be more client-side Javascript oriented.

Free online courses:
  * [Code Academy: Javascript](http://www.codecademy.com/tracks/javascript) is an online interactive course that comes highly recommend. The course says 10 hours, but one of our students reported that it took him about 17 hours over 5 days, and found Q&A forum and glossary both helpful.
  * [How to Learn JavaScript Properly](http://javascriptissexy.com/how-to-learn-javascript-properly/)
  * [Introduction to Computing Principles](http://web.stanford.edu/class/cs101/)
  * [Introduction to JavaScript Development](https://www.udemy.com/refactoru-intro-js/?dtcode=DvGKZ5c30m1I)

Some non-free online courses:
  * [Node.js Essential Training](http://www.lynda.com/JavaScript-tutorials/Nodejs-Essential-Training/141132-2.html)

### General Javascript Books and eBooks

Some general Javascript online books in eBook (in rough order of preference)

  * [Eloquent Javascript](http://eloquentjavascript.net)
  * [JavaScript Guide](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide)
  * [JavaScript Garden](http://bonsaiden.github.io/JavaScript-Garden/)

### Local-client Javascript Tools & Playgrounds

These tools can be cloned from github to allow you to use your browser to play around with various bitcoin capabilities:

  * [Bip32 Generator](https://github.com/bip32/bip32.github.io) `git clone https://github.com/bip32/bip32.github.io.git ; cd bip32.github.io ; open index.html` lets you create Bip32 Deterministic Heirarchical Keys (prefix xpub* and xprv*) based on a simple brainwallet (aka arbitrary mnemonic) passphrase.
  * [Bip39 Mnemonic Code Converter](https://github.com/dcpos/bip39) `npm install bip39 ; npm run compile` lets you create Bip39 mnemonics (typically 12 words) used for deterministic keys, typically for Bip32.

### Bitcore.js
  * [Bitcoin 101 - Getting Started With Bitcore - A Full JavaScript Implementation of Bitcoin](https://www.youtube.com/watch?v=TmkN8yYyOv8)
    * https://github.com/wobine/blackboard101/blob/master/BitcoreDayOne.html - code for the above
  * [Bitcoin Quick Tools](https://github.com/chulini/bitcoin-quick-tools) - Simple browser-based app (loaded locally) that is Bitcore.js. Has key and address generator and simple transaction tool. Useful working example of browserfy bitcore.js code.

### Testnet Faucets

You'll need bitcoin testnet coins while developing apps with bitcoin. List in rough order of reliability and number of coins offered.

  * [Sidechains Elements Project Testnet Faucet](https://testnet-faucet.elementsproject.org) - 10 BTC per day per ip
  * [TP Faucet](http://tpfaucet.appspot.com) - 1.9 BTC per request


### Bitcoin Standards
  * BIP32 - Bitcoin Deterministic Heirarchical Keys
  * Good article on [Deterministic Wallets](http://blog.richardkiss.com/?p=313)
  * Discussion of [BIP32 Advantages and Flaws](https://bitcoinmagazine.com/8396/deterministic-wallets-advantages-flaw/)
  * The [BIP0032 Standard](https://en.bitcoin.it/wiki/BIP_0032)
  * [Bip32 Generator](https://github.com/bip32/bip32.github.io) `git clone https://github.com/bip32/bip32.github.io.git ; cd bip32.github.io ; open index.html` lets you create Bip32 Deterministic Heirarchical Keys (prefix xpub* and xprv*) based on a simple brainwallet (aka arbitrary mnemonic) passphrase.
  * [Blockstack Keychains JS](https://github.com/blockstack/blockstack-keychains-js)

### Blockchain Explorers
  * [Blockr.io](http://btc.blockr.io)
  * [Blockchain.info](https://blockchain.info/)
",['ChristopherA'],0,,0.64,0,,,,,,28,,,
286346180,MDEwOlJlcG9zaXRvcnkyODYzNDYxODA=,sys_reading,pentium3/sys_reading,0,pentium3,https://github.com/pentium3/sys_reading,system paper reading notes,0,2020-08-10 01:14:17+00:00,2025-02-13 16:26:54+00:00,2022-03-03 02:24:03+00:00,,69,241,241,,1,1,1,1,0,0,13,0,0,126,,1,0,0,public,13,126,241,master,1,,"My system paper reading notes.


![image](https://user-images.githubusercontent.com/7352163/142090379-91284705-8b32-4b75-a390-fdfd7ff981e6.png)



See [**Issues**](https://github.com/pentium3/sys_reading/issues)

[**Pinned Issues**](https://github.com/pentium3/sys_reading/milestone/1)

---------------------------------------------

Some public computer system reading groups:

## General

https://courses.cs.washington.edu/courses/cse590x/22wi/resources/

https://www.bu.edu/rhcollab/events/bu-systems-bu%e2%99%bas-seminar-fall-2021/

https://pages.cs.wisc.edu/~shivaram/cs744-fa21/

https://www.systemsapproach.org/blog-archive

https://www.csail.mit.edu/events

https://about.iangneal.io/seminar/

https://systems-seminar-uiuc.github.io/fall20/index.html

https://github.com/fruffy/nyu-systems-seminar

https://systems-lunch.org/

http://www.sysnet.ucsd.edu/classes/cse294/sp20/

http://www.cs.cornell.edu/courses/cs7490/2020fa/

https://www.youtube.com/c/uwcse/playlists

https://docs.google.com/document/d/1gWQ_Uk60zIH6PvP1P4NYzz4TvrKWGCnltySBxwkradM/edit

## DB

http://pages.cs.wisc.edu/~yxy/cs764-f21/index.html

https://db.cs.cmu.edu/seminar2021/

https://www.cs.purdue.edu/homes/csjgwang/CloudNativeDB/

## Network / Distributed Sys

https://dsl.cis.upenn.edu/seminar/

https://www.youtube.com/channel/UCDjWhwewESyX335Rp6B1PEw/videos

https://www.youtube.com/channel/UCAtFAG5JdQrHac6ArIWJ-hw/videos

http://pages.cs.wisc.edu/~akella/CS740/S20/papers.html

https://gitlab.com/purdue-cs592/fall-2021/public

https://pages.cs.wisc.edu/~mgliu/teaching.html

https://people.cs.rutgers.edu/~sn624/552-F20/syllabus.html

## Sys + ML

https://github.com/ucbrise/cs294-ai-sys-sp22

https://github.com/mosharaf/eecs598

https://remziarpacidusseau.wixsite.com/mlos

http://alchem.usc.edu/ceng-seminar/

## Sys Security

https://courses.cs.duke.edu/fall21/compsci590.1/schedule.html

https://www.cs.purdue.edu/homes/pfonseca/seminar/

https://www.cs.purdue.edu/homes/pfonseca/teaching/cs590/20spring/

http://csg.csail.mit.edu/6.888Yan/schedule

https://sites.cs.ucsb.edu/~trinabh/classes/w20/syllabus.html

https://people.csail.mit.edu/nickolai/#pubs

http://srinathsetty.net/

https://www.cis.upenn.edu/~sga001/classes/cis700s21/

## Sys Reliability

https://www.cs.jhu.edu/~huang/cs817/spring21/

https://www.cs.jhu.edu/~huang/cs624/spring21/syllabus.html

https://web.eecs.umich.edu/~manosk/summer-school-2020.html

---------------------------------------------
",['pentium3'],0,,0.56,0,,,,,,7,,,
219027,MDEwOlJlcG9zaXRvcnkyMTkwMjc=,aws,appoxy/aws,0,appoxy,https://github.com/appoxy/aws,Amazon Web Services (AWS) Ruby Gem,0,2009-06-05 02:13:32+00:00,2024-09-03 00:20:46+00:00,2024-02-22 18:29:22+00:00,https://rubygems.org/gems/aws,792,236,236,Ruby,1,1,1,1,1,0,86,0,0,34,,1,0,0,public,86,34,236,master,1,1,"# DEPRECATED

Please use this gem now: https://rubygems.org/gems/aws-sdk

# Appoxy AWS Library

A Ruby gem for all Amazon Web Services.

Brought to you by: [![Appoxy](https://lh5.googleusercontent.com/_-J9DSaseOX8/TX2Bq564w-I/AAAAAAAAxYU/xjeReyoxa8o/s800/appoxy-small%20%282%29.png)](http://www.appoxy.com)

## Discussion Group

[http://groups.google.com/group/ruby-aws](http://groups.google.com/group/ruby-aws)

## Documentation

[Ruby Docs](http://rubydoc.info/gems/aws/2.4.5/frames)

## Appoxy Amazon Web Services Ruby Gems

Published by [Appoxy LLC](http://www.appoxy.com), under the MIT License. Special thanks to RightScale from which this project is forked.

## INSTALL:

    gem install aws

Then `require 'aws'` in your application.

## DESCRIPTION:

The AWS gems have been designed to provide a robust, fast, and secure interface to Amazon EC2, EBS, S3, SQS, SDB, and
CloudFront.
The AWS gems comprise:

- Aws::Ec2 -- interface to Amazon EC2 (Elastic Compute Cloud) and the associated EBS (Elastic Block Store)
- Aws::S3 and Aws::S3Interface -- interface to Amazon S3 (Simple Storage Service)
- Aws::Sqs and Aws::SqsInterface -- interface to Amazon SQS (Simple Queue Service)
- Aws::SdbInterface -- interface to Amazon SDB (SimpleDB). See [SimpleRecord for an ActiveRecord like gem](https://github.com/appoxy/simple_record).
- Aws::AcfInterface -- interface to Amazon CloudFront, a content distribution service
- Aws::ElbInterface -- interface to Amazon Load Balancing service
- Aws::MonInterface -- interface to Amazon CloudWatch monitoring service
- Aws::Iam -- for AWS Identity and Access Management

To use a single piece intead of loading all of then, you can require it explicitly for example: `require 'aws/sqs'`.

## FEATURES:

- Full programmmatic access to EC2, EBS, S3, SQS, SDB, ELB, and CloudFront.
- Complete error handling: all operations check for errors and report complete
  error information by raising an AwsError.
- Persistent HTTP connections with robust network-level retry layer using
  RightHttpConnection).  This includes socket timeouts and retries.
- Robust HTTP-level retry layer.  Certain (user-adjustable) HTTP errors returned
  by Amazon's services are classified as temporary errors.
  These errors are automaticallly retried using exponentially increasing intervals.
  The number of retries is user-configurable.
- Fast REXML-based parsing of responses (as fast as a pure Ruby solution allows).
- Uses libxml (if available) for faster response parsing. 
- Support for large S3 list operations.  Buckets and key subfolders containing
  many (> 1000) keys are listed in entirety.  Operations based on list (like
  bucket clear) work on arbitrary numbers of keys.
- Support for streaming GETs from S3, and streaming PUTs to S3 if the data source is a file.
- Support for single-threaded usage, multithreaded usage, as well as usage with multiple
  AWS accounts.
- Support for both first- and second-generation SQS (API versions 2007-05-01
  and 2008-01-01).  These versions of SQS are not compatible.
- Support for signature versions 0, 1 and 2 on all services.
- Interoperability with any cloud running Eucalyptus (http://eucalyptus.cs.ucsb.edu)
- Test suite (requires AWS account to do ""live"" testing).

## THREADING:

All AWS interfaces offer three threading options:

1. Use a single persistent HTTP connection per process. :single
2. Use a persistent HTTP connection per Ruby thread. :per_thread
3. Open a new connection for each request. :per_request
 
Either way, it doesn't matter how many (for example) Aws::S3 objects you create,
they all use the same per-program or per-thread
connection. The purpose of sharing the connection is to keep a single
persistent HTTP connection open to avoid paying connection
overhead on every request. However, if you have multiple concurrent
threads, you may want or need an HTTP connection per thread to enable
concurrent requests to AWS. The way this plays out in practice is:

1. If you have a non-multithreaded Ruby program, use the non-multithreaded setting.
2. If you have a multi-threaded Ruby program, use the multithreaded setting to enable
   concurrent requests to S3 (or SQS, or SDB, or EC2).
3. For running under Mongrel/Rails, use the non-multithreaded setting even though
   mongrel is multithreaded.  This is because only one Rails handler is invoked at
   time (i.e. it acts like a single-threaded program)

Note that due to limitations in the I/O of the Ruby interpreter you
may not get the degree of parallelism you may expect with the multi-threaded setting.

By default, EC2/S3/SQS/SDB/ACF interface instances are created in per_request mode.  Set
params[:connection_mode] to :per_thread in the initialization arguments to use
multithreaded mode.


== LICENSE:

Copyright (c) 2007-2009 RightScale, Inc. 

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
'Software'), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
","['treeder', 'mfojtik', 'marios', 'phiggins', 'NeMO84', 'rkononov', 'tobias', 'amalagaura', 'jrosengren', 'DevL', 'mariuszlusiak', 'gaffo', 'sjones4', 'apsoto', 'allgdante', 'andycroll', 'ctm', 'dgb', 'douglasr', 'friedmag', 'hubertlepicki', 'jeremyrosengren', 'goldmann', 'Meekohi', 'iconara', 'thibaudgg', 'tomassedovic', 'c4ssio', 'erichs', 'plaa']",0,,0.73,0,,,,,,11,,,
197303330,MDEwOlJlcG9zaXRvcnkxOTczMDMzMzA=,Awesome-CV-Resources,Sophia-11/Awesome-CV-Resources,0,Sophia-11,https://github.com/Sophia-11/Awesome-CV-Resources,CV领域实验室、数据库、资源,0,2019-07-17 02:51:40+00:00,2025-01-19 13:34:45+00:00,2019-09-01 11:21:22+00:00,,73,235,235,,1,1,1,1,0,0,74,0,0,21,,1,0,0,public,74,21,235,master,1,,,['Sophia-11'],0,,0.63,0,,,,,,12,,,
254014116,MDEwOlJlcG9zaXRvcnkyNTQwMTQxMTY=,HybridQA,wenhuchen/HybridQA,0,wenhuchen,https://github.com/wenhuchen/HybridQA,"Dataset and code for EMNLP2020 paper ""HybridQA: A Dataset of Multi-Hop Question Answeringover Tabular and Textual Data""",0,2020-04-08 07:19:07+00:00,2025-03-06 15:13:48+00:00,2023-06-03 02:53:23+00:00,,26581,226,226,Python,1,1,1,1,0,0,24,0,0,1,mit,1,0,0,public,24,1,226,master,1,,"# HybridQA
This repository contains the dataset and code for the EMNLP2020 paper [HybridQA: A Dataset of Multi-Hop Question Answeringover Tabular and Textual Data](https://arxiv.org/pdf/2004.07347.pdf), which is the first large-scale multi-hop question answering dataset on heterogeneous data including tabular and textual data. The whole dataset contains over 70K question-answer pairs based on 13,000 tables, each table is in average linked to 44 passages, more details in https://hybridqa.github.io/.
<p align=""center"">
<img src=""example.png"" width=""850"">
</p>
The questions are annotated to require aggregation of information from both the table and its hyperlinked text passages, which poses challenges to existing homongeneous text-based or KB-based models. 

# Requirements:
- [huggingface transformer 2.6.0](https://github.com/huggingface/transformers)
- [pytorch 1.4.0](https://pytorch.org/)
- tensorboardX
- tqdm

# Dataset Visualization
Have fun interacting with the dataset: https://hybridqa.github.io/explore.html

# Released data:
The released data contains the following files:
```
train/dev/test.json: these files are the original files, all annotated by human.
train/dev.traced.json: these files are generated by trace_answer.py to find the answer span in the given evidences.
```

# Preprocess data:
First of all, you should download all the tables and passages into your current folder
```
git clone https://github.com/wenhuchen/WikiTables-WithLinks
```
Then, you can either preprocess the data on your own,
```
python preprocessing.py
```
or use our preprocessed version from Amazon S3
```
wget https://hybridqa.s3-us-west-2.amazonaws.com/preprocessed_data.zip
unzip preprocessed_data.zip
```

# Reproduce the reported results
## Download the trained bert-base model from Amazon S3:
```
wget https://hybridqa.s3-us-west-2.amazonaws.com/models.zip
unzip BERT-base-uncased.zip
```
It will download and generate folder stage1/stage2/stage3/

## Using pretrained model to run stage1/stage2:
```
CUDA_VISIBLE_DEVICES=0 python train_stage12.py --stage1_model stage1/2020_10_03_22_47_34/checkpoint-epoch2 --stage2_model stage2/2020_10_03_22_50_31/checkpoint-epoch2/ --do_lower_case --predict_file preprocessed_data/dev_inputs.json --do_eval --option stage12 --model_name_or_path  bert-large-uncased
```
This command generates a intermediate result file
## Using pretrained model to run stage3:
```
CUDA_VISIBLE_DEVICES=0 python train_stage3.py --model_name_or_path stage3/2020_10_03_22_51_12/checkpoint-epoch3/ --do_stage3   --do_lower_case  --predict_file predictions.intermediate.json --per_gpu_train_batch_size 12  --max_seq_length 384   --doc_stride 128 --threads 8
```
This command generates the prediction file
## Compute the score
```
python evaluate_script.py predictions.json released_data/dev_reference.json
```


# Training [Default for Bert-base-uncased model]
## Train Stage1:
Running training command for stage1 using BERT-base-uncased as follows:
```
CUDA_VISIBLE_DEVICES=0 python train_stage12.py --do_lower_case --do_train --train_file preprocessed_data/stage1_training_data.json --learning_rate 2e-6 --option stage1 --num_train_epochs 3.0
```
Or Running training command for stage1 using BERT-large-uncased as follows:
```
CUDA_VISIBLE_DEVICES=0 python train_stage12.py --model_name_or_path bert-large-uncased --do_train --train_file preprocessed_data/stage1_training_data.json --learning_rate 2e-6 --option stage1 --num_train_epochs 3.0
```

## Train Stage2:
Running training command for stage2 as follows:
```
CUDA_VISIBLE_DEVICES=0 python train_stage12.py --do_lower_case --do_train --train_file preprocessed_data/stage2_training_data.json --learning_rate 5e-6 --option stage2 --num_train_epochs 3.0
```
Or BERT-base-cased/BERT-large-uncased like above.


## Train Stage3:
Running training command for stage3 as follows:
```
CUDA_VISIBLE_DEVICES=0 python train_stage3.py --do_train  --do_lower_case   --train_file preprocessed_data/stage3_training_data.json  --per_gpu_train_batch_size 12   --learning_rate 3e-5   --num_train_epochs 4.0   --max_seq_length 384   --doc_stride 128  --threads 8
```
Or BERT-base-cased/BERT-large-uncased like above.

## Model Selection for Stage1/2:
Model Selction command for stage1 and stage2 as follows:
```
CUDA_VISIBLE_DEVICES=0 python train_stage12.py --do_lower_case --do_eval --option stage1 --output_dir stage1/[OWN_PATH]/ --predict_file preprocessed_data/stage1_dev_data.json
```

# Evaluation
## Model Evaluation Step1 -> Stage1/2:
Evaluating command for stage1 and stage2 as follows (replace the stage1_model and stage2_model path with your own):
```
CUDA_VISIBLE_DEVICES=0 python train_stage12.py --stage1_model stage1/[OWN_PATH] --stage2_model stage2/[OWN_PATH] --do_lower_case --predict_file preprocessed_data/dev_inputs.json --do_eval --option stage12
```
The output will be saved into predictions.intermediate.json, which contain all the answers for non hyper-linked cells, with the hyperlinked cells, we need the MRC model in stage3 to extract the span.

## Model Evaluation Step2 -> Stage3:
Evaluating command for stage3 as follows (replace the model_name_or_path with your own):
```
CUDA_VISIBLE_DEVICES=0 python train_stage3.py --model_name_or_path stage3/[OWN_PATH] --do_stage3   --do_lower_case  --predict_file predictions.intermediate.json --per_gpu_train_batch_size 12  --max_seq_length 384   --doc_stride 128 --threads 8
```
The output is finally saved to predictoins.json, which can be used to calculate F1/EM with reference file.

## Computing the score
```
python evaluate_script.py predictions.json released_data/dev_reference.json
```

# CodaLab Evaluation
We host CodaLab challenge in [HybridQA Competition](https://codalab.lisn.upsaclay.fr/competitions/7979), you should submit your results to the competition to obtain your testing score. The submitted file should first be named ""test_answers.json"" and then zipped. The required format of the submission file is described as follows:
```
[
  {
    ""question_id"": xxxxx,
    ""pred"": XXX
  },
  {
    ""question_id"": xxxxx,
    ""pred"": XXX
  }
]
```
The reported scores are EM and F1.

# Recent Papers


**Model**                                     |  **Organization**  |**Reference**                                                             | **Dev-EM** | **Dev-F1** | **Test-EM** | **Test-F1** | 
----------|---------------------------|-----------------------------------|---------------------------------------------------------------------------|---------|----------|------------------|
S<sup>3</sup>HQA | CASIA     |  [Lei et al. (2023)](https://arxiv.org/pdf/2305.11725.pdf)                 | 68.4  | 75.3 |  67.9  | 75.5 |
MAFiD            | JBNU & NAVER | [Lee et al. (2023)](https://aclanthology.org/2023.findings-eacl.177.pdf)| 66.2  | 74.1 |  65.4  | 73.6 |
TACR             | TIT & NTU | [Wu et al. (2023)](https://arxiv.org/pdf/2305.14682.pdf)                 |   64.5  | 71.6 |  66.2   | 70.2 |
UL-20B           | Google    | [Tay et al. (2022)](https://arxiv.org/abs/2205.05131)                    |   -     | -    |  61.0  | -    |
MITQA            | IBM & IIT | [Kumar et al. (2021)](https://arxiv.org/pdf/2112.07337.pdf)              |   65.5  | 72.7 |  64.3  | 71.9 |
RHGN             | SEU       | [Yang et al. (2022)](https://link.springer.com/epdf/10.1007/s11227-022-05035-9?sharing_token=kouLCEDp9_vH1RkK8N9CAPe4RwlQNchNByi7wbcMAY4kj78xdT5rsS4-XKuj5N_XmnTRe7ko6X0kKaXyingc6wfoEGdQgx5hH9hDtcI6ivFPDd1p7A3RUWChRVmVBrgsvavXcujpAkPf2d1K1X-eE8ctae3eLrfxStzEdLP9uOs=)   |   62.8 | 70.4 |   60.6    |  68.1   |
POINTR + MATE    | Google    | [Eisenschlos et al. (2021)](https://arxiv.org/pdf/2109.04312.pdf)         |   63.3  | 70.8 |  62.7  | 70.0 |
POINTR + TAPAS   | Google    | [Eisenschlos et al. (2021)](https://arxiv.org/pdf/2109.04312.pdf)         |   63.4  | 71.0 |  62.8  | 70.2 |
MuGER<sup>2</sup>| JD AI     | [Wang et al. (2022)](https://arxiv.org/abs/2210.10350)                    | 57.1    | 67.3 | 56.3   |  66.2 |
DocHopper        | CMU       | [Sun et al. (2021)](https://arxiv.org/abs/2106.00200)                     |   47.7  | 55.0 |  46.3  | 53.3 |
HYBRIDER         | UCSB      | [Chen et al. (2020)](https://arxiv.org/abs/2004.07347)                    |   43.5  | 50.6 |  42.2  | 49.9 |
HYBRIDER-Large   | UCSB      | [Chen et al. (2020)](https://arxiv.org/abs/2004.07347)                    |  44.0   | 50.7 |  43.8  | 50.6 |
Unsupervised-QG  | NUS\&UCSB |  [Pan et al. (2020)](https://arxiv.org/abs/2010.12623)                    |    25.7 | 30.5 |   -    |  -   |



# Referenece
If you find this project useful, please use the following format to cite the paper:
```
@article{chen2020hybridqa,
  title={HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data},
  author={Chen, Wenhu and Zha, Hanwen and Chen, Zhiyu and Xiong, Wenhan and Wang, Hong and Wang, William},
  journal={Findings of EMNLP 2020},
  year={2020}
}
```


# Miscellaneous
If you have any question about the dataset and code, feel free to raise a github issue or shoot me an email. Thanks!
","['wenhuchen', 'haitian-sun', 'ZIZUN', 'lfy79001']",1,,0.63,0,,,,,,7,,,
760544859,R_kgDOLVT-Ww,Awesome-Text-to-Video-Generation,soraw-ai/Awesome-Text-to-Video-Generation,0,soraw-ai,https://github.com/soraw-ai/Awesome-Text-to-Video-Generation,"A list for Text-to-Video, Image-to-Video works",0,2024-02-20 15:53:42+00:00,2025-03-03 21:50:13+00:00,2024-12-04 19:39:21+00:00,,6020,225,225,,1,1,1,1,0,0,11,0,0,0,,1,0,0,public,11,0,225,main,1,1,,"['ray-ruisun', 'zymvszym', 'inFaaa']",0,,0.69,0,,,,,,10,,,
123210246,MDEwOlJlcG9zaXRvcnkxMjMyMTAyNDY=,KBGAN,cai-lw/KBGAN,0,cai-lw,https://github.com/cai-lw/KBGAN,"Code for ""KBGAN: Adversarial Learning for Knowledge Graph Embeddings"" https://arxiv.org/abs/1711.04071",0,2018-02-28 01:20:09+00:00,2025-01-16 11:19:38+00:00,2018-10-11 13:22:07+00:00,,14993,213,213,Python,1,1,1,1,0,0,57,0,0,2,mit,1,0,0,public,57,2,213,master,1,,"# KBGAN
Liwei Cai and William Yang Wang, ""KBGAN: Adversarial Learning for Knowledge Graph Embeddings"", in *Proceedings of The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2018)*.

Paper: https://arxiv.org/abs/1711.04071

Our lab: http://nlp.cs.ucsb.edu/index.html

## Dependencies
* Python 3
* PyTorch 0.2.0
* PyYAML
* nvidia-smi

**PyTorch 0.2.0 is REQUIRED**. Because PyTorch is not backward compatible, **newer versions will NOT work**. We understand that 0.2.0 is outdated, but we currently have no schedule of adding support to newer PyTorch versions.

## Usage
1. Unzip `data.zip`.
2. Pretrain: `python3 pretrain.py --config=config_<dataset_name>.yaml --pretrain_config=<model_name>` (this will generate a pretrained model file)
2. Adversarial train: `python3 gan_train.py --config=config_<dataset_name>.yaml --g_config=<G_model_name> --d_config=<D_model_name>` (make sure that G model and D model are both pretrained)

Feel free to explore and modify parameters in config files. Default parameters are those used in experiments reported in the paper.

Decrease `test_batch_size` in config files if you experience GPU memory exhaustion. (this would make the program runs slower, but would not affect the test result)
",['cai-lw'],1,,0.8,0,,,,,,8,,,
324901695,MDEwOlJlcG9zaXRvcnkzMjQ5MDE2OTU=,AI-Security-Paper,eastmountyxz/AI-Security-Paper,0,eastmountyxz,https://github.com/eastmountyxz/AI-Security-Paper,"This resource mainly counts papers related to APT attacks, including APT traceability, APT knowledge graph construction, APT malicious sample detection, and APT overview. Hope these summarized papers are helpful to you~",0,2020-12-28 03:23:09+00:00,2025-03-05 10:51:23+00:00,2024-04-30 04:03:22+00:00,,31998,207,207,,1,1,1,1,0,0,26,0,0,0,,1,0,0,public,26,0,207,main,1,,"# AI-Security-Paper
This resource mainly counts papers related to APT attacks, including APT traceability, APT knowledge graph construction, APT malicious sample detection, and APT overview. Hope these summarized papers are helpful to you~

- sec-deadlines：https://sec-deadlines.github.io/
- AI deadlines: https://aideadlin.es
- CCF deadlines：https://ccfddl.github.io/
- NISL@THU安全顶会论文查询：https://secpaper.cn
- letpub：https://www.letpub.com.cn/index.php?page=journalapp
- CCFrank：在dblp和Google学术的搜索结果中显示中国计算机学会CCF推荐的国际会议和期刊排名
- SCI分区查询：https://www.letpub.com.cn/index.php?journalid=2011&page=journalapp&view=detail
- 四大顶会统计：https://www.s3.eurecom.fr/~balzarot/notes/top4_2018/authors_all_conf.html
- 学术关系查询：https://www.semanticscholar.org/
- CCF DDL查询：https://ccfddl.top/
- CCF DDL查询：https://ccfddl.github.io/

**常用工具：**
- 画图图标：https://www.iconfont.cn/
- 矢量图转换：https://onlineconvertfree.com/zh/convert-format/png-to-ai/
- 矢量图转换：https://cn.office-converter.com/png-to-eps
- bmeps -c Figure-1.png Figure-1.eps 
- LaTeX绘制表格：https://www.latex-tables.com/ （Excel绘制表格粘贴生成LaTex）
- 矢量图转换（推荐）：https://cloudconvert.com/svg-to-eps （Visio转SVG再转EPS和PDF）


**安全团队介绍（国外）：**
- Dawn Song：https://people.eecs.berkeley.edu/~dawnsong/
- XiaoFeng Wang：https://homes.luddy.indiana.edu/xw7/
- Giovanni Vigna：https://sites.cs.ucsb.edu/~vigna/
- Peng Liu：https://s2.ist.psu.edu/pliu/
- Heng Yin：https://www.cs.ucr.edu/~heng/
- Jiang Ming：https://ranger.uta.edu/~ming/
- Luyi Xing：https://www.xing-luyi.com/
- Zhiyun Qian：https://www.cs.ucr.edu/~zhiyunq/
- Yanfang (Fanny) Ye, Ph.D.：http://yes-lab.org/publications.html
- Wang Gang: https://gangw.cs.illinois.edu/

**安全团队介绍（国内）：**
- NISL@THU：http://netsec.ccert.edu.cn/chs/
- NESA-lab：https://nesa.zju.edu.cn/
- 王骞教授：http://nisplab.whu.edu.cn/
- 唐杰教授：http://keg.cs.tsinghua.edu.cn/jietang/ （清华大学，ACM Fellow/IEEE Fellow）
- 何德彪教授：http://blockchain.whu.edu.cn
- 珞珈之戍：https://csp.whu.edu.cn/
- 周亚金教授：https://yajin.org/
- 王志波教授：https://person.zju.edu.cn/zhibowang#951076
- 李琦老师：http://netsec.ccert.edu.cn/chs/people/qli/
- GoSSIP(上交)：https://github.com/GoSSIP-SJTU
- 学术安全圈(川大)：https://secdr.org/ | https://chenghuang.org/
- InforSec：http://www.inforsec.org/wp/?page_id=309
- 张岑老师：https://cenzhang.github.io/publications/


**安全学术大佬博客：**
- https://github.com/hindupuravinash/the-gan-zoo （GAN巨佬）
- https://people.engr.tamu.edu/guofei/sec_conf_stat.htm
- https://b1nsec.github.io/


**安全数据集：**<br />
- https://www.unb.ca/cic/datasets/
- **Malware** <br />
BODMAS Malware Dataset：https://whyisyoung.github.io/BODMAS/
- **APT** <br />
https://github.com/mstfknn/malware-sample-library
- **IDS** <br />
NSL-KDD：https://www.unb.ca/cic/datasets/nsl.html
- **Scripts**
- **溯源图 DARPA TC** <br />
https://github.com/darpa-i2o/Transparent-Computing <br />
https://drive.google.com/drive/folders/1QlbUFWAGq3Hpl8wVdzOdIoZLFxkII4EK <br />
https://drive.google.com/drive/folders/1okt4AYElyBohW4XiOBqmsvjwXsnUjLVf <br />
https://github.com/jun-zeng/Audit-log-analysis <br />


**安全经典综述：**<br />
- `Malware Detection | Anomaly Detection` <br />
[1] [A Survey on Malware Detection Using Data Mining Techniques](https://dl.acm.org/doi/pdf/10.1145/3365001) [ACM Computing Surveys 2017] <br />
[2] [A Survey of Android Malware Detection with Deep Neural Models](https://dl.acm.org/doi/pdf/10.1145/3417978) [ACM Computing Surveys 2017] <br />
[3] [Dynamic Malware Analysis in the Modern Era - A State of the Art Survey](https://dl.acm.org/doi/pdf/10.1145/3329786) [ACM Computing Surveys 2020]  <br />
[4] [Deep learning-based anomaly detection in cyber-physical systems: Progress and opportunities](https://dl.acm.org/doi/pdf/10.1145/3453155) [ACM Computing Surveys 2021] <br />
[5] [A Survey on Data-driven Network Intrusion Detection](https://dl.acm.org/doi/pdf/10.1145/3472753) [ACM Computing Surveys 2021] <br />

- `APT` <br />
[1] [APT datasets and attack modeling for automated detection methods: A review](https://www.sciencedirect.com/science/article/pii/S0167404820300213) [Computers & Security 2020] <br />
[2] [Threat detection and investigation with system-level provenance graphs: A survey](https://www.sciencedirect.com/science/article/pii/S0167404821001061) [Computers & Security 2021] <br />
[3] [Machine Learning-Enabled IoT Security: Open Issues and Challenges Under Advanced Persistent Threats](https://arxiv.org/ftp/arxiv/papers/2204/2204.03433.pdf) [ACM Computing Surveys 2022] <br />

- `Vulnerability` <br />
[1] [A Survey on Ethereum Systems Security: Vulnerabilities, Attacks, and Defenses](https://dl.acm.org/doi/pdf/10.1145/3391195) [ACM Computing Surveys 2021] <br />
[2] [Android Source Code Vulnerability Detection: A Systematic Literature Review](https://dl.acm.org/doi/pdf/10.1145/3556974) [ACM Computing Surveys 2022] <br />

- `Other` <br />
[1] [Malware Dynamic Analysis Evasion Techniques: A Survey](https://dl.acm.org/doi/pdf/10.1145/3365001) [ACM Computing Surveys 2020] <br />
[2] [Text Mining in Cybersecurity: A Systematic Literature Review](https://dl.acm.org/doi/pdf/10.1145/3462477) [ACM Computing Surveys 2022]  <br />
[3] [When Machine Learning Meets Privacy: A Survey and Outlook](https://dl.acm.org/doi/pdf/10.1145/3436755) [ACM Computing Surveys 2022]  <br />

- CSUR搜索：https://dl.acm.org/journal/csur

**其他学习：**
- CVPR2021集合：https://github.com/amusi/CVPR2021-Papers-with-Code
- APT论文集合（作者）：https://github.com/eastmountyxz/APT-ProvenanceGraph

---

**初学者论文技巧：（——学习至中科院王老师）**

📃 目标领域重要文章
- Survey + 关键词 -> 谷歌学术
- papers + 关键词 -> DBLP/GitHub/知乎
- paperswithcode（https://paperswithcode.com） -> leaderboard
- 经典论文 -> related work & cited by

🗂 管理
- Zotero（https://www.zotero.org/），免费，多级目录，同步，全文检索

📃 读论文
- 题目+ 摘要 + Intro
- 图表 + 图表描述部分
- 分析总结：段 -> section -> 整篇文章 -> 结构/逻辑

🗂 写论文
- Word X  Latex✅ 不关心排版专注内容，引文列表
- 经典论文拆解 -> 模仿
- 句型：阅读积累 + Phrasebank （https://www.phrasebank.manchester.ac.uk）
- 翻译/纠错/润色：谷歌+ Grammarly（https://app.grammarly.com/） + Quillbot （https://quillbot.com/）

📬 投稿
- CCF推荐列表 （https://www.ccf.org.cn/Academic_Evaluation/By_category/）
- AI deadlines: https://aideadlin.es
- Sec deadlines：https://sec-deadlines.github.io/

---

**个人总结系统安全和论文撰写那些事**

- 论文撰写
- 论文投递
- 论文修改
- 论文润色

---

PS：论文后续会详细整理补充，目前忙碌中....

## 一.Classified by subject

### APT
- [1] **NDSS 2020**. Xueyuan Han, Thomas F. J.-M. Pasquier, Adam Bates, James Mickens, Margo I. Seltzer:
**Unicorn: Runtime Provenance-Based Detector for Advanced Persistent Threats**.  图结构提升隐蔽APTs检测 <br />
https://arxiv.org/abs/2001.01525 <br />
https://blog.csdn.net/xjxtx1985/article/details/106473928 【中文后续自归纳】
- [2] **S&P 2019**. Sadegh Momeni Milajerdi, Rigel Gjomemo, Birhanu Eshete, et al. **HOLMES: Real-Time APT Detection through Correlation of Suspicious Information Flows**. IEEE Symposium on Security and Privacy 2019: 1137-1152. 协同信息溯源、日志、TTPs  <br />
https://arxiv.org/pdf/1810.01594.pdf <br />
https://www.secrss.com/articles/14488 【中文后续自归纳】
- [3] **USENIX 2017**. **SLEUTH Real-time Attack Scenario Reconstruction from COTS Audit Data**
- [4] **CCS 2019**. Sadegh M. Milajerdi，Birhanu Eshete, et al. **Poirot: Aligning Attack Behavior with Kernel Audit Records for Cyber Threat Hunting**. 实体构建、攻击威胁猎杀 <br />
https://arxiv.org/pdf/1910.00056.pdf <br />
https://blog.csdn.net/sc0fie1d/article/details/105103342 【中文后续自归纳】
- [5] **TIFS 2018**. Yuqing Li, Wenkuan Dai, Jie Bai, et al. **An Intelligence-Driven Security-Aware Defense Mechanism for Advanced Persistent Threats**   <br />
https://ieeexplore.ieee.org/abstract/document/8386813
- [6] **Computer & Science**. LIU H B, WU T B, SHEN J, et al, **Advanced Persistent Threat Detection Based on Generative Adversarial Networks and Long Short-term Memory**. in Computer Science, vol.47, no.1, pp.281-286, 2020.  恶意DNS和流量分析的APT恶意软件检测  <br />
- [7] **RAID2020**. Jun Zhao, Qiben Yan, Xudong Liu, Bo Li, Guangsheng Zuo. **Cyber Threat Intelligence Modeling Based on Heterogeneous Graph Convolutional Network**. 基于异构图卷积网络的网络威胁情报建模   <br />
https://www.usenix.org/system/files/raid20-zhao.pdf  <br />
https://mp.weixin.qq.com/s/TszbHM__hpYvdHsCoMmkUQ 【中文后续自归纳】 <br />
https://raid2020.org/accepted-papers/
- [8] Yali Gao,  Xiaoyong Li, Hao Peng, Binxing Fang, Philip S. Yu. HinCTI: A Cyber  Threat Intelligence Modeling and Identification System Based on Heterogeneous  Information Network.. IEEE TKDE 2020. (CCF A)
- [9] **EuroS&P**. Kiavash Satvat, Rigel Gjomemo and V.N. Venkatakrishnan, **EXTRACTOR: Extracting Attack Behavior from Threat Reports**  <br />
https://arxiv.org/pdf/2104.08618.pdf <br />




---

### Knowledge Graph + Security
- [1] **CCS 2020**. Zhang X H, Zhang Y, Zhong M, et al. **Enhancing State-of-the-art Classifiers with API Semantics to Detect Evolved Android Malware**. API语义增强图+检测   <br />
https://dl.acm.org/doi/pdf/10.1145/3372297.3417291  <br />
https://mzgao.blog.csdn.net/article/details/114366920 【中文后续自归纳】 <br />
https://www.cnblogs.com/sjtuguyang/p/13860430.html 【中文后续自归纳】 <br />
https://blog.csdn.net/qq_30303857/article/details/111356626 【中文后续自归纳】
- [2] **CCS 2019**. Sadegh M. Milajerdi，Birhanu Eshete, et al. **Poirot: Aligning Attack Behavior with Kernel Audit Records for Cyber Threat Hunting**. 实体构建、攻击威胁猎杀 <br />
https://arxiv.org/pdf/1910.00056.pdf <br />
https://blog.csdn.net/sc0fie1d/article/details/105103342 【中文后续自归纳】
- [3] **RAID2020**. Jun Zhao, Qiben Yan, Xudong Liu, Bo Li, Guangsheng Zuo. **Cyber Threat Intelligence Modeling Based on Heterogeneous Graph Convolutional Network**. 基于异构图卷积网络的网络威胁情报建模 <br />
https://www.usenix.org/system/files/raid20-zhao.pdf  <br />
https://mp.weixin.qq.com/s/TszbHM__hpYvdHsCoMmkUQ 【中文后续自归纳】


---

### GNN\DNN\CNN\RNN + Security
- [1] **NDSS 2020**. Yue Duan, Xuezixiang Li, Jinghan Wang, Heng Yin, **DeepBinDiff: Learning Program-Wide Code Representations for Binary Diffing**. 深度学习二进制相似比对  <br />
https://www.ndss-symposium.org/wp-content/uploads/2020/02/24311-paper.pdf

- [2] 深度长文：图神经网络欺诈检测方法总结. https://mp.weixin.qq.com/s/ewzsURiU7bfG3gObzIP2Mw <br />


- [3] 腾讯科恩实验室论文Order Matters的工具 Binaryai 工具复现. https://www.anquanke.com/post/id/235580 <br />


---

### Malware Family Clustering and Classification


---

### Malware Analysis
- [1] **NDSS 2020**. Hojjat Aghakhani, Fabio Gritti, Francesco Mecca, et al. **When Malware is Packin' Heat; Limits of Machine Learning Classifiers Based on Static Analysis Features**.  加壳检测、对抗加壳 <br />
https://www.ndss-symposium.org/wp-content/uploads/2020/02/24310-paper.pdf <br />
https://github.com/ucsb-seclab/packware <br />
https://mzgao.blog.csdn.net/article/details/109822304 【中文后续自归纳】
- [2] **Usenix 2020**. Shuofei Zhu, Jianjun Shi, Limin Yang, et al. **Measuring and Modeling the Label Dynamics of Online Anti-Malware Engines**. VirusTotal分类优化  <br />
https://www.usenix.org/system/files/sec20fall_zhu_prepub.pdf



---

### Intrusion Detection System 
By: Update 2021-12-29


- [1] **Computers & Security**. Jianwu Zhang, Yu Ling, Xingbing Fu, et al. **Model of the intrusion detection system based on the integration of spatial-temporal features**. Comput. Secur. 89 (2020).  时空LSTM实现入侵检测 <br />
https://www.sciencedirect.com/science/article/pii/S0167404819302214


---

### Interesting repositories
APT资源
- https://github.com/RedDrip7/APT_Digital_Weapon
- https://github.com/mstfknn/malware-sample-library
- https://github.com/kbandla/APTnotes
- https://github.com/cyber-research/APTMalware
- https://github.com/Cherishao/APT-Sample
- https://app.any.run/submissions/

其他资源
- https://github.com/QData/deepWordBug
- https://github.com/angr/angr
- https://github.com/angr/angr-doc/tree/master/examples
- https://baimafujinji.blog.csdn.net/article/details/50926010
- https://whyisyoung.github.io/BODMAS/



---


### AI 对抗样本 

文本攻击与防御的论文概述

**(1) 文本攻击与防御的论文概述**
- Analysis Methods in Neural Language Processing: A Survey. Yonatan Belinkov, James Glass. TACL 2019.
- Towards a Robust Deep Neural Network in Text Domain A Survey. Wenqi Wang, Lina Wang, Benxiao Tang, Run Wang, Aoshuang Ye. 2019.
- Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey. Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, Chenliang Li. 2019.

**(2) 黑盒攻击**
- PAWS: Paraphrase Adversaries from Word Scrambling. Yuan Zhang, Jason Baldridge, Luheng He. NAACL-HLT 2019.
- Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems. Steffen Eger, Gözde Gül ¸Sahin, Andreas Rücklé, Ji-Ung Lee, Claudia Schulz, Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson, Iryna Gurevych.NAACL-HLT 2019.
- Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models. Tong Niu, Mohit Bansal. CoNLL 2018.
- Generating Natural Language Adversarial Examples. Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, Kai-Wei Chang. EMNLP 2018.
- Breaking NLI Systems with Sentences that Require Simple Lexical Inferences. Max Glockner, Vered Shwartz, Yoav Goldberg ACL 2018.
- AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples. Dongyeop Kang, Tushar Khot, Ashish Sabharwal, Eduard Hovy. ACL 2018.
- Semantically Equivalent Adversarial Rules for Debugging NLP Models. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin ACL 2018.
- Robust Machine Comprehension Models via Adversarial Training. Yicheng Wang, Mohit Bansal. NAACL-HLT 2018.
- Adversarial Example Generation with Syntactically Controlled Paraphrase Networks. Mohit Iyyer, John Wieting, Kevin Gimpel, Luke Zettlemoyer. NAACL-HLT 2018.
- Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers. Ji Gao, Jack Lanchantin, Mary Lou Soffa, Yanjun Qi. IEEE SPW 2018.  <br />
https://arxiv.org/pdf/1801.04354.pdf
- Synthetic and Natural Noise Both Break Neural Machine Translation. Yonatan Belinkov, Yonatan Bisk. ICLR 2018.
- Generating Natural Adversarial Examples. Zhengli Zhao, Dheeru Dua, Sameer Singh. ICLR 2018.
Adversarial Examples for Evaluating Reading Comprehension Systems. Robin Jia, and Percy Liang. EMNLP 2017.

**(3) 白盒攻击**
- On Adversarial Examples for Character-Level Neural Machine Translation. Javid Ebrahimi, Daniel Lowd, Dejing Dou. COLING 2018.
- HotFlip: White-Box Adversarial Examples for Text Classification. Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou. ACL 2018.
- Towards Crafting Text Adversarial Samples. Suranjana Samanta, Sameep Mehta. ECIR 2018.

**(4) 同时探讨黑盒和白盒攻击**
- TEXTBUGGER: Generating Adversarial Text Against Real-world Applications. Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, Ting Wang. NDSS 2019.
- Comparing Attention-based Convolutional and Recurrent Neural Networks: Success and Limitations in Machine Reading Comprehension. Matthias Blohm, Glorianna Jagfeld, Ekta Sood, Xiang Yu, Ngoc Thang Vu. CoNLL 2018.
- Deep Text Classification Can be Fooled. Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, Wenchang Shi.IJCAI 2018.

**(5) 对抗防御**
- Combating Adversarial Misspellings with Robust Word Recognition. Danish Pruthi, Bhuwan Dhingra, Zachary C. Lipton. ACL 2019.
评估

**(6) 对文本攻击和防御研究提出新的评价方法**
- On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models. Paul Michel, Xian Li, Graham Neubig, Juan Miguel Pino. NAACL-HLT 2019



https://www.cnblogs.com/zzxb/p/13246967.html

《网络攻防实践》实践作业


---

### NLP经典论文



**图神经网络**
- 01.Node2Vec：Node2Vec: Scalable Feature Learning for Networks
- 02.LINE：LINE: Large-scale Information Network Embedding
- 03.SDNE：Structural Deep Network Embedding
- 04.metapath2vec：metapath2vec:Scalable Representation Learning for Heterogeneous Networks
- 05.TransE/H/R/D:
TransE：Translating Embeddings for Modeling Multi-relational Data <br />
TransH：Knowledge Graph Embedding by Translating on Hyperplanes <br />
TransR：Learning entity and relation embeddings for knowledge graph completion <br />
TransD：Knowledge Graph Embedding via Dynamic Mapping Matrix <br />
- 06.GAT：Graph Attention Networks
- 07.GraphSAGE：Inductive Representation kearping on Large Graphs
- 08.GCN：Semi-Supervised Classification with Graph Convolutional Networks
- 09.GGNN：Gated Graph Sequence Neural Networks
- 10.MPNN：Neural Message Passing for Quantum Chemistry

**NLP精读论文目录**
- 01.Deep learning：Deep learning
- 02.word2vec：Efficient Estimation of Word Representations in Vector Space
- 03.句和文档的embedding：Distributed representations of sentences and docments
- 04.machine translation：Neural Machine Translation by Jointly Learning to Align and Translate
- 05.transformer：Transformer: attention is all you need
- 06.GloVe：GloVe: Global Vectors for Word Representation
- 07.Skip：Skip-Thought Vector
- 08.TextCNN：Convolutional Neural Networks for Sentence Classification
- 09.基于CNN的词级别的文本分类：Character-level Convolutional Networks for Text Classification
- 10.DCNN：A Convolutional Neural Network For Modelling Sentences
- 11.FASTTEXT：Bag of Tricks for Efficient Text Classification
- 12.HAN：Hierarchical Attention Network for Document Classification
- 13.PCNNATT：Neural Relation Extraction with Selective Attention over Instances
- 14.E2ECRF：End-to-end Sequence Labeling via Bi-directional LSTM-CNNS-CRF
- 15.多层LSTM：Sequence to Sequence Learning with Neural Networks
- 16.卷积seq2seq：Convolutional Sequence to Sequence Learning
- 17.GNMT：Google’s Neural Machine Translation System：Bridging the Gap between Human and Machine Translation
- 18.UMT：Phrase-Based&Neural Unsupervised Machine Translation
- 19.指针生成网络：Get To The Point:Summarization with Pointer-Generator Networks
- 20.End-to-End Memory Networks：End-to-End Memory Networks
- 21.QANet：QANet:Combining Local Convolution with Global Self-Attention for Reading Comprehension
- 22.双向Attention：Bi-Directional Attention Flow for Machine Comprehension
- 23.Dialogue：Adversarial Learning for Neural Dialogue Generation
- 24.缺
- 25.R-GCNs：Modeling Relational Data with GraphConvolutional Networks
- 26.大规模语料模型：Exploring the limits of language model
- 27.Transformer-XL：Transformer-XL:Attentive Language Models Beyond a Fixed-Length Context
- 28.TCN：An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling
- 29.Deep contextualized word representations
- 30.BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding

**NLP Baseline**
- 1.Word2Vec.Efficient Estimation of Word Representations in Vector Space
- 2.GloVe.GloVe: Global Vectors for Word Representation
- 3.C2W.Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation
- 4.TextCNN.Convolutional Neural Networks for Sentence Classification
- 5.CharCNN.Character-level Convolutional Networks for Text Classification
- 6.FastText.Bag of Tricks for Efficient Text Classification
- 7.Seq2Seq.Sequence to Sequence Learning with Neural Networks
- 8.Attention NMT.Neural Machine Translation by Jointly Learning to Align and Translate
- 9.HAN.Hierarchical Attention Network for Document Classification
- 10.SGM.SGM: Sequence Generation Model for Multi-Label Classification



---

### GAN


---

## 二.Classified by source

### Conferences & Journals Abroad


---

### Chinese Conference & Periodical



---

### Enterprise Analysis Report



----


## Time

2021-04-19：撰写恶意代码相关论文

读博艰辛，努力前行~


---



By:Eastmount 2022-09-26
",['eastmountyxz'],0,,0.55,0,,,,,,5,,,
52553541,MDEwOlJlcG9zaXRvcnk1MjU1MzU0MQ==,touchFluid,kamindustries/touchFluid,0,kamindustries,https://github.com/kamindustries/touchFluid,Fluids in TouchDesigner and GLSL,0,2016-02-25 20:10:52+00:00,2025-03-06 21:14:59+00:00,2022-10-18 03:23:51+00:00,,2353,203,203,GLSL,1,1,1,1,0,0,23,0,0,0,mit,1,0,0,public,23,0,203,master,1,,"# touchFluid
### Fluids in TouchDesigner and GLSL

touchFluid is a lightweight 2d Semi-Lagrangian fluid solver for TouchDesigner. It features vorticity confinement, temperature, buoyancy, and obstacles.

The repo has a .toe project file and a .tox component file. The project file shows examples of impulses and obstacles: holding 1, 2, and 3 on the keyboard will add velocity, density, and temperature at the mouse position. The tox is just the core simulation component with inputs and outputs for the advection of velocity, density/temperature, and RGBA color fields.

I have added the shaders as separate files for easy editing and versioning, though they are not required to run either the .toe or .tox.

touchFluid is a key component in my Masters [thesis project](http://timesequence.blogspot.com/) for the Media Arts & Technology program at the University of California Santa Barbara.

This was originally written in CUDA. That repo was forked and can be found here: [touchFluidCUDA](https://github.com/kamindustries/touchFluidCUDA).

#### References
1. Jos Stam, [_Stable Fluids_](http://dl.acm.org/citation.cfm?id=311548).
2. Ronald Fedikw, Jos Stam, and Henrik Wann Jensen, [_Visual Simulation of Smoke_](http://dl.acm.org/citation.cfm?id=383260).
3. Mark Harris, [_Fast Fluid Dynamics Simulation on the GPU_](http://http.developer.nvidia.com/GPUGems/gpugems_ch38.html).
3. Philip Rideout, [_Simple Fluid Simulation_](http://prideout.net/blog/?p=58).

#### Licensing
touchFluid code is released under the [MIT License](https://github.com/kamindustries/touchFluid/blob/master/LICENSE).
",['kamindustries'],1,,0.73,0,,,,,,18,,,
52388195,MDEwOlJlcG9zaXRvcnk1MjM4ODE5NQ==,superlu_dist,xiaoyeli/superlu_dist,0,xiaoyeli,https://github.com/xiaoyeli/superlu_dist,"Distributed memory, MPI based SuperLU",0,2016-02-23 20:10:16+00:00,2025-02-28 21:29:17+00:00,2025-02-24 22:29:43+00:00,https://portal.nersc.gov/project/sparse/superlu/,54152,198,198,C,1,1,1,1,0,0,67,0,0,70,other,1,0,0,public,67,70,198,master,1,,"# SuperLU_DIST (version 9.1.0)   <img align=center width=""55"" alt=""superlu"" src=""https://user-images.githubusercontent.com/11741943/103982988-5a9a9d00-5139-11eb-9ac4-a55e80a79f8d.png"">

[![Build Status](https://github.com/xiaoyeli/superlu_dist/actions/workflows/test.yml/badge.svg)](https://github.com/xiaoyeli/superlu_dist/actions/workflows/test.yml)
[Nightly tests](http://my.cdash.org/index.php?project=superlu_dist)

SuperLU_DIST contains a set of subroutines to solve a sparse linear system
A*X=B. It uses Gaussian elimination with static pivoting (GESP).
Static pivoting is a technique that combines the numerical stability of
partial pivoting with the scalability of Cholesky (no pivoting),
to run accurately and efficiently on large numbers of processors.

SuperLU_DIST is a parallel extension to the serial SuperLU library.
It is targeted for the distributed memory parallel machines.
SuperLU_DIST is implemented in ANSI C, with OpenMP for on-node parallelism
and MPI for off-node communications. We are actively developing multi-GPU
acceleration capabilities.
<!-- Currently, the LU factorization and triangular solution routines, -->
<!-- which are the most time-consuming part of the solution process,-->
<!-- are parallelized. The other routines, such as static pivoting and -->
<!-- column preordering for sparsity are performed sequentially. -->
<!-- This ""alpha"" release contains double-precision real and-->
<!-- double-precision complex data types.-->

Table of Contents
=================

* [SuperLU_DIST (version 9.1.0)   <a href=""https://user-images.githubusercontent.com/11741943/103982988-5a9a9d00-5139-11eb-9ac4-a55e80a79f8d.png"" target=""_blank"" rel=""nofollow""><img align=""center"" width=""55"" alt=""superlu"" src=""https://user-images.githubusercontent.com/11741943/103982988-5a9a9d00-5139-11eb-9ac4-a55e80a79f8d.png"" style=""max-width:100%;""></a>](#superlu_dist-version-910--)
* [Directory structure of the source code](#directory-structure-of-the-source-code)
* [Installation](#installation)
   * [Installation option 1: Using CMake build system.](#installation-option-1-using-cmake-build-system)
      * [Dependent external libraries: BLAS and ParMETIS](#dependent-external-libraries-blas-and-parmetis)
      * [Optional external libraries: CombBLAS, LAPACK](#optional-external-libraries-combblas-lapack)
      * [Use GPU](#use-gpu)
      * [Summary of the CMake definitions.](#summary-of-the-cmake-definitions)
   * [Installation option 2: Manual installation with makefile.](#installation-option-2-manual-installation-with-makefile)
      * [2.1 Edit the make.inc include file.](#21-edit-the-makeinc-include-file)
      * [2.2. The BLAS library.](#22-the-blas-library)
      * [2.3. External libraries.](#23-external-libraries)
         * [2.3.1 Metis and ParMetis.](#231-metis-and-parmetis)
         * [2.3.2 LAPACK.](#232-lapack)
         * [2.3.3 CombBLAS.](#233-combblas)
      * [2.4. C preprocessor definition CDEFS. (Replaced by cmake module FortranCInterface.)](#24-c-preprocessor-definition-cdefs-replaced-by-cmake-module-fortrancinterface)
      * [2.5. Multicore and GPU.](#25-multicore-and-gpu)
* [Summary of the environment variables.](#summary-of-the-environment-variables)
* [Windows Usage](#windows-usage)
* [Reading sparse matrix files](#reading-sparse-matrix-files)
* [REFERENCES](#references)
* [RELEASE VERSIONS](#release-versions)

Created by [gh-md-toc](https://github.com/ekalinin/github-markdown-toc)

# SuperLU_DIST (version 9.1.0)  <img align=center width=""55"" alt=""superlu"" src=""https://user-images.githubusercontent.com/11741943/103982988-5a9a9d00-5139-11eb-9ac4-a55e80a79f8d.png"">

[![Build Status](https://github.com/xiaoyeli/superlu_dist/actions/workflows/test.yml/badge.svg)](https://github.com/xiaoyeli/superlu_dist/actions/workflows/test.yml)
[Nightly tests](http://my.cdash.org/index.php?project=superlu_dist)

SuperLU_DIST contains a set of subroutines to solve a sparse linear system
A*X=B. It uses Gaussian elimination with static pivoting (GESP).
Static pivoting is a technique that combines the numerical stability of
partial pivoting with the scalability of Cholesky (no pivoting),
to run accurately and efficiently on large numbers of processors.

SuperLU_DIST is a parallel extension to the serial SuperLU library.
It is targeted for the distributed memory parallel machines.
SuperLU_DIST is implemented in ANSI C, with OpenMP for on-node parallelism
and MPI for off-node communications. We are actively developing GPU
acceleration capabilities.
<!-- Currently, the LU factorization and triangular solution routines, -->
<!-- which are the most time-consuming part of the solution process,-->
<!-- are parallelized. The other routines, such as static pivoting and -->
<!-- column preordering for sparsity are performed sequentially. -->
<!-- This ""alpha"" release contains double-precision real and-->
<!-- double-precision complex data types.-->


# Directory structure of the source code

```
SuperLU_DIST/README    instructions on installation
SuperLU_DIST/CBLAS/    needed BLAS routines in C, not necessarily fast
	 	       (NOTE: this version is single threaded. If you use the
		       library with multiple OpenMP threads, performance
		       relies on a good multithreaded BLAS implementation.)
SuperLU_DIST/DOC/      the Users' Guide
SuperLU_DIST/FORTRAN/  Fortran90 wrapper functions
SuperLU_DIST/EXAMPLE/  example programs
SuperLU_DIST/INSTALL/  test machine dependent parameters
SuperLU_DIST/SRC/      C source code, to be compiled into libsuperlu_dist.a
SuperLU_DIST/TEST/     testing code
SuperLU_DIST/lib/      contains library archive libsuperlu_dist.a
SuperLU_DIST/Makefile  top-level Makefile that does installation and testing
SuperLU_DIST/make.inc  compiler, compiler flags, library definitions and C
	               preprocessor definitions, included in all Makefiles.
	               (You may need to edit it to suit your system
	               before compiling the whole package.)
SuperLU_DIST/MAKE_INC/ sample machine-specific make.inc files
```

# Installation

There are two ways to install the package. The first method is to use
CMake automatic build system. The other method requires users to
The procedures are described below.

## Installation option 1: Using CMake build system.
You will need to create a build tree from which to invoke CMake.

### Dependent external libraries: BLAS and ParMETIS
If you have a BLAS library on your machine, you can link with it
with the following cmake definition:
```
-DTPL_BLAS_LIBRARIES=""<BLAS library name>""
```
Otherwise, the CBLAS/ subdirectory contains the part of the C BLAS
(single threaded) needed by SuperLU_DIST, but they are not optimized.
You can compile and use it with the following cmake definition:
```
-DTPL_ENABLE_INTERNAL_BLASLIB=ON
```

The default sparsity ordering is METIS. But, in order to use parallel
symbolic factorization function, you
need to install ParMETIS parallel ordering package and define the
two environment variables: PARMETIS_ROOT and PARMETIS_BUILD_DIR

(Note: ParMETIS library also contains serial METIS library.)

```
export PARMETIS_ROOT=<Prefix directory of the ParMETIS installation>
export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/build/Linux-x86_64
```

### Optional external libraries: CombBLAS, LAPACK

In order to use parallel weighted matching HWPM (Heavy Weight
Perfect Matching) for numerical pre-pivoting, you need to install
CombBLAS and define the environment variable:

```
export COMBBLAS_ROOT=<Prefix directory of the CombBLAS installation>
export COMBBLAS_BUILD_DIR=${COMBBLAS_ROOT}/_build
```
Then, install with cmake option:
```
-DTPL_ENABLE_COMBBLASLIB=ON
```

By default, LAPACK is not needed. Only in triangular solve routine, we
may use LAPACK to explicitly invert the dense diagonal block to improve
speed. You can use it with the following cmake option:
```
-DTPL_ENABLE_LAPACKLIB=ON
```

### Use GPU
You can enable (NVIDIA) GPU with CUDA with the following cmake option:
```
-DTPL_ENABLE_CUDALIB=TRUE
```
You can enable (AMD) GPU with HIP with the following cmake option:
```
-DTPL_ENABLE_HIPLIB=TRUE
```

Once these needed third-party libraries are in place, the installation
can be done as follows at the top level directory:

For a simple installation with default setting, do:
(ParMETIS is needed, i.e., TPL_ENABLE_PARMETISLIB=ON)
```
mkdir build ; cd build;
cmake .. \
    -DTPL_PARMETIS_INCLUDE_DIRS=""${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include"" \
    -DTPL_PARMETIS_LIBRARIES=""${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a"" \
```
For a more sophisticated installation including third-party libraries, do:
```
cmake .. \
    -DTPL_PARMETIS_INCLUDE_DIRS=""${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include"" \
    -DTPL_PARMETIS_LIBRARIES=""${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a"" \
    -DTPL_ENABLE_COMBBLASLIB=ON \
    -DTPL_COMBBLAS_INCLUDE_DIRS=""${COMBBLAS_ROOT}/_install/include;${COMBBLAS_R\
OOT}/Applications/BipartiteMatchings"" \
    -DTPL_COMBBLAS_LIBRARIES=""${COMBBLAS_BUILD_DIR}/libCombBLAS.a"" \
    -DCMAKE_C_FLAGS=""-std=c99 -g -DPRNTlevel=0 -DDEBUGlevel=0"" \
    -DCMAKE_C_COMPILER=mpicc \
    -DCMAKE_CXX_COMPILER=mpicxx \
    -DCMAKE_CXX_FLAGS=""-std=c++11"" \
    -DTPL_ENABLE_INTERNAL_BLASLIB=OFF \
    -DBUILD_SHARED_LIBS=OFF \
    -DCMAKE_INSTALL_PREFIX=.

( see example cmake script: run_cmake_build.sh )
```

You can disable CombBLAS or LAPACK with the following cmake options:
```
-DTPL_ENABLE_LAPACKLIB=FALSE
-DTPL_ENABLE_COMBBLASLIB=FALSE
```

To actually build (compile), type:
`make`

To install the libraries, type:
`make install`

To run the installation test, type:
`ctest`
(The outputs are in file: `build/Testing/Temporary/LastTest.log`)
or,
`ctest -D Experimental`
or,
`ctest -D Nightly`

**NOTE:**
The parallel execution in ctest is invoked by ""mpiexec"" command which is
from MPICH environment. If your MPI is not MPICH/mpiexec based, the test
execution may fail. You can pass the definition option ""-DMPIEXEC_EXECUTABLE""
to cmake. For example on Cori at NERSC, you will need the following:
`-DMPIEXEC_EXECUTABLE=/usr/bin/srun`

Or, you can always go to TEST/ directory to perform testing manually.

### Summary of the CMake definitions.
The following list summarize the commonly used CMake definitions. In each case,
the first choice is the default setting. After running 'cmake' installation,
a configuration header file is generated in SRC/superlu_dist_config.h, which
contains the key CPP definitions used throughout the code.
```
    -TPL_ENABLE_PARMETISLIB=ON | OFF
    -DTPL_ENABLE_INTERNAL_BLASLIB=OFF | ON
    -DTPL_ENABLE_LAPACKLIB=OFF | ON
    -TPL_ENABLE_COMBBLASLIB=OFF | ON
    -DTPL_ENABLE_CUDALIB=OFF | ON
    -DTPL_ENABLE_HIPLIB=OFF | ON
    -Denable_complex16=OFF | ON
    -DXSDK_INDEX_SIZE=32 | 64

    -DBUILD_SHARED_LIBS= OFF | ON
    -DCMAKE_INSTALL_PREFIX=<...>.
    -DCMAKE_C_COMPILER=<MPI C compiler>
    -DCMAKE_C_FLAGS=""...""
    -DCMAKE_CXX_COMPILER=<MPI C++ compiler>
    -DMAKE_CXX_FLAGS=""...""
    -DCMAKE_CUDA_FLAGS=""...""
    -DHIP_HIPCC_FLAGS=""...""
    -DXSDK_ENABLE_Fortran=OFF | ON
    -DCMAKE_Fortran_COMPILER=<MPI F90 compiler>
```

## Installation option 2: Manual installation with makefile.
Before installing the package, please examine the three things dependent
on your system setup:

### 2.1 Edit the make.inc include file.

This make include file is referenced inside each of the Makefiles
in the various subdirectories. As a result, there is no need to
edit the Makefiles in the subdirectories. All information that is
machine specific has been defined in this include file.

Sample machine-specific make.inc are provided in the MAKE_INC/
directory for several platforms, such as Cray XT5, Linux, Mac-OS, and CUDA.
When you have selected the machine to which you wish to install
SuperLU_DIST, copy the appropriate sample include file
(if one is present) into make.inc.

For example, if you wish to run SuperLU_DIST on a Cray XT5,  you can do
`cp MAKE_INC/make.xt5  make.inc`

For the systems other than listed above, some porting effort is needed
for parallel factorization routines. Please refer to the Users' Guide
for detailed instructions on porting.

The following CPP definitions can be set in CFLAGS.
```
-DXSDK_INDEX_SIZE=64
use 64-bit integers for indexing sparse matrices. (default 32 bit)

-DPRNTlevel=[0,1,2,...]
printing level to show solver's execution details. (default 0)

-DDEBUGlevel=[0,1,2,...]
diagnostic printing level for debugging purpose. (default 0)
```

### 2.2. The BLAS library.

The parallel routines in SuperLU_DIST use some BLAS routines on each MPI
process. Moreover, if you enable OpenMP with multiple threads, you need to
link with a multithreaded BLAS library. Otherwise performance will be poor.
A good public domain BLAS library is OpenBLAS (http://www.openblas.net),
which has OpenMP support.

If you have a BLAS library your machine, you may define the following in
the file make.inc:
```
BLASDEF = -DUSE_VENDOR_BLAS
BLASLIB = <BLAS library you wish to link with>
```
The CBLAS/ subdirectory contains the part of the C BLAS (single threaded)
needed by SuperLU_DIST package. However, these codes are intended for use
only if there is no faster implementation of the BLAS already
available on your machine. In this case, you should go to the
top-level SuperLU_DIST/ directory and do the following:

1) In make.inc, undefine (comment out) BLASDEF, and define:
` BLASLIB = ../lib/libblas$(PLAT).a`

2) Type: `make blaslib`
to make the BLAS library from the routines in the
` CBLAS/ subdirectory.`

### 2.3. External libraries.

  #### 2.3.1 Metis and ParMetis.

If you will use Metis or ParMetis for sparsity ordering, you will
need to install them yourself. Since ParMetis package already
contains the source code for the Metis library, you can just
download and compile ParMetis from:
[http://glaros.dtc.umn.edu/gkhome/metis/parmetis/download](http://glaros.dtc.umn.edu/gkhome/metis/parmetis/download)

After you have installed it, you should define the following in make.inc:
```
HAVE_PARMETIS = TRUE
METISLIB = -L<metis directory> -lmetis
PARMETISLIB = -L<parmetis directory> -lparmetis
I_PARMETIS = -I<parmetis directory>/include -I<parmetis directory>/metis/include
```
You can disable ParMetis with the following line in SRC/superlu_dist_config.h:
```
#undef HAVE_PARMETIS
```
  #### 2.3.2 LAPACK.
  Starting Version 6.0, the triangular solve routine can perform explicit
  inversion on the diagonal blocks, using LAPACK's xTRTRI inversion routine.
  To use this feature, you should define the following in make.inc:
```
SLU_HAVE_LAPACK = TRUE
LAPACKLIB = <lapack library you wish to link with>
```
You can disable LAPACK with the following line in SRC/superlu_dist_config.h:
```
#undef SLU_HAVE_LAPACK
```

 #### 2.3.3 CombBLAS.

You can use parallel approximate weight perfect matching (AWPM) algorithm
to perform numerical pre-pivoting for stability. The default pre-pivoting
is to use MC64 provided internally, which is an exact algorithm, but serial.
In order to use AWPM, you will need to install CombBLAS yourself, at the
download site:
[https://people.eecs.berkeley.edu/~aydin/CombBLAS/html/index.html](https://people.eecs.berkeley.edu/~aydin/CombBLAS/html/index.html)

After you have installed it, you should define the following in make.inc:
```
HAVE_COMBBLAS = TRUE
COMBBLASLIB = <combblas root>/_build/libCombBLAS.a
I_COMBBLAS=-I<combblas root>/_install/include -I<combblas root>/Applications/BipartiteMatchings
```
You can disable CombBLAS with the following line in SRC/superlu_dist_config.h:
```
#undef HAVE_COMBBLAS
```

### 2.4. C preprocessor definition CDEFS. (Replaced by cmake module FortranCInterface.)

In the header file SRC/superlu_FCnames.h, we use macros to determine how
C routines should be named so that they are callable by Fortran.
(Some vendor-supplied BLAS libraries do not have C interfaces. So the
re-naming is needed in order for the SuperLU BLAS calls (in C) to
interface with the Fortran-style BLAS.)
The possible options for CDEFS are:
```
-DAdd_: Fortran expects a C routine to have an underscore
  postfixed to the name;
  (This is set as the default)
-DNoChange: Fortran expects a C routine name to be identical to
      that compiled by C;
-DUpCase: Fortran expects a C routine name to be all uppercase.
```

### 2.5. Multicore and GPU.

To use OpenMP parallelism, need to link with an OpenMP library, and
set the number of threads you wish to use as follows (bash):

`export OMP_NUM_THREADS=<##>`

To enable NVIDIA GPU access, need to take the following step:
Add the CUDA library location in make.inc:
```
HAVE_CUDA=TRUE
INCS += -I<CUDA directory>/include
LIBS += -L<CUDA directory>/lib64 -lcublas -lcudart
endif
```
A Makefile is provided in each subdirectory. The installation can be done
completely automatically by simply typing ""make"" at the top level.


# Summary of the environment variables.
A couple of environment variables affect parallel execution.
```
    export OMP_NUM_THREADS=<...>
    export SUPERLU_ACC_OFFLOAD=1  // this enables use of GPU. Default is 1.
```
Several integer blocking parameters may affect performance. Most of them can be
set by the user through environment variables. Oherwise the default values
are provided. Various SuperLU routines call an environment inquiry function
to obtain these parameters. This function is provided in the file SRC/sp_ienv.c.
Please consult that file for detailed description of the meanings.
```
    export NREL=<...>   // supernode relaxation parameter
    export NSUP=<...>   // maximum allowable supernode size, not to exceed 512
    export FILL=<...>   // estimated fill ratio of nonzeros(L+U)/nonzeros(A)
    export MAX_BUFFER_SIZE=<...>   // maximum buffer size on GPU for GEMM
```

# Windows Usage
Prerequisites: CMake, Visual Studio, Microsoft HPC Pack
This has been tested with Visual Studio 2017, without Parmetis,
without Fortran, and with OpenMP disabled.

The cmake configuration line used was
```
'/winsame/contrib-vs2017/cmake-3.9.4-ser/bin/cmake' \
  -DCMAKE_INSTALL_PREFIX:PATH=C:/winsame/volatile-vs2017/superlu_dist-master.r147-parcomm \
  -DCMAKE_BUILD_TYPE:STRING=Release \
  -DCMAKE_COLOR_MAKEFILE:BOOL=FALSE \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=TRUE \
  -Denable_openmp:BOOL=FALSE \
  -DCMAKE_C_COMPILER:FILEPATH='C:/Program Files (x86)/Microsoft Visual Studio/2017/Professional/VC/Tools/MSVC/14.11.25503/bin/HostX64/x64/cl.exe' \
  -DCMAKE_C_FLAGS:STRING='/DWIN32 /D_WINDOWS /W3' \
  -DTPL_ENABLE_PARMETISLIB:BOOL=FALSE \
  -DXSDK_ENABLE_Fortran=OFF \
  -G 'NMake Makefiles JOM' \
  C:/path/to/superlu_dist
```

After configuring, simply do
```
  jom # or nmake
  jom install  # or nmake install
```

Libraries will be installed under
C:/winsame/volatile-vs2017/superlu_dist-master.r147-parcomm/lib
for the above configuration.

If you wish to test:
  `ctest`

# Reading sparse matrix files

The SRC/ directory contains the following routines to read different file
formats, they all have the similar calling sequence.
```
$ ls -l dread*.c
dreadMM.c              : Matrix Market, files with suffix .mtx
dreadhb.c              : Harrell-Boeing, files with suffix .rua
dreadrb.c              : Rutherford-Boeing, files with suffix .rb
dreadtriple.c          : triplet, with header
dreadtriple_noheader.c : triplet, no header, which is also readable in Matlab
```

# REFERENCES

**[1]** X.S. Li and J.W. Demmel, ""SuperLU_DIST: A Scalable Distributed-Memory
 Sparse Direct Solver for Unsymmetric Linear Systems"", ACM Trans. on Math.
 Software, Vol. 29, No. 2, June 2003, pp. 110-140.
**[2]** L. Grigori, J. Demmel and X.S. Li, ""Parallel Symbolic Factorization
 for Sparse LU with Static Pivoting"", SIAM J. Sci. Comp., Vol. 29, Issue 3,
 1289-1314, 2007.
**[3]** P. Sao, R. Vuduc and X.S. Li, ""A distributed CPU-GPU sparse direct
 solver"", Proc. of EuroPar-2014 Parallel Processing, August 25-29, 2014.
 Porto, Portugal.
**[4]** P. Sao, X.S. Li, R. Vuduc, “A Communication-Avoiding 3D Factorization
 for Sparse Matrices”, Proc. of IPDPS, May 21–25, 2018, Vancouver.
**[5]** P. Sao, R. Vuduc, X. Li, ""Communication-avoiding 3D algorithm for
 sparse LU factorization on heterogeneous systems"", J. Parallel and
 Distributed Computing (JPDC), September 2019.
**[6]** Y. Liu, M. Jacquelin, P. Ghysels and X.S. Li, “Highly scalable
 distributed-memory sparse triangular solution algorithms”, Proc. of
 SIAM workshop on Combinatorial Scientific Computing, June 6-8, 2018,
 Bergen, Norway.
**[7]** N. Ding, S. Williams, Y. Liu, X.S. Li, ""Leveraging One-Sided
 Communication for Sparse Triangular Solvers"", Proc. of SIAM Conf. on
 Parallel Processing for Scientific Computing. Feb. 12-15, 2020.
**[8]** A. Azad, A. Buluc, X.S. Li, X. Wang, and J. Langguth,
""A distributed-memory algorithm for computing a heavy-weight perfect matching
on bipartite graphs"", SIAM J. Sci. Comput., Vol. 42, No. 4, pp. C143-C168, 2020.\
**[9]** N. Ding, Y. Liu, S. Williams, X.S. Li,
""A Message-Driven, Multi-GPU Parallel Sparse Triangular Solver”,
Proceedings of SIAM Proceedings of ACDA21 conference, 2021.\
**[10]** Y. Liu, N. Ding, P. Sao, S. Williams, X.S. Li,
""Unified Communication Optimization Strategies for Sparse Triangular Solver on CPU and GPU Clusters"", Proceedings of SC23, Nov. 2023 \
**[11]** X. Li, P. Lin, Y. Liu, P. Sao, “Newly Released Capabilities in Distributed-memory SuperLU Sparse Direct Solver”,
ACM Trans. Math. Software, Volume 49, No. 1, March 2023.
  https://dl.acm.org/doi/10.1145/3577197 \
**[12]** W. Boukaram, Y. Hong Y, Y. Liu, T. Shi, X.S. Li.
  ""Batched sparse direct solver design and evaluation in SuperLU\_DIST"".
  International Journal of High Performance Computing Applications. 2024;38(6):585-598.
  doi:10.1177/10943420241268200


**Xiaoye S. Li**, Lawrence Berkeley National Lab, [xsli@lbl.gov](xsli@lbl.gov)
**Gustavo Chavez**, Lawrence Berkeley National Lab, [gichavez@lbl.gov](gichavez@lbl.gov)
**Jim Demmel**, UC Berkeley, [demmel@cs.berkeley.edu](demmel@cs.berkeley.edu)
**Nan Ding**, Lawrence Berkeley National Lab, [nanding@lbl.gov](nanding@lbl.gov)
**John Gilbert**, UC Santa Barbara, [gilbert@cs.ucsb.edu](gilbert@cs.ucsb.edu)
**Laura Grigori**, INRIA, France, [laura.grigori@inria.fr](laura.grigori@inria.fr)
**Paul Lin**, Lawrence Berkeley National Lab, [paullin@lbl.gov](paullin@lbl.gov)
**Yang Liu**, Lawrence Berkeley National Lab, [liuyangzhuan@lbl.gov](liuyangzhuan@lbl.gov)
**Piyush Sao**, Georgia Institute of Technology, [piyush.feynman@gmail.com](piyush.feynman@gmail.com)
**Meiyue Shao**, Lawrence Berkeley National Lab, [myshao@lbl.gov](myshao@lbl.gov)
**Ichitaro Yamazaki**, Univ. of Tennessee, [ic.yamazaki@gmail.com](ic.yamazaki@gmail.com)


# RELEASE VERSIONS
```
October 15, 2003    Version 2.0
October 1,  2007    Version 2.1
Feburary 20, 2008   Version 2.2
October 15, 2008    Version 2.3
June 9, 2010        Version 2.4
November 23, 2010   Version 2.5
March 31, 2013      Version 3.3
October 1, 2014     Version 4.0
July 15, 2014       Version 4.1
September 25, 2015  Version 4.2
December 31, 2015   Version 4.3
April 8, 2016       Version 5.0.0
May 15, 2016        Version 5.1.0
October 4, 2016     Version 5.1.1
December 31, 2016   Version 5.1.3
September 30, 2017  Version 5.2.0
January 28, 2018    Version 5.3.0
June 1, 2018        Version 5.4.0
September 22, 2018  Version 6.0.0
December 9, 2018    Version 6.1.0
February 8, 2019    Version 6.1.1
November 12, 2019   Version 6.2.0
February 23, 2020   Version 6.3.0
October 23, 2020    Version 6.4.0
May 10, 2021        Version 7.0.0
October 5, 2021     Version 7.1.0
October 18, 2021    Version 7.1.1
December 12, 2021   Version 7.2.0
May 22, 2022        Version 8.0.0
July 5, 2022        Version 8.1.0
October 1, 2022     Version 8.1.1
Novembe 9, 2023     Version 8.2.0
Novembe 17, 2023    Version 8.2.1
May 8, 2024         Version 9.0.0
November 10, 2024   Version 9.1.0
```
","['xiaoyeli', 'liuyangzhuan', 'piyush314', 'WajihBK', 'nanding0701', 'jrobcary', 'gchavez2', 'pwxy', 'balay', 'prj-', 'nmnobre', 'minrk', 'sebastiangrimberg', 'BarrySmith', 'eromero-vlc', 'bavier', 'jamtrott', 'jeanlucf22', 'pghysels', 'SidShi', 'jacobrking']",1,,0.71,0,,,,,,17,,,
48642866,MDEwOlJlcG9zaXRvcnk0ODY0Mjg2Ng==,Awesome-People-in-Computer-Vision,solarlee/Awesome-People-in-Computer-Vision,0,solarlee,https://github.com/solarlee/Awesome-People-in-Computer-Vision,,0,2015-12-27 11:39:00+00:00,2025-02-28 02:53:11+00:00,2017-10-31 18:01:28+00:00,,51,193,193,,1,1,1,1,0,0,46,0,0,3,,1,0,0,public,46,3,193,master,1,,,['solarlee'],0,,0.64,0,,,,,,13,,,
237353415,MDEwOlJlcG9zaXRvcnkyMzczNTM0MTU=,addchain,mmcloughlin/addchain,0,mmcloughlin,https://github.com/mmcloughlin/addchain,Cryptographic Addition Chain Generation in Go,0,2020-01-31 03:31:03+00:00,2024-12-30 09:51:09+00:00,2024-06-30 22:18:29+00:00,,441,188,188,Go,1,0,1,0,0,0,12,0,0,30,bsd-3-clause,1,0,0,public,12,30,188,master,1,,"<p align=""center"">
  <img src=""logo.svg"" width=""40%"" border=""0"" alt=""addchain"" />
  <br />
  <img src=""https://img.shields.io/github/actions/workflow/status/mmcloughlin/addchain/ci.yml?style=flat-square"" alt=""Build Status"" />
  <a href=""https://pkg.go.dev/github.com/mmcloughlin/addchain""><img src=""https://img.shields.io/badge/doc-reference-007d9b?logo=go&style=flat-square"" alt=""go.dev"" /></a>
  <a href=""https://goreportcard.com/report/github.com/mmcloughlin/addchain""><img src=""https://goreportcard.com/badge/github.com/mmcloughlin/addchain?style=flat-square"" alt=""Go Report Card"" /></a>
  <a href=""https://doi.org/10.5281/zenodo.5622943""><img src=""https://img.shields.io/badge/DOI-10.5281%2Fzenodo.5622943-007ec6?style=flat-square"" alt=""DOI: 10.5281/zenodo.5622943"" /></a>
</p>

<p align=""center"">Cryptographic Addition Chain Generation in Go</p>

`addchain` generates short addition chains for exponents of cryptographic
interest with [results](#results) rivaling the best hand-optimized chains.
Intended as a building block in elliptic curve or other cryptographic code
generators.

* Suite of algorithms from academic research: continued fractions,
  dictionary-based and Bos-Coster heuristics
* Custom run-length techniques exploit structure of cryptographic exponents
  with excellent results on Solinas primes
* Generic optimization methods eliminate redundant operations
* Simple domain-specific language for addition chain computations
* Command-line interface or library
* Code generation and templated output support

## Table of Contents

* [Background](#background)
* [Results](#results)
* [Usage](#usage)
  * [Command-line Interface](#command-line-interface)
  * [Library](#library)
* [Algorithms](#algorithms)
  * [Binary](#binary)
  * [Continued Fractions](#continued-fractions)
  * [Bos-Coster Heuristics](#bos-coster-heuristics)
  * [Dictionary](#dictionary)
  * [Runs](#runs)
  * [Optimization](#optimization)
* [Citing](#citing)
* [Thanks](#thanks)
* [Contributing](#contributing)
* [License](#license)


## Background

An [_addition chain_](https://en.wikipedia.org/wiki/Addition_chain) for a
target integer _n_ is a sequence of numbers starting at 1 and ending at _n_
such that every term is a sum of two numbers appearing earlier in the
sequence. For example, an addition chain for 29 is

```
1, 2, 4, 8, 9, 17, 25, 29
```

Addition chains arise in the optimization of exponentiation algorithms with
fixed exponents. For example, the addition chain above corresponds to the
following sequence of multiplications to compute <code>x<sup>29</sup></code>

<pre>
 x<sup>2</sup> = x<sup>1</sup> * x<sup>1</sup>
 x<sup>4</sup> = x<sup>2</sup> * x<sup>2</sup>
 x<sup>8</sup> = x<sup>4</sup> * x<sup>4</sup>
 x<sup>9</sup> = x<sup>1</sup> * x<sup>8</sup>
x<sup>17</sup> = x<sup>8</sup> * x<sup>9</sup>
x<sup>25</sup> = x<sup>8</sup> * x<sup>17</sup>
x<sup>29</sup> = x<sup>4</sup> * x<sup>25</sup>
</pre>

An exponentiation algorithm for a fixed exponent _n_ reduces to finding a
_minimal length addition chain_ for _n_. This is especially relevent in
cryptography where exponentiation by huge fixed exponents forms a
performance-critical component of finite-field arithmetic. In particular,
constant-time inversion modulo a prime _p_ is performed by computing
<code>x<sup>p-2</sup> (mod p)</code>, thanks to [Fermat's Little
Theorem](https://en.wikipedia.org/wiki/Fermat%27s_little_theorem). Square root
also reduces to exponentiation for some prime moduli. Finding short addition
chains for these exponents is one important part of high-performance finite
field implementations required for elliptic curve cryptography or RSA.

Minimal addition chain search is famously hard. No practical optimal
algorithm is known, especially for cryptographic exponents of size 256-bits
and up. Given its importance for the performance of cryptographic
implementations, implementers devote significant effort to hand-tune addition
chains. The goal of the `addchain` project is to match or exceed the best
hand-optimized addition chains using entirely automated approaches, building
on extensive academic research and applying new tweaks that exploit the
unique nature of cryptographic exponents.

## Results

The following table shows the results of the `addchain` library on popular
cryptographic exponents. For each one we also show the length of the [best
known hand-optimized addition chain](https://briansmith.org/ecc-inversion-addition-chains-01), and the
delta from the library result.

| Name | This Library | Best Known | Delta |
| ---- | -----------: | ---------: | ----: |
| [Curve25519 Field Inversion](doc/results.md#curve25519-field-inversion) | 266 | 265 | +1 |
| [NIST P-256 Field Inversion](doc/results.md#nist-p-256-field-inversion) | 266 | 266 | **+0** |
| [NIST P-384 Field Inversion](doc/results.md#nist-p-384-field-inversion) | 397 | 396 | +1 |
| [secp256k1 (Bitcoin) Field Inversion](doc/results.md#secp256k1-bitcoin-field-inversion) | 269 | 269 | **+0** |
| [Curve25519 Scalar Inversion](doc/results.md#curve25519-scalar-inversion) | 283 | 284 | **-1** |
| [NIST P-256 Scalar Inversion](doc/results.md#nist-p-256-scalar-inversion) | 294 | 292 | +2 |
| [NIST P-384 Scalar Inversion](doc/results.md#nist-p-384-scalar-inversion) | 434 | 433 | +1 |
| [secp256k1 (Bitcoin) Scalar Inversion](doc/results.md#secp256k1-bitcoin-scalar-inversion) | 293 | 290 | +3 |


See [full results listing](doc/results.md) for more detail and
results for less common exponents.

These results demonstrate that `addchain` is competitive with hand-optimized
chains, often with equivalent or better performance. Even when `addchain` is
slightly sub-optimal, it can still be considered valuable since it fully
automates a laborious manual process. As such, `addchain` can be trusted to
produce high quality results in an automated code generation tool.

## Usage

### Command-line Interface

Install a pre-compiled [release
binary](https://github.com/mmcloughlin/addchain/releases):

```
curl -sSfL https://git.io/addchain | sh -s -- -b /usr/local/bin
```

Alternatively build from source:

```
go install github.com/mmcloughlin/addchain/cmd/addchain@latest
```

Search for a curve25519 field inversion addition chain with:

```sh
addchain search '2^255 - 19 - 2'
```

Output:

```
addchain: expr: ""2^255 - 19 - 2""
addchain: hex: 7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffeb
addchain: dec: 57896044618658097711785492504343953926634992332820282019728792003956564819947
addchain: best: opt(runs(continued_fractions(dichotomic)))
addchain: cost: 266
_10       = 2*1
_11       = 1 + _10
_1100     = _11 << 2
_1111     = _11 + _1100
_11110000 = _1111 << 4
_11111111 = _1111 + _11110000
x10       = _11111111 << 2 + _11
x20       = x10 << 10 + x10
x30       = x20 << 10 + x10
x60       = x30 << 30 + x30
x120      = x60 << 60 + x60
x240      = x120 << 120 + x120
x250      = x240 << 10 + x10
return      (x250 << 2 + 1) << 3 + _11
```

Next, you can [generate code from this addition chain](doc/gen.md).

### Library

Install:

```
go get -u github.com/mmcloughlin/addchain
```

Algorithms all conform to the [`alg.ChainAlgorithm`](https://pkg.go.dev/github.com/mmcloughlin/addchain/alg#ChainAlgorithm) or
[`alg.SequenceAlgorithm`](https://pkg.go.dev/github.com/mmcloughlin/addchain/alg#SequenceAlgorithm) interfaces and can be used directly. However the
most user-friendly method uses the [`alg/ensemble`](https://pkg.go.dev/github.com/mmcloughlin/addchain/alg/ensemble) package to
instantiate a sensible default set of algorithms and the [`alg/exec`](https://pkg.go.dev/github.com/mmcloughlin/addchain/alg/exec)
helper to execute them in parallel. The following code uses this method to
find an addition chain for curve25519 field inversion:

```go
func Example() {
	// Target number: 2²⁵⁵ - 21.
	n := new(big.Int)
	n.SetString(""7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffeb"", 16)

	// Default ensemble of algorithms.
	algorithms := ensemble.Ensemble()

	// Use parallel executor.
	ex := exec.NewParallel()
	results := ex.Execute(n, algorithms)

	// Output best result.
	best := 0
	for i, r := range results {
		if r.Err != nil {
			log.Fatal(r.Err)
		}
		if len(results[i].Program) < len(results[best].Program) {
			best = i
		}
	}
	r := results[best]
	fmt.Printf(""best: %d\n"", len(r.Program))
	fmt.Printf(""algorithm: %s\n"", r.Algorithm)

	// Output:
	// best: 266
	// algorithm: opt(runs(continued_fractions(dichotomic)))
}
```

## Algorithms

This section summarizes the algorithms implemented by `addchain` along with
references to primary literature. See the [bibliography](doc/bibliography.md)
for the complete references list.

### Binary

The [`alg/binary`](https://pkg.go.dev/github.com/mmcloughlin/addchain/alg/binary) package implements the addition chain equivalent
of the basic [square-and-multiply exponentiation
method](https://en.wikipedia.org/wiki/Exponentiation_by_squaring). It is
included for completeness, but is almost always outperformed by more advanced
algorithms below.

### Continued Fractions

The [`alg/contfrac`](https://pkg.go.dev/github.com/mmcloughlin/addchain/alg/contfrac) package implements the continued fractions
methods for addition sequence search introduced by
Bergeron-Berstel-Brlek-Duboc in 1989 and later extended. This approach
utilizes a decomposition of an addition chain akin to continued fractions,
namely

```
(1,..., k,..., n) = (1,...,n mod k,..., k) ⊗ (1,..., n/k) ⊕ (n mod k).
```

for certain special operators ⊗ and ⊕. This
decomposition lends itself to a recursive algorithm for efficient addition
sequence search, with results dependent on the _strategy_ for choosing the
auxillary integer _k_. The [`alg/contfrac`](https://pkg.go.dev/github.com/mmcloughlin/addchain/alg/contfrac) package provides a
laundry list of strategies from the literature: binary, co-binary,
dichotomic, dyadic, fermat, square-root and total.

#### References

* F Bergeron, J Berstel, S Brlek and C Duboc. Addition chains using continued fractions. Journal of Algorithms. 1989. http://www-igm.univ-mlv.fr/~berstel/Articles/1989AdditionChainDuboc.pdf
* Bergeron, F., Berstel, J. and Brlek, S. Efficient computation of addition chains. Journal de theorie des nombres de Bordeaux. 1994. http://www.numdam.org/item/JTNB_1994__6_1_21_0
* Amadou Tall and Ali Yassin Sanghare. Efficient computation of addition-subtraction chains using generalized continued Fractions. Cryptology ePrint Archive, Report 2013/466. 2013. https://eprint.iacr.org/2013/466
* Christophe Doche. Exponentiation. Handbook of Elliptic and Hyperelliptic Curve Cryptography, chapter 9. 2006. http://koclab.cs.ucsb.edu/teaching/ecc/eccPapers/Doche-ch09.pdf

### Bos-Coster Heuristics

Bos and Coster described an iterative algorithm for efficient addition
sequence generation in which at each step a heuristic proposes new numbers
for the sequence in such a way that the _maximum_ number always decreases.
The [original Bos-Coster paper](https://link.springer.com/content/pdf/10.1007/0-387-34805-0_37.pdf) defined four
heuristics: Approximation, Divison, Halving and Lucas. Package
[`alg/heuristic`](https://pkg.go.dev/github.com/mmcloughlin/addchain/alg/heuristic) implements a variation on these heuristics:

* **Approximation:** looks for two elements a, b in the current sequence with sum close to the largest element.
* **Halving:** applies when the target is at least twice as big as the next largest, and if so it will propose adding a sequence of doublings.
* **Delta Largest:** proposes adding the delta between the largest two entries in the current sequence.

Divison and Lucas are not implemented due to disparities in the literature
about their precise definition and poor results from early experiments.
Furthermore, this library does not apply weights to the heuristics as
suggested in the paper, rather it simply uses the first that applies. However
both of these remain [possible avenues for
improvement](https://github.com/mmcloughlin/addchain/issues/26).

#### References

* Bos, Jurjen and Coster, Matthijs. Addition Chain Heuristics. In Advances in Cryptology --- CRYPTO' 89 Proceedings, pages 400--407. 1990. https://link.springer.com/content/pdf/10.1007/0-387-34805-0_37.pdf
* Riad S. Wahby. kwantam/addchain. Github Repository. Apache License, Version 2.0. 2018. https://github.com/kwantam/addchain
* Christophe Doche. Exponentiation. Handbook of Elliptic and Hyperelliptic Curve Cryptography, chapter 9. 2006. http://koclab.cs.ucsb.edu/teaching/ecc/eccPapers/Doche-ch09.pdf
* Ayan Nandy. Modifications of Bos and Coster’s Heuristics in search of a shorter addition chain for faster exponentiation. Masters thesis, Indian Statistical Institute Kolkata. 2011. http://library.isical.ac.in:8080/jspui/bitstream/10263/6441/1/DISS-285.pdf
* F. L. Ţiplea, S. Iftene, C. Hriţcu, I. Goriac, R. Gordân and E. Erbiceanu. MpNT: A Multi-Precision Number Theory Package, Number Theoretical Algorithms (I). Technical Report TR03-02, Faculty of Computer Science, ""Alexandru Ioan Cuza"" University, Iasi. 2003. https://profs.info.uaic.ro/~tr/tr03-02.pdf
* Stam, Martijn. Speeding up subgroup cryptosystems. PhD thesis, Technische Universiteit Eindhoven. 2003. https://cr.yp.to/bib/2003/stam-thesis.pdf

### Dictionary

Dictionary methods decompose the binary representation of a target integer _n_ into a set of dictionary _terms_, such that _n_
may be written as a sum

<pre>
n = ∑ 2<sup>e<sub>i</sub></sup> d<sub>i</sub>
</pre>

for exponents _e_ and elements _d_ from a dictionary _D_. Given such a decomposition we can construct an addition chain for _n_ by

1. Find a short addition _sequence_ containing every element of the dictionary _D_. Continued fractions and Bos-Coster heuristics can be used here.
2. Build _n_ from the dictionary terms according to the sum decomposition.

The efficiency of this approach boils down to the decomposition method. The [`alg/dict`](https://pkg.go.dev/github.com/mmcloughlin/addchain/alg/dict) package provides:

* **Fixed Window:** binary representation of _n_ is broken into fixed _k_-bit windows
* **Sliding Window**: break _n_ into _k_-bit windows, skipping zeros where possible
* **Run Length**: decompose _n_ into runs of 1s up to a maximal length
* **Hybrid**: mix of sliding window and run length methods

#### References

* Martin Otto. Brauer addition-subtraction chains. PhD thesis, Universitat Paderborn. 2001. http://www.martin-otto.de/publications/docs/2001_MartinOtto_Diplom_BrauerAddition-SubtractionChains.pdf
* Kunihiro, Noboru and Yamamoto, Hirosuke. New Methods for Generating Short Addition Chains. IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences. 2000. https://pdfs.semanticscholar.org/b398/d10faca35af9ce5a6026458b251fd0a5640c.pdf
* Christophe Doche. Exponentiation. Handbook of Elliptic and Hyperelliptic Curve Cryptography, chapter 9. 2006. http://koclab.cs.ucsb.edu/teaching/ecc/eccPapers/Doche-ch09.pdf

### Runs

The runs algorithm is a custom variant of the dictionary approach that
decomposes a target into runs of ones. It leverages the observation that
building a dictionary consisting of runs of 1s of lengths
<code>l<sub>1</sub>, l<sub>2</sub>, ..., l<sub>k</sub></code> can itself be
reduced to:

1. Find an addition sequence containing the run lengths
   <code>l<sub>i</sub></code>. As with dictionary approaches we can use
   Bos-Coster heuristics and continued fractions here. However here we have the
   advantage that the <code>l<sub>i</sub></code> are typically very _small_,
   meaning that a wider range of algorithms can be brought to bear.
2. Use the addition sequence for the run lengths <code>l<sub>i</sub></code>
   to build an addition sequence for the runs themselves
   <code>r(l<sub>i</sub>)</code> where <code>r(e) = 2<sup>e</sup>-1</code>. See
   [`dict.RunsChain`](https://pkg.go.dev/github.com/mmcloughlin/addchain/alg/dict#RunsChain).

This approach has proved highly effective against cryptographic exponents
which frequently exhibit binary structure, such as those derived from
[Solinas primes](https://en.wikipedia.org/wiki/Solinas_prime).

> I have not seen this method discussed in the literature. Please help me find references to prior art if you know any.

### Optimization

Close inspection of addition chains produced by other algorithms revealed
cases of redundant computation. This motivated a final optimization pass over
addition chains to remove unecessary steps. The [`alg/opt`](https://pkg.go.dev/github.com/mmcloughlin/addchain/alg/opt) package
implements the following optimization:

1. Determine _all possible_ ways each element can be computed from those prior.
2. Count how many times each element is used where it is the _only possible_ way of computing that entry.
3. Prune elements that are always used in computations that have an alternative.

These micro-optimizations were vital in closing the gap between `addchain`'s
automated approaches and hand-optimized chains. This technique is reminiscent
of basic passes in optimizing compilers, raising the question of whether
other [compiler optimizations could apply to addition
chains](https://github.com/mmcloughlin/addchain/issues/24)?

> I have not seen this method discussed in the literature. Please help me find references to prior art if you know any.

## Citing

If you use `addchain` in your research a citation would be appreciated.
Citing a specific release is preferred, since they are [archived on
Zenodo](https://doi.org/10.5281/zenodo.4625263) and assigned a DOI. Please use the
following BibTeX to cite the most recent [0.4.0
release](https://github.com/mmcloughlin/addchain/releases/tag/v0.4.0).

```bib
@misc{addchain,
    title        = {addchain: Cryptographic Addition Chain Generation in Go},
    author       = {Michael B. McLoughlin},
    year         = 2021,
    month        = oct,
    howpublished = {Repository \url{https://github.com/mmcloughlin/addchain}},
    version      = {0.4.0},
    license      = {BSD 3-Clause License},
    doi          = {10.5281/zenodo.5622943},
    url          = {https://doi.org/10.5281/zenodo.5622943},
}
```

If you need to cite a currently unreleased version please consider [filing an
issue](https://github.com/mmcloughlin/addchain/issues/new) to request a new
release, or to discuss an appropriate format for the citation.

## Thanks

Thank you to [Tom Dean](https://web.stanford.edu/~trdean/), [Riad
Wahby](https://wahby.org/), [Brian Smith](https://briansmith.org/) and
[str4d](https://github.com/str4d) for advice and encouragement. Thanks also to
[Damian Gryski](https://github.com/dgryski) and [Martin
Glancy](https://twitter.com/mglancy) for review.

## Contributing

Contributions to `addchain` are welcome:

* [Submit bug reports](https://github.com/mmcloughlin/addchain/issues/new) to
  the issues page.
* Suggest [test cases](https://github.com/mmcloughlin/addchain/blob/e6c070065205efcaa02627ab1b23e8ce6aeea1db/internal/results/results.go#L62)
  or update best-known hand-optimized results.
* Pull requests accepted. Please discuss in the [issues section](https://github.com/mmcloughlin/addchain/issues)
  before starting significant work.

## License

`addchain` is available under the [BSD 3-Clause License](LICENSE).
","['mmcloughlin', 'bytemare']",0,,0.63,8287,,,,,,7,,,
13096521,MDEwOlJlcG9zaXRvcnkxMzA5NjUyMQ==,teachers_pet,github-education-resources/teachers_pet,0,github-education-resources,https://github.com/github-education-resources/teachers_pet,Command line tool to help teachers use GitHub in their classrooms,0,2013-09-25 14:45:56+00:00,2024-06-14 16:39:22+00:00,2016-10-08 07:32:09+00:00,https://education.github.com/guide,398,187,187,Ruby,1,1,1,0,0,0,74,1,0,35,mit,1,0,0,public,74,35,187,master,1,1,"__Notice:__ We have [released](https://github.com/blog/2055-teachers-manage-your-courses-with-classroom-for-github) a web based tool for teachers. `teachers_pet` will remain availble, but we recommend teachers use [Classroom for GitHub](https://classroom.github.com) instead.

# teachers_pet [![Build Status](https://travis-ci.org/education/teachers_pet.svg?branch=master)](https://travis-ci.org/education/teachers_pet) [![Gem Version](https://badge.fury.io/rb/teachers_pet.svg)](http://badge.fury.io/rb/teachers_pet)

**WARNING: This documentation may contain unreleased changes. See [rubydoc.info/gems/teachers_pet](http://rubydoc.info/gems/teachers_pet) for the version of this README corresponding to the latest release.**

Command line tool to help teachers use GitHub in their classrooms.

## Philosophy

Each class is an 'organization' on GitHub. This allows the instructors (GitHub organization Owners) to create, push, pull, and administer all repositories. This achieves two goals:

* Instructors can push code starter code to all students
* Instructors can easily browse/pull student code at any time during the assignment to assist in questions, check on progress

Each student is given a team in the organization. The team name is the same as the student's GitHub username.

## Installation

[Install Ruby 1.9.3+](https://www.ruby-lang.org/en/installation/), then run

```bash
gem install teachers_pet
```

If you've used this tool before, get the newest version using

```ruby
gem update teachers_pet
```

To use the latest-and-greatest code from this repository, see the instructions in [CONTRIBUTING.md](CONTRIBUTING.md).

## Typical workflow

...when using the [sandboxing](https://education.github.com/guide/sandboxing) method with [private repositories](https://education.github.com/guide/private_repos):

### Basic setup

1. Create an organization (you will be an owner by default). The organization should reflect the name of your course. See [the classroom guide](https://education.github.com/guide#2-create-an-organization-for-your-class) for more info.
1. Have each student/instructor create GitHub accounts.
1. Create a `students` file (you can use an alternate filename and specify with the `--students` option if you like)
    * Individual assignments: one username per line
    * Group assignments: one team per line in the format `teamName username username username`
1. Add the GitHub username of all instructors to an `Owners.csv` file (one per line)
1. Run the following:

    ```bash
    teachers_pet create_student_teams ...
    teachers_pet add_to_team --members Owners.csv ...
    ```

### Assignments

```bash
teachers_pet create_repos ...
teachers_pet push_files ...
# Multiple times:
teachers_pet open_issue ...

# Then, after the assignment is due,
teachers_pet clone_repos ...
```

## Authentication

The scripts will ask for your GitHub password in order to run. If you have [two factor authentication](https://help.github.com/articles/about-two-factor-authentication) (2FA) enabled, [create a personal access token](https://help.github.com/articles/creating-an-access-token-for-command-line-use) (replace `github.com` with your host for GitHub Enterprise):

https://github.com/settings/tokens/new?description=teachers_pet&scopes=repo%2Cpublic_repo%2Cwrite%3Aorg%2Crepo%3Astatus%2Cread%3Aorg%2Cuser%2Cadmin%3Aorg

Once created, specify the token using the `--token` option, or if you add the `TEACHERS_PET_GITHUB_TOKEN` environment variable to your `.bash_profile` (or equivalent – example below), it will be picked up by `teachers_pet`.

```bash
# replace YOUR_TOKEN_HERE below
echo ""\n\nexport TEACHERS_PET_GITHUB_TOKEN=YOUR_TOKEN_HERE"" >> ~/.bash_profile
source ~/.bash_profile
```

## Actions

**To learn the options for each action, run**

```bash
teachers_pet help
# or
teachers_pet help COMMAND
```

### Giving others access

You may need to give other people access to various repositories using teams – the `add_to_team` command can help do this in bulk.

### Creating assignments

When using the [sandboxing](https://education.github.com/guide/sandboxing) setup, you will need to create the repositories for the students.  For each assignment, use the `create_repos` action to create a repository for each student.  The repositories are technically created per team, but if you use `create_student_teams` first, then there will be one team per student.

### Forks

If you need to grab the list of users who have forked a particular repository – e.g. to use with another command – you can run the `forks` command, and the results will be written to a file.

### Collaborator access

Give [collaborator access](https://help.github.com/articles/what-are-the-different-access-permissions#collaborator) to everyone who has forked your repository using `add_collaborators`.  Mostly useful for GitHub demonstrations, where usernames can quickly be collected via `forks`, and then the students can be quickly given access to a repository.

### Pushing starter files

When creating repositories for students, you will often want to include boilerplate files.  After running `create_repos`, create a canonical copy of the starter files (e.g. [`.gitignore`](https://github.com/github/gitignore#readme), `Makefile`s, etc.) in a repository.  From the local clone of the repository, use the `push_files` action to place that code in the repositories for each student.  This works by creating a Git remote for each student repository, and doing a `git push` to each one.

### Opening issues

After running `create_repos`, instructors can open issues in student repos as a way to list requirements of the assignment, goals, or instructions for patching, using the `open_issue` command.

### Clone repositories for grading

When grading, use the `clone_repos` command to clone all the repositories in the organization that match the username-repository naming scheme that is generated when `create_repos` is run.

### Merge all open pull requests

When running a GitHub workshop, it's nice to be able to merge a bunch of pull requests on a particular repository all at once. `merge_pull_requests` will handle this for you.

## Related projects

* https://education.github.com/guide
* https://github.com/hogbait/6170_repo_management
* https://github.com/UCSB-CS-Using-GitHub-In-Courses/github-acad-scripts
","['afeld', 'tarebyte', 'johndbritton', 'kelleydv', 'aronwc', 'mikehelmick', 'hartmamt', 'mkuehn10', 'svenevs']",0,,0.76,0,,"## Development

```bash
git clone https://github.com/education/teachers_pet.git
cd teachers_pet
bundle install
# then run actions using
bundle exec ./bin/COMMAND
```

## Running tests

```bash
bundle
# then
bundle exec rspec
# or
bundle exec guard
```

To see test coverage information:

```bash
bundle exec rspec
open coverage/index.html
```
",,,,25,,,
220714260,MDEwOlJlcG9zaXRvcnkyMjA3MTQyNjA=,Fakeddit,entitize/Fakeddit,0,entitize,https://github.com/entitize/Fakeddit,r/Fakeddit New Multimodal Benchmark Dataset for Fine-grained Fake News Detection,0,2019-11-09 22:57:06+00:00,2025-03-02 00:19:18+00:00,2022-10-04 11:24:03+00:00,https://fakeddit.netlify.app/,28,179,179,Python,1,1,1,1,0,0,33,0,0,10,,1,0,0,public,33,10,179,master,1,,"# Fakeddit

Kai Nakamura, Sharon Levy, and William Yang Wang. 2020. r/Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection

Website: https://fakeddit.netlify.app/

Codalab Competition: https://competitions.codalab.org/competitions/25337

Paper: https://arxiv.org/abs/1911.03854

Our lab: http://nlp.cs.ucsb.edu/index.html



## Getting Started

Follow the instructions to download the dataset. You can download text, metadata, comment data, and image data.

Note that released test set is public. Private test set is used for leaderboard (coming soon).

Please read the `Usage` section. It is important.  

Please let us know if you encounter any problems by opening an issue or by directly contacting us.

### Installation

#### Download text and metadata
Please read the USAGE section before using or downloading. 
Download the v2.0 dataset from [here](https://drive.google.com/drive/folders/1jU7qgDqU1je9Y0PMKJ_f31yXRo5uWGFm?usp=sharing) 

#### Download image data 

**Option 1: (RECOMMENDED)**
Download the images [here](https://drive.google.com/file/d/1cjY6HsHaSZuLVHywIxD5xQqng33J5S2b/view?usp=sharing).

**Option 2:**
The `*.tsv` dataset files have an `image_url` column which contain the image urls. You can use the URLs to download the images.

For convenience, we have provided a script which will download the images for you. Please follow the instructions if you would like to use the attached script.

Fork or clone this repository and install required python libraries

```
$ git clone https://github.com/entitize/Fakeddit
$ cd Fakeddit
$ pip install -r requirements.txt
```
Copy `image_downloader.py` to the same directory/folder as where you downloaded the tsv files. 

Run `image_downloader.py`  in the new directory/folder

```
$ python image_downloader.py file_name
```

#### Download comment data
Download the comment data from [here](https://drive.google.com/drive/folders/150sL4SNi5zFK8nmllv5prWbn0LyvLzvo?usp=sharing)

### Usage

Please note that results in the paper are based on multimodal samples only (samples that have both text and image). In our paper, only samples that have both image and text were used for the baseline experiments and error analysis. Thus, if you would like to compare against the results in the paper, use the samples in the `multimodal_only_samples` folder. 

If there are `Unnamed`... columns, you can ignore or get rid of them. Use the `clean_title` column to get filtered text data. 

`comments.tsv` consists of comments made by Reddit users on submissions in the entire released dataset. Use the `submission_id` column to identify which submission the comment is associated with. Note that one submission can have zero, one, or multiple comments.
",['entitize'],1,,0.78,0,,,,,,6,,,
806564993,R_kgDOMBM0gQ,SymbCoT,Aiden0526/SymbCoT,0,Aiden0526,https://github.com/Aiden0526/SymbCoT,"Codes and Data for ACL 2024 Paper ""Faithful Logical Reasoning via Symbolic Chain-of-Thought"".",0,2024-05-27 12:46:49+00:00,2025-02-27 05:42:33+00:00,2024-06-29 09:36:20+00:00,,1538,176,176,Python,1,1,1,1,0,0,18,0,0,5,mit,1,0,0,public,18,5,176,main,1,,"# SymbCoT

Codes and Data for ACL 2024 Paper [""Faithful Logical Reasoning via Symbolic Chain-of-Thought""](https://arxiv.org/abs/2405.18357#:~:text=While%20the%20recent%20Chain%2Dof,expressions%20and%20rigid%20deducing%20rules.)

Authors: [**Jundong Xu**](https://aiden0526.github.io/JundongXu/)<sup>1</sup>, [**Hao Fei**](http://haofei.vip/)<sup>1</sup><sup>*</sup> (Corresponding author), [**Liangming Pan**](http://www.liangmingpan.com/)<sup>2</sup>, [**Qian Liu**](https://profiles.auckland.ac.nz/liu-qian)<sup>3</sup>, [**Mong-Li Lee**](https://www.comp.nus.edu.sg/cs/people/leeml/)<sup>1</sup>, [**Wynne Hsu**](https://www.comp.nus.edu.sg/cs/people/whsu/)<sup>1</sup>

<sup>1</sup> National University of Singapore, Singapore, <sup>2</sup> University of California, Santa Barbara, USA, <sup>3</sup> University of Auckland, New Zealand

**Introduction**
-----
While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules.
To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely **SymbCoT**, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. 
Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain.
Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently.

![My Image](framework.png)

**Dataset**
-----
We test our framework on 5 different datasets using 2 symbolic formats.

First-order Logic: [ProntoQA](https://github.com/asaparov/prontoqa), [ProofWriter](https://allenai.org/data/proofwriter), [FOLIO](https://github.com/Yale-LILY/FOLIO)

Constraint Optimization: [LogicalDeduction](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/logical_deduction), [AR-LSAT](https://github.com/zhongwanjun/AR-LSAT)

**Setup**
-----
Please install all the required packages first by running the following command:
```
pip install -r requirements.txt
```

**Logical Inference**
-----
To use the logical inference, please run the following command:
```
python symbcot.py \
    --api_key ""Your API Key"" \
    --model_name ""Model Name [gpt-3.5-turbo | gpt-4]"" \
    --data_path ""The Path of Your Data"" \
    --dataset_name ""Dataset [ProntoQA | ProofWriter | FOLIO | LogicalDeduction | AR-LSAT]"" \
    --split dev
```
The results will be saved in the ```./results```.

**Verification**
-----
To verify the logical inference, please run the following command:
```
python verifier.py \
    --api_key ""Your API Key"" \
    --model_name ""Model Name [gpt-3.5-turbo | gpt-4]"" \
    --data_path ""The Path of Your Data"" \
    --dataset_name ""Dataset [ProntoQA | ProofWriter | FOLIO | LogicalDeduction | AR-LSAT]"" \
    --split dev
```
The verified results will be saved in the ```./verified_results``` with a suffix ```verified```.

**Evaluation**
-----
To evaluate the results, please run the following command:
```
python evaluate.py \
   --dataset_name ""Dataset [ProntoQA | ProofWriter | FOLIO | LogicalDeduction | AR-LSAT]"" \
   --model_name ""Model Name [gpt-3.5-turbo | gpt-4]"" \
   --split dev \
   --verification ""Verified or Not [True | False]""
```

**Citation**
-----
Please cite the paper if you use this framework during your research.
```
@inproceedings{
    author={Jundong Xu and Hao Fei and Liangming Pan and Qian Liu and Mong-Li Lee and Wynne Hsu},
    title={Faithful Logical Reasoning via Symbolic Chain-of-Thought},
    booktitle={The 62nd Annual Meeting of the Association for Computational Linguistics},
    year={2024},
    url={https://arxiv.org/abs/2405.18357}
}
```

","['Aiden0526', 'jdangerdolphin']",1,,0.72,0,,,,,,2,,,
93411649,MDEwOlJlcG9zaXRvcnk5MzQxMTY0OQ==,women-in-programming-languages-research,jeanqasaur/women-in-programming-languages-research,0,jeanqasaur,https://github.com/jeanqasaur/women-in-programming-languages-research,Women in Programming Languages and Software Engineering Research,0,2017-06-05 14:19:09+00:00,2024-12-21 18:18:27+00:00,2024-11-29 17:26:15+00:00,,194,169,169,,1,1,1,1,0,0,69,0,0,1,,1,0,0,public,69,1,169,master,1,,"# Women in Programming Languages and Software Engineering Research
For your program committees, seminar series, panels, etc.

A couple of comments about women and balance:
* As a result of many well-meaning people trying to balance out committees, women get asked to be on committees *a lot*. It would be good to make it easy for women to say no and, even better, come up with some more globally optimal solution to the fact that while it's important to get women on committees and in front of people so there can be more women eventually, currently there simply aren't enough women to go around.
* Balance should not come at the cost of lower standards. Dropping standards for women (or any other group) introduces **explicit** bias because you're accepting that women can't be held to the same standards. (We talk a lot about implicit bias, but a lot of the bias that exists is often explicit and comes from things like this.) The right thing to do here is to use lists like this one to increase the pool and to be thoughtful about explicit and fair standards. Making it clear that this is what's happening is helpful so that people aren't assuming that the women and minorities got somewhere because of different standards. (Unfortunately, this happens a lot.)

And for people who are wondering why gender-balanced representation is a big deal, Adrienne Porter Felt has a [good post](https://techlady.haus/blog/2017/6/11/peer-review-gender-imbalance-in-program-committees) about this.

Please add yourself, or someone who is comfortable being on this list, in alphabetical order. There are no guarantees about the comprehensiveness of this list.

## Faculty and Research Scientists (at Universities or Research Institutions)
* [Sara Achour](https://people.csail.mit.edu/sachour/), Stanford, USA
* [Amal Ahmed](http://www.ccs.neu.edu/home/amal/), Northeastern University, USA
* [Nada Amin](https://namin.seas.harvard.edu), Harvard University, USA
* [Zena M. Ariola](http://ix.cs.uoregon.edu/~ariola/), University of Oregon, USA
* [Anya Helene Bagge](http://www.ii.uib.no/~anya/), University of Bergen, Norway
* [Stephanie Balzer](http://www.cs.cmu.edu/~balzers/), Carnegie Mellon University, USA
* [Annette Bieniusa](https://softech.cs.uni-kl.de/homepage/de/staff/AnnetteBieniusa/), TU Kaiserslautern, Germany
* [Kelly Blincoe](http://kblincoe.github.io), University of Auckland, New Zealand
* [Laura Bocchi](https://www.cs.kent.ac.uk/people/staff/lb514/), University of Kent, UK
* [Viviana Bono](http://www.di.unito.it/~bono/index.html), Università di Torino, Italy
* [Kuljit Kaur Chahal](https://twitter.com/kuljitchahal2), Guru Nanak Dev University, Amritsar, India
* [Marsha Chechik](http://www.cs.toronto.edu/~chechik), University of Toronto, Canada
* [Maria Christakis](https://mariachris.github.io/), MPI-SWS, Germany
* [Youyou Cong](https://prg.is.titech.ac.jp/people/cong/), Tokyo Institute of Technology, Japan
* [Ornela Dardha](http://www.dcs.gla.ac.uk/~ornela/), University of Glasgow, UK
* [Eva Darulova](https://malyzajko.github.io/), Uppsala University, Sweden
* [Işil Dillig](http://www.cs.utexas.edu/~isil/), University of Texas Austin, USA
* [Rayna Dimitrova](https://www2.le.ac.uk/departments/informatics/people/rayna-dimitrova), University of Leicester, UK
* [Jenna Wise DiVincenzo](https://www.cs.cmu.edu/~jlwise/), Purdue University, USA
* [Sophia Drossopoulou](https://wp.doc.ic.ac.uk/sd/), Imperial College London, UK
* [Jana Dunfield](https://dunfieldlab.ca/), Queen's University, Canada
* [Sue Eisenbach](http://www.imperial.ac.uk/people/s.eisenbach), Imperial College London, UK
* [Azadeh Farzan](https://www.cs.toronto.edu/~azadeh/), University of Toronto, Canada
* [Kathleen Fisher](https://www.cs.tufts.edu/~kfisher/Kathleen_Fisher/Home.html), Tufts University, USA
* [Maria Andreina Francisco Rodriguez](http://www.it.uu.se/katalog/marfr379), Uppsala University, Sweden
* [Philippa Gardner](https://www.doc.ic.ac.uk/~pg/), Imperial College London, UK
* [Lilia Georgieva](https://www.linkedin.com/in/liliageorgievageorgieva/), Heriot Watt University, Edinburgh, UK
* [Alessandra Gorla](http://software.imdea.org/~alessandra.gorla/), IMDEA Software Institute, Madrid, Spain
* [Mary Hall](http://www.cs.utah.edu/~mhall/), University of Utah, USA
* [Regina Hebig](https://www.chalmers.se/en/staff/Pages/hebig.aspx), Chalmers Gothenburg University, Sweden
* [Görel Hedin](http://cs.lth.se/gorel-hedin/), Lund University, Sweden
* [Felienne Hermans](https://www.felienne.com/bio-and-pic), Leiden University, The Netherlands
* [Rashina Hoda](https://unidirectory.auckland.ac.nz/profile/r-hoda), University of Auckland, New Zealand
* [Limin Jia](http://www.andrew.cmu.edu/user/liminjia/), Carnegie Mellon University, USA
* [Alexandra Jimborean](http://www.it.uu.se/katalog/aleji304), Uppsala University, Sweden
* [Patricia Johann](https://cs.appstate.edu/johannp/), Appalachian State University, USA
* [Maria Jump](http://staff.kings.edu/mariajump/), Kings College, UK
* [Sara Kalvala](https://warwick.ac.uk/fac/sci/dcs/people/sara_kalvala/), University of Warwick, UK
* [Gabriele Keller](https://www.cse.unsw.edu.au/~keller/), University of New South Wales, UK
* [Marie Kerjean](https://www.lipn.univ-paris13.fr/~kerjean/), CNRS, Université Sorbonne Paris Nord, France
* [Ekaterina Komendantskaya](http://www.macs.hw.ac.uk/~ek19/), Heriot-Watt University, UK
* [Laura Kovács](http://www.cse.chalmers.se/~laurako/), TU Wien/Chalmers, Austria/Sweden
* [Chandra Krintz](http://www.cs.ucsb.edu/~ckrintz/), University of California, Santa Barbara, USA
* [Lindsey Kuper](http://composition.al),  University of California, Santa Cruz, USA
* [Marta Kwiatkowska](http://www.cs.ox.ac.uk/marta.kwiatkowska/), Oxford University, UK
* [Claire Le Goues](http://www.clairelegoues.com), Carnegie Mellon University, USA
* [Caroline Lemieux](http://www.carolemieux.com/), University of British Columbia, Vancouver, Canada
* [Crista Lopes](http://www.ics.uci.edu/~lopes/), University of California, Irvine, USA
* [Mae Milano](https://cs.princeton.edu/~mpmilano), Princeton University, USA
* [Ana Milanova](http://www.cs.rpi.edu/~milanova/), Rensselaer Polytechnic Institute, USA
* [Heather Miller](https://twitter.com/heathercmiller), Northeastern University/EPFL, USA/Switzerland
* [Mira Mezini](http://www.stg.tu-darmstadt.de/staff/mira_mezini/), TU Darmstadt, Germany
* [Brigitte Pientka](http://www.cs.mcgill.ca/~bpientka/), McGill University, Canada
* [Ruzica Piskac](http://www.cs.yale.edu/homes/piskac/), Yale University, USA
* [Nadia Polikarpova](http://people.csail.mit.edu/polikarn/), University of California, San Diego, USA
* [Azalea Raad](http://www.soundandcomplete.org/), Imperial College London, UK
* [Talia Ringer](http://tlringer.github.io/), University of Illinois Urbana-Champaign, USA
* [Christine Rizkallah](http://www.cse.unsw.edu.au/~crizkallah/), University of New South Wales, Australia
* [Paige Rodeghero](paigerodeghero.com), Clemson University, USA
* [Kristin Yvonne Rozier](laboratory.temporallogic.org), Iowa State University, USA
* [Julia Rubin](https://www.ece.ubc.ca/faculty/julia-rubin), University of British Columbia, Canada
* [Roopsha Samanta](https://www.cs.purdue.edu/homes/roopsha/), Purdue University, USA
* [Ina Schaefer](https://www.tu-braunschweig.de/isf/team/schaefer), Technische Universität Braunschweig, Germany
* [Sibylle Schupp](https://www.tuhh.de/sts/institute/prof-dr-sibylle-schupp.html), Technische Universität Hamburg-Harburg, Germany
* [Sharon Shoham Buchbinder](http://www.tau.ac.il/~sharonshoham/), Tel Aviv University, Israel
* [Alexandra Silva](http://www.alexandrasilva.org/#/main.html), Cornell University, USA
* [Ana Sokolova](http://cs.uni-salzburg.at/~anas/), TU Eindhoven, Netherlands
* [Perdita Stevens](http://homepages.inf.ed.ac.uk/perdita/), University of Edinburgh, Scotland
* [Michelle Mills Strout](http://cgi.cs.arizona.edu/~mstrout/), University of Arizona, USA
* [Shin Hwei Tan](http://www.shinhwei.com/), Southern University of Science and Technology, China
* [Laura Titolo](https://lauratitolo.github.io/), NIA/NASA Langley, USA
* [Emma Tosch](https://uvm.edu/~etosch), University of Vermont, USA
* [Caterina Urban](https://caterinaurban.github.io), INRIA & École Normale Supérieure, France
* [Niki Vazou](https://nikivazou.github.io/), IMDEA Software Institute Madrid, Spain
* [Stephanie Weirich](https://www.cis.upenn.edu/~sweirich/), University of Pennsylvania, USA
* [Aiko Yamashita](https://about.me/aiko.yamashita), CWI, The Netherlands / Akershus University of Applied Sciences, Norway
* [Nobuko Yoshida](http://mrg.doc.ic.ac.uk/people/nobuko-yoshida/), Imperial College London, UK

## Industry

 * [Cristina Cifuentes](https://labs.oracle.com/pls/apex/f?p=labs:bio:0:21), Oracle Labs, Australia
 * [Valeria de Paiva](http://vcvpaiva.github.io/), Topos Institute, USA
 * [Jennifer Paykin](http://www.cis.upenn.edu/~jpaykin/), Galois, USA
 * [Ciera Jaspan](https://research.google.com/pubs/CieraJaspan.html), Google, USA
 * [Rezwana Karim](http://paul.rutgers.edu/~rkarim/), Samsung Research America
 * [Heidy Khlaaf](http://heidyk.com/), Adelard, UK
 * [Daira Hopwood](https://github.com/daira), Jacaranda Software / Zerocoin Electric Coin Company
 * [Kathryn McKinley](https://www.cs.utexas.edu/users/mckinley/), Google, USA
 * [Darya Melicher](https://www.cs.cmu.edu/~dkurilov/), Google, USA
 * [Tatiana Shpeisman](https://www.linkedin.com/in/tatiana-shpeisman-52b1011b/), Google, USA
 * [Ezgi Cicek](https://wp.mpi-sws.org/ecicek/), Facebook, UK
 * [Marianna Rapoport](http://mrapoport.com/), Amazon, Canada
 * [Jean Yang](http://jeanyang.com), Akita Software, USA

## Open Source

 * [Anne Ogborn](http://theelginworks.com), SWI-Prolog (works at The Elgin Works and Simularity)
 * [Maxime Chevalier-Boisvert](https://pointersgonewild.com), ZetaVM

## Postdocs
* [Eleni Constantinou](http://www.econst.eu), University of Mons, Belgium
* [Eva Graversen](https://portal.findresearcher.sdu.dk/en/persons/efgraversen), University of Southern Denmark, Denmark
* [Jennifer Hackett](http://www.cs.nott.ac.uk/~pszjlh/), University of Nottingham, UK
* [Anastasia Isychev](https://aisychev.github.io/), TU Wien, Austria
* [Maria Kechagia](https://mkechagia.github.io/), University College London, UK
* [Rumyana Neykova](http://mrg.doc.ic.ac.uk/people/rumyana-neykova/), Imperial College London, UK
* [Zoe Paraskevopoulou](https://zoep.github.io/), Northeastern University, USA
* [Larisa Safina](https://lsafina.github.io/), INRIA, France
* [Malavika Samak](https://sites.google.com/site/malavikasamak/home), MIT, USA
* [Kristina Sojakova](http://www.cs.cmu.edu/~ksojakov/), Cornell University, USA
* [Hira Taqdees Syeda](https://ts.data61.csiro.au/people/?cn=Hira+Taqdees+Syeda), Data61 CSIRO and UNSW Sydney, Australia
* [Debasmita Lohar](https://dlohar.github.io/), Karlsruhe Institute of Technology, Germany

## PhD Students
* [Leif Andersen](https://leifandersen.net), Northeastern University, USA
* [Julia Belyakova](https://julbinb.github.io/), Northeastern University, USA
* [Annie Cherkaev](https://anniecherkaev.com/), University of Utah, USA
* [Maryam Dabaghchian](https://sites.google.com/site/maryamdabaghchian/), University of Utah, USA
* [Amelia Dobis](https://www.cs.princeton.edu/~ad4048/), Princeton University, USA
* [Greta Dolcetti](https://gretadolcetti.github.io/), Ca' Foscari University of Venice, Italy
* [Juliana Franco](https://www.doc.ic.ac.uk/~jvicent1/), Imperial College London, UK
* [April Gonçalves](http://cyberglot.me/), University of Strathclyde, UK
* [Anna Gommerstadt](http://anyag.net/), Carnegie Mellon University, USA
* [Sylvia Grewe](http://www.stg.tu-darmstadt.de/staff/sylvia_grewe/sylvia_grewe.en.jsp), Technische Universität Darmstadt, Germany
* [Wen Kokke](https://wenkokke.github.io/), University of Edinburgh, Scotland
* [Cristina Matache](http://users.ox.ac.uk/~scro3229/), University of Oxford, UK
* [Rachel Muir](https://www.linkedin.com/in/rachel-muir00/?originalSubdomain=uk), University of Kent, UK
* [Jeevana Priya Inala](https://jinala.github.io/), MIT, USA
* [Juliana Alves Pereira](http://wwwiti.cs.uni-magdeburg.de/~jualves/), University of Magdeburg, Germany
* [Neea Rusch](https://nkrusch.github.io), Augusta University, USA
* [Rian Shambaugh](http://cs.umass.edu/~rian), University of Massachusetts Amherst, USA
* [Jiasi Shen](http://people.csail.mit.edu/jiasi/), MIT, USA
* [Tori Vollmer](https://research.kent.ac.uk/programming-languages-systems/person/tori-vollmer/), University of Kent, UK
* [Katherine Ye](https://cs.cmu.edu/~kqy/), Carnegie Mellon University, USA
* [Uma Zalakain](https://umazalakain.info/), University of Glasgow, UK

## Other
* [Audrey Tang](https://github.com/audreyt), Taiwan's Digital Minister, Taiwan
","['jeanqasaur', 'wenkokke', 'rxg', 'dlohar', 'dorchard', 'namin', 'izycheva', 'vcvpaiva', 'mkechagia', 'lkuper', 'LeifAndersen', 'JonathanAldrich', 'heathermiller', 'fmontesi', 'etosch', 'caterinaurban', 'Maryam81609', 'maximecb', 'nadia-polikarpova', 'nkrusch', 'paigerodeghero', 'pierreganty', 'rachitnigam', 'rrnewton', 'mechtaev', 'cygnenoir', 'Tuxified', 'grammarware', 'vaibhavsagar', 'YouyouCong', 'drdreyer', 'econst', 'hkhlaaf', 'jlphackett', 'jualvespereira', 'kkc-al', 'kyrozier', 'lggeorgieva', 'dobios', 'hgommers', 'bieniusa', 'anniecherk', 'antonatem', 'ulysses4ever', 'carolemieux', 'clegoues', 'aubertc', 'daira', 'malyzajko', 'ezgicicek', 'Felienne', 'gretadolcetti', 'turingfan', 'jdunfield', 'jasonhemann', 'julbinb', 'athleens', 'kblincoe', 'amaurremi', 'mkerjean']",0,,0.3,0,,,,,,9,,,
578335933,R_kgDOIni0vQ,DiffusionDisentanglement,UCSB-NLP-Chang/DiffusionDisentanglement,0,UCSB-NLP-Chang,https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement,"Official implementation of the paper ""Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models",0,2022-12-14 20:16:53+00:00,2025-01-15 11:04:56+00:00,2023-10-08 17:54:52+00:00,,29896,165,165,Jupyter Notebook,1,1,1,1,0,0,9,0,0,3,other,1,0,0,public,9,3,165,main,1,1,"# Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models
### [Project Page](https://wuqiuche.github.io/DiffusionDisentanglement-project-page/)

[Qiucheng Wu](https://wuqiuche.github.io/)<sup>1</sup>,
[Yujian Liu](https://yujianll.github.io)<sup>1</sup>,
[Handong Zhao](https://hdzhao.github.io)<sup>2</sup>,
[Ajinkya Kale](https://dblp.org/pid/04/6453.html)<sup>2</sup>,
[Trung Bui](https://sites.google.com/site/trungbuistanford/)<sup>2</sup>,
[Tong Yu](https://dblp.org/pid/32/1593-1.html)<sup>2</sup>,
[Zhe Lin](https://dblp.uni-trier.de/pid/42/1680-1.html)<sup>2</sup>,
[Yang Zhang](https://mitibmwatsonailab.mit.edu/people/yang-zhang/)<sup>3</sup>,
[Shiyu Chang](https://code-terminator.github.io/)<sup>1</sup>
<br>
<sup>1</sup>UC, Santa Barbara, <sup>2</sup>Adobe Research, <sup>3</sup>MIT-IBM Watson AI Lab

This is the official implementation of the paper ""Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models"".

## Overview
Generative models have been widely studied in computer vision. Recently, diffusion models have drawn substantial attention due to the high quality of their generated images. A key desired property of image generative models is the ability to disentangle different attributes, which should enable modification towards a style without changing the semantic content, and the modification parameters should generalize to different images. Previous studies have found that generative adversarial networks (GANs) are inherently endowed with such disentanglement capability, so they can perform disentangled image editing without re-training or fine-tuning the network. In this work, we explore whether diffusion models are also inherently equipped with such a capability. Our finding is that for stable diffusion models, by partially changing the input text embedding from a neutral description (e.g., ""a photo of person"") to one with style (e.g., ""a photo of person with smile"") while fixing all the Gaussian random noises introduced during the denoising process, the generated images can be modified towards the target style without changing the semantic content. Based on this finding, we further propose a simple, light-weight image editing algorithm where the mixing weights of the two text embeddings are optimized for style matching and content preservation. This entire process only involves optimizing over around 50 parameters and does not fine-tune the diffusion model itself. Experiments show that the proposed method can modify a wide range of attributes, with the performance outperforming diffusion-model-based image-editing algorithms that require fine-tuning. The optimized weights generalize well to different images.

![](./assets/teaser.png)

## The workflow
Here, we demonstrate an example of disentangling target attribute ""children drawing"". In this example, $\boldsymbol{c}^{(0)}$ is the embedding of “A castle”, and $\boldsymbol{c}^{(1)}$ is the embedding of “A children drawing of castle”. The first step  (*first two rows*) is the optimization process that finds the best soft combination of $\boldsymbol{c}^{(0)}$ and $\boldsymbol{c}^{(1)}$, such that the modified image (*the second row*) changes the attribute without affecting other contents. After this, the learned text embedding can be directly applied to a new image, which leads to the same editing effect (*last row*).

![](./assets/pipeline.png)

## Requirements
Our code is based on <a href=""https://github.com/CompVis/stable-diffusion"">stable-diffusion</a>. This project requires one GPU with 48GB memory. Please first clone the repository and build the environment:
```bash
git clone https://github.com/wuqiuche/DiffusionDisentanglement
cd DiffusionDisentanglement
conda env create -f environment.yaml
conda activate ldm
```

You will also need to download the pretrained stable-diffusion model:
```bash
mkdir models/ldm/stable-diffusion-v1
wget -O models/ldm/stable-diffusion-v1/model.ckpt https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt
```

## Disentangle Attributes
```bash
python scripts/disentangle.py --c1 <neutral_prompt> --c2 <target_prompt> --seed 42 --outdir <output_dir>
```
We provide a bash file with a disentangling example:
```bash
chmod +x scripts/disentangle.sh
./scripts/disentangle.sh
```
You should obtain the following results (right one) in ```outputs/disentangle/image/```:
<img src=""./assets/result1.png"" width=""400"">

## Edit Images
```bash
python scripts/edit.py --c1 <neutral_prompt> --c2 <target_prompt> --seed 42 --input <input_image> --outdir <output_dir>
```
We provide a bash file with an image editing example:
```bash
chmod +x scripts/edit.sh
./scripts/edit.sh
```
You should obtain the following results (right one) in ```outputs/edit/image/```:
<img src=""./assets/result2.png"" width=""400"">

## Replication
To replicate our results in paper, we provide a bash file with commands used. You can run them all at once, or choose the target attributes you are interested in.
```bash
chmod +x scripts/result.sh
./scripts/result.sh
```

## Results
Our method is able to disentangle a series of global and local attributes. We demonstrate examples below. The high-resolution images can be found in ```examples``` directory.

![](./assets/example1.png)

![](./assets/example2.png)

## Parent Repository
This code is adopted from <a href="""">https://github.com/CompVis/stable-diffusion</a> and <a href="""">https://github.com/orpatashnik/StyleCLIP</a>.

",['wuqiuche'],1,,0.78,0,,,,,,4,,,
134324345,MDEwOlJlcG9zaXRvcnkxMzQzMjQzNDU=,dataspice,ropensci/dataspice,0,ropensci,https://github.com/ropensci/dataspice,:hot_pepper: Create lightweight schema.org descriptions of your datasets,0,2018-05-21 20:55:32+00:00,2025-02-06 10:41:56+00:00,2022-01-16 01:44:15+00:00,https://docs.ropensci.org/dataspice,3249,162,162,R,1,1,1,1,0,0,26,0,0,34,other,1,0,0,public,26,34,162,main,1,1,"# dataspice

![CRAN Version](https://www.r-pkg.org/badges/version/dataspice)
![CI](https://github.com/ropensci/dataspice/workflows/R-CMD-check/badge.svg)
[![Codecov test
coverage](https://codecov.io/gh/ropensci/dataspice/branch/main/graph/badge.svg)](https://codecov.io/gh/ropensci/dataspice?branch=main)
[![](https://badges.ropensci.org/426_status.svg)](https://github.com/ropensci/software-review/issues/426)

The goal of `dataspice` is to make it easier for researchers to create
basic, lightweight, and concise metadata files for their datasets by
editing the kind of files they’re probably most familiar with: CSVs. To
spice up their data with a dash of metadata. These metadata files can
then be used to:

-   Make useful information available during analysis.
-   Create a helpful dataset README webpage for your data similar to how
    [pkgdown](https://pkgdown.r-lib.org/) creates websites for R
    packages.
-   Produce more complex metadata formats for richer description of your
    datasets and to aid dataset discovery.

Metadata fields are based on
[Schema.org/Dataset](https://schema.org/Dataset) and other [metadata
standards](#resources) and represent a lowest common denominator which
means converting between formats should be relatively straightforward.

## Example

An basic example repository for demonstrating what using `dataspice`
might look like can be found at
[https://github.com/amoeba/dataspice-example](https://github.com/amoeba/dataspice-example/).
From there, you can also check out a preview of the HTML `dataspice`
generates at
[https://amoeba.github.io/dataspice-example](https://amoeba.github.io/dataspice-example/)
and how Google sees it at
<https://search.google.com/test/rich-results?url=https%3A%2F%2Famoeba.github.io%2Fdataspice-example%2F>.

A much more detailed example has been created by [Anna
Krystalli](https://annakrystalli.me) at
<https://annakrystalli.me/dataspice-tutorial/> ([GitHub
repo](https://github.com/annakrystalli/dataspice-tutorial)).

## Installation

You can install the latest version from
[CRAN](https://cran.r-project.org):

``` r
install.packages(""dataspice"")
```

## Workflow

``` r
create_spice()
# Then fill in template CSV files, more on this below
write_spice()
build_site() # Optional
```

![diagram showing a workflow for using
dataspice](man/figures/dataspice_workflow.png)

### Create spice

`create_spice()` creates template metadata spreadsheets in a folder (by
default created in the `data` folder in the current working directory).

The template files are:

-   **biblio.csv** - for title, abstract, spatial and temporal coverage,
    etc.
-   **creators.csv** - for data authors
-   **attributes.csv** - explains each of the variables in the dataset
-   **access.csv** - for files, file types, and download URLs (if
    appropriate)

### Fill in templates

The user needs to fill in the details of the four template files. These
csv files can be directly modified, or they can be edited using either
the associated helper function and/or
[Shiny](https://shiny.rstudio.com/) app.

#### Helper functions

-   `prep_attributes()` populates the **`fileName`** and
    **`variableName`** columns of the `attributes.csv` file using the
    header row of the data files.

-   `prep_access()` populates the **`fileName`**, **`name`** and
    **`encodingFormat`** columns of the `access.csv` file from the files
    in the folder containing the data.

To see an example of how `prep_attributes()` works, load the data files
that ship with the package:

``` r
data_files <- list.files(system.file(""example-dataset/"", package = ""dataspice""),
  pattern = "".csv"",
  full.names = TRUE
)
```

This function assumes that the metadata templates are in a folder called
`metadata` within a `data` folder.

``` r
attributes_path <- file.path(""data"", ""metadata"", ""attributes.csv"")
```

Using `purrr::map()`, this function can be applied over multiple files
to populate the header names

``` r
data_files %>%
  purrr::map(~ prep_attributes(.x, attributes_path),
    attributes_path = attributes_path
  )
```

The output of `prep_attributes()` has the first two columns filled out:

<table>
<thead>
<tr>
<th style=""text-align:left;"">
fileName
</th>
<th style=""text-align:left;"">
variableName
</th>
<th style=""text-align:left;"">
description
</th>
<th style=""text-align:left;"">
unitText
</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align:left;"">
BroodTables.csv
</td>
<td style=""text-align:left;"">
Stock.ID
</td>
<td style=""text-align:left;"">
NA
</td>
<td style=""text-align:left;"">
NA
</td>
</tr>
<tr>
<td style=""text-align:left;"">
BroodTables.csv
</td>
<td style=""text-align:left;"">
Species
</td>
<td style=""text-align:left;"">
NA
</td>
<td style=""text-align:left;"">
NA
</td>
</tr>
<tr>
<td style=""text-align:left;"">
BroodTables.csv
</td>
<td style=""text-align:left;"">
Stock
</td>
<td style=""text-align:left;"">
NA
</td>
<td style=""text-align:left;"">
NA
</td>
</tr>
<tr>
<td style=""text-align:left;"">
BroodTables.csv
</td>
<td style=""text-align:left;"">
Ocean.Region
</td>
<td style=""text-align:left;"">
NA
</td>
<td style=""text-align:left;"">
NA
</td>
</tr>
<tr>
<td style=""text-align:left;"">
BroodTables.csv
</td>
<td style=""text-align:left;"">
Region
</td>
<td style=""text-align:left;"">
NA
</td>
<td style=""text-align:left;"">
NA
</td>
</tr>
<tr>
<td style=""text-align:left;"">
BroodTables.csv
</td>
<td style=""text-align:left;"">
Sub.Region
</td>
<td style=""text-align:left;"">
NA
</td>
<td style=""text-align:left;"">
NA
</td>
</tr>
</tbody>
</table>

#### Shiny helper apps

Each of the metadata templates can be edited interactively using a
[Shiny](https://shiny.rstudio.com/) app:

-   `edit_attributes()` opens a Shiny app that can be used to edit
    `attributes.csv`. The Shiny app displays the current `attributes`
    table and lets the user fill in an informative description and units
    (e.g. meters, hectares, etc.) for each variable.
-   `edit_access()` opens an editable version of `access.csv`
-   `edit_creators()` opens an editable version of `creators.csv`
-   `edit_biblio()` opens an editable version of `biblio.csv`

![edit\_attributes Shiny app](man/figures/edit_attributes.png)

Remember to click on **Save** when finished editing.

#### Completed metadata files

The first few rows of the completed metadata tables in this example will
look like this:

`access.csv` has one row for each file

| fileName        | name            | contentUrl | encodingFormat |
|:----------------|:----------------|:-----------|:---------------|
| StockInfo.csv   | StockInfo.csv   | NA         | CSV            |
| BroodTables.csv | BroodTables.csv | NA         | CSV            |
| SourceInfo.csv  | SourceInfo.csv  | NA         | CSV            |

`attributes.csv` has one row for each variable in each file

| fileName        | variableName | description                                      | unitText |
|:----------------|:-------------|:-------------------------------------------------|:---------|
| BroodTables.csv | Stock.ID     | Unique stock identifier                          | NA       |
| BroodTables.csv | Species      | species of stock                                 | NA       |
| BroodTables.csv | Stock        | Stock name, generally river where stock is found | NA       |
| BroodTables.csv | Ocean.Region | Ocean region                                     | NA       |
| BroodTables.csv | Region       | Region of stock                                  | NA       |
| BroodTables.csv | Sub.Region   | Sub.Region of stock                              | NA       |

`biblio.csv` is one row containing descriptors including spatial and
temporal coverage

| title                                                                 | description                                                                                                                                                                                            | datePublished       | citation | keywords                   | license | funder | geographicDescription | northBoundCoord | eastBoundCoord | southBoundCoord | westBoundCoord | wktString | startDate           | endDate             |
|:----------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------|:---------|:---------------------------|:--------|:-------|:----------------------|----------------:|---------------:|----------------:|---------------:|:----------|:--------------------|:--------------------|
| Compiled annual statewide Alaskan salmon escapement counts, 1921-2017 | The number of mature salmon migrating from the marine environment to freshwater streams is defined as escapement. Escapement data are the enumeration of these migrating fish as they pass upstream, … | 2018-02-12 08:00:00 | NA       | salmon, alaska, escapement | NA      | NA     | NA                    |              78 |           -131 |              47 |           -171 | NA        | 1921-01-01 08:00:00 | 2017-01-01 08:00:00 |

`creators.csv` has one row for each of the dataset authors

| id  | name           | affiliation                                           | email                      |
|:----|:---------------|:------------------------------------------------------|:---------------------------|
| NA  | Jeanette Clark | National Center for Ecological Analysis and Synthesis | <jclark@nceas.ucsb.edu>    |
| NA  | Rich,Brenner   | Alaska Department of Fish and Game                    | richard.brenner.alaska.gov |

### Save JSON-LD file

`write_spice()` generates a json-ld file (“linked data”) to aid in
[dataset
discovery](https://developers.google.com/search/docs/data-types/dataset),
creation of more extensive metadata
(e.g. [EML](https://eml.ecoinformatics.org)), and creating a website.

Here’s a view of the `dataspice.json` file of the example data:

![listviewer pack output showing an example dataspice JSON
file](man/figures/listviewer.png)

### Build website

-   `build_site()` creates a bare-bones `index.html` file in the
    repository `docs` folder with a simple view of the dataset with the
    metadata and an interactive map. For example, this
    [repository](https://github.com/amoeba/dataspice-example/) results
    in this [website](https://amoeba.github.io/dataspice-example/)

![dataspice-website](man/figures/website_example.png)

### Convert to EML

The metadata fields `dataspice` uses are based largely on their
compatibility with terms from [Schema.org](https://schema.org). However,
`dataspice` metadata can be converted to Ecological Metadata Language
(EML), a much richer schema. The conversion isn’t perfect but
`dataspice` will do its best to convert your `dataspice` metadata to
EML:

``` r
library(dataspice)

# Load an example dataspice JSON that comes installed with the package
spice <- system.file(
  ""examples"", ""annual-escapement.json"",
  package = ""dataspice""
)

# Convert it to EML
eml_doc <- spice_to_eml(spice)
#> Warning: variableMeasured not crosswalked to EML because we don't have enough
#> information. Use `crosswalk_variables` to create the start of an EML attributes
#> table. See ?crosswalk_variables for help.
#> You might want to run EML::eml_validate on the result at this point and fix what validations errors are produced. You will commonly need to set `packageId`, `system`, and provide `attributeList` elements for each `dataTable`.
```

You may receive warnings depending on which `dataspice` fields you
filled in and this process will very likely produce an invalid EML
record which is totally fine:

``` r
library(EML)
#> 
#> Attaching package: 'EML'
#> The following object is masked from 'package:magrittr':
#> 
#>     set_attributes

eml_validate(eml_doc)
#> [1] FALSE
#> attr(,""errors"")
#> [1] ""Element '{https://eml.ecoinformatics.org/eml-2.2.0}eml': The attribute 'packageId' is required but missing.""                                  
#> [2] ""Element '{https://eml.ecoinformatics.org/eml-2.2.0}eml': The attribute 'system' is required but missing.""                                     
#> [3] ""Element 'dataTable': Missing child element(s). Expected is one of ( physical, coverage, methods, additionalInfo, annotation, attributeList ).""
#> [4] ""Element 'dataTable': Missing child element(s). Expected is one of ( physical, coverage, methods, additionalInfo, annotation, attributeList ).""
#> [5] ""Element 'dataTable': Missing child element(s). Expected is one of ( physical, coverage, methods, additionalInfo, annotation, attributeList ).""
```

This is because some fields in `dataspice` store information in
different structures and because EML requires many fields that
`dataspice` doesn’t have fields for. At this point, you should look over
the validation errors produced by `EML::eml_validate` and fix those.
Note that this will likely require familiarity with the [EML
Schema](https://eml.ecoinformatics.org/) and the [EML
package](https://github.com/ropensci/eml).

Once you’re done, you can write out an EML XML file:

``` r
out_path <- tempfile()
write_eml(eml_doc, out_path)
#> NULL
```

### Convert from EML

Like converting `dataspice` to EML, we can convert an existing EML
record to a set of `dataspice` metadata tables which we can then work
from within `dataspice`:

``` r
library(EML)

eml_path <- system.file(""example-dataset/broodTable_metadata.xml"", package = ""dataspice"")
eml <- read_eml(eml_path)
```

``` r
# Creates four CSVs files in the `data/metadata` directory
my_spice <- eml_to_spice(eml, ""data/metadata"")
```

## Resources

A few existing tools & data standards to help users in specific domains:

-   [Darwin Core](http://rs.tdwg.org/dwc/)
-   [Ecological Metadata
    Language](https://knb.ecoinformatics.org/#external//emlparser/docs/index.html)
    (EML) (& [`EML`](https://github.com/ropensci/EML))
-   [ISO 19115](https://www.iso.org/standard/53798.html) - Geographic
    Information Metadata
-   [ISO 19139](https://www.iso.org/standard/32557.html) - Geographic
    Info Metadata XML schema
-   [Minimum Information for Biological and Biomedical
    Investigations](https://fairsharing.org/collection/MIBBI) (MIBBI)

…And others indexed in [Fairsharing.org](https://fairsharing.org) & the
[RDA metadata
directory](http://rd-alliance.github.io/metadata-directory/standards/).

## Code of Conduct

Please note that this package is released with a [Contributor Code of
Conduct](https://ropensci.org/code-of-conduct/). By contributing to this
project, you agree to abide by its terms.

## Contributors

This package was developed at rOpenSci’s 2018 unconf by (in alphabetical
order):

-   [Carl Boettiger](https://github.com/cboettig)
-   [Scott Chamberlain](https://github.com/sckott)
-   [Auriel Fournier](https://github.com/aurielfournier)
-   [Kelly Hondula](https://github.com/khondula)
-   [Anna Krystalli](https://github.com/annakrystalli)
-   [Bryce Mecum](https://github.com/amoeba)
-   [Maëlle Salmon](https://github.com/maelle)
-   [Kate Webbink](https://github.com/magpiedin)
-   [Kara Woo](https://github.com/karawoo)
-   [Irene Steves](https://github.com/isteves)
","['amoeba', 'annakrystalli', 'aurielfournier', 'cboettig', 'karawoo', 'magpiedin', 'robitalec', 'isteves', 'ccamara', 'Pakillo', 'MattForshaw', 'maelle', 'njtierney', 'tdjames1', 'kylehamilton']",1,,0.68,0,,,,,,14,,,
38326093,MDEwOlJlcG9zaXRvcnkzODMyNjA5Mw==,leakless,ucsb-seclab/leakless,0,ucsb-seclab,https://github.com/ucsb-seclab/leakless,Function redirection via ELF tricks.,0,2015-06-30 18:30:38+00:00,2025-02-20 03:34:44+00:00,2015-06-30 18:31:54+00:00,,192,157,157,Python,1,1,1,1,0,0,26,0,0,0,,1,0,0,public,26,0,157,master,1,1,"How to test
===========

1. Build vuln.c

        gcc -fno-stack-protector vuln.c -o /tmp/vuln -m32 -O2

2. Find the offset of the saved IP

        ruby19 ""$METASPLOIT/tools/pattern_create.rb"" 256 | /tmp/vuln
        dmesg | tail
        ruby19 ""$METASPLOIT/tools/pattern_offset.rb"" $SEGFAULT_IP

3. Launch the attack with the desired parameter

        (python ./exploit.py /tmp/vuln --offset $OFFSET; echo ls) | /tmp/vuln

You can also just dump to a JSON file all the necessary information to
perform the exploit:

    python ./exploit.py /tmp/vuln --json

For debugging information, use the `--debug` parameter. For further
information on the parameters use the `--help` parameter.

The CMake build system will compile `vuln.c` for x86 and x86-64 with
different protections enabled.  There's also a CTest testsuite which
has been tested using the `ld.gold` linker and GCC 4.8.4. Different
toolchains might require minor adjustments.

To launch it just run:

    mkdir leakless-build
    cd leakless-build
    cmake ../leakless
    make
    make test

The build system has also the `length`, `json` and `ropl` targets
which, respectively, produce the length of the generated exploit for
each supported configuration and the JSON and ropl version of the
exploit.

    make length
    make json
    make ropl

Basic idea
==========

    char *buffer = .bss;
    char *new_stack = buffer + 1024;
    int *rubbish = new_stack + 4;

    strcpy(buffer, ""execve"");
    *((int *) buffer) = 'exec';
    *(((int *) buffer) + 1) = 've\0\0';
    char *name = buffer;
    buffer += strlen(buffer) + 1;

    Elf32_Sym *symbol = (Elf32_Sym *) buffer;
    symbol->st_name = name - .dynstr;
    symbol->st_value = 0;
    symbol->st_info = 0;
    symbol->st_other = 0;
    symbol->st_shndx = 0;
    buffer += sizeof(*symbol);

    Elf32_Rel *reloc = (Elf32_Rel *) buffer;
    reloc->r_offset = rubbish++;
    reloc->r_info = (R_386_JUMP_SLOT | (symbol - .dynsym) / sizeof(symbol));
    buffer += sizeof(reloc):

    pre_plt((reloc - .rel.plt) / sizeof(Elf32_Rel));

Helper classes
==============

* `MemoryArea`: data structure representing a part of memory, with its
  start address, its size, a reference to what its relative to
  (e.g. the `MemoryArea` where we'll write the relocation structure
  will be relative to the `.rela.dyn` section). `MemoryArea` also
  takes care of computing the appropriate index (`MemoryArea.index`)
  relative to the specified part of memory.
* `Buffer`: data structure holding information about a buffer where we
  want to write to things. Typically this will represent to
  `.bss`. Buffer also keeps track of what part of it has already been
  allocated (`Buffer.current` points to the next free location) and
  allows to allocate new `MemoryArea`s with the appropriate
  alignement.

`Exploit`-derived classes
=========================

* `Exploit`: the base class, contains all the architecture- and
  platform-independent parts of the exploit. It keeps the list of the
  gadgets, it takes care of collecting all the interesting information
  about the program from the ELF file and abstracting some utility and
  memory-related functions (e.g. `write_pointer` and `write_string`)
  which rely on the abstract `do_writemem` function (which is
  platform- and program-dependent). Finally, in `jump_to`, contains
  the core logic for setting up the necessary data structures in the
  buffers.
* `CommonGadgetsExploit`: inherits from `Exploit` and introduces
  architecture-dependent parts, in particular gadgets and
  function-invocation logic.
* `ExecveExploit`: very simple class implementing the logic to launch
  an `execve`, so write a `NULL` pointer, a `""/bin/sh\0""` and
  explicitly look for `execve`. Finally invoke it.
* `RawDumperExploit`: exploit useful to just collect the information
  necessary to perform the attack without actually generating the ROP
  chain. `RawDumperExploit.jump_to` will return as first result an
  array of tuples `(address, what_to_write_there)`, which, for
  instance, are used to implement the `--json` parameter.
",[],1,,0.74,0,,,,,,17,,,
276739286,MDEwOlJlcG9zaXRvcnkyNzY3MzkyODY=,OTT-QA,wenhuchen/OTT-QA,0,wenhuchen,https://github.com/wenhuchen/OTT-QA,"Code and Data for ICLR2021 Paper ""Open Question Answering over Tables and Text""",0,2020-07-02 20:28:04+00:00,2025-03-06 07:04:19+00:00,2024-01-02 04:37:33+00:00,,492873,154,154,Python,1,1,1,1,0,0,21,0,0,4,mit,1,0,0,public,21,4,154,master,1,,"# Open Table-and-Text Question Answering (OTT-QA)

This respository contains the OTT-QA dataset used in [Open Question Answering over Tables and Text
](https://arxiv.org/abs/2010.10439) published in [ICLR2021](https://openreview.net/group?id=ICLR.cc/2021/Conference) and the baseline code for the dataset [OTT-QA](https://ott-qa.github.io/). This dataset contains open questions which require retrieving tables and text from the web to answer. This dataset is re-annotated from the previous HybridQA dataset. The dataset is collected by UCSB NLP group and issued under MIT license. You can browse the examples through our [explorer](https://ott-qa.github.io/explore.html).

![overview](./figures/demo.png)

What's new compared to [HybridQA](http://hybridqa.github.io/):
- The questions are de-contextualized to be standalone without relying on the given context to understand.
- We add new dev/test set questions the newly crawled tables, which removes the potential bias in table retrieval.
- The groundtruth table and passage are not given to the model, it needs to retrieve from 400K+ candidates of tables and 5M candidates of passages to find the evidence.
- The tables in OTT-QA do not have groundtruth hyperlinks, which simulates a more general scenario outside Wikipedia.

## Results
Table Retrieval: We use page title + page section title + table schema as the representation of a table for retrieval
|     Split     |     HITS@1    |     HITS@5     |     HITS@10       |   HITS@20         | 
|---------------|---------------|----------------|-------------------|-------------------|
|Dev            | 41.0%         | 61.8%          | 68.5%              | 73.7%             |

QA Results: We use the retrieved table + retrieved text as the evidence to run HYBRIDER model (See https://arxiv.org/pdf/2004.07347.pdf for details), the results are shown as:
|     Model     |     Dev-EM        |     Dev-F1 |
|---------------|---------------|----------------|
| BERT-based-uncased |  8.7     |     10.9       |
| [BERT-large-uncased](https://drive.google.com/file/d/1a3I2HaOIP_9wES53E5kjbb2ST5IbgrVQ/view?usp=sharing) | 10.9      | 13.1         |

## Repo Structure
- released_data: this folder contains the question/answer pairs for training, dev and test data.
- data/all_plain_tables.json: this file contains the 400K+ table candidates for the dev/test set.
- data/all_passages.json: this file contains the 5M+ open-domain passage candidates for the dev/test set.
- data/traindev_tables_tok: this folder contains the train/dev tables.
- data/traindev_request_tok: this folder cotains the linked passages for train/dev in-domain tables
- table_crawling/: the folder contains the table extraction steps from Wikipedia.
- retriever/: the folder contains the script to build sparse retriever index.

## Requirements
- [Transformers 2.2.1](https://github.com/huggingface/transformers)
- [Pytorch 1.4](https://pytorch.org/)
- [scipy](https://www.scipy.org/)

We suggest using virtual environment to install these dependencies.
```
conda install pytorch torchvision cudatoolkit=10.2 -c pytorch
pip install transformers
pip install pexpect
```

## Additional Information
If you want to know more about the crawling procedure, please refer to [crawling](https://github.com/wenhuchen/OpenHybridQA/tree/master/table_crawling) for details.

If you want to know more about the retrieval procedure, please refer to [retriever](https://github.com/wenhuchen/OpenDomainHybridQA/tree/master/retriever) for details.

Or you can skip these two steps to directly download the needed files from AWS in Step1.

## Step1: Preliminary Step
## Step1-1: Download the necessary files 
```
cd data/
wget https://opendomainhybridqa.s3-us-west-2.amazonaws.com/all_plain_tables.json
wget https://opendomainhybridqa.s3-us-west-2.amazonaws.com/all_passages.json
cd ../
```
This command will download the crawled tables and linked passages from Wikiepdia in a cleaned format.
## Step1-2: Build inedx for retriever
```
cd retriever/
python build_tfidf.py --build_option text_title --out_dir text_title_bm25 --option bm25
python build_tfidf.py --build_option title_sectitle_schema --out_dir title_sectitle_schema
```
This script will generate index files under retriever/ folder, which are used in the following experiments

## Step1-3: Reproducing the retrieval results
```
python evaluate_retriever.py --split dev --model retriever/title_sectitle_schema/index-tfidf-ngram\=2-hash\=16777216-tokenizer\=simple.npz  --format question_table
```
This script will produce the table retrieval results in terms of HITS@1,5,10,20,50.

## Step2: Training
### Step2-0: If you want to download the model from [Google Drive](https://drive.google.com/file/d/1a3I2HaOIP_9wES53E5kjbb2ST5IbgrVQ/view?usp=sharing), you can skip the following training procedure.
```
unzip models.zip
```
### Step2-1: Preprocess the training data
```
python retrieve_and_preprocess.py --split train
```
This command will generate training data for different submodules in the following steps.

### Step2-2: Train the three modules in the reader.
```
python train_stage12.py --do_lower_case --do_train --train_file preprocessed_data/stage1_training_data.json --learning_rate 2e-6 --option stage1 --num_train_epochs 3.0 --model_name_or_path bert-large-uncased
python train_stage12.py --do_lower_case --do_train --train_file preprocessed_data/stage2_training_data.json --learning_rate 5e-6 --option stage2 --num_train_epochs 3.0 --model_name_or_path bert-large-uncased
python train_stage3.py --do_train  --do_lower_case   --train_file preprocessed_data/stage3_training_data.json  --per_gpu_train_batch_size 12   --learning_rate 3e-5   --num_train_epochs 4.0   --max_seq_length 384   --doc_stride 128  --threads 8 --model_name_or_path bert-large-uncased
```
The three commands separately train the step1, step2 and step3 neural modules, all of them are based on BERT-uncased-base model from HugginFace implementation.

## Step3: Evaluation
### Step3-1: Reconstruct Hyperlinked Table using built text title index
```
python evaluate_retriever.py --format table_construction --model retriever/text_title_bm25/index-bm25-ngram\=2-hash\=16777216-tokenizer\=simple.npz
python retrieve_and_preprocess.py --split dev_retrieve --model retriever/title_sectitle_schema/index-tfidf-ngram\=2-hash\=16777216-tokenizer\=simple.npz
python retrieve_and_preprocess.py --split test_retrieve --model retriever/title_sectitle_schema/index-tfidf-ngram\=2-hash\=16777216-tokenizer\=simple.npz
```
This step can potentially take a long time since it matches each cell in the 400K tables against the whole passage title pool.

### Step3-2: Evaluate with the trained model
```
python train_stage12.py --stage1_model stage1/[YOUR-MODEL-FOLDER] --stage2_model stage2/[YOUR-MODEL-FOLDER] --do_lower_case --predict_file preprocessed_data/dev_inputs.json --do_eval --option stage12 --model_name_or_path bert-large-uncased --table_path data/all_constructed_tables.json --request_path data/all_passages.json
python train_stage3.py --model_name_or_path stage3/[YOUR-MODEL-FOLDER] --do_stage3   --do_lower_case  --predict_file predictions.intermediate.json --per_gpu_train_batch_size 12  --max_seq_length 384   --doc_stride 128 --threads 8 --request_path data/all_passages.json
```
Once you have generated the predictions.json file, you can use the following command to see the results.
```
python evaluate_script.py predictions.json released_data/dev_reference.json
```
To replicate my results, please see the generated predictions.dev.json by my model.
```
python evaluate_script.py predictions.dev.json released_data/dev_reference.json
```

## CodaLab Evaluation
To obtain the score on the test set (released_data/test.blind.json), you need to participate the CodaLab challenge in [OTT-QA Competition](https://codalab.lisn.upsaclay.fr/competitions/7967). Please submit your results to obtain your testing score. The submitted file should first be named ""test_answers.json"" and then zipped. The required format of the submission file is described as follows:
```
[
  {
    ""question_id"": xxxxx,
    ""pred"": XXX
  },
  {
    ""question_id"": xxxxx,
    ""pred"": XXX
  }
]
```
The reported scores are EM and F1.

## Link Prediction in Table
We also provide the script to predict the links from the given table based on the context using GPT-2 model. To train the model, please use the following command.
```
python link_prediction.py --dataset data/traindev_tables.json --do_train --batch_size 512
```
To generate links, please run
```
python link_prediction.py --do_all --load_from link_generator/model-ep9.pt --dataset data/all_plain_tables.json --batch_size 256
```
This command will generate all the link mapping in the link_generator/ folder.

## Visualization
If you want to browse the tables, please go to [this website](https://wenhuchen.github.io/opendomaintables.github.io/) and type in your table_id like 'Serbia_at_the_European_Athletics_Championships_2', then you will see all the information related to the given table.

## Recent Papers

**Model**                                     |  **Organization**  |**Reference**                                                             | **Dev-EM** | **Dev-F1** | **Test-EM** | **Test-F1** | 
----------|---------------------------|-----------------------------------|---------------------------------------------------------------------------|---------|----------|------------------|
COS         | CMU + Microsoft Research + UIUC     | [Ma et al. (2023)](https://arxiv.org/abs/2305.03130)                    |  56.9 | 63.2  | 54.9  | 61.5 |
CORE         | CMU + Microsoft Research      | [Ma et al. (2022)](https://arxiv.org/abs/2210.12338)                    |  49.0 | 55.7  | 47.3  | 54.1 |
OTTeR        | MSRA + Beihang                | [Huang et al. (2022)](https://arxiv.org/abs/2210.05197)                 |  37.1 | 42.8 | 37.3 | 43.1 |
RINK         | JBNU + NAVER                  | [Park et al. (2023)](https://ojs.aaai.org/index.php/AAAI/article/view/26577)  | 36.7 | 42.4 | 35.5 | 41.5 |
CARP         | MSRA + Sun Yet-sen University      | [Zhong et al. (2021)](https://arxiv.org/pdf/2201.05880.pdf)                    |  33.2 | 38.6  | 32.5  | 38.5 |
Fusion+Cross-Reader         | Google      | [Chen et al. (2021)](https://arxiv.org/abs/2010.10439)                    |  28.1 | 32.5  | 27.2  | 31.5 |
Dual Reader-Parser | Amazon | [Alexander et al. (2021)](https://assets.amazon.science/09/2b/7acf41f24c998cd3c2361681e9db/dual-reader-parser-on-hybrid-textual-and-tabular-evidence-for-open-domain-question-answering.pdf)                                              |   15.8   |    -  |  -       | -        |
BM25-HYBRIDER   | UCSB      | [Chen et al. (2021)](https://arxiv.org/abs/2010.10439)                    |    10.3     | 13.0  |  9.7           | 12.8  |

## Reference
If you find this project useful, please cite it using the following format

```
  @article{chen2021ottqa,
  title={Open Question Answering over Tables and Text},
  author={Wenhu Chen, Ming-wei Chang, Eva Schlinger, William Wang, William Cohen},
  journal={Proceedings of ICLR 2021},
  year={2021}
}
```
","['wenhuchen', 'Mayer123', 'JudePark96']",1,,0.58,0,,,,,,3,,,
219044154,MDEwOlJlcG9zaXRvcnkyMTkwNDQxNTQ=,hal-fuzz,ucsb-seclab/hal-fuzz,0,ucsb-seclab,https://github.com/ucsb-seclab/hal-fuzz,Source code of HAL-fuzz,0,2019-11-01 18:49:08+00:00,2025-02-15 16:02:26+00:00,2021-02-04 06:31:37+00:00,,52324,144,144,,1,1,1,1,0,0,28,0,0,3,apache-2.0,1,0,0,public,28,3,144,master,1,1,"# hal-fuzz: An HLE-based fuzzer for blob firmware

hal-fuzz is the sleeker, faster, fuzzing-oriented version of HALucinator.
It was developed as part of our paper ""HALucinator: Firmware Re-Hosting through Abstraction Layer Emulation"" at USENIX 2020.

It was also used by the Shellphish hacking team to win the 2019 CSAW Embedded Security Challenge(https://github.com/TrustworthyComputing/csaw_esc_2019), by leveraging its rehosting, fuzzing, and debugging capabilities.
Check out a video of hal-fuzz grilling up a challenge automatically here: https://drive.google.com/file/d/1m4VzTQUBMb1xOZN9GmWQZS-Qij3v0koF/view


If you're interested in re-hosting entire multi-node systems, or re-hosting firmware needing complex interactions with the outside world, you might want to try the original, found here: https://github.com/embedded-sec/halucinator 

#### Cite us!

Using hal-fuzz for research? Please cite our USENIX paper.  More details at:
http://subwire.net/publication/halucinator/


## What is this crazy thing?

hal-fuzz is a generic emulator based on the principle of High Level Emulation (HLE), where we replace hardware-related library functions in the binary with high-level Python replacements.  While these replacements are created manually, we show (and you can experience yourself) that these high-level replacements are shockingly easy to write.  Most of them simply take arguments from the program, and do almost nothing of consequence. In fact, many do nothing at all, and are simply nop-outs.

In this fuzzing-oriented version, we replace the combo of full-system QEMU and Avatar used to create an instrumentable environment with AFL-Unicorn(https://github.com/Battelle/afl-unicorn ).
This removes the significant bulk of the previous system, and adds AFL's fork-server and block coverage information.  While we tried to engineer the system such that handlers written for one work with the other, we noticed that handlers used simply for fuzzing could be much shorter (and therefore much more performant) and opted to split up the system.  This version also forgoes concepts such as the Peripheral Server from the original HALucinator for the same reasons.

In order to make hal-fuzz work at a reasonable speed, we made a number of notable optimizations:

- *Deterministic Timers*: Timers are based on block counts, not real time.  This enables deterministic fuzzing of otherwise-asynchronous code.
- *Native Handlers*: Normal AFL-Unicorn/Python suffers from performance issues when transitioning between Unicorn and its Python bindings.  Unfortunately, Unicorn also does not provide a ""conditional hook""; you can hook every block in the program, but not a specific block.
We therefore provide a native library to enable this in a performant way, causing a massive (about 8x) speedup.
- *Interrupts* and ""faux-terrupts"": Interrupts are a part of life in embedded.  Unfortunately, one aspect that was lost when QEMU was slimmed-down into Unicorn was the entire notion of interrupts.  We add this notion back, in the usual HLE way, using a model of the Cortex-M NVIC. This implements the entry and exit procedures described in the ISA docs well enough to run actual RTOS bits.
However, many firmware/libraries will not actually need full interrupt support, and instead need something simpler, like an asynchronous callback.  This callback is passed as a function pointer, and is expected to be called in an Interrupt Service Routine (ISR).  Under normal circumstances, which ISR you use is hardware-dependent.  We'd prefer not to worry about that with HLE, so we created the new concept of ""faux-terrupts"", where a handler (or any Python hook) can freely enter interrupt mode with the IRQ of its chosing, and execute these callbacks, without needing hardware-specific knowledge.
As part of this, we also enable the manipulation of the ARM Cortex M status registers from Unicorn, a feature currently missing from the mainline version.

More details can be found in the paper.


## How do I use it?

### Initial setup

#### Shortcut: Docker

Do you not hate Docker? Skip all the stuff below and try our Dockerfile (note that we don't suggest this for real-world fuzzing of anything, but it's a great way to play around!)

Simply:
```
docker build .
```

...and eventually...

```
docker run -it <image_hash> /bin/bash
```

and you'll be dropped into a shell with everything set up!

If you want to fuzz, make sure you do the following *on the host, as root* or AFL will get mad:

```
echo core >/proc/sys/kernel/core_pattern
cd /sys/devices/system/cpu
echo performance | tee cpu*/cpufreq/scaling_governor
```

#### The old-fashioned way

If you're on Ubuntu 18.04, first make yourself a Python virtual environment:
```
mkvirtualenv -p /usr/bin/python3 halfuzz
```

...and then run:
```
./setup.sh
```

You'll need to be a user with sudo permissions.  
Cross your fingers.

### Now what?

If all goes well, you can now use the ./hal-fuzz script to use the tool.

Other useful points of interest include the `test_*.sh` scripts found in the root directly.  The ""fuzz"" scripts will start single-threaded AFL for that sample, the ""parallel"" scripts will start a huge parallel AFL ssession (be careful!) and the plain ones will simply run the binary once (useful for triage and debugging).

The `csaw_tester_*.py` scripts relate to our use of hal-fuzz in the 2019 CSAW Embedded Systems Challenge.  These are for debugging, fuzzing, or validating challenge solutions.  More info on what these do and why can be found on the CSAW ESC website.

### Getting symbols

As we mention in the paper, you need a few things in order to use this tool, the first of which is the location of the libraries in the binary-under-test.  There are two ways to get this information, which are provided to the system as a yml file:

- *Dump the symbols from an ELF*:  If you're compiling a piece of firmware yourself, or are otherwise lucky enough to have symbols around, do this.  We have a nice angr script (dump_symbols.py) that will do this for you, creating the needed yml file. In the future, we hope to add a loader (e.g., angr's CLE loader) to automate most of this.

- *Use LibMatch*: We created our own library-matcher as part of our paper, found here: (https://github.com/subwire/libmatch )
This tool also outputs the yml format you need. If you've got a real blob, this is the way to go. You'll of course need the SDK you think your firmware was built with in order to do this.

### Writing Handlers and Models

hal-fuzz doesn't do much without Handlers and Models.
If your firmware uses a library we already have Handlers for, great! (see ./configs/hal/ for what's already supported) You can skip this part.

If not, you need to make some.  This isn't hard (as we evaluate in the paper) and shouldn't take too long at all.

Handlers are the high-level replacement functions that make hardware-dependent behaviors disappear.  Models are data abstractions to allow for handlers to share a common object, such as a virtual serial port or I2C bus. Models also facilitate a common interface with the outside host.

Your goal in writing handlers is to create two things: The python handler code itself, and a YML file mapping the actual symbol names to your handlers.  

*Handler code:* A handler is a python function that takes the emulator's state (uc) as an argument, and transforms this state to appear as it would if the function was run. The three primary steps in most handlers are: 1) collecting arguments, 2) performing the actual behavior, and 3) returning a value.  For example, consider a function that adds 2 to the argument and returns it; it would look like this:

```
def add_two(uc):
    number = uc.regs.r0 # Get the argument
    number += 2         # Add two
    uc.regs.r0 = number # Return the result
```

Now this isn't the kind of function you'd normally want to intercept.  What about something with hardware in it? Let's say we have a function that takes n bytes from a serial port, and writes them into a buffer. We use the SerialModel for this.  For example:

```python
def serial_read(uc):
    serial_id = uc.regs.r0 # arg0, which serial port
    buff_ptr = uc.regs.r1 # arg1, where's the data going?
    len = uc.regs.r2 # arg2, how much data?
    buff_len = uc.regs.r3 # arg3, how long is that buffer?
    the_data = SerialModel.rx(serial_id, len) # get the actual data
    assert(buff_ptr != 0) # crash if we get a null pointer!
    assert(len <= buff_len) # crash if somebody's being bad!
    uc.mem[buff_ptr] = the_data # write it out to memory
    uc.regs.r0 = 0 # indicate success
```

In the above, we see a few new concepts.  The function's arguments and their type should be looked up in the HAL, library, or SDK's documentation.  Based on this info, we can help out our fuzzer by adding some preconditions that, if violated, tell us something has gone horribly wrong.  

See the numerous included examples (in hal_fuzz.handlers) for ideas and inspiration on writing your own handlers. 

*YAML file:*  You can create a YAML file for the HAL or library you're handling so that it can be quickly re-used for any new firmware image.  You may map multiple functions to the same handler, reuse other handlers, or leave the handler function name blank to just nop out the function.  *Note that you should use the nop-out when you can, we dynamically re-write the binary to add nop-outs to avoid calling Python code for performance reasons!*

Following on from the above examples, we might do something like:

```yaml
handlers:
    HAL_Serial_Read:
        handler: hal_fuzz.handlers.my_hal.serial_read
    HAL_Serial_Init:
        handler: 
    HAL_add_two:
        handler: hal_fuzz.handlers.my_hal.add_two
```

Once you have this YAML file for your HAL or library, just include it in your firmware's configuration (see below)

### Configuring hal-fuzz

We use a YAML configuration file per binary to set up emulation.  In this file, you need to specify the memory map, which libraries are in use, and any ancillary options that affect emulation, such as the configuration of peripherals you'd like.

Numerous examples exist in ./tests of how to do this, but the basic layout for a typical firmware sample is:

- *include*: The include directive accepts a list of other yml files to include.  If they contain duplicate entries, they will be merged, allowing you to override settings as you desire.  We provide YAML files for each HAL (as they typically do not change) and for the basic layout of the ISA-specified memory map, which can/should be included first.  Make sure to include the file containing the recovered symbols created earlier!

- *memory_map*: This is where you tell hal-fuzz how to load the binary.  Of course, since we're dealing with blobs, this information needs to be figured out first. Make sure to use the ""file="" attribute to load the firmware itself into memory somewhere!

- *handlers*: Wihle most of the handler configuration is simply included from a pre-existing file, if you have any firmware-specific overrides, you can put them here.  These could include things like removing the CRC from LwIP, switching error handlers from crashes to hangs, or anything your analysis could benefit from.


### Command-line options

With the emulation environment configured, now it's time to run the tool.  hal-fuzz accepts many command-line options (queried with ./hal-fuzz --help) which are described in detail here.

- *-c*: This is the required option where you specify the YAML file for your firmware.
- *-d/--debug*: This enables debug instrumentation.  Fuzzing something? *TURN THIS OFF* to massively boost your execution speed. None of the other debugging-related options will work without this on, however
- *-t/--trace-syms*: Print to the screen a trace of the called functions, as annotated by your collected symbol table.  This is extrmeely useful for rapid debugging and development of handlers, or diagnosing crashes
- *-M/--trace-memory*: Print to the screen a full memory trace.  This is *EXTREMELY SLOW* and will produce more output than you might expect, but is the best way to triage certain kinds of crashes.  You're not getting this kind of data out of your average emulator!
- *-b/--breakpoint*: Set a breakpoint, and drop into the SparklyUnicorn(tm) debugger.  See below for details. You can set furhter breakpoints after you hit the first one (TODO: Fix this)
- *input*: The final, positional argument is a file to be fed in as the ""fuzz"".

### Using hal-fuzz with AFL

Ready to fuzz some drones? Great! If you've followed the steps above, hal-fuzz is already ready to use with AFL.  See our examples (e.g., ./test_st_plc_parallel.sh) for examples.  The basic usage is:

./afl-fuzz -U -m none -i ./path/to/inputs -o ./path/to/outputs -- ./hal-fuzz -c ./path/to/firmware.yml @@

Note that the first time you start, AFL will probably warn you about your system settings.  Just follow its instructions and you'll be all set.

*If you're using Docker, do these steps to the host, not to the container!*

### Debugging with hal-fuzz

Found a crash? Great! You can track it down with the provided tools.

As a first step, the -d, -t,  and -M options above might be all you need -- they'll quickly tell you where in the binary the crash occured, and a quick trace of how the culprit data got there.

If you need anything beyond that, such as single-stepping through the binary, you can do this via the new SparklyUnicorn(tm) debugger. Just pass -b followed by an address, and you'll be dropped into an ipdb shell, with the binary halted.  (NOTE: at this time, Unicorn only lets us break on the first instruction in a basic block! This is the price we pay for performance)

Once your breakpoint is hit, you're left with the Unicorn object itself (uc), which has been instrumented with many new features (it's sparkly!).  

- *Stepping*: Typing ""uc.step()"" will single-step the binary.  Want a live stack or register view? Try ""uc.step; uc.regs; uc.stack"".  ipdb lets you simply repeatedly press enter to repeat the last command, giving you GDB-like convenience.

- *Registers*: Typing ""uc.regs"" produces a register view. You can read and write the reigsters using uc.regs.register, like ""uc.regs.r0 = 0"".  

- *Stack*: Similarly, uc.stack produces a stack view, annotated with points-to register information. You can expand this using uc.stack.pp(start_offset, end_offset), or slice it for the raw data, as uc.stack[-4:16]

- *Memory*: As with the stack, memory is available at uc.mem, and can be sliced to read and write the emulator's memory.

- *Breakpoints*: You can control breakpoints using add_breakpoint() and remove_breakpoint()


## TODOs / Limitations / Help Wanted / Roadmap

hal-fuzz is a research prototype.  While we feel it's extremely useful as-is, there are many rough edges to be worked on, which we look forward to addressing.

Here's an (incomplete) list:

- *Multiple architectures*: We only support M-profile ARM CPUs at this time, since that's all we had in our evaluation.  I have left TODOs in the code where things would need to be chnaged to fix this, and Unicorn itself supports a number of architectures, so there's no reason this couldn't be implemented given some time. Note that, very sadly, Cortex-M4F CPUs are *NOT* supported due to missing FPU support; this is a generic limitation that QEMU itself is still in the process of solving, and it will take an indeterminate amount of time for this to trickle down to Unicorn.

- *Integrate angr's archinfo and cle*: As part of the above, we should use archinfo to abstract architecture-dependent stuff.  This includes, but is not limited to: the ""return"" instruction, register information, calling conventions, various kinds of endness, and so on.  CLE would provide us instant support for numerous binary formats, particularly to cover the situation where you're compiling your own firmware and have more than just a blob.  Still, CLE also has support for loading blobs, even automatically, using plugins.

- *Modernize Handlers*: The way we write handlers changed a few times during hal-fuzz's development to make things easier, such as the creation of faux-terrupts, and SparklyUnicorn.  This would dramatically cut down in the ugliness of handler code, and make implementation even easier for the analyst. Using some kind of type or calling-convention information would be the ultimate in handler creation, and could allow for semi-automation of the process.

",['subwire'],1,,0.82,0,,,,,,10,,,
192248904,MDEwOlJlcG9zaXRvcnkxOTIyNDg5MDQ=,Knowledge-Aware-Reader,xwhan/Knowledge-Aware-Reader,0,xwhan,https://github.com/xwhan/Knowledge-Aware-Reader,"PyTorch implementation of the ACL 2019 paper ""Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader""",0,2019-06-17 00:14:17+00:00,2025-02-07 15:59:48+00:00,2020-02-04 21:25:31+00:00,,619,139,139,Python,1,1,1,1,0,0,24,0,0,4,,1,0,0,public,24,4,139,master,1,,"Code for the ACL 2019 paper:

## Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader

Paper link: [https://arxiv.org/abs/1905.07098](https://arxiv.org/abs/1905.07098)

Model Overview:
<p align=""center""><img width=""90%"" src=""assets/model.png"" /></p>

### Requirements
* ``PyTorch 1.0.1``
* ``tensorboardX``
* ``tqdm``
* ``gluonnlp``

### Prepare data
```
mkdir datasets && cd datasets && wget https://sites.cs.ucsb.edu/~xwhan/datasets/webqsp.tar.gz && tar -xzvf webqsp.tar.gz && cd ..
```

### Full KB setting
```
CUDA_VISIBLE_DEVICES=0 python train.py --model_id KAReader_full_kb --max_num_neighbors 50 --label_smooth 0.1 --data_folder datasets/webqsp/full/ 
```

### Incomplete KB setting
Note: The Hits@1 should match or be slightly better than the number reported in the paper. More tuning on threshold should give you better F1 score. 
#### 30% KB
```
CUDA_VISIBLE_DEVICES=0 python train.py --model_id KAReader_kb_03 --max_num_neighbors 50 --use_doc --data_folder datasets/webqsp/kb_03/ --eps 0.05
```

#### 10% KB
```
CUDA_VISIBLE_DEVICES=0 python train.py --model_id KAReader_kb_01 --max_num_neighbors 50 --use_doc --data_folder datasets/webqsp/kb_01/ --eps 0.05
```
#### 50% KB
```
CUDA_VISIBLE_DEVICES=0 python train.py --model_id KAReader_kb_05 --num_layer 1 --max_num_neighbors 100 --use_doc --data_folder datasets/webqsp/kb_05/ --eps 0.05 --seed 3 --hidden_drop 0.05
```

### Citation
```
@inproceedings{xiong-etal-2019-improving,
    title = ""Improving Question Answering over Incomplete {KB}s with Knowledge-Aware Reader"",
    author = ""Xiong, Wenhan  and
      Yu, Mo  and
      Chang, Shiyu  and
      Guo, Xiaoxiao  and
      Wang, William Yang"",
    booktitle = ""Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"",
    month = jul,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/P19-1417"",
    doi = ""10.18653/v1/P19-1417"",
    pages = ""4258--4264"",
}
```
",['xwhan'],1,,0.78,0,,,,,,6,,,
129460796,MDEwOlJlcG9zaXRvcnkxMjk0NjA3OTY=,KoopmanMPC_for_flowcontrol,arbabiha/KoopmanMPC_for_flowcontrol,0,arbabiha,https://github.com/arbabiha/KoopmanMPC_for_flowcontrol,A data-driven framework for control of nonlinear flows with Koopman Model Predictive Control,0,2018-04-13 22:41:19+00:00,2025-02-28 06:37:19+00:00,2020-06-10 03:35:22+00:00,,2341,130,130,MATLAB,1,1,1,1,0,0,44,0,0,1,mit,1,0,0,public,44,1,130,master,1,,"# KoopmanMPC_for_flowcontrol
This project demonstates the application of Koopman-MPC framework for flow control,
following the paper
*""A data-driven Koopman model predictive control framework for nonlinear flows""*
by H. Arbabi, M. Korda and I. Mezic (https://arxiv.org/pdf/1804.05291.pdf).

The Koopman-MPC framework is summarized in the below figure:

<img src=""https://github.com/arbabiha/KoopmanMPC_for_flowcontrol/blob/master/thehood/BigPic.png"" width=""700"">


### files in the root folder:

#### BurgersExample 
Runs the Burgers example as explained in the paper, it includes data collection, Extended Dynamic Mode Decomposition (EDMD) for identification of the Koopman linear system, and a run of closed-loop controlled system from some initial condition.
Feel free to play with the paremeters of the code, specially, try different observables, embedding dimension, reference signal, initial condition, etc.
The whole program, with the initial paremeter settings, runs on my personal laptop in under 2 minutes.


#### CavityExample
Runs the lid-driven cavity flow example as explained in the paper,  including  EDMD for identification of the Koopman linear system, and a run of closed-loop controlled system from some initial condition on the limit cycle. There are two options to run this code:
1- ask the code to generate data for EDMD. This is a lengthy process and for the parameter values reported in the paper takes ~10 hours on a powerful desktop (with no parallelization), or 2- go to https://ucsb.box.com/s/367tvkgnzby61x9nrh64q81748ugaw63 and download the data file ""Cavity_data_4EDMD_0"" (~3GB) which is the data used in the paper. Using the data file, the program  takes about 5 minutes to run on my laptop. 




### before you run the code:

go to ""./thehood/"" and unzip ""qpOASES-3.1.0"",
then go to subfolder "".\thehood\qpOASES-3.1.0\interfaces\matlab"" and run make.m .
This is required to activate the qpOASIS interface for solving the optimization problem.


send comments and questions to
#### arbabiha@gmail.com

H Arbabi

April 2018
",['arbabiha'],1,,0.63,0,,,,,,8,,,
562327954,R_kgDOIYRxkg,xenos,raphaelradna/xenos,0,raphaelradna,https://github.com/raphaelradna/xenos,Xenos: Xenharmonic Stochastic Synthesizer,0,2022-11-06 01:35:52+00:00,2024-12-22 13:43:07+00:00,2023-09-12 11:06:30+00:00,,292,127,127,C++,1,1,1,1,0,0,10,0,0,6,gpl-3.0,1,0,0,public,10,6,127,main,1,,"# Xenos

Xenos is a virtual instrument plug-in that implements and extends the Dynamic Stochastic Synthesis (DSS) algorithm invented by Iannis Xenakis. Programmed in C++ with the JUCE framework, Xenos is open-source, cross-platform, and can be built in a number of plug-in formats.

![](Extra/xenosInterface.png ""The Xenos interface"")

Key features include:
- Authentic DSS engine
- Xenharmonic pitch quantizer
- Custom scale import in the [Scala](https://www.huygens-fokker.org/scala/) format
- Ten stochastic distributions with up to two parameters each
- First- and second-order random walks
- Variable number of segments per wave cycle
- Variable amplitude envelope
- Polyphonic (128 voices by default)
- MIDI implementation (notes, sustain, pitch bend)
- External MIDI controller assignment
- Parameter automation
- Simple and streamlined interface
- Free and open source

Xenos was first presented to the Meta–Xenakis Global Symposium, and is the subject of a master’s degree from the [Media Arts and Technology (MAT)](https://www.mat.ucsb.edu/) program at UC Santa Barbara.

## Quick Start Video

Watch the [Quick Start video](https://youtu.be/ha5xsKm7MtE) on YouTube.

## Installation Notes

Xenos has been tested on macOS 10.14.6 and Windows 10 (64-bit).

### Build from Source (Mac or Windows)

1. Download [JUCE](https://juce.com/get-juce/download)
2. Clone or download [Xenos](https://github.com/raphaelradna/xenos/archive/refs/heads/main.zip)
3. Open Xenos.jucer in the Projucer
4. Export the project for your IDE and platform, e.g., Xcode (macOS) or Visual Studio 2019 (Windows); see [here](https://docs.juce.com/master/tutorial_new_projucer_project.html) for more information
5. Compile Xenos using your IDE
6. Move the plug-in binary, e.g., Xenos.component or Xenos.vst3, to the proper location according to your platform, host software, and plug-in format
    - e.g., `/Macintosh HD/Library/Audio/Plug-Ins/Components` (MacOS)
    - e.g., `C:\Program Files\Common Files\VST3` (Windows)
7. Open a suitable plug-in host application and add Xenos on a software instrument track

### Build from Source (Linux)

0. Install JUCE dependencies:
    ```
    sudo apt install libasound2-dev libjack-jackd2-dev ladspa-sdk \
        libcurl4-openssl-dev libfreetype6-dev libx11-dev \
        libxcomposite-dev libxcursor-dev libxext-dev libxinerama-dev \
        libxrandr-dev libxrender-dev libwebkit2gtk-4.0-dev
    ```
1. Clone Xenos:
    `git clone https://github.com/raphaelradna/xenos.git`
2. Navigate into the Xenos folder:
    `cd xenos`
3. Clone JUCE:
    `git clone https://github.com/juce-framework/JUCE.git`
4. Configure the build:
    `mkdir -p build/Release && cd build/Release && cmake -D CMAKE_BUILD_TYPE=Release -G ""Unix Makefiles"" ../..`
5. Build Xenos:
    `cmake --build ./ --config Release`

Once built, Xenos VST3 and LV2 plugins will be in `xenos/build/Release/Xenos_artefacts/Release`.
However, they should have already been automatically copied to the default location for such plugins on your computer (probably `~/.vst3` and `~/.lv2`).

### Pre-Built Binaries

1. Download the latest Xenos release for your platform from [GitHub](https://github.com/raphaelradna/xenos/releases)
2. Extract the plug-in binary, i.e., Xenos.component or Xenos.vst3, and move it to the proper location according to your platform, host software, and plug-in format
    - e.g., `/Macintosh HD/Library/Audio/Plug-Ins/Components` (MacOS)
    - e.g., `C:\Program Files\Common Files\VST3` (Windows)
    - e.g., `~/.vst3` (Linux)
3. Open a suitable plug-in host application and add Xenos on a software instrument track
","['raphaelradna', 'rodneydup']",1,,0.75,4100,,,,,,12,,,
2962766,MDEwOlJlcG9zaXRvcnkyOTYyNzY2,LuaAV,LuaAV/LuaAV,0,LuaAV,https://github.com/LuaAV/LuaAV,Audiovisual scripting with Lua,0,2011-12-12 08:10:09+00:00,2024-02-02 16:43:55+00:00,2017-07-25 17:20:26+00:00,http://lua-av.mat.ucsb.edu/,73047,123,123,C++,1,1,1,1,1,0,11,0,0,9,other,1,0,0,public,11,9,123,master,1,1,"LuaAV: Audiovisual Scripting with Lua
======================================================================
See the copyright information in the file named `COPYRIGHT`.


LuaAV is an integrated programming environment based upon extensions to the Lua programming language to enable the tight real-time integration of computation, time, sound and space.  LuaAV is first and foremost an audiovisual composition environment but can be used for general Lua scripting and prototyping as well.

Principal developers: Wesley Smith and Graham Wakefield.

LuaAV Website: http://lua-av.mat.ucsb.edu


LuaAV Concept
-----------

LuaAV is a real-time audiovisual scripting environment based on the Lua scripting language and a collection of libraries for sound, graphics, and media protocols. The goal of LuaAV is to serve as a computational platform for creative exploration that is both fluid and generative. As with the Lua language, the design strategies for LuaAV favor mechanisms over implementation in order to keep its structure as clear, compact, dynamic, and reconfigurable as possible. At any given moment, it should be possible to enact any creative thought about sound, image, space, and time and experiment with a range of possible approaches without having to divert energy away from its pursuit due to the brittleness of the computational system. Computation must become plastic. Software must become softer.

LuaAV is a generative kernel of computation, capable of spawning new structures and procedures dynamically. In essence, it is meta-software for an audiovisual composition capable of generating a vast range of possible software either at the hand of a user/programmer or even algorithmically through abstract machines. As much of LuaAV as is reasonable is generated dynamically through the built-in compiler, parser, and code generating mechanisms such that the software grows and adapts as features are employed.

LuaAV is divided between a cross-platform library called libluaav and collection platform-specific classes and functions that make up the LuaAV application. libluaav contains the core timing, scheduling, and multi-threaded messaging functionality and is designed in such a way that it can be embedded in a wide variety of contexts much like Lua itself. Possible contexts might include VST plugins, web-browser plug-ins, streaming radio stations, or other host applications such as 3D modeling programs. In such contexts, platform-specific functionality such as windowing and menus are irrelevant, which is why they are in the LuaAV application instead of libluaav.
","['grrrwaaa', 'charlieroberts', 'weshoke', 'charlieroberts-rit']",1,,0.87,0,,,,,,17,,,
141444447,MDEwOlJlcG9zaXRvcnkxNDE0NDQ0NDc=,MojiTalk,claude-zhou/MojiTalk,0,claude-zhou,https://github.com/claude-zhou/MojiTalk,"Code for ""MojiTalk: Generating Emotional Responses at Scale"" https://arxiv.org/abs/1711.04090",0,2018-07-18 14:16:31+00:00,2024-10-22 08:32:42+00:00,2018-07-18 15:10:42+00:00,,72,121,121,Python,1,1,1,1,0,0,22,0,0,3,mit,1,0,0,public,22,3,121,master,1,,"# MojiTalk
Xianda Zhou, and William Yang Wang. 2018. Mojitalk: Generating emotional responses at scale. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 1128–1137. Association for Computational Linguistics.

Paper: https://arxiv.org/abs/1711.04090

Our lab: http://nlp.cs.ucsb.edu/index.html

## Emojis
The file ```emoji-test.txt``` (http://unicode.org/Public/emoji/5.0/emoji-test.txt) provides data for loading and testing emojis. The 64 emojis that we used in our work are marked with '64' in our modified ```emoji-test.txt``` file.

Unicode and the Unicode Logo are registered trademarks of Unicode, Inc. in the U.S. and other countries.

For terms of use, see http://www.unicode.org/terms_of_use.html

## Dependencies
* Python 3.5.2
* TensorFlow 1.2.1

## Usage
1. Preparation:
	
	Set up an environment according to the dependencies.

	Dataset: https://drive.google.com/file/d/1l0fAfxvoNZRviAMVLecPZvFZ0Qexr7yU/view?usp=sharing
	
	Unzip ```mojitalk_data.zip``` to the current path, creating ```mojitalk_data``` directory where our dataset is stored. Read the ```readme.txt``` in it for the format of the dataset.

2. Base model:
	1. Set the ```is_seq2seq``` variable in the ```cvae_run.py``` to ```True```
	2. Train, test and generate: ```python3 cvae_run.py```

		This will save several breakpoints, a log file and generation output in ```mojitalk_data/seq2seq/<timestamp>/```
	
3. CVAE model:
	1. Set the ```is_seq2seq``` variable in the ```cvae_run.py``` to ```False```
	2. Set path of pretrain model: Modify line 67 of ```cvae_run.py``` to load a previously trained base model. e.g.: ```saver.restore(sess, ""seq2seq/07-17_05-49-50/breakpoints/at_step_18000.ckpt"")``` 
	3. Train, test and generate: ```python3 cvae_run.py```
	
		This will save several breakpoints, a log file and generation output in ```mojitalk_data/cvae/<timestamp>/```.
	
		Note that the choice of base model breakpoint as the pretrain setting would influence the result of CVAE training. A overfitted base model may cause the CVAE to diverge.
	
4. Reinforced CVAE model:
	1. Train the emoji classifier: ```CUDA_VISIBLE_DEVICES=0 python3 classifier.py``` 
		
		The trained model will be saved in ```mojitalk_data/classifier/<timestamp>/breakpoints``` as a tensorflow breakpoint.
	
	2. Set path of pretrain model: Modify line 63/74 of ```rl_run.py``` to load a previously trained CVAE model and the classifier.
	3. Train, test and generate: ```python3 rl_run.py```
	
		This will save several breakpoints, a log file and generation output in ```mojitalk_data/cvae/<timestamp>/```.
",['claude-zhou'],1,,0.69,0,,,,,,2,,,
12771371,MDEwOlJlcG9zaXRvcnkxMjc3MTM3MQ==,simple-dmrg,simple-dmrg/simple-dmrg,0,simple-dmrg,https://github.com/simple-dmrg/simple-dmrg,"SIMPLE DMRG tutorial, created for the Trieste summer school on quantum spin liquids.",0,2013-09-12 00:36:53+00:00,2025-02-02 01:51:49+00:00,2019-08-27 18:20:40+00:00,,73,118,118,Python,1,1,1,1,0,0,51,0,0,0,mit,1,0,0,public,51,0,118,master,1,1,#ERROR!,"['garrison', 'mishmash']",1,,0.76,0,,,,,,12,,,
102878107,MDEwOlJlcG9zaXRvcnkxMDI4NzgxMDc=,basic_simulation_training,MobleyLab/basic_simulation_training,0,MobleyLab,https://github.com/MobleyLab/basic_simulation_training,"A document for the Living Journal of Computational Molecular Science (LiveCoMS) which describes basic training for molecular simulations (oriented towards molecular dynamics (MD)), providing some training itself and linking out to other helpful information elsewhere. The intent is that this provide information on the prerequisites which will be required for understanding/following many of the other ""best practices"" documents being prepared.",0,2017-09-08 15:47:48+00:00,2025-01-27 06:09:44+00:00,2018-12-29 22:07:45+00:00,,30722,113,113,TeX,1,1,1,1,0,0,19,0,0,5,cc-by-4.0,1,0,0,public,19,5,113,master,1,1,"# Basic Simulation Training

This repository contains a document (and supporting materials) on Basic Simulation Training, intending to provide prerequisite information for people who want to begin conducting (bio)molecular simulations, especially Molecular Dynamics (MD).
Our focus here is to tell people what they need to know and why, with links/references to suitable training material. Some brief summaries of some of the key concepts may be provided, but as we are not out to write a textbook, a focus is on guiding people to the appropriate materials.
The current focus is on MD; Monte Carlo (MC) will be addressed in a separate document which will be linked from here when it is available.

## List of Authors

- Efrem Braun (Berkeley)
- Justin Gilmer (Vanderbilt)
- Heather B. Mayes (University of Michigan)
- David L. Mobley (UC Irvine)
- Jacob I. Monroe (UC Santa Barbara)
- Samarjeet Prasad (National Institutes of Health; Johns Hopkins University)
- Daniel M. Zuckerman (Oregon Health and Science University)

## List of Contributors
<!-- We suggest listing contributers in order of addition. -->
- Avisek Das (helped with outline and early brainstorming/planning of this document)
- Victoria Tran Lim provided [valuable editorial feedback](https://github.com/MobleyLab/basic_simulation_training/issues/89#issue-351693860) on the document
- Michael Shirts caught a variety of typos and other minor issues, and suggested some improvements.
- Emmanuel Karagiorgos caught some duplicated references

## Paper writing as code development
<!-- This discussion is so that people know how to contribute to your document. -->
This paper is being developed as a living document, open to changes from the community. You can read more about the concept of writing a paper in the same way one would write software code in the essay [""Paper writing as code development""](https://livecomsjournal.github.io/about/paper_code/). If you have comments or suggestions, we welcome them! Please [submit them as issues](https://guides.github.com/features/issues/) to this GitHub repository so they can be recorded and given credit for the contribution. Specific changes can be proposed [via pull requests](https://help.github.com/articles/about-pull-requests/).

## List of Released Versions
<!-- update this when you decide to release a version either by preprint or when submitted to LiveCoMS-->

## Citing this work

Version 1.0 of this work is being published in the Living Journal of Computaitonal Molecular Science (LiveCoMS) volume 1, issue 1, page/article 5957 (2019).    It is available at this DOI: [10.33011/livecoms.1.1.5957](http://dx.doi.org/10.33011/livecoms.1.1.5957).

Ongoing (non-peer reviewed) versions of this article will be assigned unique DOIs via Zenodo which will be posted here and can be cited by these DOIs; subsequent peer reviewed versions (when applicable) are planned to be published in LiveCoMS.

## Changelog
<!-- Here, record summaries of important changes. A granular discussion of changes will be kept in GitHub by issue tracking.-->
- Sept. 8, 2017: D. Mobley created repo, put in initial files from LiveCoMS.
- Spring/early summer 2018: Finalize who will be involved and write/edit first version of the paper
- Sept. 2, 2018: Final draft of version 1 submitted to LiveCoMS
- Nov. 5-6, 2018: Make editorial revisions suggested by peer reviewers and Victoria Lim.
- Nov. 23, 2018: Check references using [`fixbibtex`](https://github.com/jaimergp/fixbibtex), incorporate fixes for problems it caught; addresses a number of typos/missing references caught by Michael Shirts.
- Nov. 28, 2018: Add DOI, include citation to Grossfield et al. LiveCoMS article with DOI.
- Dec. 18, 2018: Fix duplicated references.
- Dec. 29, 2018: Finalize formatting for publication.
","['davidlmobley', 'EfremBraun', 'justinGilmer', 'hmayes', 'samarjeet', 'avisekdas', 'dmzuckerman']",1,,0.68,0,,,,,,20,,,
90395711,MDEwOlJlcG9zaXRvcnk5MDM5NTcxMQ==,trends.earth,ConservationInternational/trends.earth,0,ConservationInternational,https://github.com/ConservationInternational/trends.earth,trends.earth - measure land change,0,2017-05-05 16:25:46+00:00,2025-02-20 12:24:43+00:00,2025-03-03 19:01:18+00:00,http://trends.earth,157727,112,112,Python,1,1,1,0,0,0,45,0,0,134,gpl-2.0,1,0,0,public,45,134,112,main,1,1,"# Trends.Earth

[![Trends.Earth](https://s3.amazonaws.com/trends.earth/sharing/trends_earth_logo_bl_600width.png)](http://trends.earth)

[![Documentation Status](https://readthedocs.org/projects/trendsearth/badge/?version=latest)](https://trendsearth.readthedocs.io/en/latest/?badge=latest)
[![Zipfile Status](https://github.com/ConservationInternational/trends.earth/actions/workflows/build_zipfile.yaml/badge.svg)](https://github.com/ConservationInternational/trends.earth/actions/workflows/build_zipfile.yaml)
[![Update translations](https://github.com/ConservationInternational/trends.earth/actions/workflows/translation_update.yaml/badge.svg)](https://github.com/ConservationInternational/trends.earth/actions/workflows/translation_update.yaml)
[![Tests](https://github.com/ConservationInternational/trends.earth/actions/workflows/test.yaml/badge.svg)](https://github.com/ConservationInternational/trends.earth/actions/workflows/test.yaml)

`Trends.Earth` is a free and open source tool to understand land change: the how and why
behind changes on the ground. Trends.Earth allows users to draw on the best available
information from across a range of sources - from globally available data to customized
local maps. A broad range of users are applying Trends.Earth for projects ranging from
planning and monitoring restoration efforts, to tracking urbanization, to developing
official national reports for submission to the United Nations Convention to Combat
Desertification (UNCCD).

`Trends.Earth` is a [QGIS](http://www.qgis.org) plugin that supports monitoring of land
change, including trends in urbanization, and changes in productivity, land cover, and
soil organic carbon. The tool can support monitoring land degradation for reporting to
the Global Environment Facility (GEF) and United Nations Convention to Combat
Desertification (UNCCD), as well as tracking progress towards achievement of Sustainable
Development Goal (SDG) target 15.3, Land Degradation Neutrality (LDN).

`Trends.Earth` was produced by a partnership of Conservation International, Lund
University, and the National Aeronautics and Space Administration (NASA), with
the support of the Global Environment Facility (GEF). It was further developed
through a partnership with Conservation International, University of Bern,
University of Colorado in partnership with USDA and USAID, University of California -
Santa Barbara in partnership with University of North Carolina - Wilmington and Brown
University with additional funding from the Global Environment Facility (GEF).

## Documentation

See the [user guide](http://trends.earth) for information on how to use
the plugin.

## Installation of stable version of plugin

The easiest way to install the plugin is from within QGIS, using the [QGIS
plugin repository](http://plugins.qgis.org/plugins/LDMP/). However, It is also
possible to install the plugin manually from a zipfile, which can be useful to
access an old version of the plugin, or to install the plugin without internet.
Instructions for both of these possibilities are below.

### Stable version from within QGIS (recommended)

The easiest way to install the plugin is from within QGIS, using the [QGIS
plugin repository](http://plugins.qgis.org/plugins/LDMP/).

### Stable version from zipfile

Download a stable version of `Trends.Earth` from
[the list of available releases on
GitHub](https://github.com/ConservationInternational/trends.earth/releases). Then follow
the instructions below on [installing the plugin from a
zipfile](#installing-plugin-from-a-zipfile).

## Installation of development version of plugin

If you are interested in using the development version of the
plugin, with the very latest (but not as well tested) features, or in contributing to
the development of it, you will want to install the development version. Note
that the development version is more likely to contain bugs or other issues
that have yet to be resolved. There are two ways to install the development
version:

- Using a packaged version (zipfile)

- Cloning the github repository and installing from that code

It is easier to install the plugin from a zipfile than from github, so this
option is recommended unless you are interested in contributing to development
of the plugin.

### Development version from zipfile

[Download the latest `Trends.Earth`
zipfile](https://s3.amazonaws.com/trends.earth/sharing/LDMP_main.zip). Then
follow the instructions below on [installing the plugin from a
zipfile](#installing-plugin-from-a-zipfile).

QGIS3+ is required for the latest versions of Trends.Earth. The QGIS2 version is no
longer supported (support ended in March 2020). If you want to use a previous version of
`Trends.Earth` (e.g. versions that work with QGIS2), please refer to this
[repository](https://github.com/ConservationInternational/trends.earth/releases) where
all `Trends.Earth` releases are available.

### Development version from source

Open a terminal window and clone the latest version of the repository from
Github:

```
git clone https://github.com/ConservationInternational/trends.earth
```

Navigate to the root folder of the newly cloned repository, and install
`invoke`, a tool that assists with installing the plugin:

```
pip install invoke
```

Now run the setup task with `invoke` to pull in the external dependencies needed
for the project:

```
invoke plugin-setup
```

then you can install the plugin using invoke:

```
invoke plugin-install --profile=<profile name>
```

If you modify the code, you need to run `invoke plugin-install` to update the
installed plugin in QGIS. You only need to rerun `invoke plugin-setup` if you
change or update the plugin dependencies. After reinstalling the plugin you
will need to restart QGIS or reload the plugin. Install the ""Plugin reloader""
plugin if you plan on making a log of changes
(https://github.com/borysiasty/plugin_reloader).

## Installing plugin from a zipfile

While installing `trends.earth` directly from within QGIS is recommended, it
might be necessary to install the plugin from a zipfile if you need to install
it offline, or if you need the latest features.

To install from a zipfile, first download a zipfile of the
[stable](#stable-version-from-zipfile) version. The zipfile might be
named `LDMP.zip`or `LDMP_QGIS3.zip` depending on what
version you are installing.

When using QGIS3.10.3 or greater versions it is possible to install `trends.earth`
directly from a zipfile. To install `trends.earth` from a zipfile, open QGIS3.10.3
(or greater) and click on ""Plugins"" then on ""Manage and install plugins"" and
choose the option ""Install from ZIP"". Browse to the folder in which the zipfile
has been saved, select the zipfile and click on 'Install Plugin'.
It is not necessary to unzip the file.

Please, note that the latest version of `trends.earth` is only supported for
QGIS3.10.3 or greater versions.

Start QGIS, and click on ""Plugins"" then ""Manage and install plugins"". In the
plugins window that appears, click on ""Installed"", and then make sure there is
a check in the box next to ""Land Degradation Monitoring Tool"". The plugin is
now installed and activated. Click ""Close"", and start using the plugin.

## Getting help

### General questions

If you have questions related to the methods used by the plugin, how to use a
particular dataset or function, etc., it is best to first check the [user
guide](http://trends.earth/docs/en) to see if your question is already
addressed there. The [frequently asked questions (FAQ)
page](http://trends.earth/docs/en/about/faq.html) is another good place to
look.

If you don't find your answer in the above pages, you can also [contact the
discussion group](https://groups.google.com/forum/#!forum/trends_earth_users).

### Reporting an issue (error, possible bug, etc.)

If you think you have found a bug in Trends.Earth, report your issue to our
[issue
tracker](https://github.com/ConservationInternational/trends.earth/issues) so
the developers can look into it.

When you report an issue, be sure to provide enough information to allow us to
be able to reproduce it. In particular, be sure to specify:

- What you were doing with the plugin when the problem or error occurred (for
  example ""I clicked on 'Download Results' and got an error messaging saying
  `describe what the message said`"".
- The operating system you are using, version of the plugin are you using, and
  version of QGIS that you are using
- If you are emailing about an error or problem that occurred when downloading
  results, tell us your username, and the task start time that is listed in the
  download tool for the task you are referring to
- If the error occurred while processing data with the plugin, tell us the
  location you were analyzing with the tool (for example: ""I selected Argentina
  from the dropdown menu""). If you used your own shapefile, please send us the
  file you used.
- If you got a message saying ""An error has occurred while executing Python
  code"", send us either the text of the message, or a a screenshot of the error
  message. **Also, send us the content of the Trends.Earth log messages
  panel.** To access the Trends.Earth log messages panel, select ""View"", then
  ""Panels"", then ""Log Messages"" from within QGIS. Copy and paste the text from
  that panel and include it in your issue report. It will make it easiest for
  us to track things down (as there will be fewer log messages) if you do this
  after first starting a new QGIS session and immediately reproducing the
  error.

## License

`Trends.Earth` is free and open-source. It is licensed under the GNU General
Public License, version 2.0 or later
","['azvoleff', 'MLNoon', 'luipir', 'Samweli', 'gkahiu', 'gdaldegan', 'pre-commit-ci[bot]', 'alexbruy', 'github-actions[bot]', 'vermeulendivan', 'gabrieldaldegan', 'mgroglich', 'dimasciput', 'dependabot[bot]', 'omahs', 'elpaso', 'zacharlie', 'eliaswilz', 'jmertic', 'nyalldawson', 'mentaljam', 'vikineema']",1,,0.69,692,,,,,,19,,,
146258162,MDEwOlJlcG9zaXRvcnkxNDYyNTgxNjI=,NVSim,SEAL-UCSB/NVSim,0,SEAL-UCSB,https://github.com/SEAL-UCSB/NVSim,"NVSim - A performance, energy and area estimation tool for non-volatile memory (NVM)",0,2018-08-27 06:51:22+00:00,2025-03-05 03:10:42+00:00,2018-08-27 22:58:17+00:00,http://nvsim.org/,147,111,111,C++,1,1,1,1,0,0,53,0,0,3,,1,0,0,public,53,3,111,master,1,1,"NVSim - A performance, energy and area estimation tool
for non-volatile memory (NVM)

======================================================

Sections

    1. Overview
    2. Compiling NVSim
    3. Running NVSim
    4. Configuring NVSim
    5. Hacking NVSim
    6. README Changelog

------------------------------------------------------  

1. Overview

    NVSim models the area, timing, dynamic energy and 
    leakage power of Phase-Change Memory (PCM), Spin-
    Torque-Transfer RAM (STT-RAM), Resistive RAM
    (ReRAM) or memristor, Floating Body Dynamic RAM
    (FBDRAM) and Single-Level Cell NAND Flash.
    NVSim uses the same modeling principles as the
    well-known CACTI, but it starts from scratch on
    the basis of a brand-new frame work with more
    flexibility in terms of bank/mat/subarray
    organization and periphral circuitry design. Such
    flexibility is necessary for emerging non-volatile
    memory technologies as the current status of most 
    emerging NVMs is unknown. Thanks for trying NVSim!


------------------------------------------------------

2. Compiling NVSim

    NVSim is programmed under GNU C++, and it can be 
    compiled on both Unix-like OSes and Microsoft 
    Windows 

    2a. Under Linux

        The tool can be built using make:

        $ make

        Running through make will automatically set 
        the compile flags needed.


------------------------------------------------------

3. Running NVSim


    If no .cfg file is specified, NVSim will load the 
    default configuration file (nvsim.cfg)

    $ ./nvsim

    Actually, users can specify their own configurations
    by passing the "".cfg"" argument.

    $ ./nvsim <custom>.cfg
    

------------------------------------------------------

4. Configuring NVSim


    NVSim can be configured using the configuration files.
    Several example configuration files can be found in
    the root directory of the source code. Note that most
    of the configuration files are specified with small
    capacity (< 128MB) as the current version of NVSim 
    only models a single-bank. Multi-bank cache/memory is
    in the to-do list of NVSim 2.0.

    For example, 
    sample_STTRAM_cache.cfg - A 8MB STT-RAM cache (both 
    data and array) configured for design space exploration
    sample_PCRAM.cfg - A 16MB PCM macro with fixed bank 
    organization (H-tree, internal sensing)
    sample_NVM_macro.cfg - A generic NVM macro
    A more detailed listing of configuration parameter 
    names and potential values are on the NVSim wiki page.


------------------------------------------------------

5. Hacking NVSim


    As mentioned in the overview, NVSim is meant to 
    be flexible. Another level of flexibility is 
    enabled when experts in NVM design Write their own
    sense amplifier, voltage plane, charge pump
    circuiitry etc. This can be done by creating a new
    C++ file with a new class and instantiate the new
    class in corresponding component. 


------------------------------------------------------

Please refer to the following paper for a general
introduction of the tool,
""NVSim: A Circuit-Level Performance, Energy and Area
Model for Emerging Nonvolatile Memory"", IEEE TCAD 2012

If you have any comments, questions, or suggestions please
either create an issue on github or contact us via email.

Cong Xu <xucongpsu@gmail.com>
Xiangyu Dong <rioshering@gmail.com>
Shuangchen Li <lschen.26@gmail.com>


",['cxxz'],1,,0.87,0,,,,,,5,,,
42426999,MDEwOlJlcG9zaXRvcnk0MjQyNjk5OQ==,CASMcode,prisms-center/CASMcode,0,prisms-center,https://github.com/prisms-center/CASMcode,First-principles statistical mechanical software for the study of multi-component crystalline solids,0,2015-09-14 04:21:07+00:00,2025-02-22 12:27:57+00:00,2023-11-13 22:44:04+00:00,,15065,109,109,C++,1,1,1,1,0,0,72,0,0,109,other,1,0,0,public,72,109,109,1.X,1,1,"## CASM: A Clusters Approach to Statistical Mechanics


CASM [(https://github.com/prisms-center/CASMcode)](https://github.com/prisms-center/CASMcode) is an open source software package designed to perform first-principles statistical mechanical studies of multi-component crystalline solids. CASM interfaces with first-principles electronic structure codes, automates the construction and parameterization of effective Hamiltonians and subsequently builds highly optimized (kinetic) Monte Carlo codes to predict finite-temperature thermodynamic and kinetic properties. CASM uses group theoretic techniques that take full advantage of crystal symmetry in order to rigorously construct effective Hamiltonians for almost arbitrary degrees of freedom in crystalline solids. This includes cluster expansions for configurational disorder in multi-component solids and lattice-dynamical effective Hamiltonians for vibrational degrees of freedom involved in structural phase transitions.

This version of CASM supports:

- Constructing, fitting, and evaluating cluster expansion effective Hamiltonians with:
  - Occupational degrees of freedom
  - Strain degrees of freedom
  - Displacement degrees of freedom
- High-throughput calculations using:
  - [VASP](https://www.vasp.at)  
  - (update for 1.X in progress) [Quantum Espresso](https://www.quantum-espresso.org/)
- Monte Carlo calculations using:
  - Semi-grand canonical ensemble
  - Canonical ensemble

Collaboration is welcome and new features can be incorporated by forking the repository on GitHub, creating a new feature, and submitting pull requests. If you are interested in developing features that involve a significant time investment we encourage you to first contact the CASM development team at <casm-developers@lists.engr.ucsb.edu>.

For a particular major version number, the ``ccasm`` program interface, including file input and output formats, will remain stable and backwards compatible. The CASM library ``libcasm`` is less stable and may have some breaking changes without changing the major version number.


#### Getting Started

- [Installation and online documentation](https://prisms-center.github.io/CASMcode_docs/)

- [Workshop tutorial slides and demo projects](https://github.com/prisms-center/CASMcode_demo)


#### Developers and Contributors:

CASM is developed by the Van der Ven group, originally at the University of Michigan and currently at the University of California Santa Barbara.

**Lead developers**:  John C. Thomas and Brian Puchala

**Developers**:  John Goiri and Anirudh Natarajan

**Other contributors**: Min-Hua Chen, Jonathon Bechtel, Max Radin, Elizabeth Decolvenaere, Anna Belak, Liang Tian, Naga Sri Harsha Gunda, Julija Vinckeviciute, Sanjeev Kolli

#### Acknowledgements ####

The development of CASM was made possible with support from:

- The U.S. Department of Energy, Office of Basic Energy Sciences, Division of Materials Sciences and Engineering under Award #DE-SC0008637 that funds the PRedictive Integrated Structural Materials Science (PRISMS) Center at University of Michigan.

- The National Science Foundation under Awards DMR-1410242, DMR-1105672, DMR-1436154, and OAC-1642433.


#### Contact:

Contact the developers at <casm-developers@lists.engr.ucsb.edu>.

The CASM development team will periodically send email notifications regarding new releases,
features, and bug fixes to the CASM users notification list. To join the list send an email to <CASM-Users-join@lists.engr.ucsb.edu> or visit <https://lists.engr.ucsb.edu/mailman/listinfo/casm-users> to sign up.

#### License

GNU Lesser General Public License (LGPL). Please see the file LICENSE for details.


#### For Developers

See INSTALL.md
","['bpuchala', 'goirijo', 'jcthomas', 'skk74', 'tallakahath', 'seshasaibehara', 'max-radin', 'anirudhrn', 'julijavin', 'jonaskaufman', 'sriharsha09', 'flwalsh', 'xivh']",1,,0.77,210,,,,,,28,,,
104403797,MDEwOlJlcG9zaXRvcnkxMDQ0MDM3OTc=,OpenFermion-PySCF,quantumlib/OpenFermion-PySCF,0,quantumlib,https://github.com/quantumlib/OpenFermion-PySCF,OpenFermion plugin to interface with the electronic structure package PySCF.,0,2017-09-21 22:11:02+00:00,2025-02-27 01:05:27+00:00,2025-02-27 01:06:32+00:00,,107,107,107,Python,1,0,1,0,0,0,43,0,0,12,apache-2.0,1,0,0,public,43,12,107,master,1,1,"OpenFermion-PySCF
=================

.. image:: https://badge.fury.io/py/openfermionpyscf.svg
    :target: https://badge.fury.io/py/openfermionpyscf

.. image:: https://github.com/quantumlib/OpenFermion-PySCF/workflows/Continuous%20Integration/badge.svg
    :target: https://github.com/quantumlib/OpenFermion-PySCF/actions?query=workflow%3A%22Continuous+Integration%22


`OpenFermion <http://openfermion.org>`__ is an open source library (licensed under Apache 2) for compiling and analyzing quantum algorithms which simulate fermionic systems.
This plugin library allows the electronic structure package `PySCF <http://github.com/sunqm/pyscf>`__ (licensed under BSD-2-Clause) to interface with OpenFermion.

Installation
------------

To start using OpenFermion-PySCF, first install `PySCF
<http://github.com/sunqm/pyscf>`__.
Then, to install the latest versions of OpenFermion and OpenFermion-PySCF (in development mode):

.. code-block:: bash

  git clone https://github.com/quantumlib/OpenFermion-PySCF
  cd OpenFermion-PySCF
  python -m pip install -e .

Alternatively, to install the latest PyPI releases as libraries (in user mode):

.. code-block:: bash

  python -m pip install --user openfermionpyscf

Also be sure to take a look at the `ipython notebook demo <https://github.com/quantumlib/OpenFermion-PySCF/blob/master/examples/openfermionpyscf_demo.ipynb>`__.

How to contribute
-----------------

We'd love to accept your contributions and patches to OpenFermion-PySCF.
There are a few guidelines you need to follow.
Contributions to OpenFermion-PySCF must be accompanied by a Contributor License Agreement.
You (or your employer) retain the copyright to your contribution,
this simply gives us permission to use and redistribute your contributions as part of the project.
Head over to https://cla.developers.google.com/
to see your current agreements on file or to sign a new one.

All submissions, including submissions by project members, require review.
We use GitHub pull requests for this purpose. Consult
`GitHub Help <https://help.github.com/articles/about-pull-requests/>`__ for
more information on using pull requests.
Furthermore, please make sure your new code comes with extensive tests!
We use automatic testing to make sure all pull requests pass tests and do not
decrease overall test coverage by too much. Make sure you adhere to our style
guide. Just have a look at our code for clues. We mostly follow
`PEP 8 <https://www.python.org/dev/peps/pep-0008/>`_ and use
the corresponding `linter <https://pypi.python.org/pypi/pep8>`_ to check for it.
Code should always come with documentation.

Authors
-------

`Ryan Babbush <http://ryanbabbush.com>`__ (Google),
`Jarrod McClean <http://jarrodmcclean.com>`__ (Google),
`Kevin Sung <https://github.com/kevinsung>`__ (University of Michigan),
`Ian Kivlichan <http://aspuru.chem.harvard.edu/ian-kivlichan/>`__ (Harvard),
`Dave Bacon <https://github.com/dabacon>`__ (Google),
`Yudong Cao <https://github.com/yudongcao>`__ (Harvard),
`Chengyu Dai <https://github.com/jdaaph>`__ (University of Michigan),
`E. Schuyler Fried <https://github.com/schuylerfried>`__ (Harvard),
`Craig Gidney <https://github.com/Strilanc>`__ (Google),
`Brendan Gimby <https://github.com/bgimby>`__ (University of Michigan),
`Pranav Gokhale <https://github.com/singular-value>`__ (University of Chicago),
`Thomas Häner <https://github.com/thomashaener>`__ (ETH Zurich),
`Tarini Hardikar <https://github.com/TariniHardikar>`__ (Dartmouth),
`Vojtĕch Havlíček <https://github.com/VojtaHavlicek>`__ (Oxford),
`Oscar Higgott <https://github.com/oscarhiggott>`__ (University College London),
`Cupjin Huang <https://github.com/pertoX4726>`__ (University of Michigan),
`Josh Izaac <https://github.com/josh146>`__ (Xanadu),
`Zhang Jiang <https://ti.arc.nasa.gov/profile/zjiang3>`__ (NASA),
`Xinle Liu <https://github.com/sheilaliuxl>`__ (Google),
`Sam McArdle <https://github.com/sammcardle30>`__ (Oxford),
`Matthew Neeley <https://github.com/maffoo>`__ (Google),
`Thomas O'Brien <https://github.com/obriente>`__ (Leiden University),
`Bryan O'Gorman <https://ti.arc.nasa.gov/profile/bogorman>`__ (UC Berkeley, NASA),
`Isil Ozfidan <https://github.com/conta877>`__ (D-Wave Systems),
`Max Radin <https://github.com/max-radin>`__ (UC Santa Barbara),
`Jhonathan Romero <https://github.com/jromerofontalvo>`__ (Harvard),
`Nicholas Rubin <https://github.com/ncrubin>`__ (Google),
`Daniel Sank <https://github.com/DanielSank>`__ (Google),
`Nicolas Sawaya <https://github.com/nicolassawaya>`__ (Harvard),
`Kanav Setia <https://github.com/kanavsetia>`__ (Dartmouth),
`Hannah Sim <https://github.com/hsim13372>`__ (Harvard),
`Damian Steiger <https://github.com/damiansteiger>`__ (ETH Zurich),
`Mark Steudtner <https://github.com/msteudtner>`__  (Leiden University),
`Qiming Sun <https://github.com/sunqm>`__ (Caltech),
`Wei Sun <https://github.com/Spaceenter>`__ (Google),
`Daochen Wang <https://github.com/daochenw>`__ (River Lane Research),
`Chris Winkler <https://github.com/quid256>`__ (University of Chicago) and
`Fang Zhang <https://github.com/fangzh-umich>`__ (University of Michigan).

How to cite
-----------
When using OpenFermion-PySCF for research projects, please cite:

    Jarrod R. McClean, Kevin J. Sung, Ian D. Kivlichan, Yudong Cao,
    Chengyu Dai, E. Schuyler Fried, Craig Gidney, Brendan Gimby,
    Pranav Gokhale, Thomas Häner, Tarini Hardikar, Vojtĕch Havlíček,
    Oscar Higgott, Cupjin Huang, Josh Izaac, Zhang Jiang, Xinle Liu,
    Sam McArdle, Matthew Neeley, Thomas O'Brien, Bryan O'Gorman, Isil Ozfidan,
    Maxwell D. Radin, Jhonathan Romero, Nicholas Rubin, Nicolas P. D. Sawaya,
    Kanav Setia, Sukin Sim, Damian S. Steiger, Mark Steudtner, Qiming Sun,
    Wei Sun, Daochen Wang, Fang Zhang and Ryan Babbush.
    *OpenFermion: The Electronic Structure Package for Quantum Computers*.
    `arXiv:1710.07629 <https://arxiv.org/abs/1710.07629>`__. 2017.

as well as

    Qiming Sun, Timothy C. Berkelbach, Nick S. Blunt, George H. Booth, Sheng Guo,
    Zhendong Li, Junzi Liu, James McClain, Elvira. R. Sayfutyarova, Sandeep Sharma,
    Sebastian Wouters and Garnet Kin-Lic Chan.
    *The Python-based Simulations of Chemistry Framework (PySCF)*.
    `WIREs Compututational Molecular Science <http://onlinelibrary.wiley.com/doi/10.1002/wcms.1340/full>`__.
    2017.

We are happy to include future contributors as authors on later OpenFermion releases.

Disclaimer
----------
Copyright 2017 The OpenFermion Developers.
This is not an official Google product.
","['babbush', 'kevinsung', 'jarrodmcc', 'sunqm', 'dabacon', 'r-imai-67', 'ncrubin', 'Strilanc', 'mhucka']",1,,0.65,38,"# Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, gender identity and expression, level of
experience, education, socio-economic status, nationality, personal appearance,
race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

*   Using welcoming and inclusive language
*   Being respectful of differing viewpoints and experiences
*   Gracefully accepting constructive criticism
*   Focusing on what is best for the community
*   Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

*   The use of sexualized language or imagery and unwelcome sexual attention or
    advances
*   Trolling, insulting/derogatory comments, and personal or political attacks
*   Public or private harassment
*   Publishing others' private information, such as a physical or electronic
    address, without explicit permission
*   Other conduct which could reasonably be considered inappropriate in a
    professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, or to ban temporarily or permanently any
contributor for other behaviors that they deem inappropriate, threatening,
offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

This Code of Conduct also applies outside the project spaces when the Project
Stewards have a reasonable belief that an individual's behavior may have a
negative impact on the project or its community.

## Conflict Resolution

We do not believe that all conflict is bad; healthy debate and disagreement
often yield positive results. However, it is never okay to be disrespectful or
to engage in behavior that violates the project’s Code of Conduct.

If you see someone violating the Code of Conduct, you are encouraged to address
the behavior directly with those involved. Many issues can be resolved quickly
and easily, and this gives people more control over the outcome of their
dispute. If you are unable to resolve the matter for any reason, or if the
behavior is threatening or harassing, report it. We are dedicated to providing
an environment where participants feel welcome and safe.

Reports should be directed to quantumai-oss-maintainers@googlegroups.com,
the project stewards at Google Quantum AI. They will then work with a committee
consisting of representatives from the Open Source Programs Office and the
Google Open Source Strategy team. If for any reason you are uncomfortable
reaching out to the Project Stewards, please email opensource@google.com.

We will investigate every complaint, but you may not receive a direct response.
We will use our discretion in determining when and how to follow up on reported
incidents, which may range from not taking action to permanent expulsion from
the project and project-sponsored spaces. We will notify the accused of the
report and provide them an opportunity to discuss it before any action is taken.
The identity of the reporter will be omitted from the details of the report
supplied to the accused. In potentially harmful situations, such as ongoing
harassment or threats to anyone's safety, we may take action without notice.

## Attribution

This Code of Conduct is adapted from the Contributor Covenant, version 1.4,
available at
https://www.contributor-covenant.org/version/1/4/code-of-conduct.html
","# How to contribute

We'd love to accept your patches and contributions to this project. We do have
some guidelines to follow, covered in this document, but don't be concerned
about getting everything right the first time! Create a pull request (discussed
below) and we'll nudge you in the right direction.

## Before you begin

### Sign our Contributor License Agreement

Contributions to this project must be accompanied by a [Contributor License
Agreement](https://cla.developers.google.com/about) (CLA). You (or your
employer) retain the copyright to your contribution; the CLA simply gives us
permission to use and redistribute your contributions as part of the project.
Please visit https://cla.developers.google.com/ to see your current agreements
on file or to sign a new one. You generally only need to submit a Google CLA
once, so if you've already submitted one (even if it was for a different
project), you probably don't need to do it again.

> [!WARNING]
> Please note carefully clauses [#5](https://cla.developers.google.com/about/google-corporate#:~:text=You%20represent%20that%20each%20of%20Your%20Contributions%20is%20Your%20original%20creation)
> and [#7](https://cla.developers.google.com/about/google-corporate#:~:text=Should%20You%20wish%20to%20submit%20work%20that%20is%20not%20Your%20original%20creation%2C%20You%20may%20submit%20it%20to%20Google%20separately)
> in the CLA. Any code that you contribute to this project must be **your**
> original creation. Code generated by artificial intelligence tools **does
> not** qualify as your original creation.

### Review our community guidelines

We have a [code of conduct](CODE_OF_CONDUCT.md) to make the project an open and
welcoming community environment. Please make sure to read and abide by the code
of conduct.

## Contribution process

All submissions, including submissions by project members, require review. We
use the tools provided by GitHub for pull requests for this purpose. The
preferred manner for submitting pull requests is to fork the, create a new
branch in this fork to do your work, and when ready, create a pull request from
this branch to the main project repository. The subsections below describe the
process in more detail.

Pleae make sure to follow the [Google Style
Guides](https://google.github.io/styleguide/) in your code, particularly the
[style guide for Python](https://google.github.io/styleguide/pyguide.html).

### Repository forks

1.  Fork the OpenFermion-PySCF repository (you can use the _Fork_ button in
    upper right corner of the [repository
    page](https://github.com/quantumlib/OpenFermion-PySCF)). Forking creates a
    new GitHub repo at the location
    `https://github.com/USERNAME/OpenFermion-PySCF`, where `USERNAME` is your
    GitHub user name.

1.  Clone (using `git clone`) or otherwise download your forked repository to
    your local computer, so that you have a local copy where you can do your
    development work using your preferred editor and development tools.

1.  Check out the `main` branch and create a new [git
    branch](https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell)
    from `main`:

    ```shell
    git checkout main -b YOUR_BRANCH_NAME
    ```

    where `YOUR_BRANCH_NAME` is the name of your new branch.

### Development environment installation

Please refer to the section _Developer install_ of the [installation
instructions](docs/install.md) for information about how to set up a local copy
of the software for development.

### Tests and test coverage

Existing tests must continue to pass (or be updated) when changes are
introduced, and code should be covered by tests. We use
[pytest](https://docs.pytest.org) to run our tests and
[pytest-cov](https://pytest-cov.readthedocs.io) to compute coverage. We use the
scripts [`./check/pytest`](./check/pytest) and
[`./check/pytest-and-incremental-coverage`](./check/pytest-and-incremental-coverage)
to run these programs with custom configurations for this project.

We don't require 100% coverage, but any uncovered code must be annotated with `#
pragma: no cover`. To ignore coverage of a single line, place `# pragma: no
cover` at the end of the line. To ignore coverage for an entire block, start the
block with a `# pragma: no cover` comment on its own line.

### Lint

Code should meet common style standards for Python and be free of error-prone
constructs. We use [Pylint](https://www.pylint.org/) to check for code lint, and
the script [`./check/pylint`](./check/pylint) to run it. When Pylint produces a
false positive, it can be silenced with annotations. For example, the annotation
`# pylint: disable=unused-import` would silence a warning about an unused
import.

### Type annotations

Code should have [type annotations](https://www.python.org/dev/peps/pep-0484/).
We use [mypy](http://mypy-lang.org/) to check that type annotations are correct,
and the script [`./check/mypy`](./check/mypy) to run it. When type checking
produces a false positive, it can be silenced with annotations such as `# type:
ignore`.

### Pull requests and code reviews

1.  If your local copy has drifted out of sync with the `main` branch of the
    main OpenFermion-PySCF repo, you may need to merge the latest changes into
    your branch. To do this, first update your local `main` and then merge your
    local `main` into your branch:

    ```shell
    # Track the upstream repo (if your local repo hasn't):
    git remote add upstream https://github.com/quantumlib/OpenFermion-PySCF.git

    # Update your local main.
    git fetch upstream
    git checkout main
    git merge upstream/main
    # Merge local main into your branch.
    git checkout YOUR_BRANCH_NAME
    git merge main
    ```

    If git reports conflicts during one or both of these merge processes, you
    may need to [resolve the merge conflicts](
    https://docs.github.com/articles/about-merge-conflicts) before continuing.

1.  Finally, push your changes to your fork of the OpenFermion-PySCF repo on
    GitHub:

    ```shell
    git push origin YOUR_BRANCH_NAME
    ```

1.  Now when you navigate to the OpenFermion-PySCF repository on GitHub
    (https://github.com/quantumlib/OpenFermion-PySCF), you should see the option
    to create a new [pull
    requests](https://help.github.com/articles/about-pull-requests/) from your
    forked repository. Alternatively, you can create the pull request by
    navigating to the ""Pull requests"" tab near the top of the page, and
    selecting the appropriate branches.

1.  A reviewer from the OpenFermion-PySCF team will comment on your code and may
    ask for changes. You can perform the necessary changes locally, commit them
    to your branch as usual, and then push changes to your fork on GitHub
    following the same process as above. When you do that, GitHub will update
    the code in the pull request automatically.
","# Reporting security issues

The OpenFermion-PySCF developers and community take security bugs in
OpenFermion-PySCF seriously. We appreciate your efforts to responsibly disclose
your findings, and will make every effort to acknowledge your contributions.

Please **do not** use GitHub issues to report security vulnerabilities; GitHub
issues are public, and doing so could allow someone to exploit the information
before the problem can be addressed. Instead, please use the GitHub [""Report a
Vulnerability""](https://github.com/quantumlib/OpenFermion-PySCF/security/advisories/new)
interface from the _Security_ tab of the OpenFermion-PySCF repository.

Please report security issues in third-party modules to the person or team
maintaining the module rather than the OpenFermion-PySCF project stewards,
unless you believe that some action needs to be taken with OpenFermion-PySCF in
order to guard against the effects of a security vulnerability in a third-party
module.

## Responses to security reports

The project stewards at Google Quantum AI will send a response indicating the
next steps in handling your report. After the initial reply to your report, the
project stewards will keep you informed of the progress towards a fix and full
announcement, and may ask for additional information or guidance.

## Additional points of contact

Please contact the project stewards at Google Quantum AI via email at
quantum-oss-maintainers@google.com if you have questions or other concerns. If
for any reason you are uncomfortable reaching out to the project stewards,
please email opensource@google.com instead.
",,,16,,,
621565867,R_kgDOJQxXqw,Directional-Stimulus-Prompting,Leezekun/Directional-Stimulus-Prompting,0,Leezekun,https://github.com/Leezekun/Directional-Stimulus-Prompting,"[NeurIPS 2023] Codebase for the paper: ""Guiding Large Language Models with Directional Stimulus Prompting""",0,2023-03-30 23:33:45+00:00,2025-02-24 01:59:37+00:00,2023-05-17 04:47:10+00:00,,115233,106,106,Python,1,1,1,1,0,0,12,0,0,4,apache-2.0,1,0,0,public,12,4,106,main,1,,"# 💡DSP: Directional-Stimulus-Prompting

**Directional-Stimulus-Prompting** is a framework that uses a tuneable language model (LM) to provide guidance for the black-box frozen large language model (LLM) towards desirable properties. Specifically, we train a policy LM to generate discrete tokens as *directional stimulus* of each input, which is a hint/cue such as keywords of an article for summarization. The *directional stimulus* is then combined with the original input and fed into the LLM to guide its generation toward the desired target (an example can be seen in **Figure 1**). 

<p align=""center"">
  <img align=""center"" src=""pics/example.png"" width=""600px"" />
</p>
<p align=""left"">
  <b>Figure 1:</b> Comparison of our proposed Directional Stimulus Prompting with the standard prompting method to use the LLM such as GPT-3 on the summarization task. Our DSP uses a tuneable policy LM to generate the stimulus (highlighted in orange color), which is keywords in this case, to guide the LLM on generating the desired summary (highlighted in blue color) with higher rouge scores or other measures like human preference. 
</p>

The policy LM can be trained through (1) `supervised finetuning from annotated data (SFT)` and (2) `reinforcement learning from offline and online rewards (RL)` to explore directional stimulus that better aligns LLMs with human preferences. This framework is flexibly applicable to various LMs and tasks. An illustration of the **DSP** framework is shown in **Figure 2**.

Paper Link: https://arxiv.org/abs/2302.11520

<p align=""center"">
  <img align=""center"" src=""pics/dsp.png"" width=""600px"" />
</p>
<p align=""left"">
  <b>Figure 2:</b> Overview of our proposed framework DSP, which learns a small policy LM to improve the frozen LLM's performance on specific downstream tasks. Given the input, the policy LM generates stimulus to guide the LLM's generation, which is then evaluated with downstream performance measures or human labelers. The evaluation scores are used as rewards to optimize the policy LM with RL. The parameters of LLM are frozen while the policy LM is tuneable.
</p>

Currently, we test the framework on two benchmark tasks: 
 - Summarization
 - Dialogue Generation

Our code is based on [RL4LMs](https://github.com/allenai/RL4LMs). Users can customize the dataset, metrics, and LLM-based reward function to train transformer-based policy LMs, to provide guidance for the LLMs towards the desirable properties.


---
# Install

## Local Installation 
```bash
git clone https://github.com/leezekun/Directional-Stimulus-Prompting.git
cd Directional-Stimulus-Prompting
pip install -e .
```

## Docker
We provide also a Dockerfile for development using docker containers containing all the dependencies.
```bash
docker build . -t dsp
```

## Additional dependencies

Optionally, coreNLP libraries are required for certain metric computations (eg. SPICE) which can be downloaded through `cd rl4lms/envs/text_generation/caption_metrics/spice && bash get_stanford_models.sh`

## Setup OPENAI ACCESS KEY
You should setup your openai access key to call the api. 
`export OPENAI_API_KEY='XXXXXXXX'`


---
# Step 1: Supervised Fine Tuning (SFT)
First, we perform supervised finetuning (SFT) on the policy LM with annotated data to provide a good initial point for the further RL training. The code and data are placed in the `sft4lms` directory. We provide the script to run the SFT for the two tasks:
```bash
sh run_sft_cnndm.sh # for the summarization task on the CNN/Daily Mail dataset
sh run_sft_multiwoz.sh # for the dialogue generation task on the MultiWOZ dataset
```

---
# Step 2: RL Training with PPO/NLPO
This part is based on [RL4LMs](https://github.com/allenai/RL4LMs). A simple training API that can be invoked via train [script](https://github.com/allenai/RL4LMs/blob/main/scripts/training/train_text_generation.py) that allows to train PPO, NLPO or a supervised model by using a config file (YAML). 

We provide the scripts of training the policy LM T5 on the tasks of summarization and dialogue generation. You can run the scripts:
```bash
sh run_ppo_cnndm.sh
sh run_ppo_multiwoz.sh
```

The config files for the summarization and dialogue generation tasks can be found in the `scripts/training/task_configs/summarization_with_hint` and `scripts/training/task_configs/multiwoz_with_hint`, respectively.
You can customize the configuration files as instructed in [RL4LMs](https://github.com/allenai/RL4LMs).

## YAML file schema - Configuring building blocks

Config file contains details about hyper-parameter settings for building blocks which are described below:

- **Dataset/Task**: Dataset containing samples with input prompts and reference sentences. Available datasets are found in the class `DataPoolRegistry` in [registry](https://github.com/allenai/RL4LMs/blob/main/rl4lms/envs/text_generation/registry.py). (See how to create your own dataset [here](#adding-dataset)) 
For our experiments, we customize the datasets of CNN/Daily Mail and MultiWOZ, which are registered as `cnn_daily_mail_with_hint` and `multiwoz_with_hint`:

  ```yaml
  datapool:
    id: cnn_daily_mail_with_hint
    args:
      prompt_prefix: ""Extract the keywords: ""
      n_train: 2000
      n_val: 500
      n_test: 500
      extraction_mode: ""textrank""
      extraction_source: ""all""
  ```
  ```yaml
  datapool:
    id: multiwoz_with_hint
    args:
      version: ""2.0""
      n_train: 80
      n_val: 100
      n_test: 1000
  ```

- **Reward Function**: Reward function which computes token-level scores at each time step of MDP. Available reward functions can be found in the class `RewardFunctionRegistry`. (See how to create your own reward function [here](#adding-reward-function)) We customize the LLM-based reward functions, where the reward is measured on the generation of LLMs guided by stimulus generated by the trained policy LM.


  ```yaml
  reward_fn:
  id: summarization_with_hint
  args:
    gpt3_model: 'gpt-3.5-turbo' 
    interval: 0.5 # arguments for exponential backoff
    timeout: 20.0
    exp: 2.0
    patience: 10
    temperature: 0.7 # arguments for the LLM's inference
    max_tokens: 128
    num_seqs: 4
    top_p: 1.0
    stop_words: [""Article:"", ""Q:"",  ""A:"", ""<|im_end|>""]
    selection_strategy: ""choose_all"" # average all the inferences generated by the LLM
    prompt_prefix: ""Extract the keywords: ""
    prompt_path: ""./prompts/cnn_fs.txt""
    hint_prompt_path: ""./prompts/cnn_hint_fs.txt""
    gpt3_metric: ""rouge-avg"" # metric on the generation of the LLM
    gpt3_coef: 10.
    use_baseline: False
    t5_coef: 0.
    t5_metric: ""hint_hit"" # the customized metric on the keywords generated by the policy LM (t5)
    t5_pos_coef: 1.0
    t5_neg_coef: 0.25 # penalty for the policy LM (t5) if generated a ``wrong'' keyword
    step_reward_coef: 1.0 # set as 0 if not use step reward
    split_token: "";"" # we use "";"" to split multiple keywords
    split_token_id: 117 # token id of "";"" for t5
  ```
  
Note that we conducted the experiments using Codex (**gpt-3.5-turbo**), which has not been supported by OPENAI since March 23rd, 2023. However, you can [apply for either the Codex model access or a research subsidy](https://openai.com/form/researcher-access-program).
You can also try other models by changing the `gpt3_model`.

- **Environment**: Configures a gym-style text generation [environment](https://github.com/allenai/RL4LMs/blob/main/rl4lms/envs/text_generation/env.py) which simulates MDP episodes. Rollouts are generated using train samples from dataset consisting of input and reference texts.
Further, we wrap our env with `SubProcVecEnv` from stable-baselines that processes `n_envs` episodes in parallel using multi-processing to compute step-wise rewards.  
Further configuration settings include: 
  - `max_episode_length` : max length of the episode 
  - `max_prompt_length` - maximum length of the input text to consider 
  - `terminate_on_eos` - whether to terminate the episode as soon as EOS action is performed 
  - `prompt_truncation_side` - truncation side for the prompt text 
  - `context_start_token` - id for context token (corresponds to initial token given to decoder in encoder-decoder models)

  ```yaml
  env:
    n_envs: 10
    args:
      max_prompt_length: 512
      max_episode_length: 100
      terminate_on_eos: True
      prompt_truncation_side: ""right""
      context_start_token: 0
  ```

- **On-policy alg**: We provide implementations of 4 on-policy algorithms: PPO, NLPO, A2C and TRPO adapted from [stable-baselines3](https://github.com/DLR-RM/stable-baselines3) tailored to work with NLP tasks which can be used out-of-the-box with either a causal policy or a seq2seq LM policy. (See how to create your own [on-policy algorithm](#adding-custom-on-policy-algorithms) or [policy](#adding-custom-policies))
  - We also provide a supervised [trainer](https://github.com/allenai/RL4LMs/blob/2863116cd5860e4a4106a76486e70bfac25df2ba/rl4lms/envs/text_generation/training_utils.py#L225) for benchmarking purposes. Supervised Warm start models are already uploaded to Huggingface Hub and specified in the respective config files.
  - Hyper-parameters for the algorithm can be specified at `alg/args`. 
  - Further, all RL algorithms use adaptive KL controller to keep the LM close to original LM by setting initial KL co-efficient (`alg/kl_div/coeff`) and target KL (`alg/kl_div/target_kl`). 
  - We support two types of LM policy: **causal LM policy** (for decoder only models) and **seq2seq LM policy** (for encoder-decoder models). Further for NLPO, we also provide maskable variants of these. Policy implementations can be found [here](https://github.com/allenai/RL4LMs/blob/main/rl4lms/envs/text_generation/policy.py) in and it can be attached to algorithms by specifying `alg/policy/id` and `alg/policy/args`

    ```yaml
    alg:
      id: nlpo
      args: 
        n_steps: 512
        batch_size: 1
        verbose: 1
        learning_rate: 0.000002
        n_epochs: 5
        ent_coef: 0.0
        vf_coef: 0.5
      kl_div:
        coeff: 0.005
        target_kl: 0.5
      policy:
        id: maskable_seq2seq_lm_actor_critic_policy
        args:
          model_name: $MODEL_PATH # the initial checkpoint of the policy LM, use t5-base or the checkpoints trained with SFT in the first step
          apply_model_parallel: True
          prompt_truncation_side: ""right""
          min_tokens_to_keep: 100
          top_mask: 0.9
          mask_type: ""learned_top_p""
          target_update_iterations: 20
          generation_kwargs:
            min_length: 8
            max_new_tokens: 64
            do_sample: True
            top_k: 100         
    ```

- **Trainer Config**: We provide an [On-policy trainer](https://github.com/allenai/RL4LMs/blob/2863116cd5860e4a4106a76486e70bfac25df2ba/rl4lms/envs/text_generation/training_utils.py#L126) - a feature-complete wrapper that instantiates building blocks from their corresponding configs and provides an outer training loop consisting of *train* and *eval* iterations `train_evaluation/n_iters`. 
  - Each iteration corresponds to performing updates with `alg/args/n_steps` x `env/n_envs` of the chosen algorithm. 
  - For every `eval_every` iters, LM is evaluated on validation split using metrics listed in `train_evaluation/metrics` with generation kwargs provided in `train_evaluation/generation_kwargs` (this overrides rollout `alg/policy/generation_kwargs` for inference purposes only)

  We customize the evaluation function, which measures on the generation of the LLM and the trained policy LM T5.

  ```yaml
  # train and evaluation
  train_evaluation:
    eval_batch_size: 10
    n_iters: 20
    eval_every: 2
    save_every: 2
    metrics:
      - id: summarization_with_hint
        args: 
          gpt3_model: 'gpt-3.5-turbo'
          interval: 0.5
          timeout: 20.0
          exp: 2
          patience: 10
          temperature: 0.7
          max_tokens: 128
          num_seqs: 3
          top_p: 1.0
          stop_words: [""Article:"", ""Q:"",  ""A:""]
          selection_strategy: ""choose_all""
          split_token: "";""
          split_token_id: 117 # token id of t5 for "";""
          prompt_prefix: ""Extract the keywords: ""
          prompt_path: ""./prompts/cnn_fs.txt""
          hint_prompt_path: ""./prompts/cnn_hint_fs.txt""
          use_lower_baseline: False
          use_upper_baseline: False
          gpt3_metrics: 
            - id: meteor
              args: {}
            - id: rouge
              args: 
                use_single_ref: False
            - id: bleu
              args: {}
            - id: bert_score
              args:
                language: en
          t5_metrics: 
            - id: ""hint_hit""
              args: 
                split: "";""
    generation_kwargs: # for the trained policy LM T5
      min_length: 8
      max_new_tokens: 64
      do_sample: True
      top_k: 0
      temperature: 0.7
  ```

---
# Custom Building Blocks :wrench:
RL4LMs provide complete customizability - with respect to adding new tasks/datasets, reward functions, evaluation metric, on-policy algorithms and actor-critic policies.

## Adding dataset
Users can create their own datasets by sub-classing [TextGenPool](https://github.com/allenai/RL4LMs/blob/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/data_pools/text_generation_pool.py#L15) just by overriding `prepare(cls, split: str, **args) -> 'TextGenPool':` method to return an instance of TextGenPool. An example is shown below:


 ```python
 from rl4lms.data_pools.text_generation_pool import Sample, TextGenPool

 class MyDataPool(TextGenPool):
    @classmethod
    def prepare(cls, split: str):
        .. 
        samples = []
        for ix, item in enumerate(..):
            sample = Sample(id=f""{split}_{ix}"",
                            prompt_or_input_text=item[""document""],
                            references=[item[""target""]]
                            )
            samples.append(sample)
        pool_instance = cls(samples)
        return pool_instance

```

## Adding reward function
Custom reward funtions can be implemented easily by sub-classing [RewardFunction](https://github.com/allenai/RL4LMs/blob/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/reward.py#L12) (a callable) which takes observation ($s$), next observation ($s'$), action ($a$), done (indicating whether episode is finished) and meta info (containing other information about textual input). Here, [Observation](https://github.com/allenai/RL4LMs/blob/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/observation.py#L11) is a data class object consisting of generated text (at a particular step), prompt text, context text (at that step), reference text which can be used to compute token-level or sentence level rewards.


 ```python
from rl4lms.envs.text_generation.observation import Observation
from rl4lms.envs.text_generation.reward import RewardFunction


class MyRewardFunction(RewardFunction):
    def __init__(self, *args) -> None:
        super().__init__()

    def __call__(self, prev_observation: Observation,
                 action: int,
                 current_observation: Observation,
                 done: bool,
                 meta_info: Dict[str, Any] = None) -> float:
        if done:
            reward = ..
            return reward
        return 0
 ```

:bulb:
In addition to traditional NLG metrics, for quick prototyping, we provide two synthetic reward functions which trains LMs to [generate numbers](https://github.com/allenai/RL4LMs/blob/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/test_reward.py#L8) in increasing order and [generate dates](https://github.com/allenai/RL4LMs/blob/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/test_reward.py#L54). These can be used to quickly test different algorithms and policies. Corresponding configs can be found here ([numbers](https://github.com/allenai/RL4LMs/tree/main/scripts/training/task_configs/synthetic_generate_increasing_numbers), [dates](https://github.com/allenai/RL4LMs/tree/main/scripts/training/task_configs/synthetic_generate_dates))


## Adding custom metrics
Users can create their own evaluation metric which then will be used to periodically evaluate the model on validation split of dataset. This can be done by sub-classing [BaseMetric](https://github.com/allenai/RL4LMs/blob/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/metric.py#L20) which takes prompt texts, generated texts, reference texts, meta_infos, current LM model, split name as inputs and returns a dict with metric name as key and value consisting of tuple of sentence-level scores and corpus level scores. An example is as follows:

 ```python

from rl4lms.envs.text_generation.metric import BaseMetric

class MyMetric(BaseMetric):
    def __init__(self) -> None:
        super().__init__()

    def compute(self,
                prompt_texts: List[str],
                generated_texts: List[str],
                reference_texts: List[List[str]],
                meta_infos: List[Dict[str, Any]] = None,
                model: PreTrainedModel = None,
                split_name: str = None):
        metric_dict = {
            ""custom_metrics/my_metric"": ([0.4, 0.7, 0.9], 0.7)
        }
        return metric_dict
 ```

## Adding custom on-policy algorithms

In addition to supported on-policy algorithms (PPO, NLPO, A2C,TRPO), users can implement their own on-policy algorithms with ease by sub-classing stable-baselines3's [OnPolicyAlgorithm](https://github.com/DLR-RM/stable-baselines3/blob/a697401e032dd4fecbbd4162755ddd707df980d3/stable_baselines3/common/on_policy_algorithm.py#L20). Since we provide [wrappers](https://github.com/allenai/RL4LMs/blob/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/alg_wrappers.py#L67) for on-policy algorithms that handles rollouts using LM policies, environment, computing rewards etc, users just need to implement `train()` method with custom loss functions. 

```python
from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm

class MyOnPolicyAlgorithm(OnPolicyAlgorithm):
    def __init__(**args):
        super().__init__(**args)

    def train(self) -> None:
        # train for n_epochs epochs
        for epoch in range(self.n_epochs):
            # Do a complete pass on the rollout buffer
            for rollout_data in self.rollout_buffer.get(self.batch_size):
              # compute loss
```

## Adding custom policies

We provide LM based actor-critic policy [implementations](https://github.com/allenai/RL4LMs/blob/main/rl4lms/envs/text_generation/policy.py) that wraps causal LM and seq2seq LMs. These can be also extended (for eg: use a different critic architecture) by overriding appropriate methods (eg. `evaluate_actions()`)

## Registry
Finally, just register your custom components by adding them to corresponding [registry](https://github.com/allenai/RL4LMs/blob/main/rl4lms/envs/text_generation/registry.py), after which they can be used directly from configs similar to pre-defined components :wave:

## Crowdsourcing templates

We have provided the crowdsourcing templates we used on mechanical turk, along with example inputs in `scripts/crowdworking_templates`. You might find these a helpful starting point either for evaluating your own model's generations, or for gathering training data for a learned reward function.

---

# Logging and Experiment Results

Additionally, we support WANDB logging and warm-starting of training by storing checkpoints and other training artifacts in a user-specified path. This is especially useful for running preemptible jobs on large, scheduled clusters.

Artifacts include (1) jsonl file containing rollout infos at specified intervals (2) jsonl file containing training infos at specified intervals (3) jsonl file containing validation metrics at specified intervals (4) jsonl file containing test metrics before and after training (5) json file with validation predictions at specified intervals (6) json file with test predictions before and after training (7) trained LM model (8) config json used to run the experiment

Complete usage is as follows:

```bash 
WANDB_API_KEY=<YOUR-WANDB-API-KEY-HERE>  python scripts/training/train_text_generation.py \
--config_path <PATH-TO-CONFIG-FILE> \
--experiment_name <EXPERIMENT-NAME> \
--base_path_to_store_results <PATH-TO-STORE-RESULTS> \
--log_to_wandb
```

---

# Citation

```bibtex
@article{li2023guiding,
  title={Guiding Large Language Models via Directional Stimulus Prompting},
  author={Li, Zekun and Peng, Baolin and He, Pengcheng and Galley, Michel and Gao, Jianfeng and Yan, Xifeng},
  journal={arXiv preprint arXiv:2302.11520},
  year={2023}
}
```

# Acknowledgement?
We thank the authors of [RL4LMs](https://github.com/allenai/RL4LMs) for sharing their code. You can contact Zekun Li (`zekunli@cs.ucsb.edu`), if there are questions related to the code.
",['Leezekun'],1,,0.68,0,,,,,,2,,,
18089790,MDEwOlJlcG9zaXRvcnkxODA4OTc5MA==,HandGestureApp,eaglesky/HandGestureApp,0,eaglesky,https://github.com/eaglesky/HandGestureApp,My course project for CS290I in UCSB,0,2014-03-25 05:29:29+00:00,2024-08-14 09:50:54+00:00,2016-01-13 09:27:43+00:00,,70275,105,105,C++,1,1,1,1,0,0,53,0,0,6,mit,1,0,0,public,53,6,105,master,1,,"# Hand Gesture Recognition App

Yalun Qin (allenchin1990@gmail.com)

03/24/2014

## Acknowledgments:

1. The idea of presampling is from Simen Andresen's [blog](http://simena86.github.io/blog/2013/08/12/hand-tracking-and-recognition-with-opencv/) 
2. Used LibSVM on Android implemented by Kun Li. [link](https://github.com/cnbuff410/Libsvm-androidjni)
3. Used [aFileChooser library](https://github.com/iPaulPro/aFileChooser) to implement the file chooser.
4. To implement mapping human gestures to Android apps, [this article](http://blog.csdn.net/qinjuning/article/details/6867806) is quite helpful.

**This is the new Android Studio project! If you miss the old Eclipse ADT project, check out the other branch ""eclipse_adt_old""**

**@raquezha also made great efforts in making the original project work with Android Studio. Check out his project https://github.com/raquezha/HandGestureApp too. Thanks for his work!** 

## Installation:

1. Before importing the project into Android Studio (mine is the 1.5 version), make sure you have already installed and configured related tools, including Android SDK, Android NDK. In this project Android 4.4.2 and OpenCV 2.4.9 are used. **I have already imported the OpenCV for Android Library and its native code so you don't have to download or import it yourself ^_^**. 
You can download and install NDK from ""Project Structure"" -> ""SDK Location"" -> ""Download Android NDK"", or download from the website(http://developer.android.com/ndk/downloads/index.html) and follow the instructions to install(make sure you fill in the NDK path in ""Project Structure""/""SDK Location""/""Android NDK location""). Then **be sure to follow this post (http://kn-gloryo.github.io/Build_NDK_AndroidStudio_detail/, [video](https://www.youtube.com/watch?v=RmPuwdxR1qs)) to configure NDK tools**. This will make your life with NDK much easier! 

2. Import the project into Android Studio. Build the jni libraries libHandGestureApp.so and libsignal.so by right click on app/sr c/main/jni and choose NDK -> ndk build. This will call the ndk build tool and generate those two jni libraries in app/src/main/libs. This must be done if you change the c++ source files in the jni folder. 

3. Make sure OpenCV Manager has been installed on your Android phone and then build and run the project!

## Usage:

1. The first mode you are in upon starting the app is background sampling mode. You can lower the resolution using the option in the menu to make the app run faster. After finished, just touch the screen and you will be in the second mode: hand sampling mode. You need to cover the seven squares with your hand. This step is crucial to the performance and I suggest you bend your hand a little so that the shadow could also be sampled. 

2. After presampling you can touch the screen again and go into next mode to see the segmented hand. Touch again and you can see the extracted features, along with three buttons. In this mode you can add new gestures to the gesture database, train the SVM model or begin testing. The deletion of gestures has not been implemented yet so be careful when you want to add a new one. All the data of the gestures are stored in the ExternalStorageDirectory/MyDataSet folder. If you don't know the location on your phone, just click ""Add Gesture"" button and try to add a new gesture, the folder will be generated automatically in the external storage directory. You can use the existing data set which is located in the project directory. You can click ""Test"" button to see the realtime recognition performance of SVM classifier, but make sure the model has already been generated, i.e., trained. 

3. To add a new gesture to the training set, simply make a static gesture before the camera and click ""Add Gesture"" button. If you want to increase the number of a gesture that is already in the training set, you need to click ""Data Collection"" in the menu and choose the image representing the gesture, and then click ""Add Gesture"" button. After you click that button, the app will start gathering data immediately for a certain number of frames(currently the number is set to be 10). Finally a dialog will pop up asking you wheather to save those data or not.

4. This app can also let you quickly launch other apps using gestures. But first you need to map your gestures to the apps by selecting ""Map Apps"" in the menu. The selection is stored in the app so you don't need to map it every time you run the app. Then you can test it by choosing the ""App Test"" in the menu.

A demo of the app can be found on YouTube: http://youtu.be/PF6hY-0VuN4

Project page: http://eaglesky.github.io/2015/12/26/HandGestureRecognition/ 

If you find any problem about the code, **feel free to create an issue to report it**. Also feel free to refactor the code and send pull requests to me. It would be great if we could improve this project together!

",['eaglesky'],1,,0.74,0,,,,,,18,,,
501407352,R_kgDOHeLeeA,greed,ucsb-seclab/greed,0,ucsb-seclab,https://github.com/ucsb-seclab/greed,A symbolic execution engine for EVM smart contract binaries.,0,2022-06-08 20:52:38+00:00,2025-02-25 00:23:19+00:00,2025-02-25 00:23:15+00:00,https://ucsb-seclab.github.io/greed/,10546,102,102,Python,1,1,1,1,1,0,16,0,0,8,mit,1,0,0,public,16,8,102,main,1,1,"# greed
![ubuntu](https://img.shields.io/badge/Ubuntu-20.04+-yellow)
[![python](https://img.shields.io/badge/Python-3.8+-3776AB.svg?style=flat&logo=python&logoColor=white)](https://www.python.org)
![Version](https://img.shields.io/badge/Release-v1.0.0-red)
[![Tests](https://github.com/ucsb-seclab/greed/actions/workflows/python-app.yml/badge.svg)](https://github.com/ucsb-seclab/greed/actions/workflows/python-app.yml)
[![License](https://img.shields.io/github/license/Ileriayo/markdown-badges?style=flat)]([https://pypi.org/project/ethpwn/](https://raw.githubusercontent.com/ethpwn/ethpwn/main/LICENSE))
[![Docs](https://img.shields.io/badge/Documentation-gh_pages)](https://ucsb-seclab.github.io/greed/)

<img align=""left"" width=""250""  src=""logo.png"">

<!-- [![Tests](https://github.com/ucsb-seclab/greed/actions/workflows/python-app.yml/badge.svg)](https://github.com/ucsb-seclab/greed/actions/workflows/python-app.yml) -->

### ⚡️ Installation
```bash
# Clone this repo
git clone git@github.com:ucsb-seclab/greed.git
# Create a virtual environment (e.g., using virtualenvwrapper)
mkvirtualenv greed
# Activate the virtual environment
workon greed
# Install greed (will setup gigahorse, yices, and `pip install -e greed`)
cd greed
./setup.sh
```

### 🚀 Usage
First, the contract needs to be pre-processed with `gigahorse`. This can be done in two ways:
```bash
# IMPORTANT: create a new folder. The analyses will pollute the current working directory
mkdir /tmp/test_contract
cd /tmp/test_contract/

# OPTION 1: From the solidity source
cp <contract_source> contract.sol
analyze_source.sh contract.sol

# OPTION 2: From the contract bytecode
cp <contract_bytecode> contract.hex
analyze_hex.sh contract.hex
```

Then, to use `greed` in your python project:
```python
from greed import Project

p = Project(target_dir=""/tmp/test_contract/"")

entry_state = p.factory.entry_state(xid=0)
simgr = p.factory.simgr(entry_state=entry_state)
simgr.run()
```

Or to run `greed` from the command line:
```bash
greed /tmp/test_contract [--debug] [--find <address>]
```

### 🚦 Testing
```bash
cd greed/tests

# Run the full test suite with pytest
pytest

# Or manually run a single test
./test_math.py --debug
```

### 🧱 Architecture
#### Offline representation

* `Project`: calls the TAC_Parser to parse functions, blocks, and statements from Gigahorse
  * `Factory`: used to access several objects
  * `Function(s)`: contain blocks + an intra-procedural CFG
    * `Block(s)`: contain statements
      * `Statement(s)`: represent TAC operations. Every statement has a `.handle(state)` method that given a state applies such operations to derive its successors

#### Runtime representation

* `SimulationManager`: stores and manages states in ""stashes""
  * `State(s)`: hold the transaction context at every step
    * `Storage`: symbolic modulo 2^256 store
    * `Memory`: symbolic modulo 2^256 store
    * `Registers`: symbolic modulo 2^256 store

### 🎓 Academia 

If you are using greed for an academic publication, we would really appreciate a citation to the following work:

```
@inproceedings{gritti2023confusum,
title={Confusum contractum: confused deputy vulnerabilities in ethereum smart contracts},
author={Gritti, Fabio and Ruaro, Nicola and McLaughlin, Robert and Bose, Priyanka and Das, Dipanjan and Grishchenko, Ilya and Kruegel, Christopher and Vigna, Giovanni},
booktitle={32nd USENIX Security Symposium (USENIX Security 23)},
pages={1793--1810},
year={2023}
}

@inproceedings{ruaro2024crush,
title={Not your Type! Detecting Storage Collision Vulnerabilities in Ethereum Smart Contracts},
author={Ruaro, Nicola and Gritti, Fabio and McLaughlin, Robert and Grishchenko, Ilya and Kruegel, Christopher and Vigna, Giovanni},
booktitle={Network and Distributed Systems Security (NDSS) Symposium 2024},
year={2024}
}
```
","['ruaronicola', 'degrigis', 'robmcl4', 'ylya', 'MingxuanYao0528', 'ph4ge', 'bossjoker1', 'syang-ng', 'vtreees']",1,,0.83,0,,,,,,8,,,
243450105,MDEwOlJlcG9zaXRvcnkyNDM0NTAxMDU=,VSGNet,ASMIftekhar/VSGNet,0,ASMIftekhar,https://github.com/ASMIftekhar/VSGNet,VSGNet:Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions.,0,2020-02-27 06:43:39+00:00,2024-12-14 00:33:40+00:00,2022-12-27 15:37:41+00:00,,5940,102,102,Python,1,1,1,1,0,0,20,0,0,5,mit,1,0,0,public,20,5,102,master,1,,"# VSGNet 
### [**VSGNet:Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions**](http://openaccess.thecvf.com/content_CVPR_2020/papers/Ulutan_VSGNet_Spatial_Attention_Network_for_Detecting_Human_Object_Interactions_Using_CVPR_2020_paper.pdf) 

[Oytun Ulutan*](https://sites.google.com/view/oytun-ulutan), [A S M Iftekhar*](https://sites.google.com/view/asmiftekhar/home), [B S Manjunath](https://vision.ece.ucsb.edu/people/bs-manjunath).

Official repository of our [**CVPR 2020**](http://cvpr2020.thecvf.com/) paper.

![Overview of VSGNET](https://github.com/ASMIftekhar/VSGNet/blob/master/7850-teaser.gif?raw=true)
## Citing
If you find this work useful, please consider our paper to cite:

	 @InProceedings{Ulutan_2020_CVPR,
	author = {Ulutan, Oytun and Iftekhar, A S M and Manjunath, B. S.},
	title = {VSGNet: Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions},
	booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2020}
	}



## Results on HICO-DET and V-COCO



## Our Results on V-COCO dataset

|Method| mAP (Scenario 1)|
|:---:|:---:|
|[InteractNet](https://arxiv.org/pdf/1704.07333.pdf)| 40.0|
|[Kolesnikov et al.](http://openaccess.thecvf.com/content_ICCVW_2019/html/SGRL/Kolesnikov_Detecting_Visual_Relationships_Using_Box_Attention_ICCVW_2019_paper.html)| 41.0|
|[GPNN](https://arxiv.org/abs/1808.07962)| 44.0 |
|[iCAN](https://arxiv.org/abs/1808.10437)| 45.3  |
|[Li et al.](https://arxiv.org/abs/1811.08264)| 47.8 |
|[**VSGNet**](https://arxiv.org/abs/2003.05541)| **51.8** |

## Our Results on HICO-DET dataset

**Object Detector Pre-trained on COCO**
|Method| mAP (Full) | mAP (Rare) | mAP (None-Rare)|
|:---:|:---:|:---:|:---:|
|[HO-RCNN](http://www-personal.umich.edu/~ywchao/publications/chao_wacv2018.pdf)| 7.81 | 5.37 | 8.54 | 
|[InteractNet](https://arxiv.org/pdf/1704.07333.pdf)|9.94 | 7.16| 10.77| 
|[GPNN](https://arxiv.org/abs/1808.07962)| 10.61  | 7.78 | 11.45 | 
|[iCAN](https://arxiv.org/abs/1808.10437)| 14.84  | 10.45 | 16.15 | 
|[Li et al.](https://arxiv.org/abs/1811.08264)| 17.03   | 13.42 | 18.11 | 
|[**VSGNet**](https://arxiv.org/abs/2003.05541)| **19.8**  | **16.05** | **20.91** | 

**Object Detector Fine-Tuned on HICO**

We use the object detection results from [DRG](https://github.com/vt-vl-lab/DRG).
|Method| mAP (Full) | mAP (Rare) | mAP (None-Rare)|
|:---:|:---:|:---:|:---:|
|[UniDet](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600494.pdf)|17.58 |11.72 |19.33 |
|[IP-Net](https://arxiv.org/pdf/2003.14023.pdf) | 19.56 |12.79| 21.58 |
|[PPDM](https://arxiv.org/pdf/1912.12898v1.pdf) |21.10 |14.46| 23.09| 
|[Functional](https://arxiv.org/pdf/1904.03181.pdf) |21.96 |16.43|23.62| 
|[VCL](https://github.com/zhihou7/VCL)|23.63 |17.21 |25.55 |
|[ConsNet](https://github.com/YLiuEric/ConsNet)|24.39 |17.10 |26.56|
|[DRG](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570681.pdf)|24.53 |19.47 |26.04 |
|[IDN](https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network))|26.29|**22.61**|27.39|
|[**VSGNet**](https://arxiv.org/abs/2003.05541)| **26.54**| 21.26 | **28.12** |


## Installation
1. Clone repository (recursively):
```Shell
git clone --recursive https://github.com/ASMIftekhar/VSGNet.git
```
2. Download data,annotations,object detection results:
```Shell
bash download_data.sh
```
You need to have wget and unzip packages to execute this script. Alternatively you can download the data from [here](https://drive.google.com/drive/folders/1J8mN63bNIrTdBQzq7Lpjp4qxMXgYI-yF?usp=sharing).
If you execute the script then there will be two folders in the directory ""All\_data"" and ""infos"". This will take close to 10GB space. This contains both of the datasets and all the essential files. Also, if you just want to work with v-coco, download ""All_data_vcoco"" from the link.  

Inside the All\_data folder you will find the following subdirectories.

**a.Data_vcoco**: It will contain all training and validation images of v-coco inside train2014 subdirectory and all test images of v-coco inside val2014 subdirectory.

**b.Annotations\_vcoco**: It will contain all annotations of training, validation and testing set in three json files. The annotations are taken from v-coco API and converted into our convenient format. For example, lets consider there is only one single image annotated with two verbs ""smile"" and ""hold"" along with two person and object bounding boxes. The annotation for this image will be arranged as follows:

```
	{image_id:[{'Verbs': 'hold',
  	'object': {'obj_bbx': [305.84, 59.12, 362.34, 205.22]},
  	'person_bbx': [0.0, 0.63, 441.03, 368.86]},
 	{'Verbs': 'smile',
  	'object': {'obj_bbx': []},
  	person_bbx': [0.0, 0.63, 441.03, 368.86]}]}
```
**c.Object\_Detections\_vcoco**: It will contain all object detection results for v-coco. 

**d.v-coco**: It will contain original v-coco API. This is needed for doing evaluations.

**e.Data_hico**: It will contain all the training images of HICO-DET inside train2015 subdirectory and all test images of HICO_DET inside test2015 subdirectory.

**f.Annotations\_hico**: same as folder (b) but for HICO_DET dataset.

**g.Object\_Detections\_hico**: same as folder (c) but for HICO_DET dataset.

**h.bad\_Detections\_hico**: It will contain the list of images in HICO_DET dataset where our object detector fails to detect any person or object.

**j.hico\_infos**: It will contain additional files required to run training and testing in HICO_DET.

3. To install all packages (preferable to run in a python2 virtual environment):
```
pip2 install -r requirements.txt
```
For HICO_DET evaluation we will use python3 environment, to install those packages (preferable to run in a python3 virtual environment):
```
pip3 install -r requirements3.txt
```
Run only compute_map.sh in a python 3 enviornment. For all other use python 2 environment.

4. If you do not wish to move ""All\_data"" folder from the main directory then you dont need to do anything else to setup the repo. Otherwise you need to run setup.py with the location of All\_data. If you put it in /media/ssd2 with a new name of ""data"" then you need to execute the following command:
```
python2 setup.py -d /media/ssd2/data/
```

## Downloading the Pre-Trained Models:
To download the pre-trained models for the results reported in the paper:
```Shell
bash download_res.sh
```
This will store the model for v-coco in 'soa_paper' folder and the model for HICO_DET in 'soa_paper_hico'. Alternatively you can download the models from [here](https://drive.google.com/drive/folders/1J8mN63bNIrTdBQzq7Lpjp4qxMXgYI-yF?usp=sharing).

## Evaluation in V-COCO


To store the best result in v-coco format run(inside ""scripts/""):
```Shell
CUDA_VISIBLE_DEVICES=0 python2 main.py -fw soa_paper -ba 8 -r t -i t
```
You can use as many gpus as you wish. Just add the necessary gpu ids in the given command.

The outputs that will be shown in the console is basically Average Precision in test set without considering bounding boxes. 

To see the results in original v-coco scheme:
```Shell
python2 calculate_map_vcoco.py -fw soa_paper -sa 34 -t test
```
## Evaluation in HICO_DET


To store the best result in HICO_DET format run (inside ""scripts_hico/""):
```Shell
CUDA_VISIBLE_DEVICES=0 python2 main.py -fw soa_paper_hico -ba 8 -r t -i t
```
You can use as many gpus as you wish. Just add the necessary gpu ids in the given command.

The outputs that will be shown in the console is basically Average Precision in test set without considering bounding boxes. 

To see the results in original HICO_DET scheme run (inside ""scripts_hico/HICO_eval/"")
```Shell
bash compute_map.sh soa_paper_hico 20
```
The evaluation code has been adapted from the [No-Frills repository.](https://github.com/BigRedT/no_frills_hoi_det)Here, 20 indicates the number of cpu cores to be used for evaluation, this can be changed to any number based on the system. 
## Training in V-COCO

To train the model from scratch (inside ""scripts/""):
```
CUDA_VISIBLE_DEVICES=0 python2 main.py -fw new_test -ba 8 -l 0.001 -e 80 -sa 20 
```
**Flags description**:

**-fw:** Name of the folder in which the result will be stored.

**-ba:** Batch size.

**-l:** Learning rate.

**-e:** Number of epochs.

**-sa:** After how many epochs the model would be saved, remember by default for every epoch the best model will be saved. If someone wants to store the model at a particular epoch then this flag should be used.

To understand the flags more please consult main.py. The given example is a typical hyperparameter settings. The model converges normally within 40 epochs. Again,you can use as many gpus as you wish. Just add the necessary gpu ids in the given command. After running the model,  to store the results in v-coco format (inside ""scripts/""):
```
CUDA_VISIBLE_DEVICES=0 python2 main.py -fw new_test -ba 8 -r t -i t
 ```
Lets consider the best result is achieved at 30th epoch then to evaluate the result in original V-COCO scheme(inside ""scripts/""):
```
python2 calculate_map_vcoco.py -fw new_test -sa 30 -t test
```
## Training in HICO_DET

To train the model from scratch (inside ""scripts_hico/""):
```
CUDA_VISIBLE_DEVICES=0 python2 main.py -fw new_test -ba 8 -l 0.001 -e 80 -sa 20 
```
The flags are same as v-coco. The model converges normally within 30 epochs. Again,you can use as many gpus as you wish. Just add the necessary gpu ids in the given command. We have used 4 2080Tis to train HICO_DET with a batch size of 8 per gpu. It takes around 40 minutes per epoch.  
After running the model, to store the results in HICO_DET format (inside ""scripts_hico/""):
```
CUDA_VISIBLE_DEVICES=0 python2 main.py -fw new_test -ba 8 -r t -i t
```
To evaluate the result in original HICO_DET scheme (inside ""scripts_hico/HICO_eval/""):
```Shell
bash compute_map.sh new_test 20
```

Please contact A S M Iftekhar (iftekhar@ucsb.edu) for any queries.
","['ASMIftekhar', 'oulutan']",1,,0.75,0,,,,,,10,,,
717596293,R_kgDOKsWmhQ,CodeScope,WeixiangYAN/CodeScope,0,WeixiangYAN,https://github.com/WeixiangYAN/CodeScope,[ACL 2024] CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,0,2023-11-12 00:16:50+00:00,2025-02-18 07:00:22+00:00,2024-07-29 21:12:58+00:00,,28483,97,97,Python,1,1,1,1,0,0,9,0,0,0,mit,1,0,0,public,9,0,97,main,1,,"# [CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation](https://haitianliu22.github.io/code-scope-benchmark/)



<div align=""center"">
    <a href=""https://haitianliu22.github.io/code-scope-benchmark/""><img src=""./images/leaderboard.png"">Leaderboard</a> &nbsp;&nbsp;|&nbsp;&nbsp;
    <a href=""https://arxiv.org/abs/2311.08588"">📄 Paper</a> &nbsp;&nbsp;|&nbsp;&nbsp;
    <a href=""https://huggingface.co/datasets/WeixiangYan/CodeScope"">🤗 Access from HuggingFace datasets</a> &nbsp;&nbsp;|&nbsp;&nbsp;
    <a href=""https://drive.google.com/file/d/1kg3KICQZekpaQyCAGt_qTPR6ag5MBT_y/view?usp=sharing""><img src=""./images/google_drive.png""> Access from Google Drive datasets</a>
</div>

<br>

**CodeScope**, an execution-based, multilingual, multi-task, multi-dimensional evaluation benchmark for comprehensively gauging LLM capabilities on coding tasks. CodeScope covers **43 programming languages** and **8 coding tasks**. It evaluates the coding performance of LLMs from three dimensions (perspectives): **difficulty**, **efficiency**, and **length**.

## 🌈 Update
* **[2024.05.15]** CodeScope was accepted into the **ACL 2024 Main Conference**, thanking the academic community for its recognition.
* **[2023.11.15]** 🎉🎉🎉 CodeScope is published！🎉🎉🎉



## Datasets
🤗[Hugging Face](https://huggingface.co/datasets/WeixiangYan/CodeScope) or  <img src=""./images/google_drive.png"">[Google Drive](https://drive.google.com/file/d/1kg3KICQZekpaQyCAGt_qTPR6ag5MBT_y/view?usp=sharing) or [Github Data](https://github.com/WeixiangYAN/CodeScope/tree/main/data)


## Code
CodeScope evaluates the comprehensive ability of LLMs in **code understanding** and **code generation** from **eight** coding tasks.

### Code Understanding
1. [Code Summarization](./code_summarization)
2. [Code Smell](./code_smell)
3. [Code Review](./code_review)
4. [Automated Testing](./automated_testing)

### Code Generation
5. [Program Synthesis](./program_synthesis)
6. [Code Translation](./code_translation)
7. [Code Repair](./code_repair)
8. [Code Optimization](./code_optimization)



## Citation
Please cite the paper if you use the data or code from CodeScope.
```
@misc{yan2023codescope,
      title={CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation},
      author={Weixiang Yan and Haitian Liu and Yunkun Wang and Yunzhe Li and Qian Chen and Wen Wang and Tingyu Lin and Weishan Zhao and Li Zhu and Shuiguang Deng and Hari Sundaram},
      year={2023},
      eprint={2311.08588},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Contact
For questions, please feel free to reach out via email at ``weixiangyan@ucsb.edu``.

",['WeixiangYAN'],1,,0.63,0,,,,,,2,,,
506751748,R_kgDOHjRrBA,SpaceControl,raphaelradna/SpaceControl,0,raphaelradna,https://github.com/raphaelradna/SpaceControl,Space Control: Spatial Composition Workstation,0,2022-06-23 18:34:42+00:00,2024-10-31 02:29:30+00:00,2022-09-25 19:12:31+00:00,,5,94,94,,1,1,1,1,0,0,2,0,0,1,,1,0,0,public,2,1,94,main,1,,"# Space Control

This repository holds distributions of Space Control, a software application for sound spatialization.

![SpaceControl_1000x563](https://www.create.ucsb.edu/uploads/SpaceControl_1000x563.png)

Space Control is a multitrack workstation dedicated to the design, realization, and mixture of spatial gestures for electroacoustic music composition. With its simple interface and minimal learning curve, it makes quick and powerful spatialization available to users of all experience levels.

Features of the software include:
- Spatialization using up to 128 sources and between 4 and 24 output channels
- Real-time playback, metering, and editing
- Offline (faster than realtime) export in multi-mono or interleaved formats
- Virtual mixer for balancing sources in the spatial mix
- Per-source volume automation
- Waveform zoom functionality enabling spatialization on the microsound timescale
- Algorithmic transformations of spatial trajectories
- Bass management option with adjustable subwoofer level and crossover frequency
- Scalable interface with 4 magnification levels
- Ability to save and load user project files

The [Quick Start video](https://youtu.be/hYJ57K_lLjA) demonstrates the basic functionality of the software.

Space Control was created by the team of Professor João Pedro Oliveira, acting as project manager, and software developer Raphael Radna. It is a joint project of the UC Santa Barbara Department of Music and the Center for Research in Electronic Art Technology (CREATE), and was supported by a Faculty Research Grant from the UC Santa Barbara Academic Senate.

Space Control runs on Mac and Windows computers, and is available as a free download from the ""Releases"" section on the right-hand side of this page.

Note: Some Windows users have reported that their antivirus software blocked important program files when extracting the Space Control distribution. If you encounter problems when running Space Control on Windows, temporarily deactivate any antivirus software and reinstall.
",['raphaelradna'],1,,0.8,3533,,,,,,8,,,
624565881,R_kgDOJToeeQ,DiffSTE,UCSB-NLP-Chang/DiffSTE,0,UCSB-NLP-Chang,https://github.com/UCSB-NLP-Chang/DiffSTE,,0,2023-04-06 18:56:11+00:00,2025-02-27 05:07:08+00:00,2024-08-01 15:48:13+00:00,,6741,91,91,Python,1,1,1,0,0,0,7,0,0,8,mit,1,0,0,public,7,8,91,main,1,1,"# Improving Diffusion Models for Scene Text Editing with Dual Encoders
[Jiabao Ji](https://question406.github.io)<sup>1</sup>,
[Guanhua Zhang](https://ghzhang233.github.io)<sup>1</sup>,
[Zhaowen Wang](http://www.ifp.illinois.edu/~wang308/)<sup>2</sup>,
[Bairu Hou](https://hbr690188270.github.io)<sup>1</sup>,
[Zhifei Zhang](https://zzutk.github.io)<sup>2</sup>,
[Brian Price](https://www.brianpricephd.com)<sup>2</sup>,
[Shiyu Chang](https://code-terminator.github.io/)<sup>1</sup>
<br>
<sup>1</sup>UC, Santa Barbara, <sup>2</sup>Adobe Research


This is the official implementation of the paper ""Improving Diffusion Models for Scene Text Editing with Dual Encoders"" \[[Arxiv](https://arxiv.org/abs/2304.05568)\].

## Overview
In this work, we propose a novel Diffusion-based Scene Text Editing (DiffSTE) framework, which is able to edit scene text into different font styles and colors following given text instruction. Specifically, we propose to improve pre-trained diffusion models with a dual encoder design, which includes a character encoder for better text legibility and an instruction encoder for better style control. We then utilize an instruction tuning framework to train our model learn the mapping from the text instruction to the corresponding image with either the specified style or the style of the surrounding texts in the background. Such a training method further brings our model the zero-shot generalization ability to the following three scenarios: generating text with unseen font variation, e.g. italic and bold, mixing different fonts to construct a new font, 
and using more relaxed forms of natural language as the instructions to guide the generation task.

## Quick Start
Assuming conda has already installed, you could use the following commands to try our model for a quick start.
```bash
conda create -n diffste python=3.8
conda activate diffste
pip install -r requirements.txt
gdown https://drive.google.com/uc?id=1fc0RKGWo6MPSJIZNIA_UweTOPai64S9f
python generate.py --ckpt_path diffste.ckpt --in_image examples/sample0.png --in_mask examples/mask0.png --text wizards --out_dir ./
```

## Requirements
Build the environment with the following command:
```bash
conda create -n diffste python=3.8
conda activate diffste
pip install -r requirements.txt
```
Our pretrained model can be downloaded from [here](https://drive.google.com/file/d/1fc0RKGWo6MPSJIZNIA_UweTOPai64S9f/view?usp=share_link).

## Generation samples

### Scene text editing
Run following command to edit scene text. The mask file indicates the region where the generated text locates.
```bash
python generate.py --ckpt_path ${model_path} --in_image examples/sample0.png --in_mask examples/mask0.png --text wizards --out_dir ${output_dir}
```
You should be able to get a similar result:

<img src=""assets/edit.png"" width=""400"">

### Specify text style
Specify the font and color of the generated text by adding `--font` and `--color` arguments.
```bash
python generate.py --ckpt_path ${model_path} --in_image examples/sample1.png --in_mask examples/mask1.png --text five --font Courgette --color red --out_dir ${output_dir}
```
You should be able to get a similar result:

<img src=""assets/stylecond.png"" width=""400"">

Specify the text style with a natural language instruction.
```bash
python generate.py --ckpt_path ${model_path} --in_image examples/sample2.png --in_mask examples/mask2.png --text STAFF --instruction ""The word \""STAFF\"" is colored in a delicate, ladylike shade of lilac"""" --out_dir ${output_dir}
```
You should be able to get a similar result:

<img src=""assets/naturallang.png"" width=""400"">

### Font variation
Generate text with unseen font variation, e.g. italic and bold. Notice that NovaMono font has no italic and bold version from google-fonts library.
```bash
python generate.py --ckpt_path ${model_path} --in_image examples/sample3.png --in_mask examples/mask3.png --text STATION --font NovaMono --out_dir ${output_dir}
python generate.py --ckpt_path ${model_path} --in_image examples/sample3.png --in_mask examples/mask3.png --text STATION --font NovaMono-Italic --out_dir ${output_dir}
python generate.py --ckpt_path ${model_path} --in_image examples/sample3.png --in_mask examples/mask3.png --text STATION --font NovaMono-Bold --out_dir ${output_dir}
python generate.py --ckpt_path ${model_path} --in_image examples/sample3.png --in_mask examples/mask3.png --text STATION --font NovaMono-BoldItalic --out_dir ${output_dir}
```
You should be able to get similar results:

<img src=""assets/font-extro.png"" width=""600"">

Mix two different font styles.
```bash
python generate.py --ckpt_path ${model_path} --in_image examples/sample4.png --text Reload --font Allura --out_dir ${output_dir}
python generate.py --ckpt_path ${model_path} --in_image examples/sample4.png --text Reload --font Mohave --out_dir ${output_dir}
python generate.py --ckpt_path ${model_path} --in_image examples/sample4.png --text Reload --font ""Allura and Mohave"" --out_dir ${output_dir}
```

You should be able to get similar results:

<img src=""assets/font-intro.png"" width=""500"">

## Train the model
You can train the model on a combination of real world scene text data and synthetic scene text data.
### Prepare Data 
1. Download real world dataset: 
```bash
sh scripts/down_data.sh
```
2. Generate synthetic dataset: 
```bash
pip install -r synthgenerator/requirements.txt
sh scripts/gen_synth.sh
```
Notice that you may need to first download fonts from google fonts library, we include a list of font names for our released model in `synthgenerator/resources/100fonts` and background images from [SynthText Project](https://github.com/ankush-me/SynthText).

The donwloaded real world data and synthetic data will be in folder `data/ocr-dataset`.

### Train script
The main training script is `train.py`. You can train the model by running
```bash
python train.py --base ${config_paths} --stage fit --name ${run_name} --project ${project_name} --base_logdir ${log_directory}
```
Logs and model will be saved in `${log_directory}/${project_name}/${time}_${run_name}`.
An example config file is in `configs` folder, which defines the hyper parameter and other information required for training.

## Reference
Our code use `pytorch-lightning` as the main framework and `diffusers` for loading pretrained stable-diffusion model. We mainly follow the implementation of [stable-diffusion](https://github.com/runwayml/stable-diffusion).

## Citation
If you find our work useful in your research, please consider citing our paper:
```bibtex
@misc{ji2023improving,
      title={Improving Diffusion Models for Scene Text Editing with Dual Encoders}, 
      author={Jiabao Ji and Guanhua Zhang and Zhaowen Wang and Bairu Hou and Zhifei Zhang and Brian Price and Shiyu Chang},
      year={2023},
      eprint={2304.05568},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```
","['Question406', 'ghzhang233']",1,,0.83,0,,,,,,4,,,
43461933,MDEwOlJlcG9zaXRvcnk0MzQ2MTkzMw==,baredroid,ucsb-seclab/baredroid,0,ucsb-seclab,https://github.com/ucsb-seclab/baredroid,,0,2015-09-30 22:02:50+00:00,2024-11-11 22:03:37+00:00,2017-03-15 16:26:52+00:00,,2780,91,91,Python,1,1,1,1,0,0,24,0,0,1,,1,0,0,public,24,1,91,master,1,1,"# **BareDroid** #

BareDroid allows for bare-metal analysis on Android devices. See the paper [here](https://www.cs.ucsb.edu/~vigna/publications/2015_ACSAC_Baredroid.pdf)

## Folders ##

**backup**: used to create a backup of the device. It is a bash script which stores a copy of:
* userdata
* system
* boot 
in the *store* partition (partition number 30). This is needed only when you want to setup a new device.

**restore**: used to restore a previous backup. It is a bash script which restore a) system, b) userdata and c) userdatanew partition.

**backup_and_restore**: used to create a backup and restore it. It is a merge of the previous scripts.

**setup_device**:
*	setup: used to setup a device. Basically, this script creates the new partitions;

---

**update**: contains the python scripts used to manage the infrastructure.
*	adb.py: provides APIs used to send command through adb shell;
*	device.py: contains information about the update_manager associated to the device (deprecated, not used in the next version);
*	analysis.py: it bridges the gap between the infrastructure and the experiments. For a detailed description see below;
*	util.py: provides utilities (deprecated);
*	manager.py: manages the command line interface;
*	updtae_manager.py: it is the core of the infrastructure. It is a separate process which manages the update and analysis process;
*	update_manager_device.py: represents a separate process used to update the device during the analysis;
*	update_manager_recovery.py: represents a separate process used to update the device during the reboot. it is used to perform a relabel of the userdata partition ( SELinux :) );

**update/config**: contains cfg and info files used to setup the python scripts.
*	config.cfg: contains the information about where to save the logs, and which scripts use to perform the analysis
*	devices.info: contains general information about the devices (e.g., user, AndroidId)

## How to backup ##
1.	connect the device
2.	run the ""./script"" file (the script reboots the device in recovery mode and run the script)
3.	reboot the device


## How to restore ##
1. connect the device
2. run the ""./script"" file (the script reboots the device in recovery mode and run the script)
3. reboot the device


## Analysis script ##
The goal of this script is to provide a wrapper between the experiment and the infrastructure, *SetupAndStart* is the main method and the only one called by the infrastructure (i.e., update_manager.py line 178). It setups the environment for the experiment (e.g., adb root) and run the experiment.
If you want to use your code in an experiment you need to:
1.	modify the config.cfg file. In the stanza 'Project' put the absolute path to the code that you want to use;
2.	create an python script containing the class used in the stanza 'class' of the config.cfg file;
3.	define a 'run' method in the config.cfg file. This is the method used by the wrapper to start the experiment.

## how to add a new device to the infrastructure ##

1.	see 'setup_device';
2.	run the backup and restore scripts;
3.	add info to the 'update/config/device.info' file (e.g., AndroidId); [mandatory]

general consideration:

4.	device -> Settings -> Storage -> USB computer connection -> disable  MTP
5.	device -> Security -> Screen lock -> None
6.	be sure you can install untrusted app (i.e., from outside the market)


## How to run an experiment ##
Architectural overview:

![alt tag](https://docs.google.com/drawings/d/1UXaQkFElMduaZckbcicz3zloDz9SOA5aap_CV0FFMhQ/pub?w=465&amp;h=259)


1.	to run an experiment you need to include the absolute path of the folder containing the samples to analyze;
2.	run the manager start script against the folder containing the apps to analyze;

example

```
./start_baredroid path/to/folder/containing/samples
```

3.	the script will prompt a command line interface which allows the user to interact with the infrastructure (e.g., start experiment);
4.	select option '2' to run the experiment;
5.	when the experiment is finished click on 'q';
6.	the results are stored in the 'update/experiment' folder.

---
*email:* simone.mutti@unibg.it
",['smutti'],1,,0.77,0,,,,,,20,,,
625048643,R_kgDOJUF8Qw,Diffusion-SpaceTime-Attn,UCSB-NLP-Chang/Diffusion-SpaceTime-Attn,0,UCSB-NLP-Chang,https://github.com/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn,"Official implementation of the paper ""Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis""",0,2023-04-07 23:31:20+00:00,2024-12-27 10:41:02+00:00,2023-10-02 06:49:49+00:00,,88324,90,90,Jupyter Notebook,1,1,1,0,0,0,3,0,0,4,mit,1,0,0,public,3,4,90,main,1,1,"# Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis

[Qiucheng Wu](https://wuqiuche.github.io/)<sup>1</sup>\*,
[Yujian Liu](https://yujianll.github.io)<sup>1</sup>\*,
[Handong Zhao](https://hdzhao.github.io)<sup>2</sup>,
[Trung Bui](https://sites.google.com/site/trungbuistanford/)<sup>2</sup>,
[Zhe Lin](https://research.adobe.com/person/zhe-lin/)<sup>2</sup>,
[Yang Zhang](https://mitibmwatsonailab.mit.edu/people/yang-zhang/)<sup>3</sup>,
[Shiyu Chang](https://code-terminator.github.io/)<sup>1</sup>
<br>
<sup>1</sup>UC, Santa Barbara, <sup>2</sup>Adobe Research, <sup>3</sup>MIT-IBM Watson AI Lab

\*denotes equal contribution.

This is the official implementation of the paper ""Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis"".

## Overview

Diffusion-based models have achieved state-of-the-art performance on text-to-image synthesis tasks. However, one critical limitation of these models is the low fidelity of generated images with respect to the text description, such as missing objects, mismatched attributes, and mislocated objects. One key reason for such inconsistencies is the inaccurate cross-attention to text in both the spatial dimension, which controls at what pixel region an object should appear, and the temporal dimension, which controls how different levels of details are added through the denoising steps. In this paper, we propose a new text-to-image algorithm that adds explicit control over spatial-temporal cross-attention in diffusion models. We first utilize a layout predictor to predict the pixel regions for objects mentioned in the text. We then impose spatial attention control by combining the attention over the entire text description and that over the local description of the particular object in the corresponding pixel region of that object. The temporal attention control is further added by allowing the combination weights to change at each denoising step, and the combination weights are optimized to ensure high fidelity between the image and the text. Experiments show that our method generates images with higher fidelity compared to diffusion-model-based baselines without fine-tuning the diffusion model.

![](./assets/teaser.png)

## The workflow
Here, we demonstrate our text-to-image generation pipeline at one denoising step. Given input text $\boldsymbol{D}$, we first parse it and extract all objects mentioned, constructing local descriptions $\boldsymbol{L}_{i}$. Next, the layout predictor predicts the pixel region for each object in the text. The diffusion model attends to the global description $\boldsymbol{D}$ and additionally attends to the local description in the object's region. The final attention output is a weighted combination of attention to both global and local descriptions, where the combination weights sum up to 1 for each pixel and are optimized for each denoising step to achieve a high fidelity with $\boldsymbol{D}$.

![](./assets/pipeline.png)

## Requirements
Our code is based on <a href=""https://github.com/CompVis/stable-diffusion"">stable-diffusion</a>. This project requires one GPU with around 48GB memory. Please first clone the repository and build the environment:
```bash
git clone https://github.com/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn
cd Diffusion-SpaceTime-Attn/attention_optimization/stable-diffusion
bash environment-prepare.sh
conda activate ldm
cd ../../
```
You will also need to download the pretrained stable-diffusion model:
```bash
mkdir -p attention_optimization/stable-diffusion/models/ldm/stable-diffusion-v1
cd attention_optimization/stable-diffusion
wget -O models/ldm/stable-diffusion-v1/model.ckpt https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt
```

## Run the Layout Predictor
To train the layout predictor, first download <a href=""https://cocodataset.org/#download"">training data</a> from MSCOCO (2017 Train/Val annotations [241MB] and 2017 Stuff Train/Val annotations [1.1GB]). Extract the annotations to ```data/coco/annotations```.

Training code:
```bash
cd layout_predictor/LayoutTransformer
python train.py --cfg_path ./configs/coco/coco_seq2seq_v9_ablation_4.yaml
```
We provide a pretrained model <a href=""https://drive.google.com/file/d/147UdbmtAgnB1tjglvxljXVn_OxswuY1f/view?usp=sharing"">here</a>. Extract the model to ```saved/coco_F_seq2seq_v9_ablation_4/```.

Inference code:
```bash
python inference/inference_coco.py --sentence 'The silver bed was situated to the right of the white couch.'
```
The output will be similar to the following:
```bash
Sentence: The silver bed was situated to the right of the white couch.
The silver bed position: (0.574, 0.503)
the white couch position: (0.269, 0.442)
```
This means the predicted coordinates of bed and couch are (0.574, 0.503) and (0.269, 0.442) respectively.

## Run the whole pipeline (3 datasets)
### GPT-Synthetic Dataset
The dataset can be found in ```datasets/gpt.txt```. As introduced in paper, this dataset is created by querying Chat-GPT. Please refer to the paper for more details.

To run on this dataset:
```bash
cd attention_optimization/stable-diffusion
python scripts/txt2img-gpt.py --plms --ddim_steps 50 --prompt """"
```

### MS-COCO Dataset
The dataset can be found in ```datasets/mscoco.txt```. As introduced in paper, this dataset is from MS-COCO's caption. Please refer to the paper for more details.

To run on this dataset:
```bash
cd attention_optimization/stable-diffusion
python scripts/txt2img-mscoco.py --plms --ddim_steps 50 --prompt """"
```

### VSR Dataset
The dataset can be found in ```datasets/vsr.txt```. As introduced in paper, this dataset is from VSR's caption. Please refer to the paper for more details.

To run on this dataset:
```bash
cd attention_optimization/stable-diffusion
python scripts/txt2img-vsr.py --plms --ddim_steps 50 --prompt """"
```

You can find the generated images in ```result_outputs/```.


## Evaluation (3 datasets)
First, we need to install environment for the object detector. This step exactly follows the detrex official documentation (https://detrex.readthedocs.io/en/latest/tutorials/Installation.html):
```bash
cd attention_optimization/stable-diffusion
bash environment-prepare-detrex1.sh
```

We are aware that some users may encouter nvcc/cudatoolkit errors when following the above instruction. In that case, please follow instructions below.
```bash
cd attention_optimization/stable-diffusion
git clone https://github.com/IDEA-Research/detrex.git
cd detrex
git submodule init
git submodule update
conda create -n detrex python=3.9 -y
conda activate detrex
conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch -y
conda install -c ""nvidia/label/cuda-11.3.0"" cuda-nvcc -y
conda install cudatoolkit-dev=11.3 -c conda-forge -y
export PATH=/home/user/miniconda3/envs/detrex/bin/:$PATH
export CUDA_HOME=/home/user/miniconda3/envs/detrex/
export LD_LIBRARY_PATH=""/home/user/miniconda3/envs/detrex/lib""
python -m pip install -e detectron2 
pip install -e .
wget https://github.com/IDEA-Research/detrex-storage/releases/download/v0.2.0/dino_swin_large_384_4scale_36ep.pth 
```

Then you can evaluate result. Put ```attention_optimization/stable-diffusion/evaluation/*``` into ```attention_optimization/stable-diffusion/detrex/demo/```.
```bash
cd attention_optimization/stable-diffusion/detrex/demo/
CUDA_VISIBLE_DEVICES=1 python detector_result_gpt.py
CUDA_VISIBLE_DEVICES=1 python relation_result_gpt.py
```

## Results
We demonstrate examples of generated images by our method and baselines below. Typical errors of baselines include missing objects, mismatched attributes, and mislocated objects. Ours (1)/(2) show the results with two different random seeds.

![](./assets/example1.png)

![](./assets/example2.png)

## Parent Repository
This code is adopted from <a href="""">https://github.com/CompVis/stable-diffusion</a>, <a href="""">https://github.com/davidhalladay/LayoutTransformer</a>, and <a href="""">https://github.com/orpatashnik/StyleCLIP</a>.

",['wuqiuche'],1,,0.81,0,,,,,,4,,,
9360594,MDEwOlJlcG9zaXRvcnk5MzYwNTk0,RHESSys,RHESSys/RHESSys,0,RHESSys,https://github.com/RHESSys/RHESSys,The Regional Hydro-Ecologic Simulation System,0,2013-04-11 02:32:19+00:00,2025-01-07 19:19:32+00:00,2025-02-19 19:48:00+00:00,,92504,90,90,C,1,1,1,1,0,0,60,0,0,89,,1,0,0,public,60,89,90,trunk,1,1,"```diff
+ TRUNK IS NOW THE DEFAULT BRANCH (REPLACING MASTER).
+ THE MASTER BRANCH WILL REMAIN FOR ARCHIVAL PURPOSES, BUT TRUNK SHOULD NOW BE USED AS THE MAIN BRANCH.
- SEE NEW CHANGES.    
- NEW CODE MAY NOT BE BACKWARD COMPATIBLE WITH YOUR CURRENT FILES.    
- PLEASE REVIEW INFORMATION ABOUT CHANGES ON THE WHAT'S NEW WIKI PAGE   
```
https://github.com/RHESSys/RHESSys/wiki/What's-New

RHESSys - The Regional Hydro-Ecologic Simulation System
=======================================================

Github is the new home for the RHESSys code repository.

The project homepage is at http://fiesta.bren.ucsb.edu/~rhessys/

The old SVN repository was at http://sourceforge.net/projects/rhessys/ 

Branches
--------
The ""develop"" branch should be used for day-to-day development, with
RHESSys releases pushed to the ""trunk"" (formerly ""master"") branch periodically (for example
yearly).


Continuous Build and Test
-------------------------

We are using Travis-CI (http://travis-ci.org) to host our continuous integration efforts.  Continuous integration helps us run our test suite upon every commit to this repository and let us know if and when we break the build.

The current build status is: [![Build Status](https://travis-ci.org/RHESSys/RHESSys.png?branch=develop)](https://travis-ci.org/RHESSys/RHESSys)

The above icon should be clickable and point to the latest build at Travis-CI: https://travis-ci.org/RHESSys/RHESSys

The `.travis.yml` configuration file defines how this project is hooked to Travis-CI.  Github has a post-commit hook that is fired upon every commit to this repository.  This post-commit hook uses an authentication token to login to Travis-CI and run the configured steps on a virtual machine.  A return value of 0 means success and generates a 'green' status indicator (hopefully illustrated in the previous paragraph).

While the code is successfully compiling and running, there are a significant number of compiler warnings at this time:

    $ GISBASE=/usr/lib/grass64 make 2>&1 | grep warning | wc -l
    1233

Tests
-----

The Create Flowpaths subproject has a growing suite of tests that can be run via `make test`.  Tests are defined as .c files in the `cf/test/src` directory and will automatically get compiled and run by the `make test` target.

Code Coverage
-------------

The Create Flowpaths subproject also has a code coverage script.  This script will use [gcov](http://gcc.gnu.org/onlinedocs/gcc/Gcov.html) and [lcov](http://ltp.sourceforge.net/coverage/lcov.php) to generate an HTML coverage report and show where more tests are needed by illustrating which lines of code are not being exercised by the existing tests.

Run the code coverage script:

    cd cf/
    ./generate_coverage.sh

This will generate an HTML report in the newly formed `cf/coverage_report/` directory.  This new directory can be copied to a webserver or opened directly in your web browser.

Static Analysis
---------------

RHESSys can be analyzed by [cppcheck](http://cppcheck.sourceforge.net/) in a few seconds with the following command:

    cppcheck . --quiet

OR to see the output, and save the errors out to a textfile:

    cppcheck . 2> err.txt
    cat err.txt

    $ wc -l err.txt
    16

Static analysis will show things like memory leaks, out-of-bound references, and null pointers.  It is generally assumed a good thing to have your code be ""static analysis clean"".
","['naomitague', 'selimnairb', 'xiaoli-chen', 'mkenn', 'wburke24', 'ejhanan', 'LouisGraup', 'ryanrbart', 'renjianning', 'aubreyd', 'jschoate', 'sbpcs', 'mingliangwsu', 'trel', 'esgarcia', 'hcjiv1', 'jjreyes', 'charlieroberts', 'scaife']",1,,0.73,0,,,,,,35,,,
107615812,MDEwOlJlcG9zaXRvcnkxMDc2MTU4MTI=,Neural-Networks-on-Silicon,wjc852456/Neural-Networks-on-Silicon,0,wjc852456,https://github.com/wjc852456/Neural-Networks-on-Silicon,This is a collection of works on neural networks and neural accelerators.,0,2017-10-20 01:11:02+00:00,2024-03-16 05:50:38+00:00,2019-03-03 10:44:18+00:00,,151,40,40,,1,1,1,1,0,0,14,0,0,0,,1,0,0,public,14,0,40,master,1,,,"['fengbintu', 'kentaroy47', 'waterbearbee', 'Aayush-Ankit', 'sung-kim']",0,,0.65,0,,,,,,8,,,