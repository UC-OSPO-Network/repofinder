id,node_id,name,full_name,private,owner,html_url,project_type,description,fork,created_at,updated_at,pushed_at,homepage,size,stargazers_count,watchers_count,language,has_issues,has_projects,has_downloads,has_wiki,has_pages,has_discussions,forks_count,archived,disabled,open_issues_count,license,allow_forking,is_template,web_commit_signoff_required,visibility,forks,open_issues,watchers,default_branch,score,contributors,organization,readme,manual_label,prediction,prediction_nn,code_of_conduct,contributing,security_policy,issue_templates,pull_request_template,release_downloads,subscribers_count,ai_prediction,gpt_category
753836075,R_kgDOLO6gKw,grab-o-matic-3000,ericdvet/grab-o-matic-3000,0,ericdvet,https://github.com/ericdvet/grab-o-matic-3000,EDU,,0,2024-02-06 21:59:14+00:00,2024-03-01 02:07:09+00:00,2024-03-19 05:45:14+00:00,,169182,0,0,Python,1,1,1,1,1,0,1,0,0,0,,1,0,0,public,1,0,0,main,1,['ericdvet'],,"# Grab-O-Matic 3000

**Team Members:**
- Eric David Vetha, email: evetha@ucsc.edu
- Connor Guzikowski, email: cguzikow@ucsc.edu

**Course Instructor:**
- Dr. Tae Huh, email: thuh@ucsc.edu

# Installation

## Downloading Webots

1. Visit the Webots website at [cyberbotics.com](https://cyberbotics.com/).
2. Navigate to the ""Download"" section of the website.
3. Choose the appropriate version of Webots for your operating system (Windows, macOS, or Linux).
4. Click on the download link to start downloading the installer file.

To install the necessary dependencies for running Grab-O-Matic 3000, please execute the following command:

```bash
pip install -r requirements.txt
```

## Acknowledgement

[**webots**](https://cyberbotics.com/)
```bibtex
@inproceedings{webots2022,
  title={Webots: Professional Mobile Robot Simulation},
  author={Olivier Michel and Fabien Rohrer and David Mansolino and Hans W. Guesgen},
  booktitle={Proceedings of the 2022 Conference on Robot Simulation and Synthesis (RoSS)},
  year={2022}
}

```
",,EDU,0.7,,,,,,0,1,0.95,EDU
922265388,R_kgDONvinLA,AIEA-LAB-Work,ananyabatra04/AIEA-LAB-Work,0,ananyabatra04,https://github.com/ananyabatra04/AIEA-LAB-Work,EDU,,0,2025-01-25 18:46:24+00:00,2025-02-23 23:43:01+00:00,2025-02-23 23:42:58+00:00,,77,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['ananyabatra04'],,"# AIEA-LAB-Work

This repository contains my onboarding task work for the Artificial Intelligence Explainability and Accountability (AIEA) Lab at UC Santa Cruz. I am working on the LLM/Logic project. This repository contains my code for my AI/LLM related projects.
",,EDU,0.73,,,,,,0,1,0.9,EDU
358882462,MDEwOlJlcG9zaXRvcnkzNTg4ODI0NjI=,Modded-Rocket-Patrol,alickxie/Modded-Rocket-Patrol,0,alickxie,https://github.com/alickxie/Modded-Rocket-Patrol,EDU,The UCSC CMPM120 HW2,0,2021-04-17 13:12:33+00:00,2021-04-19 17:34:22+00:00,2021-04-19 17:34:20+00:00,,1386,0,0,JavaScript,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['alickxie'],,"# Modded-Rocket-Patrol
The UCSC CMPM120 HW2
04/20/2021

Name: Yufeng Xie
Time: cost 8-9hours

Points breakdown:
Track a high score that persists across scenes and display it in the UI (5)
Allow the player to control the Rocket after it's fired (5)
Implement a simultaneous two-player mode (30)
Redesign the game's artwork, UI, and sound to change its theme/aesthetic (to something other than sci-fi) (60)
TOTAL: (5 + 5 + 30 + 60 = 100)

Credit:
I got some help for the sound effect:
8bit Menu Select Sound: https://opengameart.org/content/8bit-menu-select, OpenGameArt.ORG, Fupi, Sunday, June 7, 2020 - 04:29
Arrow hit twang Sound: https://opengameart.org/content/arrow-hit-twang, OpenGameArt.ORG, qubodup, Monday, April 12, 2010 - 12:15
Farm animals Duck Sound: https://opengameart.org/content/farm-animals, OpenGameArt.ORG, Secretlondon, Sunday, October 11, 2009 - 05:29
",,EDU,0.82,,,,,,0,1,0.8,EDU
550020133,R_kgDOIMikJQ,collectioncard,collectioncard/collectioncard,0,collectioncard,https://github.com/collectioncard/collectioncard,OTHER,Its me!,0,2022-10-12 04:47:12+00:00,2024-06-09 21:35:57+00:00,2024-06-09 21:35:54+00:00,,2,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['collectioncard'],,"### Hi there üëã

I'm Thomas, a Senior at the University of California, Santa Cruz, studying Computer Science: Computer Game Design.

I've had the pleasure of working on some projects that I'm really proud of! Check out [Vera](https://github.com/Team-Creative-Name/Vera) and [Pizza with Gun](https://lifehckr.itch.io/pizza-with-gun), two of my latest projects!

You can also find more of my work on my [itch.io page](https://collectioncard.itch.io/).

Currently, I'm focused on updating my portfolio website to showcase my favorite projects. Stay tuned for the link!

Please feel free to [shoot me a message](mailto:turpleturtle12@gmail.com) if you find anything that I'm doing interesting to you! I love talking about projects and topics that interest me.


<!--
**collectioncard/collectioncard** is a ‚ú® _special_ ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
- üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->
",,OTHER,0.8,,,,,,0,1,0.7,OTHER
730522200,R_kgDOK4riWA,cse-163-data-visualizations,dianaflores55d/cse-163-data-visualizations,0,dianaflores55d,https://github.com/dianaflores55d/cse-163-data-visualizations,EDU,"This collection showcases a range of engaging data visualization projects from my UC Santa Cruz course, CSE 163 (Data Programming for Visualization). Using D3.js, I created an animated bar graph, a dynamic multi-line chart, and an interactive geographic map.",0,2023-12-12 05:30:34+00:00,2023-12-12 07:51:02+00:00,2024-01-09 04:05:00+00:00,https://dianaflores55d.github.io/cse-163-data-visualizations/,30,0,0,JavaScript,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['dianaflores55d'],,"<h1 align=""center"">
    Data Visualizations
</h1>

This collection showcases a range of engaging data visualization projects from my UC Santa Cruz course, CSE 163 (Data Programming for Visualization). Using D3.js, I created an animated bar graph, a dynamic multi-line chart, and an interactive geographic map.

Links to Visualizations
------------------------
* [Animated Bar Graph](https://dianaflores55d.github.io/cse-163-data-visualizations/BarGraph/BarGraphSamplev5.html)
* [Dynamic Multi-Line Chart](https://dianaflores55d.github.io/cse-163-data-visualizations/MultiLineChart/MultiLineindex.html)
* [Interactive Geographic Map](https://dianaflores55d.github.io/cse-163-data-visualizations/GeoMap/geomap.html)
    > If you're unable to see the visualization via this link, follow the guide below or scroll to the bottom to see a demo.

Using [Brackets](https://brackets.io/) to see and interact with visualizations locally
-----------------------------------------------------------------------------------------------
1. Install [Brackets](https://brackets.io/).
2. After cloning the repo, in the project root, open the Brackets app and open the cloned repo folder.
3. On the upper left side, click on the HTML file inside the desired visualization. On the upper right side, click on the lightning bolt symbol. The visualization should appear in a new tab!
![GeoMap1](https://github.com/dianaflores55d/cse-163-data-visualizations/assets/19867603/b8905581-0042-426d-855c-f63c99b01677)

Interactive Geographic Map Demo
-------------------------------
![GeoMap2](https://github.com/dianaflores55d/cse-163-data-visualizations/assets/19867603/963cab70-9b26-45c4-8d20-40e02d82212d)
![GeoMap3](https://github.com/dianaflores55d/cse-163-data-visualizations/assets/19867603/34f2876c-860f-431d-9b09-c6dc8db7273a)
![GeoMap4](https://github.com/dianaflores55d/cse-163-data-visualizations/assets/19867603/28b8f6ba-ee77-4fda-97eb-2402d441a822)
",,EDU,0.75,,,,,,0,1,0.8,EDU
428070275,R_kgDOGYPVgw,MusicGenGA,matthewfritsch/MusicGenGA,0,matthewfritsch,https://github.com/matthewfritsch/MusicGenGA,DEV,,0,2021-11-14 23:58:01+00:00,2021-12-18 21:01:54+00:00,2021-12-12 04:59:31+00:00,,35498,1,1,Python,1,1,1,1,0,0,2,0,0,0,,1,0,0,public,2,0,1,master,1,"['matthewfritsch', 'Edywang', 'AdamCarter11', 'Str4wbeary']",,"
MusicGenGA is a project by four students at University of California, Santa Cruz. It is intended as a genetic algorithm for developing music for indie game designers.

Please check out our website here for more information:
https://music-generator-ga.readthedocs.io/

",,EDU,0.7,,,,,,10,2,0.8,EDU
24901994,MDEwOlJlcG9zaXRvcnkyNDkwMTk5NA==,50th-site,luckyluke007/50th-site,0,luckyluke007,https://github.com/luckyluke007/50th-site,WEB,UC Santa Cruz 50th site,0,2014-10-07 17:19:53+00:00,2022-10-06 18:20:55+00:00,2022-10-06 18:27:10+00:00,,224239,0,0,JavaScript,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,"['LuckyLuke001', 'knice', 'luckyluke007']",,"# UC Santa Cruz 50th anniversary web site

[![Build Status](https://travis-ci.org/luckyluke007/50th-site.svg?branch=master)](https://travis-ci.org/luckyluke007/50th-site)

Built with [Jekyll](http://jekyllrb.com)

## Plug-ins used

- Search
    * https://github.com/slashdotdash/jekyll-lunr-js-search
- Image and asset path
    * https://github.com/imathis/octopress/blob/master/plugins/image_tag.rb
    * https://github.com/samrayner/jekyll-asset-path-plugin
- Javascript
    * [Fancybox](http://fancybox.net)
    * [Bigvideojs](http://dfcb.github.io/BigVideo.js/)

## Short URLs

- http://joshualande.com/short-urls-jekyll/",,WEB,0.93,,,,,,0,2,0.9,WEB
416965405,R_kgDOGNpjHQ,GameAI-P3,DrSlowpokePhd/GameAI-P3,0,DrSlowpokePhd,https://github.com/DrSlowpokePhd/GameAI-P3,EDU,Assignment P3 for CMPM146 at UC Santa Cruz,0,2021-10-14 02:38:19+00:00,2021-10-18 16:30:39+00:00,2021-10-18 16:30:35+00:00,,3981,0,0,Python,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,main,1,"['DrSlowpokePhd', 'Str4wbeary']",,"# GameAI-P3
Assignment P3 for CMPM146 at UC Santa Cruz
",,EDU,0.81,,,,,,0,1,0.9,EDU
220716651,MDEwOlJlcG9zaXRvcnkyMjA3MTY2NTE=,prolog-flights,daniel-escoto/prolog-flights,0,daniel-escoto,https://github.com/daniel-escoto/prolog-flights,EDU,,0,2019-11-09 23:26:22+00:00,2023-01-28 13:48:24+00:00,2020-07-08 18:37:22+00:00,,3,0,0,Prolog,1,1,1,1,0,0,0,1,0,0,,1,0,0,public,0,0,0,master,1,['daniel-escoto'],,"# prolog-flights
UC Santa Cruz CSE 112 Fall 2019 Program 3

install swipl in terminal:
```
brew tap homebrew/x11
brew install swi-prolog --HEAD
```

to run program:
```
swipl --goal=main -c prog2.pl  
a.out  
| ""starting airport code"".  
| ""ending airport code"".  
```
",,EDU,0.75,,,,,,0,1,0.8,EDU
8274926,MDEwOlJlcG9zaXRvcnk4Mjc0OTI2,ucsc-hadoop,hluu/ucsc-hadoop,0,hluu,https://github.com/hluu/ucsc-hadoop,EDU,Sample codes for student of Hadoop course,0,2013-02-18 18:58:05+00:00,2013-10-07 23:06:05+00:00,2013-10-07 23:06:02+00:00,,3844,0,0,JavaScript,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['hluu'],,"ucsc-hadoop
===========

Sample codes for students of Hadoop course
",,EDU,0.81,,,,,,0,2,0.8,EDU
213734821,MDEwOlJlcG9zaXRvcnkyMTM3MzQ4MjE=,LangProjects,wchunl/LangProjects,0,wchunl,https://github.com/wchunl/LangProjects,EDU,Functional and Object-oriented programming projects,0,2019-10-08 19:30:27+00:00,2019-10-08 19:32:13+00:00,2019-10-08 19:32:10+00:00,,2095,0,0,OCaml,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,[],,"*Projects created to learn language styles, philosophy, and design principles of programming langauges, including functional and object-oriented programming.*

**Institution:** University of California, Santa Cruz<br/>
**Course:** Fundamentals of Compiler Design I<br/>
**Professor:** Wesley, Mackey<br/>
**Student:** Wai Chun Leung

### Projects (click to see more information)
1. [**Scheme**   - Basic Interpreter](./asg1)
2. [**Ocmal**    - Basic Interpreter](./asg2)
3. [**Smalltalk** - Huffman Coding](./asg3)
4. [**Perl**     - gmake Written in Perl](./asg4)
5. [**Prolog**   - Flight Reservation System](./asg5)
",,EDU,0.69,,,,,,0,1,0.9,EDU
702895424,R_kgDOKeVVQA,crowoh,crowoh/crowoh,0,crowoh,https://github.com/crowoh/crowoh,OTHER,,0,2023-10-10 08:06:08+00:00,2024-04-29 18:02:13+00:00,2024-04-29 18:02:10+00:00,,62,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['crowoh'],,"### Hello! I am Cooper Rubens üëã

---
I am a student studying at the University of California - Santa Cruz interested in Data Analysis, Cyber-Security, and AI technologies. I love to both learn and share knowledge.  

Currently working on a 2D Top-down Fantasy RPG game in Godot 4!

---


### Tech Stack/Skills
<img src=""https://img.shields.io/badge/Python-FFD43B?style=for-the-badge&logo=python&logoColor=blue"" /><img src=""https://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white"" /><img src=""https://img.shields.io/badge/coding%20ninjas-DD6620?style=for-the-badge&logo=codingninjas&logoColor=white"" /><img src=""https://img.shields.io/badge/JavaScript-323330?style=for-the-badge&logo=javascript&logoColor=F7DF1E"" /><img src=""https://img.shields.io/badge/Lua-2C2D72?style=for-the-badge&logo=lua&logoColor=white"" /><img src=""https://img.shields.io/badge/Numpy-777BB4?style=for-the-badge&logo=numpy&logoColor=white"" /><img src=""https://img.shields.io/badge/C%2B%2B-00599C?style=for-the-badge&logo=c%2B%2B&logoColor=white"" /><img src=""https://img.shields.io/badge/Unity-100000?style=for-the-badge&logo=unity&logoColor=white"" /><img src=""(https://img.shields.io/badge/Oracle-F80000?style=for-the-badge&logo=Oracle&logoColor=white)"" /><img src=""https://img.shields.io/badge/Oracle-F80000?style=for-the-badge&logo=oracle&logoColor=black"" /><img src=""https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white"" /><img src=""https://img.shields.io/badge/MySQL-005C84?style=for-the-badge&logo=mysql&logoColor=white"" /><img src=""https://img.shields.io/badge/JSS-F7DF1E?style=for-the-badge&logo=JSS&logoColor=white"" /><img src=""https://img.shields.io/badge/Pandas-2C2D72?style=for-the-badge&logo=pandas&logoColor=white"" /><img src=""https://img.shields.io/badge/adobe-%23FF0000.svg?style=for-the-badge&logo=adobe&logoColor=white)"" /><img src=""https://img.shields.io/badge/IntelliJIDEA-000000.svg?style=for-the-badge&logo=intellij-idea&logoColor=white"" /><img src=""https://img.shields.io/badge/shell_script-%23121011.svg?style=for-the-badge&logo=gnu-bash&logoColor=white"" /><img src=""https://img.shields.io/badge/typescript-%23007ACC.svg?style=for-the-badge&logo=typescript&logoColor=white"" /><img src=""https://img.shields.io/badge/unrealengine-%23313131.svg?style=for-the-badge&logo=unrealengine&logoColor=white"" /><img src=""https://img.shields.io/badge/WebGL-990000?logo=webgl&logoColor=white&style=for-the-badge"" />
<hr>

### Featured Projects | All Created by Myself Unless Stated Otherwise

- [Discord Internship bot](https://github.com/Adam101k/Internship-Discord-Bot): Discord bot that searches and lists job openings based on specific criteria. Created and collaborated with a group of friends.

- [GPT A.I. Nutrition Calculator](https://chat.openai.com/g/g-457qBAOqm-macro-calculator): A nutrition calculator powered by GPT AI that displays nutrient information based on user input. Compatible with images and text descriptions.
  
- [SenateShowdownz](https://github.com/crowoh/SenateShowdownz): Website program that shows lobbyism information for legislators and parses other general information.
  
- [NeutralNBA](https://github.com/crowoh/NeutralNBA): Data visualization tool designed to analyze and compile the travel routes, carbon emissions, and environmental impact of NBA players over the past 10 seasons.
",,OTHER,0.82,,,,,,0,1,0.7,OTHER
90785913,MDEwOlJlcG9zaXRvcnk5MDc4NTkxMw==,nanopore-RNN,UCSC-nanopore-cgl/nanopore-RNN,0,UCSC-nanopore-cgl,https://github.com/UCSC-nanopore-cgl/nanopore-RNN,DEV,Collaboration repository with Nanopore Group,0,2017-05-09 19:50:10+00:00,2018-03-22 00:16:07+00:00,2018-05-10 22:35:40+00:00,,189490,0,0,Python,1,1,1,1,0,0,4,0,0,0,mit,1,0,0,public,4,0,0,master,1,['adbailey4'],1,"[![Build Status](https://travis-ci.org/UCSC-nanopore-cgl/nanopore-RNN.svg?branch=master)](https://travis-ci.org/UCSC-nanopore-cgl/nanopore-RNN)
[![codecov](https://codecov.io/gh/UCSC-nanopore-cgl/nanopore-RNN/branch/master/graph/badge.svg)](https://codecov.io/gh/UCSC-nanopore-cgl/nanopore-RNN)

# NanoTensor

We propose a series of scripts, which are still in development, to label and train a multilayer Bidirectional long short-term memory recurrent neural network to base-call ONT-nanopore reads with modified bases.



## INSTALLATION
Install BWA and make sure executable is in the PATH:
* `git clone https://github.com/lh3/bwa.git`
* `cd bwa`
* `make`
* `export PATH=$PATH:$PWD`

The easiest way to deal with the dependencies is to download Anaconda
* Download and Install Anaconda https://docs.continuum.io/anaconda/install
* `git clone --recursive https://github.com/BD2KGenomics/nanopore-RNN`
* `cd nanopore-RNN`
* `conda env create --file requirements.yml`
* `source activate nanotensor`
* `make`  
* C executables created by signalAlign need to be in the PATH
* `export PATH=$PATH:$PWD/signalAlign/bin`
* Test installation
* `make test`

## USAGE

##### TODO

#### Training using Nanonet

If you want to use Nanonet to train a network, you can use the script located [here](https://github.com/adbailey4/signalAlign/blob/embed_labels/src/signalalign/embed_signalalign.py) to embed aligned kmers into the fast5 file which can then be used with Nanonet.


### Contributions

If you decide to help contribute please use pylint so our code is consistent.
https://www.pylint.org/
",,DEV,0.65,,,,,,0,5,0.9,DEV
29558730,MDEwOlJlcG9zaXRvcnkyOTU1ODczMA==,budz,ChrisEgan/budz,0,ChrisEgan,https://github.com/ChrisEgan/budz,DEV,"The greatest, most revolutionary web2py application of the twenty first century!!!!",0,2015-01-20 22:34:33+00:00,2015-03-27 03:43:57+00:00,2015-03-27 03:43:56+00:00,,25529,0,0,Python,1,1,1,1,0,0,1,0,0,0,other,1,0,0,public,1,0,0,master,1,['ChrisEgan'],,"Justin Bates - Jujbates@ucsc.edu
Chris Egan - cjegan@ucsc.edu

SEE THE ONLINE VERSION!
https://manfrommars47.pythonanywhere.com/brainyape/

Project Description:
    Our project's proposal was to provide an easy and quick way to connect UC Santa Cruz tutors with tutees and vice versa. We wanted the user to be sucked in by our site's user interface, making it easy to find our tutor and student listings. One of the ways we made it easy for the user was to create a painless login and register. We choose to make the user log in via their UCSC google account. We then held the user's hand as we provide multiple pathways to our main features, the Tutor and Student listings. BrainyApe also provides a crash course on 'How to be a Tutor' for those user's that want to become entrepreneurs. As this web application might be simple we put a lot of time and took heavy yet friendly advice from friends and other fellow UCSC scholars.


Challenges:

-The biggest challenge we came across was being able to access web2py‚Äôs behind the scenes data bases to tweak data after it was already entered in. (auth_user)


		+ We created a profile form that would enter the information in a profile db, but thats not really what we wanted, we wanted the auth_user so we can link all the user data into one single db. 

		
		+Once we were able to link to the users auth_user db and update data to that auth_user db, that was a game changer. 


				> Ran into problems later with being able to create a stable link because we didnt want the users the have usernames. We wanted the site to be more personable. 


				> For that reason we had to create fields in the auth user for linking called ‚Äòname‚Äô and another called ‚Äòfancy_name‚Äô. fancy_name being used to display the First name of user and last name of users. These were working at first for when we had just the profile db but after we found out how to not insert and not add but update the auth_user. 

-Other problems we had was trying to implement and emailing messaging system. we wanted a form to be a part of the viewing another users profile but just couldn‚Äôt figure that out. we left the code that we thought should of made it work in. So if its possible for an email on what was wrong with implementation because it worked for a day and then i think when we did our big db switch i think it might of messed it up or something. 

- We tried to run a plug-in like our other friends project but with a tutor rating scrubber bar but that wouldn‚Äôt work for some reason so we dropped it for a while and continued to work and do reading on what would help grab the users eye and make them think that we were better than the other guy. 

- We also tried to implement a search feature last minute where the students could search by only the following Major, College , Subject of tutors, on-off campus housing, and price of tutor but we didn't want to use the given web2py SQL Grid layout. We wanted it to have a more attractive look to the user‚Äôs eye to get them to come back to the site. So after struggling and not finding help from Web2py 


Challenges outside out code:

-Finding the right idea.

-Trying to find time to brainstorm and do a precoding stage where we take a step back and really look at what we want our user to be able to do and who we wanted the user to be. 
			> we choose to do implement features that would attract ucsc students looking to tutor and tutors that are looking to expand their business.


Goals: 

Our goal was to allow students to find tutors through a very simple interface. Students will be able to post classes they need a tutor for, while tutors will be able to post their schedule of availabilities. Once user is logged in and registers, with our easy login with goggle with a single button press, since we know our user would all have gmail(user@ucsc.edu)accounts. We wanted to user this as an opportunity to see the psychology behind the user habits and what drives the user back to a website. We found that color can catch the eye and give a higher probability of a users return to reuse this specific web app. 



List of features: 

Tutor listing
Student listing
Easy google log in
custom slideshow on homepage with 
self-profile viewing/ other-user profile viewing
>almost had messaging, super sad it stopped working (please email use if you can help with this bug, so we don't run into it and so we can keep finishing the project and implementing our dream features like the rate the tutor scrubber bar.

Github: https://github.com/ChrisEgan/budz
pythonanywhere: https://manfrommars47.pythonanywhere.com/brainyape/
(the two versions are slightly different, due to the directory names and janrain's functionality only being possible over a network connection.)
",,DEV,0.89,,,,,,0,2,0.95,DEV
326873011,MDEwOlJlcG9zaXRvcnkzMjY4NzMwMTE=,cse20IntroToPython,pieguy744/cse20IntroToPython,0,pieguy744,https://github.com/pieguy744/cse20IntroToPython,EDU,Labs from intro to python class at University of California: Santa Cruz,0,2021-01-05 03:17:02+00:00,2021-01-05 03:53:58+00:00,2021-01-05 03:53:56+00:00,,14,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['pieguy744'],,,,EDU,0.73,,,,,,0,1,0.7,EDU
113206542,MDEwOlJlcG9zaXRvcnkxMTMyMDY1NDI=,ucsc-comm-core-functionality-plugin,Herm71/ucsc-comm-core-functionality-plugin,0,Herm71,https://github.com/Herm71/ucsc-comm-core-functionality-plugin,DEV,Core functionality plugin for UCSC WordPress sites,0,2017-12-05 16:30:56+00:00,2022-04-15 16:11:05+00:00,2018-02-13 18:06:33+00:00,,76,0,0,PHP,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['Herm71'],,"![Blackbird Consulting](http://www.blackbirdconsult.com/images/blackbird_header_logo.jpg ""Blackbird Consulting"")

## UC Santa Cruz WordPress Core Functionality Plugin ##
@author UC Santa Cruz
 
@link http://www.blackbirdconsult.com/

@more https://github.com/Herm71/blackbird-core-functionality-plugin


This plugin can contain your site's core functionality. It contains the following library structure; however, it can be expanded greatly:

* metaboxes.php -- for registering custom metaboxes

* post-types.php -- for registering custom post types

* shortcodes.php -- for writing custom shortcodes

* sidebars.php -- for registering custom sidebars

* taxonomies.php -- for custom taxonomies

## Additional Scripts ##

UC Santa Cruz hosts WordPress and Drupal sites at [Pantheon.io](https://pantheon.io). Pantheon hosts their own Git repositories. Pantheon's Git repositories consist of the entire [WordPress](https://wordpress.org) install (minus ``/wp-content/uploads/`` directory, which can be synced using [rsync](https://rsync.samba.org/)). 

While this configuration keeps all your code in one place, it makes it difficult to develop",,DEV,0.86,,,,,,0,2,0.95,DEV
90406007,MDEwOlJlcG9zaXRvcnk5MDQwNjAwNw==,crime_prediction,arnab64/crime_prediction,0,arnab64,https://github.com/arnab64/crime_prediction,EDU,Class project for TIM245 UCSC (Spring 2017),0,2017-05-05 18:37:35+00:00,2017-05-06 01:37:32+00:00,2017-06-05 08:03:08+00:00,,67874,0,0,Python,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,master,1,[],,"# crime_prediction
Class project for TIM245 UCSC (Spring 2017)
",,EDU,0.77,,,,,,0,1,0.8,EDU
127357667,MDEwOlJlcG9zaXRvcnkxMjczNTc2Njc=,xenahub,UCSC-Treehouse/xenahub,0,UCSC-Treehouse,https://github.com/UCSC-Treehouse/xenahub,DEV,Stand-alone Dockerized Xena Hub,0,2018-03-29 23:27:13+00:00,2021-02-01 20:32:33+00:00,2021-02-01 20:32:30+00:00,https://treehouse.xenahubs.net,10,0,0,Dockerfile,1,1,1,1,0,0,3,0,0,0,,1,0,0,public,3,0,0,master,1,['e-t-k'],1,"# Looking for Treehouse data on Xena?<br>‚ûú [treehouse.xenahubs.net](https://treehouse.xenahubs.net)

## xenahub
Stand-alone Dockerized Xena Hub

## Requirements:
- docker
- docker-compose
- make (optional)


## Notes
You will need to provide a certificate file and keyfile.
They should be placed in the xena/certs subdirectory. (When running in standalone mode.
See below for running behind a reverse proxy.)

The hub must be accessible at the URL that the certificate is for.
Otherwise you will get an ""ERR_CERT_COMMON_NAME_INVALID"" error.

## Hosted Files
To load a data file into xenahub, you will need two files:
- the TSV data file itself, eg `example.tsv`. This must not be gzipped.
- a json-formatted metadata file, eg `example.tsv.json`. [Xena's FAQ provides details on the format](https://ucsc-xena.gitbook.io/project/local-xena-hub/loading-data-from-the-command-line).

Place both of these in the`xenahub/xena/files` directory.
Then, from the `xenahub` directory, run:

`make load file=example.tsv`

The request will complete within a few seconds and the file will then load asynchronously into the xena db.
Leave the TSV file within the `files` subdirectory, or the download link on its xena page will not work.

For backups of the metadata for Treehouse's hosted files, see [the reference-file-info repo](https://github.com/UCSC-Treehouse/reference-file-info).

## Root-Squash
If your xena files are stored on an NFS mount that has been ""root-squashed"", it is a little
bit complicated to make the permissions work.

The way I am doing it is to make everything group-readable, then run the docker with user
root (the default) but the same group id as the group on the host.
You will have to edit your docker-compose.yml file to update the group ID to the correct one.
To find the desired group id:

`getent group GROUPNAME`

If you get the following error, set the `xena` dir to be group-writable:

`Error opening database: ""Could not save properties /root/xena/database.lock.db""`

## Running behind Apache reverse proxy
This is how to set it up on a shared server running on a high-level port, with Apache proxying to it.
Instructions may be approximate.

### DNS
Set up a CNAME to your shared server with your hub URL:

xenahub.example.com 28800 CNAME sharedserver.example.com

### Apache
Apache is responsible for holding the certificates for the `xenahub.example.com` site in this mode.
For example, store in `/etc/httpd/xena_certs`:
`chain.crt  xena.crt  xena.csr  xena.key`

Then, in `/etc/httpd/conf.d/ssl.conf`, include the following VirtualHost.
This will send https requests to Xena's 7223 (https) port. ProxyPreserveHost ensures that xena knows
that its hostname is xenahub.example.com instead of 127.0.0.1.

```
<VirtualHost *:443>
  ServerName xenahub.example.com
  SSLEngine on
  SSLProtocol all -SSLv2 -SSLv3
  SSLCipherSuite HIGH:3DES:!aNULL:!MD5:!SEED:!IDEA
  SSLCertificateFile /etc/httpd/xena_certs/xena.crt
  SSLCertificateKeyFile /etc/httpd/xena_certs/xena.key
  SSLCertificateChainFile /etc/httpd/xena_certs/chain.crt

  ProxyRequests Off
  ProxyPreserveHost On
  SSLProxyEngine On
  SSLProxyVerify none
  SSLProxyCheckPeerCN off
  SSLProxyCheckPeerName off
  SSLProxyCheckPeerExpire off
  ProxyPass ""/""  ""https://127.0.0.1:7223/""
  ProxyPassReverse ""/""  ""https://127.0.0.1:7223/""
</VirtualHost>
```

### Xena Docker
For behind reverse proxy, you will want to use the no-certs configuration & docker build.
This is currently the default.
",1,DEV,0.87,,,,,,0,0,0.95,DEV
876599250,R_kgDOND_X0g,skku-projects,racheliee/skku-projects,0,racheliee,https://github.com/racheliee/skku-projects,EDU,projects in uni (ÏÑ±Í∑†Í¥ÄÎåÄÌïôÍµê Í≥ºÏ†úÎì§),0,2024-10-22 08:43:11+00:00,2025-02-19 05:34:02+00:00,2025-02-19 05:33:56+00:00,,35566,1,1,C,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,main,1,"['racheliee', 'shj1081']",,"# skku
> ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Í∞Å ÎîîÎ†âÌÜ†Î¶¨Ïùò `README.md`Î•º Ï∞∏Í≥†ÌïòÏÑ∏Ïöî. <br>
> For more information, please refer to the `README.md` in each directory.

## 23-2

- [23-2 ÏãúÏä§ÌÖúÌîÑÎ°úÍ∑∏Îû® | System Program (prof. ÏóÑÏòÅÏùµ)](./ÏãúÏä§ÌÖúÌîÑÎ°úÍ∑∏Îû®%20|%20System%20Program%20(SWE2001)/)
- [23-2 Ïª¥Ìì®ÌÑ∞Íµ¨Ï°∞Í∞úÎ°† | Introduction to Computer Architecture (prof. Ï°∞ÌòïÎØº)](./Ïª¥Ìì®ÌÑ∞Íµ¨Ï°∞Í∞úÎ°†%20|%20Intro%20to%20Comp%20Arch%20(SWE2024)/)
- [23-2 ÏïåÍ≥†Î¶¨Ï¶òÍ∞úÎ°† | Algorithms (prof. ÍπÄÌòïÏãù)](./ÏïåÍ≥†Î¶¨Ï¶òÍ∞úÎ°†%20|%20Algorithms%20(SWE2016)/)
- [23-2 ÏûêÎ∞îÌîÑÎ°úÍ∑∏ÎûòÎ∞çÎ∞èÏã§Ïäµ | Java Programming Lab (prof. ÌÉÄÎ©îÎ•¥)](./ÏûêÎ∞îÌîÑÎ°úÍ∑∏ÎûòÎ∞çÏã§Ïäµ%20|%20Java%20Programming%20Lab%20(SWE2023)/)


## 24-1

- [24-1 Ïö¥ÏòÅÏ≤¥Ï†ú | Operating Systems (prof. ÏÑúÏùòÏÑ±)](./Ïö¥ÏòÅÏ≤¥Ï†ú%20|%20Operating%20Systems%20(SWE3004)/)
- [24-1 Î™®Î∞îÏùºÏï±ÌîÑÎ°úÍ∑∏ÎûòÎ∞çÏã§Ïäµ | Mobile App Programming Lab (prof. ÍπÄÏòÅÌõà)](./Î™®Î∞îÏùºÏï±ÌîÑÎ°úÍ∑∏ÎûòÎ∞çÏã§Ïäµ%20|%20Mobile%20App%20Programming%20Lab%20(SWE3047)/)
- [24-1 ÏãúÏä§ÌÖúÌîÑÎ°úÍ∑∏ÎûòÎ∞çÏã§Ïäµ | System Programming Lab (prof. ÌïúÌôòÏàò)](./ÏãúÏä§ÌÖúÌîÑÎ°úÍ∑∏ÎûòÎ∞çÏã§Ïäµ%20|%20System%20Programming%20Lab%20(SWE2024)/)


## 24-2
- [study abroad at UC Santa Cruz](https://github.com/racheliee/ucsc-projects)
  - 24-2 CSE 140 Artificial Intelligence (prof. Niloofar Montazeri)
  - 24-2 CSE 113 Parallel & Concurrent Programming (prof. Mohsen Lesani)
  - 24-2 CSE 150 Computer Networks (prof. Christina Parsa)
  - 24-2 CSE 183 Web Applications (prod. Luca De Alfaro)
  - 24-2 CSE 232 Distributed Systems (prof. Lindsey Kuper)",,EDU,0.73,,,,,,0,1,0.95,EDU
78159916,MDEwOlJlcG9zaXRvcnk3ODE1OTkxNg==,toil-rnaseq-sc,BD2KGenomics/toil-rnaseq-sc,0,BD2KGenomics,https://github.com/BD2KGenomics/toil-rnaseq-sc,DEV,Toil-based RNA-seq pipeline for processing single-cell data,0,2017-01-06 00:37:34+00:00,2021-10-10 22:09:55+00:00,2017-08-19 14:45:24+00:00,,154,2,2,Python,1,0,1,1,0,0,3,0,0,3,apache-2.0,1,0,0,public,3,3,2,master,1,"['jvivian', 'tpesout']",1,"## University of California, Santa Cruz Genomics Institute
### Guide: Running the Single Cell RNA-seq Pipeline using Toil

This guide attempts to walk the user through running this pipeline from start to finish. If there are any questions
please contact John Vivian (jtvivian@gmail.com). If you find any errors or corrections please feel free to make a 
pull request. Feedback of any kind is appreciated.

- [Dependencies](#dependencies)
- [Installation](#installation)
- [Inputs](#inputs)
- [Usage](#general-usage)
- [Methods](#methods)


## Overview

RNA-seq fastqs generated from 10x Chromium single-cell experiments are quantified to produce a gene by cell matrix.
Additional QC plots are generated 

This pipeline produces a tarball (tar.gz) file for a given sample that contains n subdirectories:

- 
- 
- 

The output tarball is prepended with the UUID for the sample (e.g. UUID.tar.gz). 

# Dependencies

This pipeline has been tested on Ubuntu 14.04, but should also run on other unix based systems.  `apt-get` and `pip`
often require `sudo` privilege, so if the below commands fail, try prepending `sudo`.  If you do not have `sudo` 
privileges you will need to build these tools from source, or bug a sysadmin about how to get them (they don't mind). 

#### General Dependencies

    1. Python 2.7
    2. Curl         apt-get install curl
    3. Docker       http://docs.docker.com/engine/installation/

#### Python Dependencies

    1. Toil         pip install toil
    2. S3AM         pip install --pre s3am (optional, needed for uploading output to S3)
    
    
#### System Dependencies


# Installation

 
# Inputs

The CGL RNA-seq pipeline requires an index file in order to run. This file is hosted on Synapse and can 
be downloaded after creating an account which takes about 1 minute and is free. 

* Register for a [Synapse account](https://www.synapse.org/#!RegisterAccount:0)
* Either download the samples from the [website GUI](https://www.synapse.org/#!Synapse:syn5886029) or use the Python API
* `pip install synapseclient`
* `python`
    * `import synapseclient`
    * `syn = synapseclient.Synapse()`
    * `syn.login('foo@bar.com', 'password')`
    * Get the Kallisto index reference
        * `syn.get('syn5889216', downloadLocation='.')`
        
 
All samples and inputs must be submitted as URLs with support for the following schemas: 
`http://`, `file://`, `s3://`, `ftp://`.

Samples consisting of tarballs with fastq files inside _must_ follow the file name convention of ending in an 
R1/R2 or \\_1/\\_2 followed by `.fastq.gz`, `.fastq`, `.fq.gz` or `.fq.`.

# General Usage

Type `toil-rnaseq` to get basic help menu and instructions
 
1. Type `toil-rnaseq-sc generate` to create an editable manifest and config in the current working directory.
2. Parameterize the pipeline by editing the config.
3. Fill in the manifest with information pertaining to your samples.
4. Type `toil-rnaseq-sc run [jobStore]` to execute the pipeline.

### Example Commands

Run sample(s) locally using the manifest

1. `toil-rnaseq-sc generate`
2. Fill in config and manifest
3. `toil-rnaseq-sc run ./example-jobstore`

Toil options can be appended to `toil-rnaseq run`, for example:
`toil-rnaseq-sc run ./example-jobstore --retryCount=1 --workDir=/data`

For a complete list of Toil options, just type `toil-rnaseq run -h`

Run a variety of samples locally

1. `toil-rnaseq-sc generate-config`
2. Fill in config
3. `toil-rnaseq-sc run ./example-jobstore --retryCount=1 --workDir=/data --samples \\
    s3://example-bucket/sample_1.tar file:///full/path/to/sample_2.tar https://sample-depot.com/sample_3.tar`

### Example Config

```
kallisto-index: s3://cgl-pipeline-inputs/rnaseq_cgl/kallisto_hg38.idx
output-dir: /data/my-toil-run
ssec: 
ci-test:
```


## Distributed Run

To run on a distributed AWS cluster, see [CGCloud](https://github.com/BD2KGenomics/cgcloud) for instance provisioning, 
then run `toil-rnaseq-sc run aws:us-west-2:example-jobstore-bucket --batchSystem=mesos --mesosMaster mesos-master:5050`
to use the AWS job store and mesos batch system. 

# Methods

## Tools
    
All tool containers can be found on our [quay.io account](quay.io/organization/ucsc_cgl).

## Reference Data


## Tool Options

",,DEV,0.89,,,,,,0,4,0.95,DEV
68487116,MDEwOlJlcG9zaXRvcnk2ODQ4NzExNg==,Advance-C_class-projects_UCSC-Extension,vjison/Advance-C_class-projects_UCSC-Extension,0,vjison,https://github.com/vjison/Advance-C_class-projects_UCSC-Extension,EDU,"My code for the Advanced C course at UCSC-Extension.  As someone who does not touch code at work, this was an opportunity to brush up on my C programming skills",0,2016-09-18 01:17:39+00:00,2019-04-26 14:15:06+00:00,2019-04-26 13:58:20+00:00,,1814,0,0,C,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['vjison'],,"# Advance-C_class-projects_UCSC-Extension

This repository contains projects and homework assignments from my Advance C programming class at UC Santa Cruz Extension
",,EDU,0.71,,,,,,0,0,0.8,EDU
181765252,MDEwOlJlcG9zaXRvcnkxODE3NjUyNTI=,CMPS12B-M-gradingScript,Evelynchengusa/CMPS12B-M-gradingScript,0,Evelynchengusa,https://github.com/Evelynchengusa/CMPS12B-M-gradingScript,EDU,,0,2019-04-16 20:50:17+00:00,2019-08-10 15:29:33+00:00,2019-08-10 15:29:31+00:00,,128,0,0,C,1,1,1,1,0,0,5,0,0,0,,1,0,0,public,5,0,0,master,1,['Evelynchengusa'],," THIS IS FOR UCSC INTRODUCTION OF DATASTRUCTURE.
 
 Summer 2019
",,EDU,0.75,,,,,,0,0,0.7,EDU
565293256,R_kgDOIbGwyA,safety-first-gracehacks,msaini26/safety-first-gracehacks,0,msaini26,https://github.com/msaini26/safety-first-gracehacks,DEV,A web application to help students feel safe walking back home at night with heatmaps displaying the frequency of foot traffic and regions of brighter lights.,0,2022-11-12 23:31:52+00:00,2023-11-01 22:43:04+00:00,2022-11-16 03:47:04+00:00,,13049,0,0,CSS,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,"['msaini26', 'parnap', 'parnapraveen']",,"# safety-first-gracehacks

# Update 
Won ""Best UI/UX"" Hack at GraceHacks

## Inspiration

Walking alone at night on campus with uncertainty is the worst feeling as a student. As the number of CruzAlerts (Safety Alert System organized by the University of California, Santa Cruz) regarding incidents that have made students and faculty feel unsafe increased, we decided to take matters into our own hands. With Safety First, we aim to help students get home safer feeling empowered.

## What it does
We included a updated newsfeed that highlights recent university safety alerts and events. To guide students and faculty back home safely when walking late at night, we build a map with pinpoints on the UCSC campus that are well-lit and popular walking paths. We also developed an empowerment section where individuals can feel mentally stronger through power yoga poses designed to improve confidence through our machine learning pose detection model. Strike a pose with pride!

## How we built it
We built the structure of our webpage using HTML, design using CSS, and functionality using Javascript. We utilized the Google Map API to pinpoint crucial safety spots on campus with lampost icons to indicate popular walking paths and well-lit regions. For the empowerment feature, we utilized TensorFlow.js to develop our machine learning model that takes visual user input to detect poses. We classified our data set based on each pose type and were able to reach 95% accuracy overall.

## Challenges we ran into
Our goal in GraceHacks was to stretch our learning through learning new technologies and expanding our skillset overall. This was our first time utilizing the Google Maps API to drop pinpoints at frequently used paths and well-lit regions. We ran into issues trying to highlight each cluster of lights (red: more frequent path and well-light; blue: less frequent and possible light) respectively. A solution we found was grouping pinpoints based on their relative location to highlight more frequent walking regions with good lighting. Also, in our first training session for our machine learning pose detection model, we did not include enough data points to account for edge cases. For example, this might include someone doing the pose from far away or closer towards the camera. After spending many hours training and testing our model, we were able to achieve an approximate of 96% accuracy.


## Accomplishments that we're proud of
Starting off with a steep learning curve, we are proud of the final implementation of our safety map and the empowerment feature that we successfully embedded in our webpage; the process was difficult yet rewarding. Although modifying minor design details and image cohesion was tedious, we are proud of what we achieved! 

## What we learned
Parna learned how to start an HTML project from scratch. Managing time properly and breaking down a large project into smaller parts is also something that she learned is beneficial when beginning to hack. Embedding APIs and other software involved a learning curve but it helped make our project more versatile. This was Mansi's first time learning how to integrate the Google Maps API by dropping pinpoints on crucial locations and labeling them accordingly based on their location. She learned the importance of defining a clear project scope and defining its respective components ahead of time before diving straight into the development process. 


## What's next for Safety First
We want to include a discussion forum on our webpage to encourage further community building and discussion amongst UCSC student and staff. Providing additional safety havens across campus is also a goal so having a section on the webpage where students can talk about their experiences would help students feel less isolated. 
",,DEV,0.86,,,,,,0,1,0.75,DEV
870961490,R_kgDOM-nRUg,ireed3282,ireed3282/ireed3282,0,ireed3282,https://github.com/ireed3282/ireed3282,OTHER,Config files for my GitHub profile.,0,2024-10-11 01:55:53+00:00,2025-02-05 06:41:05+00:00,2025-02-05 06:41:02+00:00,https://github.com/ireed3282,6,0,0,,0,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['ireed3282'],,"- üëã Hi, I‚Äôm @ireed3282
- üëÄ I‚Äôm interested in learning more about AI and its applications to real world.
- üå± I‚Äôm currently attending UC Santa Cruz obtaining my C.S. B.S. degree
- üíûÔ∏è I‚Äôm looking to collaborate on ML projects, I am super interested in learning more about computer vision and its uses in healthcare
- üì´ How to reach me [email](mailto::isabellajanereed@gmail.com) [LinkedIn](https://www.linkedin.com/in/isabella-reed-87624924b/)
- üòÑ Pronouns: she/her
- ‚ö° My other hobbies include rides on my motorcycle, taking hikes on campus after class, and sitting down with my most recent knitting project

<!---
ireed3282/ireed3282 is a ‚ú® special ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
",,OTHER,0.84,,,,,,0,1,0.7,OTHER
928173526,R_kgDON1LN1g,C-for-Everyone,DavidMuha/C-for-Everyone,0,DavidMuha,https://github.com/DavidMuha/C-for-Everyone,EDU,Course note-taking & code,0,2025-02-06 07:27:16+00:00,2025-02-10 16:29:39+00:00,2025-02-10 16:29:36+00:00,,106,0,0,C,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['DavidMuha'],,"C for Everyone: Programming Fundamentals
by University of California, Santa Cruz

About this Course

In the new world we live in, coding is a universally valuable skill, whether you're a scientist, artist, or a humanist. 
Algorithms are everywhere, and we all have to understand how they work. 
The C language is particularly well suited as an introduction to coding: It's a tried-and-true language, and it allows you to understand computing processes at a deep level. 

Taught by:
Ira Pohl, Professor
Computer Science
",,EDU,0.65,,,,,,0,1,0.9,EDU
822773272,R_kgDOMQqGGA,medicine_preprocessing-on-entire-dataset,shaivimalik/medicine_preprocessing-on-entire-dataset,0,shaivimalik,https://github.com/shaivimalik/medicine_preprocessing-on-entire-dataset,DEV,"Reproducing ""Characterization of Term and Preterm Deliveries using Electrohysterograms Signatures""",0,2024-07-01 19:43:51+00:00,2024-11-09 00:28:57+00:00,2024-11-09 00:28:54+00:00,,55333,0,0,Jupyter Notebook,1,1,1,1,0,0,0,0,0,1,mit,1,0,0,public,0,1,0,main,1,"['shaivimalik', 'ffund']",,"## Reproducing ""Characterization of Term and Preterm Deliveries using Electrohysterograms Signatures""

In this sequence of notebooks, we will reproduce the results from 

> M. U. Khan, S. Aziz, S. Ibraheem, A. Butt and H. Shahid, ""Characterization of Term and Preterm Deliveries using Electrohysterograms Signatures,"" 2019 IEEE 10th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), Vancouver, BC, Canada, 2019, pp. 0899-0905, doi: 10.1109/IEMCON.2019.8936292.

which predicts pre-term birth based on raw EHG signals from pregnant women. It claims an accuracy of 95.5% on the test set using an SVM classifier with RBF kernel. We achieve a similar high accuracy (97%) by following the steps in that paper:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shaivimalik/medicine_preprocessing-on-entire-dataset/blob/main/notebooks/Reproducing_Original_Result.ipynb) Reproducing ""Characterization of Term and Preterm Deliveries using Electrohysterograms Signatures""

However, it turns out (as discussed in [2]) that the reported 95.5% accuracy is much higher than we would achieve on *new* EHG signals when the model is used in practice. This is because we oversampled the dataset before splitting it into training and test sets. Consequently, test set samples were used to generate synthetic samples for the training set and training set samples were used to generate synthetic samples for the test set.

We explore this issue further in the following examples - 

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shaivimalik/medicine_preprocessing-on-entire-dataset/blob/main/notebooks/Exploring_Oversampling-Adult.ipynb) Exploring Oversampling on the Income Dataset

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shaivimalik/medicine_preprocessing-on-entire-dataset/blob/main/notebooks/Exploring_Oversampling-Synthetic.ipynb) Exploring Oversampling on Synthetic Data
 
Finally, we repeat the original pre-term birth prediction, but without the data leakage error - we keep training and test sets separate in oversampling, rather than oversampling all together. We show that the original accuracy was based on an ""overly optimistic"" evaluation, and the true performance of the model is much less.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shaivimalik/medicine_preprocessing-on-entire-dataset/blob/main/notebooks/Correcting_Original_Result.ipynb) ""Characterization of Term and Preterm Deliveries using Electrohysterograms Signatures"" Without Data Leakage

---

### Running the Project

#### Google Colab

Click on the ""Open in Colab"" buttons above to run the notebooks in Google Colab.

#### Chameleon

This resource may be executed on [Chameleon](https://chameleoncloud.org/). If using Chameleon, start by running the `reserve.ipynb` notebook in the Chameleon Jupyter environment.

#### Local Machine

1. Clone the repository:
   ```
   $ git clone https://github.com/shaivimalik/medicine_preprocessing-on-entire-dataset.git
   $ cd medicine_preprocessing-on-entire-dataset
   ```

2. Install the required dependencies:
   ```
   $ pip install -r requirements.txt
   ```

3. Launch Jupyter Notebook:
   ```
   $ jupyter notebook
   ```

---


### Acknowledgements

This project was part of the 2024 Summer of Reproducibility organized by the [UC Santa Cruz Open Source Program Office](https://ucsc-ospo.github.io/).

* Contributor: [Shaivi Malik](https://github.com/shaivimalik)
* Mentors: [Fraida Fund](https://github.com/ffund), [Mohamed Saeed](https://github.com/mohammed183)

---

### References

[1] M. U. Khan, S. Aziz, S. Ibraheem, A. Butt and H. Shahid, ""Characterization of Term and Preterm Deliveries using Electrohysterograms Signatures,"" 2019 IEEE 10th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), Vancouver, BC, Canada, 2019, pp. 0899-0905, doi: 10.1109/IEMCON.2019.8936292.

[2] Gilles Vandewiele, Isabelle Dehaene, Gy√∂rgy Kov√°cs, Lucas Sterckx, Olivier Janssens, Femke Ongenae, Femke De Backere, Filip De Turck, Kristien Roelens, Johan Decruyenaere, Sofie Van Hoecke, Thomas Demeester, Overly optimistic prediction results on imbalanced data: a case study of flaws and benefits when applying over-sampling, Artificial Intelligence in Medicine, Volume 111, 2021, 101987, ISSN 0933-3657, https://doi.org/10.1016/j.artmed.2020.101987.",,EDU,0.54,,,,,,0,2,0.8,EDU
881215559,R_kgDONIZIRw,Scarab,camsterrrr/Scarab,0,camsterrrr,https://github.com/camsterrrr/Scarab,EDU,Lab submission for CSE220 Computer Arcitecture course at UC Santa Cruz.,0,2024-10-31 05:50:14+00:00,2024-11-15 05:42:09+00:00,2024-12-06 21:46:04+00:00,,1884,0,0,C,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,0,main,1,"['spruett', 'siavashzk', 'bencplin', 'AliFakhr', '5surim', 'chestercai1995', 'hlitz', 'SleepBook', 'camsterrrr', 'hpsresearchgroup', 'aniket-de', 'FlyingFish800', 'dependabot[bot]', 'yql5510718']",,"# Scarab Quick Start Guide
Install:
1. Install exact PIN version ([PIN 3.15](https://www.intel.com/content/www/us/en/developer/articles/tool/pin-a-binary-instrumentation-tool-downloads.html))
2. Export the following paths
  - `export PIN_ROOT=/path/to/pin-3.15`
  - `export SCARAB_ENABLE_PT_MEMTRACE=1`
3. `cd src && make`

Run:
1. Copy: `src/PARAMS.sunny_cove` into your run directory and rename to `PARAMS.in`
2. Run: `src/scarab --frontend memtrace --cbp_trace_r0=<MEMTRACE_FILE> --memtrace_modules_log=<MODULES_LOG_AND_BINARIES_DIR>`

# Scarab

Scarab is a cycle accurate simulator for state-of-the-art, high performance,
multicore chips. Scarab's goal is to be highly accurate, while also being
fast and easy to work with.

##### Simulator Features:
* Accurate: Scarab is detailed cycle accurate uArchitecture model
* Fast: 600 KIPS trace-driven, 100 KIPS exec-driven
* SimPoint Support: Checkpoints, Fast-Forward, Marker Instructions
* Execute-at-Fetch: Easier support for oracle features, faster development of new features

##### v.2.0 Release Features:
* Support for Dynomrio Memtrace and Intel Processor Trace (PT) frontends
* Wrong-path execution for trace-based frontends (instruction replay)

##### What Code Can Scarab Run?
* Single-threaded x86\\_64 programs that can be run on Intel's [PIN](https://software.intel.com/en-us/articles/pin-a-dynamic-binary-instrumentation-tool)

##### Scarab uArchitecture:
* All typical pipeline stages and out-of-order structures (Fetch, Decode, Rename, Retire, ROB, R/S, and more...)
* Multicore 
* Wrong path simulation
* Cache Hierarchy (Private L1, Private MLC, Private/Shared LLC)
* Ramulator Memory Simulator (DDR3/4, LPDDR3/4, GDDR5, HBM, WideIO/2, and more...)  
* Interface to McPat and CACTI for system level power/energy modeling
* Support for DVFS
* Latest Branch Predictors and Data Prefetchers (TAGE-SC-L, Stride, Stream, 2dc, GHB, Markov, and more...)

##### v.2.0 uArchitecture Extensions:
* Decoupled Frontend
* Micro-op Cache
* Register Renaming (limited GRF/rename stalls)
* Updated branch predictor and increased recovery accuracy
* FDIP/UDP prefetcher

##### Code Limitations
* 32-bit binaries not supported (work in progress)
* Performance of System Code not modeled
* No cooperative multithreaded code

##### uArch Limitations
* No SMT
* No real OS virtual to physical address translation
* Shared bus interconnect only (ring, mesh, and others are in progress.)

##### Credits 
Scarab was created in collaboration with HPS and SAFARI. This project was sponsored by Intel Labs.
Scarab v.2.0 was created and is currently maintained by UCSC.

The Scarab v.2.0 artifact is the result of our UDP ISCA 2024 paper. If you are using Scarab v.2.0 in your research please cite:

```
@inproceedings{oh2024udp,
  author = {Oh, Surim and Xu, Mingsheng and Khan, Tanvir Ahmed and Kasikci, Baris and Litz, Heiner},
  title = {UDP: Utility-Driven Fetch Directed Instruction Prefetching},
  booktitle = {Proceedings of the 51st International Symposium on Computer Architecture (ISCA)},
  series = {ISCA 2024},
  year = {2024},
  month = jun,
}
```

## License & Copyright
Please see the [LICENSE](LICENSE) for more information.

## Getting Started

1. [System requirements and software prerequisites.](docs/system_requirements.md)
2. [Compiling Scarab.](docs/compiling-scarab.md)
3. [Setting up and running auto-verification on Scarab.](docs/verification.md)
4. Running a single program on Scarab.
5. Running multiple jobs locally or on a batch system. (coming soon!)
6. Viewing batch job status and results. (coming soon!)
7. [Simulating dynamorio memtraces](docs/memtrace.md)
8. Solutions to common Scarab problems.

## Contributing to Scarab

Found a bug? [File a bug report.](https://github.com/hpsresearchgroup/scarab/issues/new/choose)

Request a new feature? [File a feature request.](https://github.com/hpsresearchgroup/scarab/issues/new/choose)

Have code you would like to commit? [Create a pull request.](https://github.com/hpsresearchgroup/scarab/pulls)

## Other Resources


1) Auto-generated software documentation can be found [here](docs/doxygen/index.html).

* Please run this command in this directory to auto-generate documentation files.
> make -C docs
",,EDU,0.7,,,,Directory exists,,0,1,0.9,EDU
631091062,R_kgDOJZ2vdg,nlp-203,Kdotseth7/nlp-203,0,Kdotseth7,https://github.com/Kdotseth7/nlp-203,EDU,Repository for assignments of NLP-203,0,2023-04-21 22:57:59+00:00,2023-04-28 03:47:04+00:00,2023-06-16 09:54:01+00:00,,82117,0,0,Jupyter Notebook,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['Kdotseth7'],,"# NLP-203 HW Repository

## Kushagra Seth

## kuseth@ucsc.edu

### 2005986
",,EDU,0.73,,,,,,0,1,0.9,EDU
88892765,MDEwOlJlcG9zaXRvcnk4ODg5Mjc2NQ==,Tutoring,douglaswsilva/Tutoring,0,douglaswsilva,https://github.com/douglaswsilva/Tutoring,EDU,CMPS 121 - Mobile Applications,0,2017-04-20 17:30:44+00:00,2017-04-20 17:35:08+00:00,2017-05-19 04:17:09+00:00,,170,0,0,Java,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,master,1,['douglaswsilva'],,"# Tutoring
CMPS 121 - Mobile Applications 

Repo contains Android sample projects used for tutoring on Mobile Aplications course at UC Santa Cruz, Spring 2017
",,EDU,0.89,,,,,,0,0,0.9,EDU
406443438,MDEwOlJlcG9zaXRvcnk0MDY0NDM0Mzg=,Spebby,Spebby/Spebby,0,Spebby,https://github.com/Spebby/Spebby,OTHER,,0,2021-09-14 16:27:47+00:00,2025-02-13 23:59:05+00:00,2025-02-13 23:59:01+00:00,,17,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['Spebby'],,"### Hello there,

<hr>

My name is Thom (Spebby) Mott,
I'm a game developer hoping to make something enjoyable, while continuing to learn more about computers and the ways in which we communicate with them. I'm currently a student at the University of California, Santa Cruz persuing a batchelors in their Game Design program, and a minor in their Computer Science program. I am most comfortable with the C family of languages, but have experience in Python, Java and JS/TypeScript.

* What am I cooking?
  * An AR Mobile application meant to build community and climate resilience
  * A Chess Engine using bitboards and other advanced techniques built in Dear ImGui
  * A curated modding project for the video game [Unturned](https://store.steampowered.com/app/304930/Unturned/)


[![wakatime](https://wakatime.com/badge/user/6c1b4d80-35ad-487a-a081-efc861c8d411.svg)](https://wakatime.com/@6c1b4d80-35ad-487a-a081-efc861c8d411)

<a href=""https://wakatime.com""><img src=""https://wakatime.com/share/@6c1b4d80-35ad-487a-a081-efc861c8d411/78c7b7f5-3ea9-4cbb-8145-43eeb184d6e3.png""/></a>

<a href=""https://wakatime.com""><img src=""https://wakatime.com/share/@6c1b4d80-35ad-487a-a081-efc861c8d411/40d02a12-58df-4001-899a-23c4423e7751.png""/></a>

WakaTime since 12/Feb/2025
",,OTHER,0.85,,,,,,0,1,0.8,OTHER
101673508,MDEwOlJlcG9zaXRvcnkxMDE2NzM1MDg=,cellBrowser,maximilianh/cellBrowser,0,maximilianh,https://github.com/maximilianh/cellBrowser,DEV,"main repo: https://github.com/ucscGenomeBrowser/cellBrowser/ - Python pipeline and Javascript scatter plot library for single-cell datasets, http://cellbrowser.rtfd.org",0,2017-08-28 18:19:26+00:00,2025-02-27 05:54:28+00:00,2023-01-20 13:21:51+00:00,https://github.com/ucscGenomeBrowser/cellBrowser/,83633,110,110,JavaScript,1,1,1,1,0,0,44,0,0,28,gpl-3.0,1,0,0,public,44,28,110,develop,1,"['maximilianh', 'matthewspeir', 'mxposed', 'braneyboo', 'pcm32', 'brittneydwick', 'christopherLee1', 'kriemo', 'redst4r', 'inodb', 'ivirshup', 'gusevfe', 'flying-sheep', 'rachadele']",,".. image:: https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat
   :target: http://bioconda.github.io/recipes/ucsc-cell-browser/README.html

UCSC Single Cell Browser
========================

The UCSC Cell Browser is a viewer for single cell data. You can click on and
hover over cells to get meta information, search for genes to color on and
click clusters to show cluster-specific marker genes. 

To look at a list of selected single cell datasets, see http://cells.ucsc.edu

To setup your own cell browser, from Cellranger, Seurat, Scanpy or text files 
(tsv/csv), or just a single cell expression matrix, read the documentation
at http://cellbrowser.rtfd.io. If you use the UCSC Cell Browser in your research, please cite
`our Bioinformatics paper <https://dx.doi.org/10.1093/bioinformatics/btab503>`_.
If you are also using data from a specific dataset we host, please also cite
the original authors of that dataset (visible under 'Info & Download' while viewing that dataset).

If you want us to add a single cell dataset to the website http://cells.ucsc.edu, 
please contact us at cells@ucsc.edu. We are happy to add any dataset.

This is a viewer for a static, precomputed layout. If you're looking for an interative layout, where you can 
move the cells around and run some algorithms interactively, try Chan-Zuckerberg's own cellxgene or Spring.
A website with both datasets and some analysis is `Scope <http://scope.aertslab.org/>`_.

Many labs host their data at cells.ucsc.edu by sending it to us, but some groups have setup their own cell browsers:

* Alexander Misharin Lab, Northwester University, https://www.nupulmonary.org/resources/
* Accelerating Medicine Partnership Consortium, https://immunogenomics.io/cellbrowser/, used in `Zhang et al. 2018 <https://www.biorxiv.org/content/10.1101/351130v1>`_ and `Der et al 2018 <https://www.biorxiv.org/content/10.1101/382846v1>`_
* Ovarian Cancer Cell Laboratory, Nuffield Department of Women's & Reproductive Health, University of Oxford, https://ovariancancercell.github.io/
* Zemans Lab, Ann Arbor, https://rnabioco.github.io/lung-scrna/
* Unpublished work, unknown lab?, http://covid19ocularsurface.org/
* Chinese University of Hong Kong, Testis cis-element atlas, http://testisatlas.s3-website-us-west-2.amazonaws.com/CB.html
* Sansom Lab, Oxford, https://sansomlab.github.io (they use an older version of the Cell Browser source code, reached out to update with our bugfix but did not get reply), for `Croft et al, Nature 2019 <https://www.nature.com/articles/s41586-019-1263-7>`_ 
* https://www.genomique.eu/cellbrowser/HCA/ Zaragosi group at IPMC CNRS Nice, for the manuscript https://dev.biologists.org/content/early/2019/09/25/dev.177428.abstract
* Same group seems to host a COVID-19 dataset: https://www.genomique.eu/cellbrowser/COVID/
* Bin Ren lab, CAAtlas http://catlas.org/mousebrain/#!/home
* Conrad lab at Charite Berlin: http://singlecell.charite.de/
* `STAB: a spatio-temporal cell atlas of the human brain <https://stab.comp-sysbio.org/tool/cellbrowser/index.html>`_ from  `Song et al NAR 2021 <https://academic.oup.com/nar/article/49/D1/D1029/5911746>`_.
* UCLA: http://mergeomics.research.idre.ucla.edu/PVDSingleCell/
* Lako Lab at Newcastle University, UK: http://retinalstemcellresearch.co.uk/CorneaCellAtlas/ from `Collins et al. 2021. The Ocular Surface. <https://www.sciencedirect.com/science/article/pii/S1542012421000215>`_
* RNA Bioscience Initiative: https://www.pneuroonccellatlas.org/ and https://github.com/rnabioco/lung-scrna
* Paul Gontarz, WUSTL, http://regmedsrv1.wustl.edu/Public_SPACE/pgontarz/Public_html/cellbrower/Exp1/


These papers have cell browsers made at UCSC:

* organoidatlas: https://www.sciencedirect.com/science/article/pii/S221112472030053X
* dros-brain: https://elifesciences.org/articles/50354
* kidney-atlas: https://science.sciencemag.org/content/365/6460/1461.abstract
* allen-celltypes/mouse-cortex: https://www.biorxiv.org/content/10.1101/2020.03.30.015214v1.full
* organoidreportcard: https://www.nature.com/articles/s41586-020-1962-0

Before judging this project by the number of issue tickets or PRs, note that at UCSC we use an internal
ticket system with more features and that a lot of communication with wetlab users is by email at cells@ucsc.edu, as we 
do not require a Github account for feedback. But we do reply to issues here, as you can see from the Github 
account and also use Github for source control.

Additional availability
-----------------------

* The preferred installation is via pip https://pypi.org/project/cellbrowser/, for documentation see https://cellbrowser.readthedocs.io
* Bioconda: this tool is available to install via `bioconda <https://bioconda.github.io/recipes/ucsc-cell-browser/README.html>`_. Note that the conda release is usually a bit outdated relative to the pip release, so use pip if possible. If you cannot use pip, please contact us. 
* Biocontainers: there is a biocontainer automatically generated from the bioconda package available `here <https://quay.io/repository/biocontainers/ucsc-cell-browser>`_
* The Seurat3Wizard, demo at http://nasqar.abudhabi.nyu.edu/SeuratV3Wizard, builds a cell browser as its last step
* Galaxy: there is a Galaxy tool for UCSC CellBrowser, which can be installed on any Galaxy instance via its `Galaxy Toolshed entry <https://toolshed.g2.bx.psu.edu/view/ebi-gxa/ucsc_cell_browser>`_ or it can be directly used by users at the `Human Cell Atlas Galaxy instance <https://humancellatlas.usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/ebi-gxa/ucsc_cell_browser/ucsc_cell_browser>`_ or as part of the example workflows, such as the `Human Cell Atlas / Scanpy CellBrowser workflow <https://humancellatlas.usegalaxy.eu/u/pmoreno/w/humancellatlas-scanpy-cellbrowser>`_ or the `EBI Single Cell Expression Atlas / Scanpy / CellBrowser workflow <https://humancellatlas.usegalaxy.eu/u/pmoreno/w/atlas-scanpy-cellbrowser-imported-from-uploaded-file>`_

This project was funded by the California Institute of Regenerative Medicine and the
Chan-Zuckerberg Initiative https://www.chanzuckerberg.com/. In 2020, it is funded through a supplement to the NHGRI Genome Browser grant.

This is early research software. You are likely to find bugs. Please open a Github
ticket or email us at cells@ucsc.edu, we can usually fix them quickly.

Citation
--------

If you use the UCSC Cell Browser in your work, please cite `Speir et al, Biorxiv 2020 <https://www.biorxiv.org/content/10.1101/2020.10.30.361162v1>`_ 
",,DEV,0.66,"# Contributor Code of Conduct

Please be nice, respectful of differing viewpoints and experiences. Note that every piece of code has a history that is not free of time/money/personal constraints.

You may wonder about the coding style and libraries used. The project does not use every popular Javascript framework. This has the disadvantage that some things are slower to implement at first, but has the advantage that we potentially can get contributions from programmers who are not yet on the most recent frameworks and also that when something breaks, we do not have to dig through the implicit behavior that is part of the framework. We hope that contributors are not put off by this. We do appreciate any ideas on how to improve the UI system, possibly by adding a UI framework in a lightweight way or just in any way, as our experience with Javascript frameworks is extremely limited.

This is a relatively small software project and all contributors have all been extremely nice, which is why this Code of Conduct is very short. 
",,,,,0,10,0.95,DEV
584585174,R_kgDOItgP1g,jchaodubs,jchaodubs/jchaodubs,0,jchaodubs,https://github.com/jchaodubs/jchaodubs,OTHER,,0,2023-01-03 01:41:21+00:00,2025-01-29 08:18:42+00:00,2025-01-29 08:18:39+00:00,,36,0,0,,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,0,main,1,['jchaodubs'],,"

<!--
**jchaodubs/jchaodubs** is a ‚ú® _special_ ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
- üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->
### Hey there! I'm Jeffrey
üí°  I like to explore new technologies and develop software solutions.<br/>
üéì  I'm currently studying Computer Science at the University of California, Santa Cruz.<br/>
‚úçÔ∏è  In my free time, I pursue Graphic Design and sports as well as dilly dallying.<br/>
üí¨  Feel free to reach out for any inquiries!<br/>
‚úâÔ∏è  You can shoot me an email at jchao11@ucsc.edu! I'll try to respond as soon as I get the feeling to respond to messages!.<br/>
üìÑ  Please have a look at my [R√©sum√©](https://drive.google.com/file/d/1AG5laF6WdzR-6KfxDZAFqpYw6prNF_wF/view?usp=sharing) or my [website](https://jeffrey-chao.com/) for more details about me. I'm open to feedback and suggestions!




![216644507-4f06ea29-bf55-4356-aac0-d42751461a9d](https://github.com/jchaodubs/jchaodubs/assets/87839757/7e4bb7fa-618e-4bfd-b357-a746f92fa63f)

",,OTHER,0.87,,,,,,0,1,0.7,OTHER
131026535,MDEwOlJlcG9zaXRvcnkxMzEwMjY1MzU=,theme-campus-calendar,ucsc/theme-campus-calendar,0,ucsc,https://github.com/ucsc/theme-campus-calendar,DEV,Silk Wrapper for UCSC's Localist events calendar,0,2018-04-25 15:24:43+00:00,2022-12-12 19:52:02+00:00,2023-05-09 17:06:43+00:00,,594,0,0,HTML,1,0,1,1,1,0,1,0,0,0,,1,0,0,public,1,0,0,master,1,"['Herm71', 'knice']",1,"# ucsc-localist-silk-wrapper

This is the SILK WRAPPER for the [UC Santa Cruz Events Calendar](https://calendar.ucsc.edu/), hosted on the [Localist](https://www.localist.com/) platform.

## Dependencies

- [npm](https://www.npmjs.com/)
- [Node.js](https://nodejs.org/en/)

## Install and build

Clone repo locally, `cd` into it and run `npm install`.

[Webpack](https://webpack.js.org/) is used to compile `html`, `js`, and `css` from the `src/` to the `dist/` directory. Final code is deployed from the `dist/` directory.

**scripts:** the `package.json` has three scripts:

- `start` starts the webserver serving from the `dist` directory
- `build` builds the source files into the `dist` directory
- `build-watch` watches the source files for changes and automatically builds
- 'release' for running `standard-version`

## Master vs Dev branches

Master branch is hosted on Netlify and dev branches are hosted on Github Pages
",,WEB,0.94,,,,,,0,4,1,WEB
234991012,MDEwOlJlcG9zaXRvcnkyMzQ5OTEwMTI=,CSE150,ashwinchidambaram/CSE150,0,ashwinchidambaram,https://github.com/ashwinchidambaram/CSE150,EDU,Computer Networking at UCSC,0,2020-01-20 00:47:54+00:00,2020-09-06 05:35:15+00:00,2020-03-24 23:30:30+00:00,,1408,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['ashwinchidambaram'],,"# CSE150
Computer Networking at UCSC
",,EDU,0.64,,,,,,0,1,0.8,EDU
78243004,MDEwOlJlcG9zaXRvcnk3ODI0MzAwNA==,TextAnnotationGraphs,CreativeCodingLab/TextAnnotationGraphs,0,CreativeCodingLab,https://github.com/CreativeCodingLab/TextAnnotationGraphs,DEV,"A modular annotation system that supports complex, interactive annotation graphs embedded on top of sequences of text.",0,2017-01-06 22:17:24+00:00,2025-02-22 13:07:41+00:00,2021-12-21 17:51:58+00:00,https://creativecodinglab.github.io/TextAnnotationGraphs/,8004,95,95,JavaScript,1,1,1,1,1,0,18,0,0,13,mit,1,0,0,public,18,13,95,master,1,"['hmsklee', 'ZechyW', 'myedibleenso', 'aznmonkey']",1,"# TextAnnotationGraphs (TAG)

A modular annotation system that supports complex, interactive annotation graphs embedded on top of sequences of text. An additional view displays a subgraph of selected connections between words/phrases using an interactive network layout.

![TAG](figs/OneRow.png)

---

![TAG](figs/taxonomyColors.png)

---

![TAG](figs/TwoRows.png)

---

![TAG](figs/trees.png)

## Development

TAG was developed by Angus Forbes (UC Santa Cruz) and Kristine Lee (University of Illinios at Chicago), in collaboration with Gus Hahn-Powell, Marco Antonio Valenzuela Esc√°rcega, Zechy Wong, and Mihai Surdeanu (University of Arizona). Contact angus@ucsc.edu for more information.

# Citing TAG

If you use TAG in your work, please use the following citation:

```TeX
@inproceedings{TAG-2018,
    author = {Angus Forbes and Kristine Lee and Gus Hahn-Powell and Marco A. Valenzuela-Esc√°rcega and Mihai Surdeanu},
    title = {Text Annotation Graphs: Annotating Complex Natural Language Phenomena},
    booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC'18)},
    year = {2018},
    month = {May},
    date = {7-12},
    address = {Miyazaki, Japan},
    editor = {Sara Goggi and H√©l√®ne Mazo},
    publisher = {European Language Resources Association (ELRA)},
    language = {english}
}
```

## Write-up

A paper describing TAG was accepted to [LREC'18](http://lrec2018.lrec-conf.org/en/conference-programme/accepted-papers/). A pre-print can be found at [https://arxiv.org/abs/1711.00529](https://arxiv.org/abs/1711.00529)

## Installation

TAG can be built and installed using [`npm`](https://docs.npmjs.com/getting-started/installing-node).

### Via `npm`

```JavaScript
npm install git+https://github.com/CreativeCodingLab/TextAnnotationGraphs.git
```

## Usage

To use TAG with your own applications, first include the library in your script:

#### Browserify (CommonJS)

```JavaScript
const TAG = require(""text-annotation-graphs"");
```

#### ES6

```JavaScript
import TAG from ""text-annotation-graphs"";
```

Next, register one or more annotation format Parsers with the library. TAG comes with parsers for the BRAT and Odin annotation formats -- See the demo sources (`demo/src/demo.js`) and the Parsers readme (`Parsers/README.md`) for more information.

```JavaScript
const OdinParser = require(""text-annotation-graphs/Parsers/odin"");
TAG.registerParser(new OdinParser(), ""odin"");
```

Then initialise the visualisation on an element, optionally specifying the initial data set to load and any overrides to the default options. For more details, consult the [full API documentation](docs/index.html).

```JavaScript
const graph = TAG.tag({
  container: $container,
  data: {...},
  format: ""odin"",
  options: {...}
});
```

## Development

Tasks are managed via [`npm` scripts](https://docs.npmjs.com/misc/scripts) and the [`runjs` build tool](https://github.com/pawelgalazka/runjs). The most commonly used tasks are listed in `package.json`, and details for the various sub-tasks can be found in `tasksfile.js`.

### Demo

See the live demo [here](demo/index.html).

After cloning the repository and installing the project dependencies via `npm install`, you can run the interactive demo using `npm run demo` and directing your browser to `localhost:8080`.

To run the demo on a different port, set the `PORT` environmental variable. For example, running `PORT=9000 npm run demo` will start the demo server on `localhost:9000` instead.

### Building the source

TAG is written in ES6, and uses [Sass](https://sass-lang.com/) for its styles.

Assuming you've cloned the repository and installed its dependencies, run `npm run build` to transpile the source to ES2015 and generate the library bundles (`dist/tag`) and documentation (`docs`).

To rebuild the sources for the demo, run `npm run demo-build`.

### Live monitoring of changes

For convenience, you can monitor changes to the library's sources (in the `src` folder) with the following `npm` task, which will regenerate the files in the `dist/tag` folder:

```
npm run watch
```

If you are experimenting with the demo and would like to monitor changes to the demo sources (in the `demo` folder), use the following `npm` task instead:

```
npm run demo-watch
```

The `demo-watch` task will also catch changes to the library sources and rebuild the demo bundles, but it will _not_ directly rebuild the library bundles in `dist/tag`, so be sure to run `npm run build` separately if you intend to redistribute/repackage your changes to the library sources.

### Generating documentation

TAG uses [JSDoc](http://usejsdoc.org/) to generate its documentation. By default, the documentation is generated using the template in `src/jsdoc-template` (adapted from the [Braintree JSDoc Template](https://github.com/braintree/jsdoc-template)) and stored in the `docs` folder.

To regenerate the documentation, use the following `npm` task:

```javscript
npm run generate-docs
```
",,DEV,0.62,,,,,,0,27,0.8,DEV
93094843,MDEwOlJlcG9zaXRvcnk5MzA5NDg0Mw==,HyEQ_Toolbox_old,pnanez/HyEQ_Toolbox_old,0,pnanez,https://github.com/pnanez/HyEQ_Toolbox_old,DEV,"The Hybrid Equation (HyEQ) Toolbox is implemented in MATLAB/Simulink for the simulation of hybrid dynamical systems. This toolbox is capable of simulating individual and interconnected hybrid systems with inputs. Examples of systems that can be simulated include a bouncing ball on a moving platform, fireflies synchronizing their flashing, and more. The Simulink implementation includes four basic blocks that define the dynamics of a hybrid system. These include a flow map, flow set, jump map, and jump set. The flows and jumps of the system are computed by the integrator system which is comprised of blocks that compute the continuous dynamics of the hybrid system, trigger jumps, update the state of the system and simulation time at jumps, and stop the simulation. Also includes a ‚Äúlite simulator‚Äù which allows for faster simulation without using Simulink.",0,2017-06-01 20:02:36+00:00,2022-01-21 19:56:00+00:00,2017-06-01 20:10:59+00:00,,3475,0,0,HTML,1,1,1,1,0,0,1,0,0,0,apache-2.0,1,0,0,public,1,0,0,master,1,['pnanez'],,"# HyEQ_Toolbox
The Hybrid Equation (HyEQ) Toolbox is implemented in MATLAB/Simulink for the simulation of hybrid dynamical systems. This toolbox is capable of simulating individual and interconnected hybrid systems with inputs. Examples of systems that can be simulated include a bouncing ball on a moving platform, fireflies synchronizing their flashing, and more. The Simulink implementation includes four basic blocks that define the dynamics of a hybrid system. These include a flow map, flow set, jump map, and jump set. The flows and jumps of the system are computed by the integrator system which is comprised of blocks that compute the continuous dynamics of the hybrid system, trigger jumps, update the state of the system and simulation time at jumps, and stop the simulation. Also includes a ‚Äúlite simulator‚Äù which allows for faster simulation without using Simulink.

README FILE

[Hybrid Equations (HyEQ) Toolbox] for [Simulating Hybrid Systems in MATLAB/Simulink], [V2.04], by 
[Hybrid Dynamics and Control Lab @ University of California Santa Cruz]
[05/11/17]

CONTENTS
I.	HOW TO UNISTALL PREVIOUS TOOLBOX VERSIONS
II.	MINIMUM SYSTEM REQUIREMENTS
III.	HOW TO INSTALL THE TOOLBOX
IV.	SOFTWARE DOWNLOADS AND MANUAL

I. HOW TO UNISTALL PREVIOUS TOOLBOX VERSIONS

Windows
1. Open Matlab.
2. Go to the Matlab's standard toolbox folder (e.g, C:\\Program Files\\Matlab\\toolbox\\HyEQ_Toolbox_vxx). 
3. Run the unistall file >> tbclean.m This procedure erase all the files in the toolbox folder.
4. Close Matlab.

Macintosh
1. Open Matlab.
2. Go to the Matlab's standard toolbox folder (e.g, ~/matlab/HyEQ_Toolbox_vxx). 
3. Run the unistall file >> tbclean.m This procedure erase all the files in the toolbox folder.
4. Close Matlab.


II.	MINIMUM SYSTEM REQUIREMENTS

In order to run simulations using the Lite HyEQ Solver, MATLAB R13 or newer is required.

In order to run simulations using the HyEQ Simulator, MATLAB/Simulink and a supported ANSI, C, or C++ 32-bit compiler must be installed. Please check on 
http://www.mathworks.com/matlabcentral/fileexchange/41372-hybrid-equations-toolbox-v2-02
to instructions about how to install necessary compilers for Windows and Mac. For more information on supported compilers, please visit 
http://www.mathworks.com/support/compilers/R2012a/win32.html

III.	HOW TO INSTALL THE TOOLBOX

1. Save the folder InstallHyEQ_Toolbox_V2_04 into some location different than the matlab root folder (e.g. ~/matlab/ or C:\\Program Files\\Matlab). 
2. Open Matlab.
3. Locate and execute the file install.m by in matlab‚Äôs command window by (do not open install.m in the editor)
>> install
4. Follow the instructions

IV.	TO GET STARTED, PLEASE GO TO

Software downloads:
https://hybrid.soe.ucsc.edu/software
http://www.mathworks.com/matlabcentral/fileexchange/41372-hybrid-equations-toolbox-v2-02
Manual:
https://hybrid.soe.ucsc.edu/biblio/2014/hybrid-equations-hyeq-toolbox
Examples:
http://hybridsimulator.wordpress.com/
",,DEV,0.59,,,,,,0,1,0.9,DEV
746912740,R_kgDOLIT75A,B573_Assignment_2,nthamilton57/B573_Assignment_2,0,nthamilton57,https://github.com/nthamilton57/B573_Assignment_2,EDU,This is the repository for B573 Assignment 2 which serves as an introduction to Python.,0,2024-01-22 22:46:52+00:00,2024-01-22 22:58:07+00:00,2024-01-22 22:49:15+00:00,,64,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['nthamilton57'],,"# B573 Assignment 2: Introduction to Python

## Programming Language: Python

## Date: September 15, 2023

## Description:

This repository is designed to perform manipulations such as creating the reverse complement sequence and print certain portions of a sequence obtained from the University of California, Santa Cruz (UCSC) Genome browser. Additionally, the repository takes user input to generate a list of fibonacci numbers and two other lists derived from the first. Lastly, a matrix is created and different calculations are computed. This repository uses the programming language Python and serves as an introduction guide to the language.
### Required Files

<code>chr1_GL383518v1_alt.fa</code>  contains the sequence to be used in python scrips

<code>assignment_2.1.py</code>   outputs the 10th and 758th letter of the sequence

<code>assignment_2.2.py</code>   creates the reverse complement of the sequence and print the 79th & the 500th through the 800th letters 

<code>assignment_2.3.py</code>   generates the fibonacci sequence and associated lists of a user defined length

<code>assignment_2.4.py</code>   creates a matrix and calculates associated properties of it

<code>assignment_2.5.py</code>   generates the base totals for each kilobase of the sequence


### Required Packages

There are no specific packages required for this repository.

### Execution

<ins>Steps to Run the Script</ins>

1. Clone repository

2. Change working directory to location of files

    ```
    cd path/to/files
    ```

3. Run the scripts using the following commands: 

    ```
    python assignment_2.1.py 
    python assignment_2.2.py
    python assignment_2.3.py
    python assignment_2.4.py
    python assignment_2.5.py
    ```

4. Each script will output results in the command line.

    a. <code>assignment_2.3.py</code> will require user input into the command line.

### Output Files

No Output Files
",,EDU,0.6,,,,,,0,1,0.7,EDU
184997858,MDEwOlJlcG9zaXRvcnkxODQ5OTc4NTg=,MesKit,Niinleslie/MesKit,0,Niinleslie,https://github.com/Niinleslie/MesKit,DEV,A tool kit for dissecting cancer evolution from multi-region derived tumor biopsies via somatic mutations,0,2019-05-05 07:50:22+00:00,2024-09-27 15:44:39+00:00,2022-01-28 03:44:04+00:00,,45174,35,35,R,1,1,1,1,0,0,9,0,0,10,gpl-3.0,1,0,0,public,9,10,35,master,1,"['Niinleslie', 'Ninomoriaty', 'chenjy327', 'likelet', 'nturaga', 'Wangxin555']",,"
<img src=""https://github.com/Niinleslie/MesKit/blob/mnliu/vignettes/logo.png"" height=""80"" width=""240"" alt = ""Github logo"" /> 

[![check in Biotreasury](https://img.shields.io/badge/Biotreasury-collected-brightgreen)](https://biotreasury.rjmart.cn/#/tool?id=23)
# [M]()ulti-region [e]()xome [s]()equencing analysis tool [Kit]()

Intra-tumor heterogeneity (ITH) is now thought to be a key factor contributing to the therapeutic failures and drug resistance, which have attracted increasing attention in the cancer research field. Here, we present an R package, MesKit, for characterizing cancer genomic ITH and inferring the history of tumor evolution via implementation of well-established computational and statistical methods. 
The source code and documents are freely available through Github (https://github.com/Niinleslie/MesKit). A shiny application was developed to provide easier analysis and visualization.


## Installation

```R
# install via Bioconductor
if (!requireNamespace(""BiocManager"", quietly = TRUE))
    install.packages(""BiocManager"")

BiocManager::install(""MesKit"")

#  install the latest version from GitHub
if(!require(devtools)) install.packages(""devtools"")
devtools::install_github(""Niinleslie/MesKit"")
```

## Usage
The structured documentation of MesKit can be found at [http://meskit.renlab.org/](http://meskit.renlab.org/).   
<div  align=""left"">   

<img src=""https://github.com/Niinleslie/MesKit/blob/mnliu/vignettes/MesKit_workflow.png"" height=""500"" width=""600"" alt = ""MesKit Workflow""/>

</div>

## Shiny APP

For GUI-based analysis, users can use the following code to launch Shiny app build with the package.

```R
pkg.suggested <- c('shiny','shinyBS','shinydashboard', 'shinyWidgets', 'shinycssloaders', 'DT',
	'BSgenome.Hsapiens.UCSC.hg19')
## if genomic reference version is hg18/hg38, change 'BSgenome.Hsapiens.UCSC.hg19' to 'BSgenome.Hsapiens.UCSC.hg18' or 'BSgenome.Hsapiens.UCSC.hg38'

# Install the required packages
checkPackages <- function(pkg){
  if (!requireNamespace(pkg, quietly = TRUE)) {
    stop(paste0(""Package "", pkg, "" needed for shiny app. Please install it.""), call. = FALSE)
  }
}
lapply(pkg.suggested, checkPackages)
# run shiny app from shiny package
shiny::runApp(system.file(""shiny"", package = ""MesKit""))
```

Also, you can run the shiny interface by:

```R
runMesKit()
```

The guidance video for MesKit Shiny APP can be found at http://meskit.renlab.org/video.html.

## Configure Shiny APP with Docker 

We provided a docker image for a quick configuration of shiny app bundle with shiny-server, please see the simple commands [here](https://github.com/Niinleslie/MesKit/blob/master/MesKit.docker.md).

## Authors
This software was mainly developed by:

* Mengni Liu, liumn5@mail2.sysu.edu.cn, Sun Yat-sen university 
* Jianyu Chen, chenjy327@mail2.sysu.edu.cn, Sun Yat-sen university 
* Xin Wang, wangx555@mail2.sysu.edu.cn, Sun Yat-sen university

## Supervised by 

* [Jian Ren](renjian@sysucc.org.cn) and [Qi Zhao](zhaoqi@sysucc.org.cn) from Bioinformatic Center of Sun Yat-sen University Cancer Center 

## Maintainer
[Mengni Liu](liumn5@mail2.sysu.edu.cn), Sun Yat-sen university  <br/>

## Copyright

Copyright ¬© 2014-2021. RenLab from SYSUCC. All Rights Reserved<br/>
For more useful tools/applications, please go to [renlab.org](http://www.renlab.org)

## Citation

_Mengni Liu, Jianyu Chen, Xin Wang, Chengwei Wang, Xiaolong Zhang, Yubin Xie, Zhixiang Zuo, Jian Ren, Qi Zhao, MesKit: a tool kit for dissecting cancer evolution of multi-region tumor biopsies through somatic alterations, GigaScience, Volume 10, Issue 5, May 2021, giab036, https://doi.org/10.1093/gigascience/giab036_
",,DEV,0.52,,,,,,0,4,0.95,DEV
302535217,MDEwOlJlcG9zaXRvcnkzMDI1MzUyMTc=,AM229F20,abhishekhalder/AM229F20,0,abhishekhalder,https://github.com/abhishekhalder/AM229F20,EDU,"Convex Optimization, Fall 2020, UCSC",0,2020-10-09 04:47:13+00:00,2020-12-15 01:35:55+00:00,2020-12-15 01:35:53+00:00,,1987,1,1,Jupyter Notebook,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,1,main,1,['abhishekhalder'],,"# AM229F20
Convex Optimization, Fall 2020, UCSC
",,EDU,0.74,,,,,,0,1,0.9,EDU
129478931,MDEwOlJlcG9zaXRvcnkxMjk0Nzg5MzE=,DevOps01,joarreola/DevOps01,0,joarreola,https://github.com/joarreola/DevOps01,EDU,For UCSC-extention DevOps class.,0,2018-04-14 03:49:05+00:00,2018-04-24 20:49:06+00:00,2018-04-24 20:49:05+00:00,,6,0,0,Shell,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['joarreola'],,"# DevOps01
For UCSC-extention DevOps class.
",,EDU,0.88,,,,,,0,0,0.8,EDU
616583700,R_kgDOJMBSFA,Virtual-Worlds-with-Cylinders,ivalmart/Virtual-Worlds-with-Cylinders,0,ivalmart,https://github.com/ivalmart/Virtual-Worlds-with-Cylinders,EDU,A documentation of creating my own virtual world in a Computer Graphics class offered at UCSC.,0,2023-03-20 17:10:09+00:00,2023-03-20 17:12:35+00:00,2023-04-24 18:11:58+00:00,,3529,0,0,JavaScript,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['ivalmart'],,"# Virtual Worlds with Cylinders
Projects 1-4 done by: Ivan Martinez-Arias<br>
Project 5 done by: Ivan Martinez-Arias & Alan Chen

## Description
These projects were done for a Computer Graphics class at the University of California, Santa Cruz to create a virtual world using low-level graphics language (WebGL). At the end of the class with the experience of creating graphics with WebGL, we were tasked with exploring high-level graphics libraries.<br>

As our final project, we had to create a scene with similar aspects from the previous projects, containing a dynamic scene with lighting, camera, and objects where users can manipulate the objects and/or camera interactivity.",,EDU,0.77,,,,,,0,1,0.9,EDU
25713841,MDEwOlJlcG9zaXRvcnkyNTcxMzg0MQ==,edibility,jtsommers/edibility,0,jtsommers,https://github.com/jtsommers/edibility,EDU,UCSC dining hall menu application with push notifications,0,2014-10-25 00:07:27+00:00,2014-11-24 04:52:05+00:00,2014-11-24 04:52:05+00:00,,4006,0,0,JavaScript,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,master,1,"['jtsommers', 'huanghunz', 'JoeWise']",,#ERROR!,,DEV,0.81,,,,,,0,1,0.95,DEV
193173334,MDEwOlJlcG9zaXRvcnkxOTMxNzMzMzQ=,UCSC-AMS147,nbaledio/UCSC-AMS147,0,nbaledio,https://github.com/nbaledio/UCSC-AMS147,EDU,AMS147: Computational Methods and Applications,0,2019-06-22 00:14:57+00:00,2025-02-13 06:15:54+00:00,2019-06-22 00:27:10+00:00,,1498,0,0,MATLAB,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['nbaledio'],,"# University of California Santa Cruz
# AMS147: Computational Methods and Applications
# Quarter: Winter 2018
# Professor: Daniele Venturi
",,EDU,0.8,,,,,,0,0,0.8,EDU
662342272,R_kgDOJ3qKgA,RSA-Encryption,BrandonChuangXRD/RSA-Encryption,0,BrandonChuangXRD,https://github.com/BrandonChuangXRD/RSA-Encryption,EDU,"An RSA key generator, encryptor, and decryptor in C that I made for a class at UC Santa Cruz",0,2023-07-04 23:36:53+00:00,2023-07-04 23:38:41+00:00,2023-07-04 23:38:37+00:00,,77,0,0,C,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,[],,"# RSA Encryption/Decryption/Key Generation
An RSA library developed in C for a class I took in UC Santa Cruz.

**Installation**\\
A makefile is included, which will do the following with the given flags:
| Flag | Description |
| --- | --- |
| all | creates all files and binaries |
| keygen | creates all files and binaries related to ```keygen``` |
| encrypt | creates all files and binaries related to ```encrypt``` |
| decrypt | creates all files and binaries related to ```decrypt``` |
| clean | remove object (*.o) and executable binary files |
| cleankeys | remove any files with the extension ```.pub``` and ```.priv``` |
| format | formats all code using clang-format |

if it says that ```gmp.h``` is not found, then make sure to run ```sudo apt install pkg-config libgmp-dev```

**Usage**
### keygen:

```./keygen [-b] [-i] [-n] [-d] [-s] [-v] [-h]```
| Flag | Description |
| --- | --- |
| -b | specifies the amount of bits required for a key |
| -i | specifies the number of Miller-Rabin iterations |
| -n | specifies the public key file (default: rsa.pub) |
| -d | specifies the private key file (default: rsa.priv) |
| -s | specifies the seed fo RNG (default: time in seconds since UNIX epoch) |
| -v | enables verbose output |
| -h | prints out help message |

### encrypt/decrypt:

```./encrypt [-i] [-o] [-n] [-v] [-h]```

```./decrypt [-i] [-o] [-n] [-v] [-h]```
| Flag | Description |
| --- | --- |
| -i | specifies the input file |
| -o | specifies the output file |
| -n | specifies the key file (public key for encrypt, private key for decrypt) |
| -v | enables verbose output |
| -h | prints out help message |

**CREDITS:**\\
Part of the makefile was made by a TA only noted by ""Sloany Liu""

As with the other assignments, this copyright was included in the ```sets.h``` file, so it may be the case that these .h files were created by professor Darell Long:
```
BSD 2-Clause License
Copyright (c) 2021, Darrell Long
All rights reserved.
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:
1. Redistributions of source code must retain the above copyright notice, this
list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
this list of conditions and the following disclaimer in the documentation
and/or other materials provided with the distribution.
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
```

**KNOWN ERRORS**\\
None.
",,EDU,0.62,,,,,,0,1,0.8,EDU
474461592,R_kgDOHEe1mA,CSE110A-sp2022,SorensenUCSC/CSE110A-sp2022,0,SorensenUCSC,https://github.com/SorensenUCSC/CSE110A-sp2022,EDU,class website for CSE110A spring 2022 at UCSC,0,2022-03-26 20:40:54+00:00,2023-10-08 11:09:06+00:00,2023-04-03 01:35:54+00:00,,38074,2,2,HTML,1,1,1,1,1,0,1,0,0,0,,1,0,0,public,1,0,2,main,1,"['tyler-utah', 'xuyanwen2012', 'SorensenUCSC']",,"# CSE110A-sp2022
class website for CSE110A spring 2022 at UCSC
",1,WEB,0.74,,,,,,0,3,1,WEB
765392,MDEwOlJlcG9zaXRvcnk3NjUzOTI=,simple-ftp,sunaku/simple-ftp,0,sunaku,https://github.com/sunaku/simple-ftp,EDU,My final project for CMPE-150 at UCSC during Spring 2004.,0,2010-07-09 06:16:00+00:00,2023-10-09 01:23:10+00:00,2015-05-11 22:08:54+00:00,,143,27,27,C,1,1,1,1,0,0,21,0,0,2,,1,0,0,public,21,2,27,master,1,['sunaku'],,"Suraj Kurapati <skurapat@ucsc.edu>
CMPE-150, Spring 2004
Network Programming Project

CONTENTS:

    files
    instructions
    specification
    known problems
    comments
    references

FILES:

    html/
        API documentation generated by Doxygen tool.

    Makefile
        Compilation script.

    README
        General project overview.

    siftp.[ch]
        Provide syntax & semantics for the SimpleFTP protocol. They also
        provide transport level services such as sending and receiving
        messages.

    service.[ch]
        Provide a common interface for mainly session level services. They
        also provide some application level services that are common to both
        server & client.

    client.[ch]
        The SimpleFTP client.

    server.[ch]
        The SimpleFTP server. Is able to handle multiple connections and has a
        simple authorization system via a common password.

INSTRUCTIONS:

    Compiling the programs.

        Run the command 'make all'.

        To view debugging messages, enable the debugging flag in the Makefile.

    Using the SimpleFTP client.

        Usage:
            siftp <server name or IP> <port number>

        Info:
            Once connected to the server, the user will be presented with a
            password prompt. The default password is ""ce150"".

            Once authorized, the user will be presented with an application
            prompt, at which the command 'help' can be issued for details on
            supported commands.

    Using the SimpleFTP server.

        Usage:
            siftpd <starting directory> <port number>

        Info:
            Connected users will have their initial current working directory
            set to <starting directory>.

            The server should only be terminated when no users are connected
            in order to prevent stale sockets.


SPECIFICATION:

    The SimpleFTP protocol.

        The purpose of this protocol is to allow for file transfer between a
        file server and authorized clients.

        This is a request and response based protocol which is largely derived
        from the FTP protocol (RFC 959).

        The notion of request and response are implicit, and are not
        specifically tied to the client nor server. For example, in some cases
        the server sends a request to the client and expects a response.

    Message format.

        A message consists of a 'Verb' and a 'Parameter' (see Figure 1). The
        Verb specifies the subject, action, or intent of the message. The
        Parameter specifies information pertaining to the Verb. For a full
        list of Verbs and their use of the Parameter field, see file
        'siftp.h'.


            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                       _____________________________________________
                      | VERB           | PARAMETER                  |
                      |________________|____________________________|

                      |<------------- 1024 bytes ------------------>|

                      |<-- 4 bytes --->|<-------- 1020 bytes ------>|

            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            Figure 1. The message format. Note that these sizes are defined in
            the file <siftp.h> and can be changed.


        When sent over the network, the message is serialized into a string of
        contiguous bytes. The format of the serialized message is: the Verb
        followed by the Parameter. The sizes of these fields are defined in
        'siftp.h'.

    Command & Status dialogue.

        The client issues commands to the server and expects a response
        message containing the status of the command. This status is then used
        to decide the next action. For example if a client requests a file for
        download, the server can respond with a failed status (because it
        doesn't have read access to the requested file). Now the client
        doesn't have to wait for data because it knows of the command's
        status.

    Authentication dialogue.

        After the client establishes a TCP connection to the server, the
        client must be authorized before being allowed access to the files on
        the server.

        Client: I want to connect.
        Server: Identify yourself.
        Client: My user name is <name>.
        Server: Accepted/Denied.
        Client: My password is <pass>.
        Server: Accepted/Denied.
        - session established -

    File upload dialogue.

        Client: I want to upload a file named <filename>.
        Server: Proceed/Abort.
        Client: I am sending <length> number of bytes.
        Client: Here is some data.
            .
            .
            .
        Client: I have finished sending data.
        - transfer complete -

    File download dialogue.

        Client: I want to download a file named <filename>.
        Server: Proceed/Abort.
        Server: I am sending <length> number of bytes.
        Server: Here is some data.
            .
            .
            .
        Server: I have finished sending data.
        - transfer complete -

KNOWN PROBLEMS:

    In the SimpleFTP client, the application prompt initially is displayed
    twice.

    Stale sockets are present if the SimpleFTP server process is forcibly
    terminated while there are connected clients.

COMMENTS:

    Caveat: although I use the layer terminology of the OSI network model in
    describing the components of this program, they all really belong to OSI
    application layer.

    I have some network programming experience in Java, but this was the first
    time I worked on a network programming project in C.

    I originally planned to use the frame transmission strategies of using
    flags to delimit start and end of a frame [1], for performing Message
    transmissions. This model allows the transport layer to take care of
    breaking, say, a huge data file, into IP packets. However, I chose later
    on to use a fixed length message model, where this application took on the
    role of splitting, say, a huge data file into discrete Messages because it
    was easier to implement.

    I relied heavily on [2] for socket initialization in both server and
    client programs, as I was unfamiliar with BSD sockets.

REFERENCES (in MLA citation format):

    1. Tanenbaum, Andrew. Computer Networks. Fourth edition. New Delhi, India:
       Prentice Hall of India, 2003.

    2. Hall, Brian. ""Beej's Guide to Network Programming"". 8 October 2001. 22
       May 2004 <http://www.ecst.csuchico.edu/~beej/guide/net/>.

",,EDU,0.78,,,,,,0,6,0.9,EDU
33798988,MDEwOlJlcG9zaXRvcnkzMzc5ODk4OA==,104a-A1,Nicojaw/104a-A1,0,Nicojaw,https://github.com/Nicojaw/104a-A1,EDU,,0,2015-04-12 01:33:50+00:00,2015-04-14 05:05:40+00:00,2015-04-14 05:05:40+00:00,,168,0,0,C++,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['Nicojaw'],,"# 104a-A1

This program was completed using pair programming.
Partner: Nico Williams (nijowill@ucsc.edu)
Partner: Brandon Rullamas (brullama@ucsc.edu)

We acknowledge that each partner in a programming pair should
drive roughly 50% of the time the pair is working together, and
at most 25% of an individual's effort for an assignment should
be spent working alone.  Any work done by a solitary programmer
must be reviewed by the partner.  The object is to work
together, learning from each other, not to divide the work into
two pieces with each partner working on a different piece.

Nico Williams spent			3 hours working alone.
Brandon Rullamas spent		2 hours working alone.
We spent					10 hours working together.
Nico Williams spent			5 hours driving.
Brandon Rullamas spent 		5 hours driving.

Please grade the work submitted by nijowill@ucsc.edu
and not the work submitted by brullama@ucsc.edu.
",,EDU,0.74,,,,,,0,2,0.9,EDU
351582820,MDEwOlJlcG9zaXRvcnkzNTE1ODI4MjA=,CAL,UCSC-REAL/CAL,0,UCSC-REAL,https://github.com/UCSC-REAL/CAL,DEV,A Second-Order Approach to Learning with Instance-Dependent Label Noise (CVPR'21 oral),0,2021-03-25 21:39:56+00:00,2025-01-17 16:07:11+00:00,2022-11-24 17:25:59+00:00,,416,42,42,Python,1,1,1,1,0,0,9,0,0,2,mit,1,0,0,public,9,2,42,main,1,['zwzhu-d'],1,"# A Second-Order Approach to Learning with Instance-Dependent Label Noise (CVPR'21 oral)
This code is a PyTorch implementation of the paper ""[A Second-Order Approach to Learning with Instance-Dependent Label Noise](https://arxiv.org/abs/2012.11854)"" accepted by CVPR 2021 as oral presentation.


# Prerequisites
Python 3.6.9

PyTorch 1.4.0

Torchvision 0.5.0


# Instructions
**Run the code:**

CIFAR10:
```python
python run_exptPRLD_C10_CAL.py
```

CIFAR100:
```python
python run_exptPRLD_C100_CAL.py
```

The following changes also apply to CIFAR100.


# Run the code step-by-step
## Step-1: **Construct $\\hat D$:**

Modify Lines 27-34 of *run_exptPRLD_C10_CAL.py* as: 
```python
#-------------- customized parameters --------------#
noise_rate = 0.6 # noise rates = 0.2, 0.4, 0.6

lossfunc = ""crossentropy""  # use this lossfunc for constructing D
# lossfunc = ""crossentropy_CAL"" # use this lossfunc for CAL

gpu_idx = ""0""   # Choose one GPU index
#---------------------------------------------------#
```

## Step-2: **Train CAL:**
Modify Lines 27-34 of *run_exptPRLD_C10_CAL.py* as: 
```python
#-------------- customized parameters --------------#
noise_rate = 0.6 # noise rates = 0.2, 0.4, 0.6

# lossfunc = ""crossentropy""  # use this lossfunc for constructing D
lossfunc = ""crossentropy_CAL"" # use this lossfunc for CAL

gpu_idx = ""0""   # Choose one GPU index
#---------------------------------------------------#
```


## Citation

If you find this code useful, please cite the following paper:

```
@inproceedings{zhu2021second,
  title={A second-order approach to learning with instance-dependent label noise},
  author={Zhu, Zhaowei and Liu, Tongliang and Liu, Yang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10113--10123},
  year={2021}
}
```
##
Corresponding authors:

Dr. [Zhaowei Zhu](https://users.soe.ucsc.edu/~zhaoweizhu/): zwzhu@ucsc.edu

Prof. [Yang Liu](http://www.yliuu.com/): yangliu@ucsc.edu
",,DEV,0.92,,,,,,0,2,0.95,DEV
510837923,R_kgDOHnLEow,LMRG-Mission,ABRFLMRG/LMRG-Mission,0,ABRFLMRG,https://github.com/ABRFLMRG/LMRG-Mission,DOCS,Mission,0,2022-07-05 17:43:30+00:00,2022-07-05 17:44:34+00:00,2022-07-05 17:47:14+00:00,,3,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['erikawee1290'],1,"
Our goal is to promote scientific exchange between researchers, specifically those in core facilities in order to increase our general knowledge and experience. We seek to provide a forum for multi-site experiments exploring ‚Äústandards‚Äù for the field of light microscopy.
https://www.abrf.org/light-microscopy-lmrg-

Current LMRG Members:

Benjamin Abrams - University of California Santa Cruz

Amanda Ammer - West Virginia University

Constadina Arvanitis - Northwestern University

Linda Callahan - University of Rochester Medical Center

Richard Cole - Wadsworth Center

Joseph Dragavon - University of Colorado

Kari Herrington - University of California, San Francisco

Jessica Hornick - Northwestern University

Michelle Itano - University of North Carolina

Daniel Keeley - University of North Carolina

Justine Kigenyi (EB liaison) - Kansas University Medical Center

Soyeon Kim - University of California, San Francisco

Kristopher Kubow (Chair) - James Madison University

DeLaine Larsen - University of California, San Francisco

Willa Ma - University of Southern California

Guillermo Marques - University of Minnesota

Arvydas Matiukas - SUNY Upstate Medical University

Thomas Pengo - University of Minnesota

James Powers - Indiana University

Josh Rappoport - Boston College

Mark Sanders - University of Minnesota

Erika Wee - Cold Spring Harbor Laboratory
",,WEB,0.6,,,,,,0,1,0.7,WEB
32295933,MDEwOlJlcG9zaXRvcnkzMjI5NTkzMw==,HSPuzzleScript,mobramaein/HSPuzzleScript,0,mobramaein,https://github.com/mobramaein/HSPuzzleScript,EDU,A Haskell Interpreter for PuzzleScript,0,2015-03-16 02:10:01+00:00,2023-07-17 16:33:06+00:00,2015-03-19 21:02:45+00:00,,2231,2,2,Haskell,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,2,master,1,"['dlederle', 'mobramaein']",,"# Haskell Compiler for PuzzleScript

This is the project readme for HSPuzzleScript a Haskell based Compiler for PuzzleScript.

This is an ongoing project at the University of California, Santa Cruz that started as an offshoot of the final project for CMPS203 (Programming Languages). 
",,EDU,0.84,,,,,,0,2,0.95,EDU
91496628,MDEwOlJlcG9zaXRvcnk5MTQ5NjYyOA==,c-cpp-a,Kelvinson/c-cpp-a,0,Kelvinson,https://github.com/Kelvinson/c-cpp-a,EDU,The homework and lecture code for UC Santa Cruz C Plus Plus for C Programmer course on Coursera,0,2017-05-16 19:30:02+00:00,2017-05-16 19:43:29+00:00,2017-05-25 07:51:13+00:00,,14,0,0,C++,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,[],,"#The homework and lecture code for UC Santa Cruz C Plus Plus for C Programmer course on Coursera
",,EDU,0.7,,,,,,0,2,0.9,EDU
315139645,MDEwOlJlcG9zaXRvcnkzMTUxMzk2NDU=,Project-2-Data-visualization,zsu44/Project-2-Data-visualization,0,zsu44,https://github.com/zsu44/Project-2-Data-visualization,EDU,,0,2020-11-22 21:53:41+00:00,2020-11-24 06:25:59+00:00,2020-11-24 06:25:57+00:00,,101,0,0,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,"['G4m3rpri3st', 'zsu44']",,"# Project-2-Data-Visualization
Members: Zhiheng Su, Logan Flansaas ||
Email: zsu44@ucsc.edu, lflansaa@ucsc.edu ||
Example: https://www.d3-graph-gallery.com/graph/connectedscatter_select.html
",,EDU,0.78,,,,,,0,1,0.8,EDU
386002398,MDEwOlJlcG9zaXRvcnkzODYwMDIzOTg=,travel-ucsc,ucsc/travel-ucsc,0,ucsc,https://github.com/ucsc/travel-ucsc,WEB,Github Repo for travel.ucsc.edu WordPress codebase,0,2021-07-14 16:18:03+00:00,2021-07-14 21:17:43+00:00,2021-07-14 21:17:40+00:00,,59088,0,0,PHP,1,1,1,1,0,0,0,0,0,0,other,1,0,0,public,0,0,0,master,1,"['danielbachhuber', 'mboynes', 'greg-1-anderson', 'Herm71', 'nullvariable', 'pwtyler', 'bensheldon', 'mikevanwinkle', 'ataylorme', 'scottbasgaard', 'gmcinnes', 'MikeNGarrett', 'tripflex', 'rachelwhitton', 'timani', 'TimothyBJacobs', 'westonruter', 'joemiller']",1,"# WordPress

This is a WordPress repository configured to run on the [Pantheon platform](https://pantheon.io).

Pantheon is website platform optimized and configured to run high performance sites with an amazing developer workflow. There is built-in support for features such as Varnish, Redis, Apache Solr, New Relic, Nginx, PHP-FPM, MySQL, PhantomJS and more.

## Getting Started

### 1. Spin-up a site

If you do not yet have a Pantheon account, you can create one for free. Once you've verified your email address, you will be able to add sites from your dashboard. Choose ""WordPress"" to use this distribution.

### 2. Load up the site

When the spin-up process is complete, you will be redirected to the site's dashboard. Click on the link under the site's name to access the Dev environment.

![alt](http://i.imgur.com/2wjCj9j.png?1, '')

### 3. Run the WordPress installer

How about the WordPress database config screen? No need to worry about database connection information as that is taken care of in the background. The only step that you need to complete is the site information and the installation process will be complete.

We will post more information about how this works but we recommend developers take a look at `wp-config.php` to get an understanding.

![alt](http://i.imgur.com/4EOcqYN.png, '')

If you would like to keep a separate set of configuration for local development, you can use a file called `wp-config-local.php`, which is already in our .gitignore file.

### 4. Enjoy

![alt](http://i.imgur.com/fzIeQBP.png, '')
",,WEB,0.82,,,,,,0,3,0.9,WEB
779384023,R_kgDOLnR01w,T2T-Browser,marbl/T2T-Browser,0,marbl,https://github.com/marbl/T2T-Browser,WEB,Genome browser hub for the T2T genomes and resources,0,2024-03-29 17:58:20+00:00,2025-02-21 20:41:30+00:00,2025-02-21 20:41:25+00:00,,13878,17,17,HTML,1,1,1,0,0,0,3,0,0,0,,1,0,0,public,3,0,17,main,1,"['arangrhie', 'nhansen', 'edmtorres', 'alekseyzimin']",1,"# T2T-Browser
UCSC Genome browser hub for the T2T genomes and resources

## Links to the browser

- Stable Release version (Recommended)
  - Hub Connect: [US](https://genome.ucsc.edu/cgi-bin/hgHubConnect?hgHub_do_redirect=on&hgHubConnect.remakeTrackHub=on&hgHub_do_firstDb=1&hubUrl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2Tgenomes/hub.txt)
   | [Euro](https://genome-euro.ucsc.edu/cgi-bin/hgHubConnect?hgHub_do_redirect=on&hgHubConnect.remakeTrackHub=on&hgHub_do_firstDb=1&hubUrl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2Tgenomes/hub.txt)
   | [Asia](https://genome-asia.ucsc.edu/cgi-bin/hgHubConnect?hgHub_do_redirect=on&hgHubConnect.remakeTrackHub=on&hgHub_do_firstDb=1&hubUrl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2Tgenomes/hub.txt)
  - Hub gateway: [US](http://genome.ucsc.edu/cgi-bin/hgGateway?genome=T2T-CHM13v2.0&hubUtl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2Tgenomes/hub.txt)
    | [Euro](http://genome-euro.ucsc.edu/cgi-bin/hgGateway?genome=T2T-CHM13v2.0&hubUtl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2Tgenomes/hub.txt)
    | [Asia](http://genome-asia.ucsc.edu/cgi-bin/hgGateway?genome=T2T-CHM13v2.0&hubUtl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2Tgenomes/hub.txt) 
  - Hub URL: https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2Tgenomes/hub.txt

- Developmental version
  - Hub Connect: [US](https://genome.ucsc.edu/cgi-bin/hgHubConnect?hgHub_do_redirect=on&hgHubConnect.remakeTrackHub=on&hgHub_do_firstDb=1&hubUrl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2T_test/hub.txt)
    | [Euro](https://genome-euro.ucsc.edu/cgi-bin/hgHubConnect?hgHub_do_redirect=on&hgHubConnect.remakeTrackHub=on&hgHub_do_firstDb=1&hubUrl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2T_test/hub.txt)
    | [Asia](https://genome-asia.ucsc.edu/cgi-bin/hgHubConnect?hgHub_do_redirect=on&hgHubConnect.remakeTrackHub=on&hgHub_do_firstDb=1&hubUrl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2T_test/hub.txt)
  - Hub gateway: [US](http://genome.ucsc.edu/cgi-bin/hgTracks?genome=T2T-CHM13v2.0&hubUrl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2T_test/hub.txt)
    | [Euro](http://genome-euro.ucsc.edu/cgi-bin/hgTracks?genome=T2T-CHM13v2.0&hubUrl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2T_test/hub.txt)
    | [Asia](http://genome-asia.ucsc.edu/cgi-bin/hgTracks?genome=T2T-CHM13v2.0&hubUrl=https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2T_test/hub.txt)
  - Hub URL: https://research.nhgri.nih.gov/CustomTracks/T2T_hubs/T2T_test/hub.txt
  
## What's in the browser
Latest tracks made publicly available for the following genomes:
- T2T-CHM13v2.0
- mGorGor1_v2.0
- mPanPan1_v2.0
- mPanTro3_v2.0
- mPonAbe1_v2.0
- mPonPyg2_v2.0
- mSymSyn1_v2.0
- HG002v1.0.1.MAT
- HG002v1.0.1.PAT
- HG002v1.1.MAT
- HG002v1.1.PAT

For convenience, the non-human primate genome browsers are available for its primary assembly versions as well in addition to the diploid version.

## Additional browser links
- Genome Archive [GenArk](https://hgdownload.soe.ucsc.edu/hubs/): Automatically built by UCSC Genome Browser for each NCBI genomes
- Legacy T2T-CHM13 browsers: [v1.0](http://genome.ucsc.edu/cgi-bin/hgTracks?genome=t2t-chm13-v1.0&hubUrl=http://t2t.gi.ucsc.edu/chm13/hub/hub.txt) | [v1.1](http://genome.ucsc.edu/cgi-bin/hgTracks?genome=t2t-chm13-v1.1&hubUrl=http://t2t.gi.ucsc.edu/chm13/hub/hub.txt)
- GenArk version of [T2T-CHM13v2.0](https://genome.ucsc.edu/h/GCA_009914755.4)
",,WEB,0.61,,,,,,0,8,0.9,WEB
126662193,MDEwOlJlcG9zaXRvcnkxMjY2NjIxOTM=,Intergalactic,CreativeCodingLab/Intergalactic,0,CreativeCodingLab,https://github.com/CreativeCodingLab/Intergalactic,DEV,The IGM-Vis project to Interactively visualize intergalactic medium (IGM) and circumgalactic medium (CGM) data in a Cosmic Web context. Demo available at:,0,2018-03-25 03:43:52+00:00,2022-07-07 17:42:23+00:00,2019-11-15 20:13:41+00:00,https://creativecodinglab.github.io/Intergalactic/intergalactic.html,157429,15,15,JavaScript,1,1,1,1,1,0,3,0,0,3,mit,1,0,0,public,3,3,15,master,1,"['davramov', 'jnburchett', 'AlfredoCuevas', 'JazzTap', 'angusforbes', 'apophenicPCG2019', 'cassiaa']",1,"# IGM-Vis: Analyzing Intergalactic and Circumgalactic Medium Absorption Using Quasar Sightlines in a Cosmic Web Context

[![IGM-Vis](images/IGM-Vis_overview.png)](images/IGM-Vis_overview.png ""IGM-Vis"")


The Intergalactic Media Visualization, or IGM-Vis, is a novel visualization and data analysis platform for investigating galaxies and the gas that surrounds them in context with their larger scale environment, the Cosmic Web.  Environment is an important factor in the evolution of galaxies from actively forming stars to a quiescent state with little, if any, discernible star formation activity. The gaseous halos of galaxies (the circumgalactic medium, or CGM) play a critical role in their evolution, because the gas necessary to fuel star formation and any gas expelled from widely observed galactic winds must encounter this interface region between galaxies and the intergalactic medium (IGM). 

The blue and red spheres represent star-forming and quiescent galaxies, respectively, and the
'skewers' piercing the volume are QSO sightlines. Mouse over a galaxy to see an image and a few
of its properties in the lower panel.  Mouse over a skewer and the right-hand panels show the spectral region where H I or C IV (or other available sperctra) fall within a specified redshift range.  If a galaxy has an impact parameter within the range indicated by the bottom-right slider, a
vertical line will appear at its redshift in the spectral window.  Use the two drop-down boxes to
control the height and thickness of these lines according to different properties.  Lastly, control
the range of redshift shown in the spectral windows with the sizing bar between the drop-downs and
the slider.

Galaxy/absorber pairs can be tracked using the numeric keys '0' through '9' (skghtlines) and 'G' (galaxies), and then saved to disk for further analysis by pressing 'D'. 

## Video Documentation

A video tutorial with an example use case can be seen here: [https://www.youtube.com/watch?v=3ZVaExEVZOk](https://www.youtube.com/watch?v=3ZVaExEVZOk)

[![IGM-Vis](images/IGM-video.jpg)](https://www.youtube.com/watch?v=3ZVaExEVZOk ""IGM-Vis"")

## Web Demo

The Web demo of IGM-Vis is located at: [https://creativecodinglab.github.io/Intergalactic/intergalactic.html](https://creativecodinglab.github.io/Intergalactic/intergalactic.html) 


## Quick Reference<br/>
Arrow keys: move reference point (in 3D view) <br/>
0 - 9 : store selected skewer to Spectrum Panel<br/>
E : obtain equivalent width measurements (see video tutorial) <br/>
G : store selected galaxy in Galaxy Panel<br/>
D : download json file of all stored skewers with galaxy neighbor pairs<br/>
S : show / hide skewers in 3D view  <br/>
T : show / hide text in 3D view  <br/>
Z : zoom camera to selected galaxy in 3D view <br/>


## Screenshots of IGM-Vis


[![IGM-Vis](images/IGM-Vis_zoomAndFilter.png)](images/IGM-Vis_zoomAndFilter.png ""IGM-Vis"")


[![IGM-Vis](images/IGM-Vis_skwererSpectra.png)](images/IGM-Vis_skwererSpectra.png ""IGM-Vis"")


[![IGM-Vis](images/IGM-Vis_EquivalentWidthPlot.png)](images/IGM-Vis_EquivalentWidthPlot.png ""IGM-Vis"")


[![IGM-Vis](images/IGM-Vis_Coherence.png)](images/IGM-Vis_Coherence.png ""IGM-Vis"")


[![IGM-Vis](images/IGM-Vis_galaxies.png)](images/IGM-Vis_galaxies.png ""IGM-Vis"")

## Data Processing

Galaxies positions and metadata are loaded from `data/galaxies.json` file using the `loadGalaxyData()` function. Once this file has been read, the function `loadSkewerData()` is called and reads the list of QSOs `qsosInSdssSlice_viz.dat` for their name, Right Ascension (RA) and Declination (DEC). The HI and CIV spectra for each QSO listed in `qsosInSdssSlice_viz.dat` is loaded from these folders respectively:  `data/spectra_HI_norm` and `data/spectra_CIV_norm`. Calculating the projected distance between every skewer and galaxy can be done using the function `computeProjections()`, which uses the `haversine()` function to calculate an angular distance (impact parameter) between the two objects. In order to do this calculation, redshift must converted to physical units, which is done with the `cosmcalc()` function. In order to save time, a lookup table stored in `data/projections/lookUp.json` is referenced, which was created using the `cosmcalc()` function. If values outside of the range contained in the lookup table are needed, the computation is done on demand. In order to quicken the initial loading time even further, the impact parameter values for this dataset were precomputed in Megaparsecs using the steps outlined above and are stored in the folder `data/projections` as a separate file for each quasar with the function `loadP()`. Data can be downloaded into a .json file using the function `exportData('example.json',JSON.stringify([an array]))`. The data object that is downloaded on the ""D"" key press can be modified in the `onKeyDown(event)` event handler.


## Authors

IGM-Vis was created by an interdisciplinary team of researchers at University of California, Santa Cruz.

- Dept. of Astronomy & Astrophysics: Joseph N. Burchett, J. X. Prochaska;
- Dept. of Computational Media: David Abramov, Cassia Artanegara, Jasmine Otto, and Angus G. Forbes

An article describing IGM-Vis is accepted to EuroVis‚Äô19, and a pre-print is available on ArXiv at https://arxiv.org/abs/1812.07092





",,DEV,0.75,,,,,,0,7,0.95,DEV
29315637,MDEwOlJlcG9zaXRvcnkyOTMxNTYzNw==,RTree,xtang2/RTree,0,xtang2,https://github.com/xtang2/RTree,EDU,,0,2015-01-15 20:03:08+00:00,2015-01-15 20:25:41+00:00,2015-01-15 20:25:40+00:00,,156,0,0,C++,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,0,master,1,['xtang2'],,"# RTree
Created by Xiaoli Tang and Luisa Fernandos 
An RTree made for CMPS 181 (Spring 2014) at University of California, Santa Cruz.
",,EDU,0.74,,,,,,0,1,0.8,EDU
52459254,MDEwOlJlcG9zaXRvcnk1MjQ1OTI1NA==,cfDNA,shendurelab/cfDNA,0,shendurelab,https://github.com/shendurelab/cfDNA,DEV,Analysis of epigenetic signals captured by fragmentation patterns of cell-free DNA,0,2016-02-24 17:00:49+00:00,2025-02-28 09:15:45+00:00,2020-07-22 11:43:36+00:00,,32921,67,67,Python,1,1,1,0,1,0,30,0,0,1,mit,1,0,0,public,30,1,67,master,1,"['makirc', 'mwsnyder']",1,"Analysis of epigenetic signals captured by fragmentation patterns of cell-free DNA
==================================================================================

The following sections provide details for the analyses performed in:

Snyder MW, Kircher M, Hill AJ, Daza RM, Shendure J. Cell-free DNA Comprises 
an In Vivo Nucleosome Footprint that Informs Its Tissues-Of-Origin. Cell. 2016 Jan 
14;164(1-2):57-68. doi: 10.1016/j.cell.2015.11.050. PubMed PMID: [26771485](http://www.ncbi.nlm.nih.gov/pubmed/26771485)

*All scripts and binaries are provided as is, without any warrenty and for use at your own risk. This is not the release of a software package. We are only providing this information and code in addition to a description of methods for making it easier to reproduce our analyses. We are __not__ providing any support for these scripts.* 

Versions and dependencies
-------------------------

Our scripts largely depend on Python 2, the [pysam](https://pysam.readthedocs.io/en/latest/api.html), [bx](https://pypi.org/project/bx-python/) and [numpy](https://numpy.org/) libraries. Here a list of versions that we used:

```
python/2.7.3
numpy/1.7.0
pysam/0.7.5
bx-python/0.6.0
```

We were also using [R 3.1.0](https://cran.r-project.org/), [UCSC binaries](http://hgdownload.cse.ucsc.edu/admin/exe/) for working with bigWig and bigBed files and tools like tabix and samtools from [HTSlib](http://www.htslib.org/).

Please consider using a software management tool like conda to install the respective packages. You can try more recent versions of the packages, but please keep in mind that Python 3 cannot be used to interpret Python 2.7 code.

### Samtools version used

Please note that a samtools binary is included with these scripts. Among other things, this samtools binary allows filtering reads based on insert size/read length. This is an early version of the samtools branch released on https://github.com/mpieva/samtools-patched. For samtool calls that are not filtering for read length/insert size, other (more recent) versions of samtools might be used. Please note though that parameters might be named differently and that the ASCII encoding of read filters is also special to the samtools version that we used. 

Primary data processing of sequencing data
------------------------------------------

Barcoded paired-end (PE) sequencing data was split allowing up to one substitution in the barcode sequence. Fragments shorter than or equal to the read length were consensus-called and adapter-trimmed. Remaining consensus single-end reads (SR) and the individual PE reads were aligned to the human reference genome (GRCh37, 1000 Genomes phase 2 technical reference, ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/) using the ALN algorithm in BWA v0.7.10 ([Li and Durbin, 2010](http://www.ncbi.nlm.nih.gov/pubmed/20080505)). PE reads were further processed with BWA SAMPE to resolve ambiguous placement of read pairs or to rescue missing alignments by a more sensitive alignment step around the location of one placed read end. Aligned SR and PE data were stored in BAM format using the samtools API ([Li et al., 2009](http://www.ncbi.nlm.nih.gov/pubmed/19505943)). BAM files for each sample were merged across lanes and sequencing runs. BAM files for each sample were deposited in the NCBI Gene Expression Omnibus (GEO) with accession GSE71378 (http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE71378). Note that GEO is distributing BAM files through SRA (http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?study=SRP061633 and http://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP061633). *Update:* SRA has recently converted the provided BAM files to SRA format. You can use the SRA tools to convert back. We are providing a copy of the original BAM files [here](https://kircherlab.bihealth.org/download/cfDNA/).

Simulated reads and nucleotide frequencies
------------------------------------------

Aligned sequencing data was simulated (SR if shorter than 45 bp, PE 45 bp otherwise) for all major chromosomes of the human reference (GRC37h). Kmer nucleotide frequencies (k=2 is presented in the manuscript) were determined from real data on both fragment ends and both strand orientations (`referenceKMers.py`), and for the reference genome on both strands (`BAM2FragKMers.py`). The insert size distribution of the real data was extracted for the 1-500 bp range (`BAM_RG_Length.py --noRG`). Reads were simulated procedurally (implemented in `simulate_reads.py`): at each step (i.e., at least once at each genomic coordinate, depending on desired coverage), (1) the strand is randomly chosen, (2) the ratio of the kmer frequency in the real data to that in the reference sequence is used to randomly decide whether the initiating kmer is considered, (3) an length is sampled from the insert size distribution, and (4) the frequency ratio of the terminal kmer is used to randomly decide whether the generated alignment is reported. The simulated coverage was matched to that of the original data after PCR duplicate removal (by adjusting how often sequences are sampled at each position).

```bash
# Extract kmers (here 2mers) from reference genome 
./referenceKMers.py -k 2 -r '' -o grch37_regChroms_2mers.tsv

# Extact the length distribution from the sample BAM, keep only the 1-500bp range 
./BAM_RG_Length.py --noRG -p tmp_outfile BAMFILE.bam
head -n 501 tmp_outfile_ > ${SAMPLE}_lenDist.tsv

# Extract kmers (here 2mers) of left and right read ends from the SAMPLE file
./BAM2FragKMers.py --kmerLE=2 --kmerRE=2 -v -r 'ALL' BAMFILE.bam --outfileLE=${SAMPLE}_left_2mer --outfileRE=${SAMPLE}_right_2mer

# Run simulation for each ""regular"" chromosome (1..22, X, Y) -- should be run in parallel
for i in $(head -n 24 grch37_1000g_phase2/whole_genome.fa.fai | awk '{ print $1"":1-""$2 }'); do \\
  ./simulate_reads.py -v -d ${SAMPLE}_lenDist.tsv --fwdKMerGenome=grch37_regChroms_2mers.tsv --revKMerGenome=grch37_regChroms_2mers.tsv --fwdPKMers=${SAMPLE}_left_2mer_f.tsv --fwdMKMers=${SAMPLE}_left_2mer_r.tsv --revPKMers=${SAMPLE}_right_2mer_f.tsv --revMKMers=${SAMPLE}_right_2mer_r.tsv -s 20  -r \\'""$i""\\' -o ${SAMPLE}_2mer_sim_chr$( echo $i | cut -f 1 -d':').bam; \\
done

# Combine chromosome files, index and regenerate some stats
./samtools merge -u ${SAMPLE}_chr*.bam | ./samtools sort - ${SAMPLE}_allChrom
./samtools index ${SAMPLE}_allChrom.bam
./samtools flagstat ${SAMPLE}_allChrom.bam > ${SAMPLE}_allChrom.bam_stats
./samtools view -F _2 ${SAMPLE}_allChrom.bam | cut -f 3 | uniq -c > ${SAMPLE}_allChrom.byChromCounts.txt
./BAM2FragmentationPatterns.py -r ALL -o ${SAMPLE}_allChrom.fragpatterns.txt ${SAMPLE}_allChrom.bam
./BAM_RG_Length.py --noRG -p ${SAMPLE}_allChrom.length ${SAMPLE}_allChrom.bam
```

Analysis of nucleotide composition of 167 bp fragments
------------------------------------------------------

Fragments with inferred lengths of exactly 167 bp were filtered within samples to remove duplicates. Dinucleotide frequencies were calculated in a strand-aware manner, using a sliding 2 bp window and reference alleles at each position, beginning 50 bp upstream of one fragment endpoint and ending 50 bp downstream of the other endpoint. Observed dinucleotide frequencies at each position were compared to expected dinucleotide frequencies determined from a set of simulated reads reflecting the same cleavage biases calculated in a library-specific manner. 

See the folder `nucleotide_composition` for more details.

Coverage, fragment endpoints, and windowed protection scores
------------------------------------------------------------

Fragment endpoint coordinates were extracted from BAM files with the SAMtools API. Both outer alignment coordinates of PE data were extracted for properly paired reads. Both end coordinates of SR alignments were extracted when PE data was collapsed to SR data by adapter trimming. A fragment‚Äôs coverage is defined as all positions between the two (inferred) fragment ends, inclusive of endpoints. We define the Windowed Protection Score (WPS) of a window of size k as the number of molecules spanning the window minus those with an endpoint within the window. We assign the determined WPS to the center of the window. For 35-80 bp fragments (short fraction, S-WPS), k=16; for 120-180 bp fragments (long fraction, L-WPS), k=120. We store coverage, read starts and WPS in gzip-compressed WIG files. We used wigToBigWig from the UCSC tools (http://hgdownload.cse.ucsc.edu/admin/exe/) for converting these to BigWig (bw) for visualization in genome viewers.

Running the extraction of coverage, read starts and WPS for the long fraction:
```bash
./samtools view -u -m 120 -M 180 BAMFILE.bam $EXTREGION | ./FilterUniqueBAM.py -p | ./extractReadStartsFromBAM2Wig.py -p -r $REGION -w 120 -c COVERAGE.wig.gz -n WPS.wig.gz -s STARTS.wig.gz
```

Running the extraction of coverage, read starts and WPS for the short fraction:
```bash
./samtools view -u -m 35 -M 80 BAMFILE.bam $EXTREGION | ./FilterUniqueBAM.py -p | ./extractReadStartsFromBAM2Wig.py -p -r $REGION -w 16 -c COVERAGE.wig.gz -n WPS.wig.gz -s STARTS.wig.gz
```

Please note the variables `EXTREGION` and `REGION` above. When piping reads from a region, we might miss the forward read of a read pair and the provided scripts usually only extract information from the first read of a pair. Thus, `EXTREGION` should include additional bases around the region (i.e. 200bp or another value that guarantees that all read insert sizes are included; here 180bp and 80bp would be sufficient; e.g. if REGION is 1:1000-2000, EXTREGION should be 1:800-2200).


Nucleosome peak calling
-----------------------

For nuclesome peak calling, the L-WPS is locally adjusted to a running median of zero in 1 kb windows and smoothed using a Savitzky-Golay filter ([Savitzky and Golay, 1964](http://pubs.acs.org/doi/abs/10.1021/ac60214a047)) (window size 21, 2nd order polynomial). The L-WPS track is then segmented into above-zero regions (allowing up to 5 consecutive positions below zero). If the resulting region is 50-150 bp, we identify the median L-WPS value of that region and search for the maximum-sum contiguous window above the median. We report the start, end and center coordinates of this window as the ‚Äúpeak,‚Äù or local maximum of nucleosome protection. All calculations involving distances between peaks are based on these center coordinates. A score for each peak is determined as the distance between maximum value in the window and the average of the two adjacent L-WPS minima neighboring the region. If the identified region is 150-450 bp, we apply the same above median contiguous window approach, but only report those windows that are 50-150 bp. For score calculation of multiple windows derived from 150-450 bp regions, we set the neighboring minima to zero. We discard regions <50 bp or >450 bp.

Peak calling is implemented in `callPeaks.py` and expects WIG on STDIN:

```bash
# Note that EXTREGION should be larger than REGION (by the maximum read length, i.e. 180) to prevent skipping alignments
./samtools view -u -m 120 -M 180 BAMFILE.bam $EXTREGION | ./FilterUniqueBAM.py -p | ./extractReadStartsFromBAM2Wig.py -p -r $REGION -w 120 -c OFF -s OFF | ./callPeaks.py -s > calls.bed

# or from bigWig (http://hgdownload.cse.ucsc.edu/admin/exe/) and save as block-gzip compressed (http://www.htslib.org/doc/tabix.html) BED file:

bigWigToWig -chrom=chr1 -start=12000000 -end=13000000 ${SAMPLE}.bw /dev/stdout | ./callPeaks.py -s | bgzip -c > calls.bed.gz
```

Again note the variables `EXTREGION` and `REGION` above. When piping reads from a region, we might miss the forward read of a read pair and the provided scripts usually only extract information from the first read of a pair. Thus, `EXTREGION` should include additional bases around the region (i.e. 200bp or another value that guarantees that all read insert sizes are included; here 180bp would be sufficient; e.g. if REGION is 1:1000-2000, EXTREGION should be 1:800-2200).


Bed files can be converted to bigBed using the above mentioned [UCSC tools](http://hgdownload.cse.ucsc.edu/admin/exe/). We uploaded bigBed (bb) files of our nucleosome calls to GEO for [BH01](ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE71nnn/GSE71378/suppl/GSE71378_BH01.bb), [IH01](ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE71nnn/GSE71378/suppl/GSE71378_IH01.bb), [IH02](ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE71nnn/GSE71378/suppl/GSE71378_IH02.bb), [CH01](ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE71nnn/GSE71378/suppl/GSE71378_CH01.bb), and [CA01](ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE71nnn/GSE71378/suppl/GSE71378_CA01.bb).

Analysis of DHS sites
---------------------

DHS peaks for 349 primary tissue and cell line samples were downloaded from http://www.uwencode.org/proj/Science_Maurano_Humbert_et_al/data/all_fdr0.05_hot.tgz. Samples derived from fetal tissues (233 of 349) were removed from the analysis as they behaved inconsistently within tissue type, possibly because of unequal representation of cell types within each tissue sample. 116 DHS callsets from a variety of cell lineages were retained for analysis. For the midpoint of each DHS peak in a particular set, the nearest upstream and downstream calls in the CH01 callset were identified, and the distance between the centers of those two calls was calculated. The distribution of all such distances was visualized for each DHS peak callset using a smoothed density estimate calculated for distances between 0 and 500 bp.

See the folder `DHS` for more details and supporting files.  

Analysis of A/B compartments
----------------------------

We downloaded A/B compartment calls with 100 kb resolution from [GSE63525](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE63525), specifically the file `GSE63525_GM12878_subcompartments.bed.gz`. A/B segmentation calls were compared to peak spacing within equivalent 100 kb genomic windows.  In a separate analysis, peak density was compared to windowed GC content in 10 kb bins.

See the folder `peak_density` for more details and supporting files.

Annotation of TFBSs and genomic features
----------------------------------------

We downloaded clustered FIMO (motif-based) intervals ([Grant et al., 2011](http://www.ncbi.nlm.nih.gov/pubmed/21330290); [Maurano et al., 2012](http://www.ncbi.nlm.nih.gov/pubmed/22955828), http://www.uwencode.org/proj/Maurano_et_al_func_var/) defining a set of computationally predicted TFBSs (`hg19.taipale.1e-4.starch`). For a subset of clustered TFs (AP-2-2, AP-2, CTCF_Core-2, E2F-2, EBF1, Ebox-CACCTG, Ebox, ESR1, ETS, MAFK, MEF2A-2, MEF2A, MYC-MAX, PAX5-2, RUNX2, RUNX-AML, STAF-2, TCF-LEF, YY1), we retained only predicted TFBSs that overlap with ENCODE ChIP-seq peaks (TfbsClusteredV3 set downloaded from http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeRegTfbsClustered/). Details on download and basic processing of these data sets are available in `data/Maurano_et_al_func_var/README` and `data/ENCODE_TfbsClusteredV3/README`.

Genomic coordinates of transcription start sites, transcription end sites, start codons, and splice donor and acceptor sites were obtained from Ensembl Build version 75 (ftp://ftp.ensembl.org/pub/release-75/gtf/homo_sapiens/Homo_sapiens.GRCh37.75.gtf.gz, details are available in `data/Ensembl_v75/README`). 

Filtering active CTCF sites
---------------------------

CTCF sites first included clustered FIMO binding site predictions (described above). This set was intersected with ENCODE ChIP-seq peaks (TfbsClusteredV3, described above), and then further intersected with a set of CTCF binding sites experimentally observed to be active across 19 tissues (Wang et al., 2012; details on the download of this data set are available in `data/Wang_et_al_CTCF/README`), to produce three increasingly stringent sets. For each CTCF site, distances between each of 20 flanking nucleosomes  (10 upstream and 10 downstream) were calculated. 

The mean S-WPS and L-WPS at each position relative to the center of the CTCF binding motifs were calculated within bins defined by spacing between -1 and +1 nucleosomes (>160 bp, 161-200 bp, 201-230 bp, 231-270 bp, 271-420 bp, 421-460 bp, and >460 bp). In figures, CTCF binding sites are shifted such that the zero coordinate on the x-axis is the center of its 52 bp binding footprint ([Ong and Corces, 2014](http://www.ncbi.nlm.nih.gov/pubmed/24614316)). 

Overlaying WPS at aligned genomic features
------------------------------------------

WPS values for a specific data set (in most cases CH01) and the corresponding simulation were extracted for each position in a 5 kb window around the start coordinate of each TFBS, and was aggregated within each TF cluster. The mean WPS of the first and last 500 bp (which is predominantly flat and represents a mean offset) of the 5 kb window was subtracted from the original WPS at each position. For L-WPS only, a sliding window mean is calculated using a 200 bp window and subtracted from the original signal. Finally, the corrected WPS profile for the simulation is subtracted from the corrected WPS profile of the data set to correct for signal that is a product of fragment length and ligation bias. This final profile is plotted and termed the Adjusted WPS. 

We provide Python and R scripts for extracting and plotting WPS overlays from block-gzip compressed WIG files. Such files are generated by `extractReadStartsFromBAM2Wig.py` or `normalizeWPSwigs.py` and need to be tabix-indexed to retrieve specific genomic regions using the tabix routines of [pysam](http://pysam.readthedocs.org/en/latest/api.html).

###Motif-based TF binding site predictions

We outline how S-WPS and L-WPS plots were generated from clustered FIMO (motif-based) sites in a README in `WPS_overlays/Maurano_et_al_TFclusters`.

###Motif-based TF binding sites overlapped with ENCODE ChIP-seq peaks

Please find a README in `WPS_overlays/Maurano_et_al_plus_ChIP` which outlines how S-WPS and L-WPS plots were generated for a subset of clustered FIMO (motif-based) sites overlapped with ENCODE ChIP-seq peaks of AP-2-2, AP-2, CTCF_Core-2, E2F-2, EBF1, Ebox-CACCTG, Ebox, ESR1, ETS, MAFK, MEF2A-2, MEF2A, MYC-MAX, PAX5-2, RUNX2, RUNX-AML, STAF-2, TCF-LEF, and YY1.

###Active CTCF from ChIP-seq in 19 cell-lines

`WPS_overlays/CTCF_19CellLines` has a README file which outlines how S-WPS and L-WPS plots were generated for this subset of CTCF sites.

###Adjusted WPS around genic features 

A README describing how the adjusted WPS was extracted and overlayed around TSS, TSE, splice acceptor/donor, and start/stop codon can be found in `WPS_overlays/genicFeatures`.

###Adjusted WPS around genic features in five NB-4 expression bins

The file `WPS_overlays/genicFeatures_by_expression/README` describes how adjusted WPS was extracted and overlayed around TSS, TSE, splice acceptor/donor, and start/stop codon for the five NB-4 expression bins and how the genes in each bin were defined.

Gene expression analysis
------------------------

###Human Protein Atlas

FPKM gene expression (GE) values measured for 20,344 Ensembl gene identifiers in 44 human cell lines and 32 primary tissues by the Human Protein Atlas ([Uhl√©n et al., 2015](http://www.ncbi.nlm.nih.gov/pubmed/25613900)) was downloaded from http://www.proteinatlas.org/download/rna.csv.zip. Genes with 3 or more non-zero expression values were retained (n=19,378 genes). The GE data set is provided with one decimal precision for the FPKM values. Thus, a zero GE value (0.0) indicates expression in the interval [0, 0.05) Unless otherwise noted, we set the minimum GE value to 0.04 FPKM before log2-transformation..

###Fast Fourier transformation (FFT) and smoothing of trajectories

We use parameters to smooth (3 bp Daniell smoother; moving average giving half weight to the end values) and de-trend the data (i.e. subtract the mean of the series and remove a linear trend). A recursive time series filter implemented in R was used to remove high frequency variation from trajectories. 24 filter frequencies (1/seq(5,100,4)) were used, and the first 24 values of the trajectory were taken as init values. The 24-value shift in the resulting trajectories was corrected by repeating the last 24 values of the trajectory.

###FFT intensity correlation with expression

L-WPS was used to calculate periodograms of genomic regions using Fast Fourier Transform (FFT, spec.pgram in R) with frequencies between 1/500 and 1/100 bases. Intensity values for the 120-280 bp frequency range were determined from smooth FFT periodograms. S-shaped Pearson correlation between GE values and FFT intensities was observed around the major inter-nucleosome distance peak, along with a pronounced negative correlation in the 193-199 bp frequency range. The mean intensity in this frequency range was correlated with the average intensity with log2-transformed GE values for downstream analysis. 

###Running the analysis

```bash

zcat transcriptAnno-GRCh37.75.tsv.gz | awk 'BEGIN{ FS=""\\t""; OFS=""\\t"" }{ if ($5 == ""+"") { print $1,$2,$3-10000,$3,$5 } else { print $1,$2,$4,$4+10000,$5 } }' > transcriptAnno-GRCh37.75.upstream.tsv                                                   
zcat transcriptAnno-GRCh37.75.tsv.gz | awk 'BEGIN{ FS=""\\t""; OFS=""\\t"" }{ if ($5 == ""+"") { print $1,$2,$4,$4+10000,$5 } else { print $1,$2,$3-10000,$3,$5 } }' > transcriptAnno-GRCh37.75.downstream.tsv                                                 
zcat transcriptAnno-GRCh37.75.tsv.gz | awk 'BEGIN{ FS=""\\t""; OFS=""\\t"" }{ if ($5 == ""+"") { print $1,$2,$3-1,$3-1+10000,$5 } else { print $1,$2,$4-1-10000,$4-1,$5 } }' > transcriptAnno-GRCh37.75.body.tsv

# Extract counts for a sample
mkdir -p body/$SAMPLE
./extractReadStartsFromBAM_Region_WPS.py --minInsert=120 --maxInsert=180 -i transcriptAnno-GRCh37.75.body.tsv -o 'body/$SAMPLE/block_%s.tsv.gz' BAMFILE.bam

# Run FFT & convert to summary tbl
mkdir -p /tmp/body/$SAMPLE/fft
( cd body/$SAMPLE/counts; ls block_*.tsv.gz ) | xargs -n 500 Rscript fft_path.R body/$SAMPLE/counts /tmp/body/$SAMPLE/fft
mkdir -p body/fft_summaries/
./convert_files.py -a transcriptAnno-GRCh37.75.body.tsv -t /tmp/ -r  -p body -i $SAMPLE
rm -fR /tmp/body/$SAMPLE/fft

# Correlate the intensities with the expression data and generate PDFs in R
R --vanilla --quiet < plots.R
```
",,DEV,0.44,,,,,,0,12,0.95,DEV
75242029,MDEwOlJlcG9zaXRvcnk3NTI0MjAyOQ==,CliqueCounting,sjain12/CliqueCounting,0,sjain12,https://github.com/sjain12/CliqueCounting,DEV,"Code for the algorithms and experiments in ""A Fast and Provable Method for Estimating Clique Counts Using Tur√°n's Theorem"" by S. Jain and C. Seshadhri",0,2016-12-01 01:12:21+00:00,2018-11-30 18:09:35+00:00,2016-12-01 02:44:44+00:00,,24748,2,2,C++,1,1,1,1,0,0,2,0,0,0,,1,0,0,public,2,0,2,master,1,['sjain12'],,"# CliqueCounting
Code for the algorithms and experiments in ""A Fast and Provable Method for Estimating Clique Counts Using Tur√°n's Theorem"" by S. Jain and C. Seshadhri

Steps to run the code:

1. Store the graph to be analyzed as a list of edges in a .txt file in the graphs folder. cd into the python folder and run:

  python sanitize.py ../graphs amazon0601.txt
  
  This will create a file 'amazon0601.edges' in the graphs folder.
  
2. To run the Tur√°nShadow algorithm, run:
  
  ./test_cliques_turan_shadow -i amazon0601.edges -m 5 -M 5 -r 10 -n 50000
  
  This will save the tree stats in '/amazon0601_5_50000_params' csv file in results/cliques/ and the estimates of the 10 runs in 'amazon0601_5_50000_10_data'
  
3. To run the Edge Sampling algorithm, run:

  ./test_cliques_edge_sampling -i amazon0601.edges -m 5 -M 5 -r 10 -p 0.75
  
  This will save the estimates from 10 runs of the algorithm in amazon0601_5_0.75_10_es in results/cliques/
  
4. To run the GRAFT algorithm, run:

  ./test_cliques_graft -i amazon0601.edges -m 5 -M 5 -r 10 -n 100000
  
  This will save the estimates from 10 runs of the algorithm in amazon0601_5_100000_10_graft in results/cliques/
  
5. To run the Color Coding algorithm, run:

  ./test_cliques_color_coding -i amazon0601.edges -m 5 -M 5 -r 10 
  
  This will save the estimates from 10 runs of the algorithm in amazon0601_5_10_cc in results/cliques/
  
6. To run the brute force clique counting algorithm, run:

  ./test_cliques_brute_force -i amazon0601.edges -m 5 -M 5 -r 10 
  
  This will save the estimates from 10 runs of the algorithm in amazon0601_5_10_bf in results/cliques/

For questions, contact Shweta Jain at sjain12@ucsc.edu.


  
",,DEV,0.66,,,,,,0,0,0.8,DEV
170924098,MDEwOlJlcG9zaXRvcnkxNzA5MjQwOTg=,helen,kishwarshafin/helen,0,kishwarshafin,https://github.com/kishwarshafin/helen,DEV,H.E.L.E.N. (Homopolymer Encoded Long-read Error-corrector for Nanopore),0,2019-02-15 20:28:09+00:00,2024-12-18 14:55:01+00:00,2020-10-28 03:26:44+00:00,,7623,70,70,Python,1,0,1,1,0,0,9,0,0,1,mit,1,0,0,public,9,1,70,master,1,"['kishwarshafin', 'tpesout', 'esrice', 'cgjosephlee']",,"# POLISHER UPDATE: P.E.P.P.E.R.
We have released a new polisher [PEPPER](https://github.com/kishwarshafin/pepper/releases/tag/v0.1) that replaces `MarginPolish-HELEN`. If you have newer data `Guppy >= 3.0.5` please use `PEPPER` instead of `MarginPolish-HELEN`. `PEPPER` is fully supported by our team.

## H.E.L.E.N.
H.E.L.E.N. (Homopolymer Encoded Long-read Error-corrector for Nanopore)

[![Build Status](https://travis-ci.com/kishwarshafin/helen.svg?branch=master)](https://travis-ci.com/kishwarshafin/helen)
___________________________________________________________
HELEN is published in Nature Biotechnology:
#### [Nanopore sequencing and the Shasta toolkit enable efficient de novo assembly of eleven human genomes](https://www.nature.com/articles/s41587-020-0503-6)
__________________________________________________________

## Overview
`HELEN` uses a Recurrent-Neural-Network (RNN) based Multi-Task Learning (MTL) model that can predict a base and a run-length for each genomic position using the weights generated by `MarginPolish`.

¬© 2020 Kishwar Shafin, Trevor Pesout, Benedict Paten. <br/>
Computational Genomics Lab (CGL), University of California, Santa Cruz.

## Why MarginPolish-HELEN ?
* `MarginPolish-HELEN` outperforms other graph-based and Neural-Network based polishing pipelines.
* Simple installation steps.
* `HELEN` can use multiple GPUs at the same time.
* Highly optimized pipeline that is faster than any other available polishing tool.
* We have <b>sequenced-assembled-polished 11 samples</b> to ensure robustness, runtime-consistency and cost-efficiency.
* We tested GPU usage on `Amazon Web Services (AWS)` and `Google Cloud Platform (GCP)` to ensure scalability.
* Open source [(MIT License)](LICENSE).

## Walkthrough
* [Docker based installation walkthrough](./docs/walkthrough_docker.md).
* [Local installation walkthrough](./docs/walkthrough_local.md).

## Installation
`MarginPolish-HELEN` is supported on  <b>`Ubuntu 16.10/18.04`</b> or any other Linux-based system.
√Ç
#### Install prerequisites
Before you follow any of the methods, make sure you install all the dependencies:
```bash
sudo apt-get -y install git cmake make gcc g++ autoconf bzip2 lzma-dev zlib1g-dev \\
libcurl4-openssl-dev libpthread-stubs0-dev libbz2-dev liblzma-dev libhdf5-dev \\
python3-pip python3-virtualenv virtualenv
```

#### Method 1: Install MarginPolish-HELEN from GitHub
You can install from the `GitHub` repository:
```bash
git clone https://github.com/kishwarshafin/helen.git
cd helen
make install
. ./venv/bin/activate

helen --help
marginpolish --help
```
Each time you want to use it, activate the virtualenv:
```bash
. <path/to/helen/venv/bin/activate>
```

#### Method 2: Install using PyPi
Install  prerequisites and the install `MarginPolish-HELEN` using pip:
```bash
python3 -m pip install helen --user

python3 -m helen.helen --help
python3 -m helen.marginpolish --help
```

Update the installed version:
```bash
python3 -m pip install update pip
python3 -m pip install helen --upgrade
```

You can also add module locations to path:
```bash
echo 'export PATH=""$(python3 -m site --user-base)/bin"":$PATH' >> ~/.bashrc
source ~/.bashrc

marginpolish --help
helen --help
```

#### Method 3: Use docker image

##### CPU based docker:
```bash
# SEE CONFIGURATION
docker run --rm -it --ipc=host kishwars/helen:latest helen --help
docker run --rm -it --ipc=host kishwars/helen:latest marginpolish --help

docker run -it --ipc=host --user=`id -u`:`id -g` --cpus=""16"" \\
-v </directory/with/inputs_outputs>:/data kishwars/helen:latest \\
helen --help
```

##### GPU based docker:
```bash
sudo apt-get install -y nvidia-docker2
# SEE CONFIGURATION
nvidia-docker run -it --ipc=host kishwars/helen:latest helen torch_stat
nvidia-docker run -it --ipc=host kishwars/helen:latest helen --help
nvidia-docker run -it --ipc=host kishwars/helen:latest marginpolish --help

# RUN HELEN
nvidia-docker run -it --ipc=host --user=`id -u`:`id -g` --cpus=""16"" \\
-v </directory/with/inputs_outputs>:/data kishwars/helen:latest \\
helen --help
```
## Usage
`MarginPolish` requires a draft assembly and a mapping of reads to the draft assembly. We commend using `Shasta` as the initial assembler and `MiniMap2` for the mapping.

#### Step 1: Generate an initial assembly
Generate an assembly using one of the ONT assemblers:
* [Shasta long read assembler](https://github.com/chanzuckerberg/shasta).
* [Flye assembler](https://github.com/fenderglass/Flye)
* [Canu assembler](https://github.com/marbl/canu)
* [WTDBG2 assembler](https://github.com/ruanjue/wtdbg2)

#### Step 2: Create an alignment between reads and shasta assembly
We recommend using `MiniMap2` to generate the mapping between the reads and the assembly. You don't have to follow these exact commands.
```bash
minimap2 -ax map-ont -t 32 shasta_assembly.fa reads.fq | samtools view -hb -F 0x904 > unsorted.bam;
samtools sort -@32 -o reads_2_assembly.0x904.bam unsorted.bam;
samtools index -@32 reads_2_assembly.0x904.bam
```
#### Step 3: Generate images using MarginPolish
##### Download Model
```bash
helen download_models \\
--output_dir <path/to/mp_helen_models/>
```

##### Run MarginPolish
You can generate images using MarginPolish by running:
```bash
marginpolish reads_2_assembly.bam \\
Assembly.fa \\
</path/to/model_name.json> \\
-t <number_of_threads> \\
-o <path/to/marginpolish_images> \\
-f
```

You can find the models by downloading them.

#### Step 4: Run HELEN
Next, run `HELEN` to polish using a RNN.
```bash
helen polish \\
--image_dir </path/to/marginpolish_images/> \\
--model_path </path/to/model.pkl> \\
--batch_size 256 \\
--num_workers 4 \\
--threads <num_of_threads> \\
--output_dir </path/to/output_dir> \\
--output_prefix <output_filename.fa> \\
--gpu_mode
```

If you are using `CPUs` then remove the `--gpu_mode` argument.

## Help
Please open a github issue if you face any difficulties.

## Acknowledgement
We are thankful to [Segey Koren](https://github.com/skoren) and [Karen Miga](https://github.com/khmiga) for their help with `CHM13` data and evaluation.

We downloaded our data from [Telomere-to-telomere consortium](https://github.com/nanopore-wgs-consortium/CHM13) to evaluate our pipeline against `CHM13`.

We acknowledge the work of the developers of these packages: </br>
* [Shasta](https://github.com/chanzuckerberg/shasta/commits?author=paoloczi)
* [pytorch](https://pytorch.org/)
* [ssw library](https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library)
* [hdf5 python (h5py)](https://www.h5py.org/)
* [pybind](https://github.com/pybind/pybind11)
* [hyperband](https://github.com/zygmuntz/hyperband)

## Fun Fact
<img src=""https://vignette.wikia.nocookie.net/marveldatabase/images/e/eb/Iron_Man_Armor_Model_45_from_Iron_Man_Vol_5_8_002.jpg/revision/latest?cb=20130420194800"" alt=""guppy235"" width=""240p""> <img src=""https://vignette.wikia.nocookie.net/marveldatabase/images/c/c0/H.E.L.E.N._%28Earth-616%29_from_Iron_Man_Vol_5_19_002.jpg/revision/latest?cb=20140110025158"" alt=""guppy235"" width=""120p""> <br/>

The name ""HELEN"" is inspired from the A.I. created by Tony Stark in the  Marvel Comics (Earth-616). HELEN was created to control the city Tony was building named ""Troy"" making the A.I. ""HELEN of Troy"".

READ MORE: [HELEN](https://marvel.fandom.com/wiki/H.E.L.E.N._(Earth-616))



¬© 2020 Kishwar Shafin, Trevor Pesout, Benedict Paten.
",,DEV,0.58,,,,,,0,5,0.9,DEV
90321157,MDEwOlJlcG9zaXRvcnk5MDMyMTE1Nw==,ocaml_dc,Slugpotato/ocaml_dc,0,Slugpotato,https://github.com/Slugpotato/ocaml_dc,EDU,Desk calculator written in OCaml for CMPS112 at UCSC,0,2017-05-05 00:23:56+00:00,2017-12-18 18:23:23+00:00,2014-11-29 23:18:49+00:00,,176,0,0,OCaml,0,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['wcork'],,"This program was completed using pair programming.
Partner: Nicholas Wood (njwood@ucsc.edu)
Partner: William Cork (wcork@ucsc.edu)

Nicholas Wood spent   0  hrs alone
William Cork  spent   0  hrs alone
Both of us    spent   30 hrs together (pair programming the whole way)

Please grade the work submitted by njwood@ucsc.edu
and not the work submitted by wcork@ucsc.edu.

submitted on unix.ucsc.edu via:
For Nicholas Wood (echo wcork > PARTNER):
submit cmps112-wm.f14 asg4 README PARTNER
For William Cork (echo njwood > PARTNER):
submit cmps112-wm.f14 asg4 Makefile bigint.ml bigint.mli maindc.ml\\
scanner.mll README PARTNER

Notes:

All features implemented and tests pass.

Handling divide by zero errors was not specified in the assignment sheet
or seen in the tests but, we decided to mimic the behaviour of dc
anyway. The behaviour is to output the error to stderr and leave the
elements on the stack.

",,EDU,0.83,,,,,,0,0,0.9,EDU
117299950,MDEwOlJlcG9zaXRvcnkxMTcyOTk5NTA=,arcsv,SUwonglab/arcsv,0,SUwonglab,https://github.com/SUwonglab/arcsv,DEV,Complex structural variant detection from WGS data,0,2018-01-13 00:08:07+00:00,2025-02-26 14:50:27+00:00,2025-01-10 05:06:46+00:00,,5354,28,28,Python,1,1,1,1,0,0,8,0,0,0,mit,1,0,0,public,8,0,28,master,1,"['jgarthur', 'ghm17']",1,"# ARC-SV: Automated Reconstruction of Complex Structural Variants #

ARC-SV is a structural variant caller for paired-end, whole-genome sequencing data. For methodological details, please see our preprint: [https://doi.org/10.1101/200170].

This software was developed in the [Wong Lab](https://web.stanford.edu/group/wonglab/) at Stanford University with funding from the NSF Grant DGE-114747 and NIH grants T32-GM096982, P50-HG007735, and R01-HG007834.

# Table of Contents #

   * [Requirements](#requirements)
   * [Installation](#installation)
      * [Example installation from scratch](#example-installation-from-scratch)
      * [Getting reference resources](#getting-reference-resources)
   * [Usage](#usage)
      * [Calling SVs](#calling-svs)
      * [Running the example](#running-the-example)
      * [Filtering and merging output files](#filtering-and-merging-output-files)
      * [Description of output](#description-of-output)

<!-- ## Table of contents ## -->

<!-- * [Requirements](#requirements) -->
<!-- * [Installation](#installation) -->
<!--   * [Example installation from scratch](#installation-example) -->
<!--   * [Getting reference resources](#reference) -->
<!-- * [Usage](#usage) -->
<!--   * [Running the example](#example) -->
<!--   * [Filtering and merging output files](#filtering) -->
<!--   * [Description of output](#output) -->
<!--   <\\!-- * [Usage](#usage) -\\-> -->
<!--   <\\!--   * [STDIN](#stdin) -\\-> -->
<!--   <\\!--   * [Local files](#local-files) -\\-> -->
<!--   <\\!--   * [Remote files](#remote-files) -\\-> -->
<!--   <\\!--   * [Multiple files](#multiple-files) -\\-> -->
<!--   <\\!--   * [Combo](#combo) -\\-> -->
<!--   <\\!-- * [Tests](#tests) -\\-> -->
<!--   <\\!-- * [Dependency](#dependency) -\\-> -->

# Installation #

ARC-SV and its dependencies can be installed as follows:

```

git clone https://github.com/SUwonglab/arcsv.git
cd arcsv
pip3 install --user .
```

*OS X users with a `brew`ed Python installation should ignore `--user` above.*

The installed location of the main script, `arcsv`, must be in your path. The correct folder is probably `/usr/bin`, `/usr/local/bin`, or `~/.local/bin`.

## Example: Using `conda`

If an isolated environment is desired, or if installing ARC-SV using `pip` is causing problems, it is recommended to use [conda](https://docs.conda.io/projects/conda/en/stable/index.html).

```
conda create -n arcsv --strict-channel-priority -c conda-forge -c bioconda \\
  python=3 pysam numpy scipy scikit-learn matplotlib python-igraph

cd /path/to/arcsv
pip3 install .
```

To run ARC-SV in future login sessions, the conda environment must first be activated using `conda activate arcsv`.

## Example: installing system dependencies without `conda` ##

The following commands should install ARC-SV and all dependencies on a fresh copy of Ubuntu:

```

# update packages
sudo apt-get update

# install pip and setuptools
sudo apt install python3-pip
pip3 install -U pip setuptools

# extra requirements needed for igraph
sudo apt install libxml2-dev zlib1g-dev

# arcsv setup
sudo apt install git
git clone https://github.com/jgarthur/arcsv.git
cd arcsv
pip3 install --user .

# add this to your .bash_profile
export PATH=""~/.local/bin/:$PATH""

```

## Getting reference resources ##

You will need a bed file containing the locations of assembly gaps in your reference genome. The `resources/` folder contains files for hg19, GRCh37, and hg38, which were retrieved as follows:

```
curl http://hgdownload.cse.ucsc.edu/goldenpath/hg19/database/gap.txt.gz | \\
     zcat | \\
     cut -f2-4 > hg19_gap.bed
     
curl http://hgdownload.cse.ucsc.edu/goldenpath/hg38/database/gap.txt.gz | \\
     zcat | \\
     cut -f2-4 > hg38_gap.bed
```

or for the NCBI reference (with ""2"" instead of ""chr2""):

```

curl http://hgdownload.cse.ucsc.edu/goldenpath/hg19/database/gap.txt.gz | \\
     zcat | \\
     cut -f2-4 | \\
     sed 's/^chr//' \\
     > GRCh37_gap.bed

```

# Usage #

## Calling SVs ##

To call SVs:

```
arcsv call -i reads.bam -r chrom[:start-end] -R reference.fasta -G reference_gaps.bed -o output_dir

# To see more detailed documentation on all possible arguments
arcsv call -h
```

## Running the example ##

The folder `example/` in this repository contains files to test the ARC-SV installation:

```
arcsv call -i example/input.bam -r 20:0-250000 -o my_example_output \\
  -R example/reference.fa -G example/gaps.bed
  
diff my_example_output/arcsv_out.tab example/expected_output.tab
```

## Filtering and merging output files ##

ARC-SV works on a single chromosome at a time. Supposing your output folders are named ""arcsv_chr#"", you can merge and/or filter the results as follows:

```

# Recommended settings
arcsv filter-merge --min_size 50 --no_insertions arcsv_chr*

# If no filtering is desired
arcsv filter-merge arcsv_chr*

```

## Description of output ##

For each cluster of candidate breakpoints, ARC-SV attempts to resolve the local structure of both haplotypes. The output file `arcsv_out.tab` contains one line for each non-reference haploype called. A call typically consists of a single SV (simple or complex), but some contain multiple variants that were called together. 

Where multiple values are given, as in svtype, the order is left to right in the alternate haplotype, which is shown in the **rearrangement** column.

All genomic positions in `arcsv_out.tab` are 0-indexed for compatibility with BED files. (arcsv_out.vcf is still 1-indexed as required.)

Output field | Description
------------ | -----------
chrom | chromosome name
minbp | position of first novel adjacency
maxbp | position of last novel adjacency
id | identifier consisting of the region in which the event was called
svtype | classification of each simple SV/complex breakpoint in this event
complextype | complex SV classification
num_sv | number of simple SVs + complex SV breakpoints in this call
bp | all breakpoints, i.e. boundaries of the blocks in the ""reference"" column (including the flanking blocks)
bp_uncertainty | width of the uncertainty interval around each breakpoint in `bp`. For odd widths, there is 1 bp more uncertainty on the right side of the breakpoint
reference | configuration of genomic blocks in the reference. Blocks are named A through Z, then a through z, then A1 through Z1, etc.
rearrangement | predicted configuration of genomic blocks in the sample. Inverted blocks are followed by a tick mark, e.g., A', and insertions are represented by underscores _
len_affected | length of reference sequence affected by this rearrangement  (plus the length of any novel insertions). For complex SVs with no novel insertions, this is often smaller than maxbp - minbp, i.e., the ""span"" of the rearrangement in the reference
filter | currently, this is `INSERTION` if there is an insertion present, otherwise `PASS`
sv_bp | breakpoint positions for each simple SV/complex breakpoint in the event (there are `num_sv` pairs of non-adjacent reference positions, each one describing a novel adjacency)
sv_bp_uncertainties | breakpoint uncertainties for each simple SV/complex breakpoint in the event
gt | genotype [either `HET` or `HOM`]
af | allele fraction for the called variant [either 0.5 or 1.0, unless `--allele_fraction_list` was set]
inslen | length of each insertion in the call
sr_support | number of supporting split reads for each simple SV and complex breakpoint (length = num_sv)
pe_support | number of supporting discordant pairs for each simple SV and complex breakpoint (length = num_sv)
score_vs_ref | log-likelihood ratio score for the call: `log( p(data | called genotype) / p(data | reference genotype) )`
score_vs_next | log-likelihood ratio score for the call vs the next best call: `log( p(data | called genotype) / p(data | next best genotype) )`
rearrangement_next | configuration of genomic blocks for the next best call (may contain more blocks than the ""reference"" and ""rearrangement"" columns
num_paths | number of paths through this portion of the adjacency graph. The called haplotype corresponds to one such path
",,DEV,0.62,,,,,,0,2,0.95,DEV
517825518,R_kgDOHt1j7g,zhouconghao,zhouconghao/zhouconghao,0,zhouconghao,https://github.com/zhouconghao/zhouconghao,OTHER,,0,2022-07-25 21:27:00+00:00,2025-03-04 18:41:35+00:00,2025-03-04 18:41:31+00:00,,2932,2,2,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,2,main,1,"['github-actions[bot]', 'zhouconghao']",,"<div align=""center"">
<h1> Hi! Zhou here! üëã </h1>


Welcome to my GitHub page, where I store my code! 

I am a physics graduate student at UC Santa Cruz. 

More info about me can be found at [zch.info](http://www.zch.info).
",,OTHER,0.71,,,,,,0,2,0.8,OTHER
667649236,R_kgDOJ8uE1A,Hangman-Public,amishana/Hangman-Public,0,amishana,https://github.com/amishana/Hangman-Public,EDU,Python: The game Hangman where the computer picks a word based on the specifications given by the user and the user tries to guess the word.,0,2023-07-18 02:15:31+00:00,2023-07-18 02:15:32+00:00,2023-07-18 02:16:29+00:00,,5,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['amishana'],,"# Hangman-Public
### Description:<br />
This game follows the rules of the traditional game of ""Hangman"" except it is a one-player version. Once the game is started you will be asked how many letters the word should be. Then, you will be asked how many lives you would like to play with. After that, guess an individual letter each time you are prompted, with the aim of guessing the full word correctly. Once you either correctly guess the word or lose all your lives, the game will end and the computer will ask if you would like to play again. The computer will prompt you with each of these actions. When asked for a number or letter, if you input a different character than expected, the computer will continue asking until the correct type of character is inputted. 

### How to Gain Access to the Full Repository:<br />
I submitted this project as a part of a class at UC Santa Cruz, and am not allowed to post it publicly. If you would like to see this project, please message me on LinkedIn (at www.linkedin.com/in/amishanambiar) and I will grant you access. Thank you for your understanding.

### How to Run This Project:<br />
First, install a Python IDE. I used PyCharm for this project, but others such as IDLE or Visual Studio should work as well. Then, download all the files in the repository and open them in your IDE. Run hangman.py to play the game or hangman_test.py to play the game with a shortened dictionary for testing purposes.

### Authors:<br />
#### Amisha Nambiar and Professor Larissa Munishkina from the University of California Santa Cruz
Amisha Nambiar authored hangman.py and hangman_test.py.<br />
Amisha Nambiar and Professor Larissa Munishkina authored the pseudocode for hangman.py and hangman_test.py.<br />
Professor Larissa Munishkina provided dictionary.txt and dictionary-short.txt.<br />

### Files in This Repository:<br />
1. dictionary.txt contains all the possible words for the computer to choose from.<br />
2. dictionary-short.txt contains one word of each possible word length for the computer to choose from.<br />
3. hangman.py contains the code for the game using dictionary.txt as the inputted dictionary file.<br />
4. hangman_test.py contains the same code as hangman.py except the inputted dictionary file is dictionary-short.txt. This will allow the user to test if the game mechanics are working correctly using a known word.
",,EDU,0.83,,,,,,0,1,0.9,EDU
597546587,R_kgDOI53WWw,CruzHacksBeats,ahzengyang/CruzHacksBeats,0,ahzengyang,https://github.com/ahzengyang/CruzHacksBeats,DOCS,,0,2023-02-04 21:46:26+00:00,2023-02-04 21:46:26+00:00,2023-02-05 11:32:29+00:00,,5,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,"['Fenchpress', 'ahzengyang']",,"# CruzHacksBeats
https://www.youtube.com/watch?v=tuIrd0GOjcs
## Inspiration
There was a time in Bryan Fenchel's life that he spent playing, composing, and producing music every waking moment. Bryan believed that someday some of this sonic brilliance would reach someone‚Äôs ears and It would have a profound impact.  Everyone wants to be heard; this is one thing we all have in common.  But when do we listen?  We listen when we want to connect with others.  When we are curious and seeking to connect with the world that we are all a part of.  Wouldn‚Äôt it be wonderful if small artists were able to connect with audiences all over the world in meaningful ways and reach beyond the screen to dance into the real world?

## What it does
This juxtaposition and synchronization of the real and virtual world creates strings that interweave one another, surpassing geographical obstacles. With the help of Niantic Lightship Visual Positioning System(VPS), we are building the foundations of a digital music wonderland; our first contribution to the real world meta-verse. The platform we will have started to build together, here at CruzHacks 2023, will allow users to step ‚Äúthrough the looking glass‚Äù and experience a world of wonder together.

By tossing a virtual rose, a user can show their support triggering an audio visual augmented reality(AR) performance of Renee Harmoni, a rising star and artist on the label of my close friend and business partner, Donyea Gooding(Starchild Yeezo). We are grateful to have been given permission to use her unreleased song for our hack.  Though she is in Los Angeles, she is here today virtually on the Stevenson Stage alongside the beautiful grand piano.
https://www.tiktok.com/@officialreneeharmoni?_t=8YXL2GB3jeo&_r=1

## How we built it
We came together as a team, a joint effort of 2 CS students at USF and 2 students at UC Santa Cruz. We fleshed out the details and troubleshooted together, which resulted in a product much greater than which any of us had pictured.  Teamwork makes the dreamwork.  We started by attending the Niantic Lightship workshops to get a better understanding of what and who we would have the pleasure of working with.

## Challenges we ran into
Though it was difficult, the challenge was a well welcome pleasure to all of us. Though we encountered hindrances we were lucky and grateful to have the assistance of Niantic representatives to help us cross the finish line.

## Accomplishments that we're proud of
We are very proud of our team member Renier‚Äôs work on implementing the physics of the throwing of roses to make the toss more natural and varied.  It was a much more rigorous task than anticipated.  

## What we learned
It was challenging to learn new tools and languages that we all were not familiar with such as Niantic Lightship VPS, C#, Unity.  We also learned the importance of both strong leadership and the power of teamwork.

## What's next for Find The Beat
We are focused on the needs of creators and their audiences.  If we build something that can support multiple live AR performances in multiple locations, while supporting the exchange of NFT‚Äôs, and enabling artists to engage with fans in meaningful individualized, memorable ways unique to the AR/VR live performance experience; that would be something truly special. We are currently developing AI tools to enable any electronic musician to live stream a performance and instantly transform streaming live audio into a multi media interactive immersive live performance. What if these AI driven AR performances were linked to virtual performance locations in order to also support a VR audience? 
We aim to serve creators by giving them a safe place to perform and share there ideas and experiences with an audience of infinite size and scale in real and virtual places, limited only by the creators imagination and enhanced by the capability, opportunity and access of Web 3.0 technologies: An end to end Web 3 virtual performance solution built for everyone. 
We seek to foster meaningful connections with Niantic and continue to contribute to the real-world meta-verse.  
",1,EDU,0.73,,,,,,0,1,0.7,EDU
211955480,MDEwOlJlcG9zaXRvcnkyMTE5NTU0ODA=,workshops,datascienceucsc/workshops,0,datascienceucsc,https://github.com/datascienceucsc/workshops,EDU,Source files for club workshops,0,2019-09-30 20:56:37+00:00,2020-10-27 01:35:08+00:00,2020-10-27 01:35:05+00:00,,5066,1,1,Jupyter Notebook,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,master,1,"['Jswig', 'ryangdar', 'jlehrer1']",1,"# Data Science Slugs Workshops
*Anders Poirel*

This (WIP) repository hosts all the deliverables for Data Science Slugs @ UCSC's workshops and activities - or at least those were a member who bothered to use version control was in charge.

## Structure

`w2020` contains all the materials for the Winter Quarter of 2020 as UC Santa Cruz, and so on.

## Contributing

Non-text files (`.pdfs`, `.png`, `.ipynb`) should be tracked exclusively with 
[Git Large File Storage](https://git-lfs.github.com/). This is necessary to 
avoid unmanageable repository sizes as we add several quarter's worth of files

## Use

You are free to re-use any material here for any purpose, but crediting us in your presentations/projects is always appreciated üòÉ
",,EDU,0.79,,,,,,0,1,0.95,EDU
757786594,R_kgDOLSrn4g,OSPO-git-party,emmet0r/OSPO-git-party,0,emmet0r,https://github.com/emmet0r/OSPO-git-party,OTHER,Internal git sandbox for UCSC's OSPO üè¢,0,2024-02-15 01:13:06+00:00,2024-02-15 01:13:07+00:00,2024-02-15 22:57:08+00:00,,13,0,0,,1,1,1,1,0,0,2,0,0,0,,1,0,0,public,2,0,0,main,1,"['emmet0r', 'slieggi', 'yelenamartyno']",,"# Internal git sandbox for UCSC's OSPO üè¢

- [How to link GitHub issues and Pull Requests (PRs)](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue)
- [Markdown Cheat Sheet](https://enterprise.github.com/downloads/en/markdown-cheatsheet.pdf)
",,OTHER,0.84,,,,,,0,1,1,OTHER
165562987,MDEwOlJlcG9zaXRvcnkxNjU1NjI5ODc=,ML-movie-project,aahchn/ML-movie-project,0,aahchn,https://github.com/aahchn/ML-movie-project,EDU,,0,2019-01-13 22:46:33+00:00,2019-05-01 20:30:34+00:00,2019-05-01 20:30:32+00:00,,3756,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['aahchn'],,"Prabhmeet Gill - (psgill@ucsc.edu - 1474279)
Tarun Sahl (tsalh@ucsc.edu - 1594485) 
Aaron Chan (aahchan@ucsc.edu - 1462870)


How to Run:
Put the project.py file into the same directory as the .csv file and enter the command ""python project.py XXXXXX.csv""

",,EDU,0.79,,,,,,0,1,0.9,EDU
17254626,MDEwOlJlcG9zaXRvcnkxNzI1NDYyNg==,sage,ucsc-proglang/sage,0,ucsc-proglang,https://github.com/ucsc-proglang/sage,EDU,The dependently & gradually typed Sage programming language,0,2014-02-27 16:01:09+00:00,2024-07-14 12:35:15+00:00,2014-02-28 16:38:13+00:00,,2160,9,9,FORTRAN,1,1,1,1,0,0,0,0,0,0,apache-2.0,1,0,0,public,0,0,9,master,1,['kennknowles'],1,"The Sage Programming Language
=============================

http://sage.soe.ucsc.edu/ 
https://github.com/ucsc-proglang/sage

A prototype implementation of a language with:

 - Executable refinement types
 - First-class types (aka full dependent types a la Cayenne)
 - Dynamic types (aka gradual typing)

with an implementation supported by:

 - An off-the-shelf theorem prover (in this case, hard-coded to Simplify)
 - A compile-time interpreter
 - A database of run-time failures (every run-time failure occurs ""at most once"")

Disclaimer: This software is a very rough prototype! It is provided for
scientific and pedagogical purposes.

*Note*: The Simplify binaries included here are not part of the Sage materials.
They are distributed for your convenience under the Java Programming Toolkit software license, 
available at http://www.hpl.hp.com/downloads/crl/jtk/agreement.html


Building and ""installing""
-------------------------

Try this

```bash
    $ make
```

If all goes well, you should have a sage executable.

Two copies of the binary executable of the Simplify theorem prover are
included. For `sage` to run correctly (without the `-nosimplify` flag), an
executable named `Simplify` must exist.

On Linux, the following should work:

```bash
  $ ln -s Simplify-1.5.4.linux Simplify
```

On Windows, this is better:

```
  cp Simplify-1.5.4.exe.win Simplify.exe
```


Usage
-----

You can experiment with Sage via its interactive toplevel by running `sage` with no arguments.
We have prepared a tutorial at http://sage.soe.ucsc.edu/tutorial.txt

```ocaml
$ ./sage
creating new database default.db.
     Welcome to Sage

# let x = 3;;
Binding for: x
Type: (Refine Int (fn (x:Int) => (inteq x 3)))
Evaluation: 3

# let add_one (x:Int) = x + 1;;
Binding for: add_one
Type: (x:Int -> (Refine Int (fn (z:Int) => (inteq z (add x 1)))))
Evaluation: (fn (x:Int) => (add x 1))

# add_one 3;;
Type: (Refine Int (fn (z:Int) => (inteq z (add 3 1))))
Evaluation: 4
```

The fun never ends.

To type check and run a file of sage code, use `sage <filename>`.
A number of example programs are in the `tests` subdirectory.
For example, try `sage tests/polylist.f`

Running `sage -help` shows the command-line options, many of which are
are for debugging purposes.


Contributors
------------

 * [Kenn Knowles](https://github.com/kennknowles) ([@kennknowles](https://twitter.com/KennKnowles))
 * [Cormac Flanagan](http://cs.ucsc.edu/~cormac)
 * [Stephen Freund](http://dept.cs.williams.edu/~freund/)
 * [Jessica Gronski](https://www.facebook.com/jgronski)
 * [Aaron Tomb](http://corp.galois.com/aaron-tomb/)


Readings
--------

Do you like the ideas in Sage? You may enjoy deep theoretical and practical readings about their development.

The most thorough treatment of the theory, and a good place to jump off to related work, is Kenn's dissertation.

 - _[Executable Refinement Types](http://kennknowles.com/research/kknowles-dissertation.pdf)_  
   Kenneth Knowles. Doctoral dissertation, 2014.

If you prefer smaller bits, the theory has been published in pieces in conferences and journals over the years.

 - [Sage: Unified Hybrid Checking for First-Class Types, General Refinement Types, and Dynamic](http://sage.soe.ucsc.edu/sage-tr.pdf)  
   Kenneth Knowles, Aaron Tomb, Jessica Gronski, Stephen N. Freund, and Cormac Flanagan.  
   _Scheme Workshop_ 2006
 - [Hybrid type checking](http://users.soe.ucsc.edu/~cormac/papers/toplas09.pdf)  
   Kenneth Knowles and Cormac Flanagan  
   _Transactions on Programming Languages and Systems_ (TOPLAS) 2010
   Revised and extended from Cormac Flanagan's work in _Principles of Programming Languages_ (POPL) 2006
 - [Hybrid Types, Invariants, and Refinements for Imperative Objects](http://www.cs.ucsc.edu/%7Ecormac/papers/fool06.pdf)  
   Cormac Flanagan, Stephen N. Freund, and Aaron Tomb  
   _Foundations of Object Oriented Languages_ (FOOL) 2006
 - [Type Reconstruction for General Refinement Types](http://kennknowles.com/research/knowles-flanagan.esop.07.type.pdf)  
   Kenneth Knowles and Cormac Flanagan  
   _European Symposium on Programming_ (ESOP) 2007
 - [Unifying Hybrid Types and Contracts](http://sage.soe.ucsc.edu/tfp07-gronski-flanagan.pdf)  
   Jessica Gronski and Cormac Flanagan  
   Presented at Trends in Functional Programming (TFP) 2007
 - [Space Efficient Gradual Typing](http://sage.soe.ucsc.edu/tfp07-herman-tomb-flanagan.pdf)  
   Dave Herman, Aaron Tomb, and Cormac Flanagan  
   _Trends in Functional Programming_ (TFP) 2007
 - [Compositional and Decidable Checking for Dependent Contract Types](http://kennknowles.com/research/knowles-flanagan.plpv.09.compositional.pdf)  
   Kenneth Knowles and Cormac Flanagan  
   _Programming Languages meets Program Verification_ (PLPV) 2009

(It took a few years to discover the name ""Executable Refinement Types"" but these are all talking about the same sorts of type systems, where the refinements are closely tied to the ability to run-time behavior to enable Hybrid Type Checking)


Other interesting projects
--------------------------

If you are interested in exciting modern developments in refinement types, be sure to check these other projects out!

 - [Liquid Haskell](https://github.com/ucsd-progsys/liquidhaskell) adds refinement types to Haskell.
 - [F7](http://research.microsoft.com/en-us/projects/f7/) adds refinement types to F#.
 - [Ynot](http://ynot.cs.harvard.edu/) adds stateful programming capabilities to Coq.
 - [HCC](http://pauillac.inria.fr/~naxu/research/hcc.html) adds hybrid contract checking to OCaml.

A bit more tangential, but still related, you may also enjoy following the progress of dependently typed
programming via
[Agda](http://wiki.portal.chalmers.se/agda/pmwiki.php),
[Epigram](http://code.google.com/p/epigram/),
[Coq](http://coq.inria.fr/),
[Ur](http://www.impredicative.com/ur/),
etc.


Acknowledgments
---------------

The example code from Benjamin Pierce's _Types and Programming Languages_ was used with permission as a starting
point for this prototype.


License
-------

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
",,EDU,0.71,,,,,,0,1,0.95,EDU
385919520,MDEwOlJlcG9zaXRvcnkzODU5MTk1MjA=,UCSCSCALAFP,kaviabey/UCSCSCALAFP,0,kaviabey,https://github.com/kaviabey/UCSCSCALAFP,EDU,testing and learning purpose only.,0,2021-07-14 11:42:15+00:00,2022-07-10 04:51:47+00:00,2022-09-19 17:49:52+00:00,,21,0,0,Scala,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['kaviabey'],,,,EDU,0.85,,,,,,0,1,0.7,EDU
90965418,MDEwOlJlcG9zaXRvcnk5MDk2NTQxOA==,EWAS-fusion,jinghuazhao/EWAS-fusion,0,jinghuazhao,https://github.com/jinghuazhao/EWAS-fusion,DEV,Epigenomewide Association Studies (EWAS) with FUnctional Summary-based ImputatiON (FUSION),0,2017-05-11 10:05:29+00:00,2024-05-13 16:14:23+00:00,2023-03-24 17:14:31+00:00,https://jinghuazhao.github.io/EWAS-fusion/,92081,12,12,R,1,1,1,1,1,0,4,0,0,0,,1,0,0,public,4,0,12,master,1,['jinghuazhao'],,"---
title: EWAS-fusion
output:
  html_document:
    mathjax:  default
    fig_caption:  true
    toc: true
    section_numbering: true
bibliography: EWAS-fusion.bib
csl: american-journal-of-medical-genetics.csl
nocite: |
  @freund18, @raj18, @turner18, @zhao07, @zhao5069993
vignette: >
  %\\VignetteEngine{knitr::rmarkdown}
  %\\VignetteIndexEntry{EWAS-fusion}
  %\\VignetteEncoding{UTF-8}
---
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5069993.svg)](https://zenodo.org/record/5069993)

## INTRODUCTION

The title represents pipeline for Epigenomewide association study (EWAS) and Functional Summary-based Imputation (FUSION) association analyses.

Transcriptomewide association statistic $z_{TWAS}$ was originally proposed for gene expression data. For a given Trait of interest **T** for which GWAS summary statistics $z_T$ is available, the corresponding Wald statistic for TWAS is defined such that

$$
z_{TWAS} = \\frac{w^T_{ge}z_T}{\\sqrt{w^T_{ge}Vw_{ge}}}

$$

where $w_{ge}$ is a weight associated with gene expression and **V** covariance matrix for $z_T$, respectively.

By analogy, an epigenomewide association statistic $z_{EWAS}$ is defined through methylation data so that

$$
z_{EWAS} = \\frac{w^T_{me}z_T}{\\sqrt{w^T_{me}Vw_{me}}}

$$

where $w_{me}$ is the weight associated with methylation. Both approaches allow for imputation using GWAS summary statistics. The derivation of these weights and imputation were done using methods as described in @gusev16 called TWAS as well as in @mancuso17 called Functional Summary-based Imputation (FUSION). The TWAS statistics from both approaches agreed very well.

```mermaid
graph TB;
SNP[""LD reference panel (bed,bim,fam)""] --> |""EWAS reference panel(top1, blup, lasso, enet, bslmm)""| Methylation;
Methylation --> Trait;
SNP --> |""GWAS summary statistics (SNP, A1, A2, Z)""| Trait;
```

A total of 442,920 CpG sites based on Illumina humanmethylation450 chips on 1,.146 individuals in EPIC-Norfolk study were available. Among these, 1,117 individuals also had genotype data from Affymetrix BioBank Axiom chips. HapMap2 SNPs from genetic data of these individuals were extracted via PLINK2 according to cis-positions of each probe and subsequently used to build weight analogous to gene expression data as implemented in computer software TWAS. We filtered probes according to their heritabilities estimated from software GCTA at significant level of 0.01. We then performed EWAS for given GWAS summary statistics. The weight generation and methylation imputation was implemented in software called TWAS-pipeline, which allows for whole epigenome computation. After filtering, 78,133 probes reached significant level 0.01.

The FUSION framework has several advantages: First, it integrates heritability estimation and covariate adjustment for whole-chromosomes with additional models such as LASSO, elastic net, BLUP. Second, it offers cross-validation, joint/conditional analyses with the output also informing top hit SNPs and inferred methylation quantitative trait locus (meQTL). Besides, the new software uses modified GCTA software (gcta\\_nr\\_robust) leading to higher yield of probes with heritabilities reaching statistical significance, GEMMA giving BSLMM estimates and ability to align strands with reference panels. As both the increased number of models and cross-validation led to excessive computing time, we dropped BSLMM models and conducted five cross-validations. As a result our reference panel for EWAS imputation contains 77,372 probes reaching the heritability p value threshold of 0.01. The association as well as joint/conditional analysis using our weights and LD panel is implemented in software called EWAS-fusion. Like the original TWAS, our implementation will enable a range of GWAS summary statistics to be used coupled with downstream analysis.

EWAS-fusion is reminiscent of Mendelian Randomisation as shown below,

```mermaid
graph TB;
SNP --> |""EWAS reference panel(top1, blup, lasso, enet, bslmm)""| Methylation;
Methylation --> Trait;
SNP --> |""GWAS summary statistics (SNP, A1, A2, Z)""| Trait;
```

## INSTALLATION

* To begin, the software [FUSION](http://gusevlab.org/projects/fusion/) including dependencies such as `plink2R` and `reshape` is required. The latest version also requires [jlimR](https://github.com/cotsapaslab/jlim). Other facilities to be required are Sun grid engine (sge) or GNU parallel for Linux clusters. A summary is also available from here: [https://jinghuazhao.github.io/software-notes/AA/](https://jinghuazhao.github.io/software-notes/AA/).
* Install the repository on your system, you will need weights based on epigenetic data or to generate them as described in **Weight generation** below.


| FILE                  | Description                |
| ----------------------- | ---------------------------- |
| EWAS-weights/         | directory for EWAS weights |
| glist-hg19            | Probe list                 |
| LDREF/                | Reference for LD           |
| EWAS-weights.pos      | Definition of regions      |
| EWAS-weights.profile* | Probe profiles             |

`*` It contains information about the probes but not directly involved in the association analysis. Earlier version of EWAS-fusion used EWAS/, RDat.pos, and RDat.profile.

## USAGE

The syntax is as follows,

```
ewas-fusion.sh input-file
```

These will send jobs to the Linux clusters. The sge error and output, if any, should be called EWAS.e and EWAS.o in your HOME directory.

## Input

The input file contains GWAS summary statistics similar to [.sumstats](https://github.com/bulik/ldsc/wiki/Summary-Statistics-File-Format) as in [LDSC](https://github.com/bulik/ldsc) with the following columns.


| Column | Name | Description                             |
| -------- | ------ | ----------------------------------------- |
| 1      | SNP  | RS id of SNPs                           |
| 2      | A1   | Effect allele (first allele)            |
| 3      | A2   | Other allele (second allele)            |
| 4      | Z    | Z-scores, taking sign with repect to A1 |

## Output

The results will be in input-file.tmp/ directory.

## Annotation

This is furnished with contribution from Dr Alexia Cardona, alexia.cardona@mrc-epid.cam.ac.uk, as follows,

```
Rscript ewas-annotate.R input-file.tmp
```

It is assumed that `HumanMethylation450_15017482_v1-2.csv` is available from the directory containing `ewas-annotate.R`but this can be at different location

```
Rscript ewas-annotate.R input-file.tmp manifest_location=/at/different/location
```

Q-Q and Manhattan plots using R/gap can be obtained from

```
Rscript ewas-plot.R input-file.tmp
```

## Example

The script [test.sh](test.sh) uses data reported in @wood14. It downloads and generates an input file called `height` to `ewas-fusion.sh`.

```
ewas-fusion.sh height
```

The results will be in `height.tmp/` once it is done.

The annotation is done with

```
Rscript ewas-annotate.R height.tmp
```

The Q-Q and Manhattan plots are generated with

```
Rscript ewas-plot.R height.tmp
```

## Weight generation

This is a revised and much simplified implementation of codes available from TWAS-pipeline. Under our sge it is furnished with

```
qsub get_weight.qsub
```

or

```
qsub get_weight.qsub 22
```

for chromosome 22.

Inputs to these are summarised as follows,


| File         | Description                                            |
| -------------- | -------------------------------------------------------- |
| FUSION.pheno | PLINK phenotype file containing data for all probes    |
| FUSION.covar | PLINK covariate file containing covariates such as PCs |
| CpG.txt      | CpG ID, chromosome and position                        |

In addition, PLINK binary pedigree file for each CpG also requires to be prepared, as in [files](files). Although it was not done, it is possible to use code as in [1KG.sh](files/1KG.sh) to get around gerneration of these individual files by using a combined one. Note the setup takes advantage of the compact storage of non-genetic data.

The results will be available from the EWAS-fusion directory to be profiled and used for association analysis above. As the number of files is fairly large, [cp_weight.qsub](files/cp_weight.qsub) is written to put weights from their temporary directories in place while [ewas-profile.sh](files/ewas-profile.sh) profiles these weights as well as prepares for LDREF. For [the version with FUSION](https://data.broadinstitute.org/alkesgroup/FUSION/LDREF.tar.bz2), it can be done as follows,

```bash
wget -qO- https://data.broadinstitute.org/alkesgroup/FUSION/LDREF.tar.bz2 | tar xfj - --strip-components=1
seq 22|awk -vp=1000G.EUR. '{print p $1}' > merge-list
plink-1.9 --merge-list merge-list --make-bed --out FUSION
rm 1000G.EUR.* merge-list
sort -k2,2 FUSION.bim > EUR.bim
```

To mirror FUSION, which uses [glist-hg19](https://www.cog-genomics.org/static/bin/plink/glist-hg19), an equivalent for EWAS needs to be built.

## ACKNOWLEDGEMENTS

We wish to thank colleagues and collaborators for their invaluable contributions to make this work possible.

## APPENDIX

Additional information for Illumina infinium humanmethylation450 beadchip as in [Illumina website](https://support.illumina.com/array/array_kits/infinium_humanmethylation450_beadchip_kit/downloads.html)


| **Column Name**             | **Description**                                                                                          |
| ----------------------------- | ---------------------------------------------------------------------------------------------------------- |
| Index                       | Probe Index                                                                                              |
| TargetID                    | Identifies the probe name. Also used as a key column for data import.                                    |
| ProbeID_A                   | Illumina identifier for probe sequence A                                                                 |
| ProbeID_B                   | Illumina identifier for probe sequence B                                                                 |
| IlmnID                      | Unique CpG locus identifier from the Illumina CG database                                                |
| Name                        | Unique CpG locus identifier from the Illumina CG database                                                |
| AddressA_ID                 | Address of probe A                                                                                       |
| AlleleA_ProbeSeq            | Sequence for probe A                                                                                     |
| AddressB_ID                 | Address of probe  B                                                                                      |
| AlleleB_ProbeSeq            | Sequence for probe B                                                                                     |
| Infinium_Design_Type        | Defines Assay type - Infinium I or Infinium II                                                           |
| Next_Base                   | Base added at SBE step - Infinium I assays only                                                          |
| Color_Channel               | Color of the incorporated base√° (Red or Green) - Infinium I assays only                                 |
| Forward_Sequence            | Sequence (in 5'-3' orientation) flanking query site                                                      |
| Genome_Build                | Genome build on which forward sequence is based                                                          |
| CHR                         | Chromosome - genome build 37                                                                             |
| MAPINFO                     | Coordinates - genome build 37                                                                            |
| SourceSeq                   | Unconverted design sequence                                                                              |
| Chromosome_36               | Chromosome - genome build 36                                                                             |
| Coordinate_36               | Coordinates - genome build 36                                                                            |
| Strand                      | Design strand                                                                                            |
| Probe_SNPs                  | Assays with SNPs present within probe >10bp from query site                                              |
| Probe_SNPs_10               | Assays with SNPs present within probe ?10bp from query site (HM27 carryover or recently discovered)      |
| Random_Loci                 | Loci which were chosen randomly in the design proccess                                                   |
| Methyl27_Loci               | Present or absent on HumanMethylation27 array                                                            |
| UCSC_RefGene_Name           | Gene name (UCSC)                                                                                         |
| UCSC_RefGene_Accession      | Accession number (UCSC)                                                                                  |
| UCSC_RefGene_Group          | Gene region feature category (UCSC)                                                                      |
| UCSC_CpG_Islands_Name       | CpG island name (UCSC)                                                                                   |
| Relation_to_UCSC_CpG_Island | Relationship to Canonical CpG Island: Shores - 0-2 kb from CpG island; Shelves - 2-4 kb from CpG island. |
| Phantom                     | FANTOM-derived promoter                                                                                  |
| DMR                         | Differentially methylated region (experimentally determined)                                             |
| Enhancer                    | Enhancer element (informatically-determined)                                                             |
| HMM_Island                  | Hidden Markov Model Island                                                                               |
| Regulatory_Feature_Name     | Regulatory feature (informatically determined)                                                           |
| Regulatory_Feature_Group    | Regulatory feature category                                                                              |
| DHS                         | DNAse hypersensitive site (experimentally determined)                                                    |

## Bioconductor packages

These are **IlluminaHumanMethylation450kanno.ilmn12.hg19** and **IlluminaHumanMethylation450kmanifest** as shown in **minfiDataEPIC**.

```r
library(IlluminaHumanMethylation450kanno.ilmn12.hg19)
data(IlluminaHumanMethylation450kanno.ilmn12.hg19)
options(width=200)
annotation.table <- getAnnotation(IlluminaHumanMethylation450kanno.ilmn12.hg19)
head(Locations)
head(Manifest)
head(annotation.table)
```

with `IlluminaHumanMethylation450kanno.ilmn12.hg19` as follows,

```
IlluminaMethylationAnnotation object
Annotation
  array: IlluminaHumanMethylation450k
  annotation: ilmn12
  genomeBuild: hg19
Available annotation
  Islands.UCSC
  Locations
  Manifest
  Other
  SNPs.132CommonSingle
  SNPs.135CommonSingle
  SNPs.137CommonSingle
  SNPs.138CommonSingle
  SNPs.141CommonSingle
  SNPs.142CommonSingle
  SNPs.144CommonSingle
  SNPs.146CommonSingle
  SNPs.147CommonSingle
  SNPs.Illumina
Defaults
  Locations
  Manifest
  SNPs.137CommonSingle
  Islands.UCSC
  Other
```

## REFERENCES

Freund MK, Burch KS, Shi H, Mancuso N, Kichaev G, Garske KM, Pan DZ, Miao Z, Mohlke KL, Laakso M, Pajukanta P, Pasaniuc B, Arboleda VA. 2018. Phenotype-specific enrichment of mendelian disorder genes near GWAS regions across 62 complex traits. Am J Hum Genet 103: 535‚Äì552.

Gusev A, Ko A, Shi H, Bhatia G, Chung W, Penninx BW, Jansen R, Geus EJ de, Boomsma DI, Wright FA, Sullivan PF, Nikkola E, Alvarez M, Civelek M, Lusis AJ, Lehtim√§ki T, Raitoharju E, K√§h√∂nen M, Sepp√§l√§ I, Raitakari OT, Kuusisto J, Laakso M, Price AL, Pajukanta P, Pasaniuc B. 2016. Integrative approaches for large-scale transcriptome-wide association studies. Nat Genet 48: 245‚Äì52.

Mancuso N, Shi H, Goddard P, Kichaev G, Gusev A, Pasaniuc B. 2017. Integrating gene expression with summary association statistics to identify genes associated with 30 complex traits. Am J Hum Genet 100: 473‚Äì487.

Raj T, Li YI, Wong G, Humphrey J, Wang M, Ramdhani S, Wang YC, Ng B, Gupta I, Haroutunian V, Schadt EE, Young-Pearse T, Mostafavi S, Zhang B, Sklar P, Bennett DA, De Jager PL. 2018. Integrative transcriptome analyses of the aging brain implicate altered splicing in alzheimer‚Äôs disease susceptibility. Nat Genet 50: 1584‚Äì1592.

Turner SD. 2018. Qqman: An R package for visualizing GWAS results using q-q and manhattan plots. Journal of Open Source Software 3: 731.

Wood AR, Esko T, Yang J, Vedantam S, Pers TH, Gustafsson S, Chu AY, Estrada K, Luan J, Kutalik Z, Amin N, Buchkovich ML, Croteau-Chonka DC, Day FR, Duan Y, Fall T, Fehrmann R, Ferreira T, Jackson AU, Karjalainen J, Lo KS, Locke AE, M√§gi R, Mihailov E, Porcu E, Randall JC, Scherag A, Vinkhuyzen AA, Westra HJ, Winkler TW, Workalemahu T, Zhao JH, Absher D, Albrecht E, Anderson D, Baron J, Beekman M, Demirkan A, Ehret GB, Feenstra B, Feitosa MF, Fischer K, Fraser RM, Goel A, Gong J, Justice AE, Kanoni S, Kleber ME, Kristiansson K, Lim U, Lotay V, Lui JC, Mangino M, Mateo Leach I, Medina-Gomez C, Nalls MA, Nyholt DR, Palmer CD, Pasko D, Pechlivanis S, Prokopenko I, Ried JS, Ripke S, Shungin D, Stanc√°kov√° A, Strawbridge RJ, Sung YJ, Tanaka T, Teumer A, Trompet S, Laan SW van der, Setten J van, Van Vliet-Ostaptchouk JV, Wang Z, Yengo L, Zhang W, Afzal U, Arnl√∂v J, Arscott GM, Bandinelli S, Barrett A, Bellis C, Bennett AJ, Berne C, Bl√ºher M, Bolton JL, B√∂ttcher Y, Boyd HA, Bruinenberg M, Buckley BM, Buyske S, Caspersen IH, Chines PS, Clarke R, Claudi-Boehm S, Cooper M, Daw EW, De Jong PA, et al. 2014. Defining the role of common variation in the genomic and biological architecture of adult human height. Nat Genet 46: 1173‚Äì86.

Zhao JH. 2007. Gap: Genetic analysis package. 2007 23: 18.

Zhao JH. 2021. Weights of EWAS-fusion. https://zenodo.org/record/5069993.
",,DEV,0.74,,,,,,0,1,0.75,DEV
243135874,MDEwOlJlcG9zaXRvcnkyNDMxMzU4NzQ=,site-ucsc-notable-women,ucsc/site-ucsc-notable-women,0,ucsc,https://github.com/ucsc/site-ucsc-notable-women,WEB,UC Santa Cruz phenomenal  women..,0,2020-02-26 00:55:25+00:00,2020-10-20 06:59:39+00:00,2023-04-11 23:20:26+00:00,,1571,0,0,HTML,1,1,1,1,0,0,0,0,0,4,mit,1,0,0,public,0,4,0,master,1,"['LuckyLuke001', 'luckyluke007', 'dependabot[bot]']",1,"# jekyll-docker-base
 Create a static site using Jekyll running in a Docker container.


### Site Deploy Status
[![Netlify Status](https://api.netlify.com/api/v1/badges/09db9ddb-dc7e-4394-b8b3-025ad87c111e/deploy-status)](https://app.netlify.com/sites/site-shenomenal/deploys)

### [Codepen Layout](https://codepen.io/luckyluke007/pen/ExjgNYE)

All CSS and Javascript editing should be make on Codepen.

- [W3bits Mansonry grid](https://w3bits.com/css-grid-masonry/)
- [Font Awesome](https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css)
- [Google webfonts](https://fonts.googleapis.com/css?family=Lora|Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i&amp;display=swap)
- [Jquery 3.4.1](https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js)



",1,WEB,0.96,,,,,,0,5,0.95,WEB
174352694,MDEwOlJlcG9zaXRvcnkxNzQzNTI2OTQ=,cadaver,notroj/cadaver,0,notroj,https://github.com/notroj/cadaver,DEV,Command-line WebDAV client,0,2019-03-07 13:50:54+00:00,2025-03-04 17:27:42+00:00,2025-02-03 20:52:58+00:00,,1193,97,97,C,1,0,1,0,0,1,8,0,0,7,gpl-2.0,1,0,0,public,8,7,97,master,1,"['notroj', 'hughmcmaster', 'yarikoptic', 'thesamesam', 'elboulangero', 'macrohumanity']",,"
[![Build and test](https://github.com/notroj/cadaver/actions/workflows/ci.yml/badge.svg)](https://github.com/notroj/cadaver/actions/workflows/ci.yml)

# cadaver

_cadaver_ is a command-line WebDAV client, with support for file
upload, download, on-screen display, in-place editing, namespace
operations (move/copy), collection creation and deletion, property
manipulation, and resource locking.

GitHub: https://github.com/notroj/cadaver | Web: https://notroj.github.io/cadaver/

Bugs, feature requests and patches can be sent in via the Github
repository: https://github.com/notroj/cadaver/issues

~~~
cadaver is Copyright (C) 1999-2024 Joe Orton
Portions are:
Copyright (C) 85, 88, 90, 91, 1995-1999 Free Software Foundation, Inc.
Copyright (C) Free Software Foundation, Inc.
Copyright (C) GRASE Lab, UCSC
~~~
",,DEV,0.77,,,,Directory exists,,0,5,0.7,DEV
75044349,MDEwOlJlcG9zaXRvcnk3NTA0NDM0OQ==,enviz,admbarre/enviz,0,admbarre,https://github.com/admbarre/enviz,EDU,Enrollment visualization tool for CS BS degree at UCSC,0,2016-11-29 04:50:17+00:00,2016-12-02 03:35:23+00:00,2018-01-12 02:39:53+00:00,,55,0,0,JavaScript,1,1,1,1,1,0,2,0,0,1,,1,0,0,public,2,1,0,master,1,"['nbmonahelis', 'admbarre', 'ruiokada']",,"#enviz
Enrollment visualization tool for CS BS degree at UCSC

Motivation
====
Planning one's academic path towards a degree is a daunting task. Often the only resources available to a student
are a catalog of courses and counselors. UCSC provides a curriculum chart in the form of a tree to show prerequisites
and a natural progression of classes towards the degree. This is a good thing that benefits CS students and could be
a benefit to others. However, it is a static document and offers little insight on its own. With JavaScript and D3
we can build a similar curriculum chart from course and enrollment data and provide helpful information and visualizations 
to students.

Objective
===
Currently this project is being pursued as the final project for the CMPS 165 Data Visualization capstone course at UCSC
taught by Suresh Lodha. It is limited in scope to a subset of courses in the CS BS degree as a result. However, this project's
goal is to be further generalized to benefit other degree programs, students wishing to transfer from community 
colleges to UCs, as well as other use cases. As time goes on a roadmap and timeline of objectives will be posted here 
detailing the project's growth.

Inspiration
===
This project is heavily inspired by the skill tree progressions present in many video games, specifically RPGs. Such a
representation is clear and intuitive enough to be grasped without help from outside resources by all age groups. 
There is also a program available to students of the San Mateo Community College District called Degree Works. Its purpose 
is to help students plan their classes and track their progress. While a useful tool, it is not very intuitive and it
is easy to get lost while using it.
",,EDU,0.78,,,,,,0,2,0.95,EDU
538700432,R_kgDOIBvqkA,python-notes,chrisliu298/python-notes,0,chrisliu298,https://github.com/chrisliu298/python-notes,EDU,,0,2022-09-19 21:17:52+00:00,2022-09-21 07:57:13+00:00,2022-09-28 17:31:32+00:00,,403,0,0,,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,0,main,1,['chrisliu298'],,"# Python Notes

This repository contains materials I prepared for [CSE20: Beginning Programming in Python](https://courses.soe.ucsc.edu/courses/cse20), a course I have TAed multiple times at [UC Santa Cruz](https://www.ucsc.edu). The content here will be updated before each week's discussion sections.

## Content

1. [Installing Python](1_installation/install_python.md)
",,EDU,0.65,,,,,,0,1,0.95,EDU
436475062,R_kgDOGgQUtg,UCSCTriaxFunctions,kkokamot/UCSCTriaxFunctions,0,kkokamot,https://github.com/kkokamot/UCSCTriaxFunctions,DEV,,0,2021-12-09 03:53:13+00:00,2023-08-08 21:02:06+00:00,2023-12-06 23:22:19+00:00,,4300,1,1,MATLAB,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,main,1,['kkokamot'],,"# UCSC Triax Functions

These functions are used in analyzing friction data from the UCSC triaxial deformation apparatus in matlab.  

The function **readUCSCtriax** reads in autolab data as a matlab table (ignores headerlines). 

**calc_mu_UCSC** calculates friction for the L-block setup. It saves a figure of friction calculated from the axial intensifier pressure (Pac) and from the load cell. The final experiment table is the initial experiment table with added columns for shear, shear as calculated form the axial control pressure (shear_Pac), friction, and friction as calculated from the axial control pressure (friction_Pac).

**find_holds** creates a mat file of all the hold locations.

**find_k_holds** uses the mat file from find_holds to find the stiffness of each hold as well as delta_mu, delta_mu_c for various steady state possibilities.

**plot_overlapping_holds** uses the mat file from find_holds to plot the decay curve for all the holds both in linear space and log(time) space

Typically, **readUCSCtriax** and **calc_mu_UCSC** are used for each experiment. For example: 

                            test1 = readUCSCtriax('UC0001.csv');  
                            [test1_final] = calc_mu_UCSC(test1, 1, 'UC0001');
",,DEV,0.75,,,,,,0,1,0.75,DEV
105691733,MDEwOlJlcG9zaXRvcnkxMDU2OTE3MzM=,ucscrocketry,UCSC-Rocket-Club/ucscrocketry,0,UCSC-Rocket-Club,https://github.com/UCSC-Rocket-Club/ucscrocketry,WEB,a repo for hosting ucscrocketry.org,0,2017-10-03 19:06:38+00:00,2020-01-21 05:23:55+00:00,2021-02-25 03:32:00+00:00,,812553,1,1,HTML,1,1,1,1,1,0,3,0,0,1,other,1,0,0,public,3,1,1,master,1,"['Windog224', 'skyler-stewart', 'catherinelee274', 'DuncanBark', 'ImStackOverflow', 'a-zarzar', 'olivewong', 'DanielThurau', 'connickshields']",1,"Welcome!!!
This is the new branch Website file!!!
the link to our website is: https://ucscrocketry.org/

   Enjoy!!!
",,WEB,0.88,,,,,,0,10,0.85,WEB
205020071,MDEwOlJlcG9zaXRvcnkyMDUwMjAwNzE=,site-specialevents,ucsc/site-specialevents,0,ucsc,https://github.com/ucsc/site-specialevents,WEB,UC Santa Cruz Special Events - Jekyll,0,2019-08-28 20:44:00+00:00,2025-01-13 23:46:24+00:00,2025-01-13 23:46:21+00:00,,225141,0,0,HTML,1,1,1,1,0,0,0,0,0,3,mit,1,0,0,public,0,3,0,master,1,"['luckyluke007', 'LuckyLuke001', 'Herm71', 'dependabot[bot]']",1,"# UC Santa Cruz Special Events: Jekyll CMS

- [Site](https://specialevents.ucsc.edu)
- [Installing Jekyll](https://jekyllrb.com)
- [Theme templates](https://github.com/luckyluke007/specialevents-redesign)

slight edit.
",,WEB,0.9,,,,,,0,4,0.95,WEB
121691378,MDEwOlJlcG9zaXRvcnkxMjE2OTEzNzg=,app-redirector,ucsc/app-redirector,0,ucsc,https://github.com/ucsc/app-redirector,DEV,Sinatra app to redirect CNAMEs to other URLs.,0,2018-02-15 22:36:42+00:00,2023-01-28 10:17:17+00:00,2021-02-27 01:06:07+00:00,https://ucsc-redirector.herokuapp.com,29,0,0,Ruby,1,0,1,0,0,0,0,1,0,1,,1,0,0,public,0,1,0,master,1,"['knice', 'luckyluke007']",1,"# Redirector

A Sinatra app to redirect CNAMEs to other sites and internal pages.

## Purpose

This app acts as a simple relay from a CNAME to another URL without needing a web server as a middle layer. We track the traffic to the domains in this app by sending 'redirect' events to Google Analytics each time a visitor is redirected.

We use this app to:

- Redirect CNAMEs to internal pages on other sites.
- Retire CNAMEs on the UC Santa Cruz domain (ucsc.edu) by pointing them to a different site.

## How this app works

1. This app is written in Ruby, using [Sinatra][1]. It runs on a Heroku Hobby Dyno.
2. It uses a [hash stored in the settings block][2] of `app.rb` to store CNAMEs and corresponding redirect URLs.
3. When a request comes in, the request's `SERVER_NAME` value is used to check the hash for a corresponding key.
4. If there is a key for `SERVER_NAME`, the app will redirect the request to the value for that key, a URL. If there is no key, the app will show a 404 screen and let the user know there is no redirect configured for the requested `SERVER_NAME`.

## How to add a new redirect

1. Edit `app.rb` to add a CNAME and redirect to the [redirects settings hash][2].
2. In Heroku, [add the domain to the project][3].
3. [Request a new DNS record][4] for the CNAME by submitting a help ticket. The record should be a CNAME record. The target should follow this pattern: `<SUBDOMAIN>.ucsc.edu.herokudns.com`. Replace `<SUBDOMAIN>` with the CNAME you wish to configure.
4. Once the DNS record takes effect, visit your CNAME and you will be redirected to the URL you specificed in the settings hash.
5. HTTPS is configured by default on all Heroku custom domains. So the redirect will work over HTTP or HTTPS requests.

## Checking to see if a CNAME is configured

Go to [/debug/`<DOMAIN>`][5] and replace `<DOMAIN>` with a CNAME to see if that CNAME is configured. **Note:** this does not check that DNS settings are correct. You can check DNS configuration by typing `dig <DOMAIN>` in a terminal window on your computer.

## Google Analytics

This app uses the [staccato gem][6] to track redirects as `events` in Google Analytics.

[1]: http://sinatrarb.com
[2]: https://github.com/ucsc/ucsc-redirector/blob/master/app.rb#L8
[3]: https://devcenter.heroku.com/articles/custom-domains
[4]: https://its.ucsc.edu/network/hostnames/
[5]: https://ucsc-redirector.herokuapp.com/debug/
[6]: https://rubygems.org/gems/staccato
",,DEV,0.88,,,,,,0,6,0.95,DEV
119742484,MDEwOlJlcG9zaXRvcnkxMTk3NDI0ODQ=,OCSUrbanEmissions,Timothy-W-Hilton/OCSUrbanEmissions,0,Timothy-W-Hilton,https://github.com/Timothy-W-Hilton/OCSUrbanEmissions,DEV,Collaboration with Gara Villalba M√©ndez at Universitat Aut√≤noma de Barcelona,0,2018-01-31 20:57:44+00:00,2018-01-31 20:58:33+00:00,2018-02-02 22:24:39+00:00,,228,0,0,Fortran,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,[],,"# OCSUrbanEmissions #

This repository contains code developed for the urban carbon dioxide flux experiments being run by [Gara Villalba M√©ndez](http://ictaweb.uab.cat/personal_detail.php?id=8) at [Universitat Aut√≤noma de Barcelona](http://www.uab.cat).  Code developed by [Timothy W. Hilton](https://www.researchgate.net/profile/Timothy_Hilton), [University of California, Santa Cruz](https://www.ucsc.edu).

### Summary ###

Broadly, the code here performs four tasks, all of which either pre- or post-process data for/from the [Sulfur Transport and dEposition Model (STEM)](https://github.com/Timothy-W-Hilton/STEM).  STEM source code is not included here, but is [also on Github](https://github.com/Timothy-W-Hilton/STEM).  The STEM code that is edited to compile with [gfortran](https://gcc.gnu.org/fortran/) at [PIC](https://www.pic.es) is in the [branch GNU_compilers](https://github.com/Timothy-W-Hilton/STEM/tree/GNU_compilers).

The code is divided into four subdirectories: LivermorePlotting, make_GRIDDESC, Preprocessor, and SiB_Regrid.  Their contents are described below.  Each subdirectory contains a readme file with further information.

#### STEM pre-processing tasks performed ####
1) build a [EDSS/Models-3 I/O API](https://www.cmascenter.org/ioapi/documentation/all_versions/html/index.html) GRIDDESC file for a 1-km resolution domain around Barcelona, Spain from WRF output.  See subdirectory make_GRIDDESC/.

2) build input files for STEM from WRF output.  This is known as the STEM preprocessor.  The preprocessor creates STEM input files meteo3d (3-D WRF meteorology), meteo2d (2-D WRF meteorology), wrfheight (WRF eta levels), and topo (latitude, longitude, and topography for the STEM grid).  See subdirectory Preprocessor/.

3) Interpolate global [SiB mechanistic OCS fluxes](http://dx.doi.org/10.1002/jgrg.20068) to the 1-km Barcelona STEM grid using the [EDSS/Models-3 I/O API](https://www.cmascenter.org/ioapi/documentation/all_versions/html/index.html). See subdirectory SiB_Regrid.

#### STEM post-processing tasks performed ####

1) plots STEM-simulated carbonyl sulfide (OCS) concentrations on a map for a set of STEM simulations in the San Francisco Bay Area, USA.  An example plot:
![STEM-simulated OCS concentrations](./livermore_map030.png ""STEM-simulated OCS concentrations"").
See subdirectory LivermorePlotting/.
",,DEV,0.83,,,,,,0,1,0.8,DEV
273850110,MDEwOlJlcG9zaXRvcnkyNzM4NTAxMTA=,paths_too_long,aa-dank/paths_too_long,0,aa-dank,https://github.com/aa-dank/paths_too_long,DEV,"This toolset was built to fix filepaths that have more characters than the 260 character limit of the Windows documents server at the University of California, Santa Cruz. It consists of three scripts: one for scraping the server for file paths that are longer than the character limit. The second script launches a gui for collecting the desired corrections of the file paths from the user. And the third script implements the path corrections from the second script.",0,2020-06-21 06:44:17+00:00,2021-12-08 01:36:09+00:00,2021-12-08 01:36:06+00:00,,744,0,0,Python,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,master,1,['aa-dank'],,"# paths_too_long

This a toolset was built to fix filepaths that have more characters than
the 260 character limit of the Windows documents server at the
university of California, Santa Cruz. It consists of three scripts: one
for scraping the server for file paths that are longer than the
character limit. The second script launches a gui for collecting the
desired corrections of the file paths from the user. And the third
script implements the path corrections from the second script. Here is a
more detailed breakdown of the three scripts: <br/> <br/>

###### paths_too_long.py

paths_too_long.py is a relatively simple script that prompt the user for
a path in the console and iterates through the directories within
populating a dataframe with the attributes of the files. It then removes
all the files that do not have a path length more than 260 characters.
THe resulting dataframe is saved into a csv file. <br/> <br/>

###### fix_paths2long.py

fix_paths2long.py launches a tkinter GUI using spreadsheet generated by
paths_too_long.py to elicit changes to directory names in the path from
the user that reduce the path length. It starts by eliciting the name of
the csv file. Note that the file should be in the same directory as the
fix_paths2long.py file. From the csv file a dataframe is generated and
iterated through each row, creating a tkinter frame for each path, the
ChooseSegmentFrame. An image of this frame can be found in image
'fix_paths2long-ChooseSegmentFrame-example.jpg'. This gui frame includes
four rows:
- Row 1 is just the entire path (the app window can become quite wide).
- Row 2 is a set of buttons that correspond to each directory in the
  path. Pushing one of these buttons selects the directory name to be
  change in the second tkinter frame.
- Row 3 is the number of other file paths in the dataframe that share
  the path up each directory in the path. This allows the user to know
  how many filepaths would be effected by changing the name of each
  directory in the path, facilitating batch changing of the filepath.
  This batch changing mechanism is the critical time-saving benefit of
  using these scripts.
- Row 4 has the length of the longest filepath from the dataframe that
  includes the corresponding directory. Note that the length of the
  current path (in row 1) is given at the last entry in this row. This
  row allows the user to know how many characters need to be removed
  from each directory in the filepath to meet the 260 character windows
  path limit.

When the button corresponding to a directory is selected from row 2 in
the ChooseSegmentFrame it is used to generate the next frame, called the
FixSegmentFrame. An image of this frame can be seen in
fix_paths2long-FixSegmentFrame-example.jpg. It includes the directory
name, a text entry box pre-populated with the directory name, an 'enter'
button, and a 'write changes to csv' button. The user should make the
desired changes to the directory name in the the text field and press
the 'enter' button which saves the name change into a modified path for
all the affected file paths in the dataframe. THe 'write changes to csv'
button on the FixSegmentFrame frame saves the dataframe with the
corrected paths to the csv file, effectively saving the users progress.
Note that the new shoretened filepath are now stored in an additional
row within the csv file. <br/> <br/>

###### apply_file2long_fixes.py

The purpose of this script is to realize the directory and file name
changes recorded in the fix_paths2long.py script. It starts by eliciting
the name of the csv file generated by paths_too_long.py with correction
by fix_paths2long.py. The csv file should be in the same directory as
the script. The script then goes through the dataframe generated from
the csv file, adding an additional attribute, 'depth', which is the
number directories between the directory that was changed in
fix_paths2long.py and the root. Then starting with file paths that
require the deepest directory or file name changes, the script iterates
through the dataframe making the desired change and recording the
changed path for all the affected rows of the dataframe in a column
labeled 'intermediary_paths'. The latest filepath change is recorded in
a 'result_path' column and if any errors are encountered when trying to
locate or change a path, those are recorded in an 'error' column. The
result is a dataframe where a list of all the filepath changes are
recorded into the intermediary_paths column, the actually resuling
location of the file is recorded in the result_path column. That
dataframe is then saved into the csv file.

",,DEV,0.68,,,,,,0,1,0.75,DEV
533455416,R_kgDOH8viOA,openram_testchip2,VLSIDA/openram_testchip2,0,VLSIDA,https://github.com/VLSIDA/openram_testchip2,DEV,,0,2022-09-06 18:33:11+00:00,2025-02-08 09:35:37+00:00,2023-09-12 07:15:39+00:00,,785483,3,3,Verilog,1,1,1,1,0,0,0,0,0,0,apache-2.0,1,0,0,public,0,0,3,main,1,"['jeffdi', 'Manarabdelaty', 'marwaneltoukhy', 'hadirkhan10', 'kareefardi', 'a-omla', 'passant5', 'mkkassem', 'RTimothyEdwards', 'mguthaus', 'mo-hosni', 'jcirimel', 'russellfriesenhahn', 'ax3ghazy', 'proppy', 'agorararmard', 'M0stafaRady', 'mattvenn', 'donn', 'Patarimi', 'erendn', 'shalan', 'rb-efabless', 'm-usama-z']",1,"# Caravel User Project

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![UPRJ_CI](https://github.com/efabless/caravel_project_example/actions/workflows/user_project_ci.yml/badge.svg)](https://github.com/efabless/caravel_project_example/actions/workflows/user_project_ci.yml) [![Caravel Build](https://github.com/efabless/caravel_project_example/actions/workflows/caravel_build.yml/badge.svg)](https://github.com/efabless/caravel_project_example/actions/workflows/caravel_build.yml)

| :exclamation: Important Note            |
|-----------------------------------------|

# Overview

This project contains a test chip for several OpenRAM memory configurations. The
configurations have varying levels of verification. In particular, it has these sizes:
* sky130_sram_1kbyte_1rw1r_8x1024_8 SRAM0
* sky130_sram_1kbyte_1rw1r_32x256_8 SRAM1
* sram_2kbyte_32b_2bank SRAM2 (2 x sky130_sram_1kbyte_1rw1r_32x256_8)
* sky130_sram_2kbyte_1rw1r_32x512_8 SRAM3
* sky130_sram_4kbyte_1rw1r_32x1024_8 SRAM4
* sky130_sram_2kbyte_1rw1r_32x512_8 SRAM5
* sky130_sram_4kbyte_1rw1r_32x1024_8 SRAM6
* sky130_sram_1kbyte_1rw_32x256_8 SRAM8
* sky130_sram_2kbyte_1rw_32x512_8 SRAM9
* sky130_sram_2kbyte_1rw_32x512_8 SRAM10

# Test Modes

There are three test modes available. Each one inputs a packet that
configures the read and write operations of a particular SRAM. The
io_in[1] and io_in[0] determines the clock the design runs on.
```
{io_in[1], io_in[0]}
2'b00 : clock is provided through LA (la test mode)
2'b01 : clock is provided through io_in[11] (gpio test mode)
2'b10 : clock is provided through wb_clk_i (wishbone test mode)
```

## Test Packet

The test packet is a 112-bit value that has the follow signals and bit size:
* chip_select (4)
* addr0 (16)
* din0 (32)
* csb0 (1)
* web0 (1)
* wmask0 (1)
* addr1 (16)
* din1 (32)
* csb1 (1)
* web1 (1)
* wmask1 (4)

During a read operation, the din bits are replaced with the data
output bits so that they can be verified.

Note: The 64-bit memory leaves the middle 32-bits as a value of 0 and
instead reads/writes the upper and lower 16-bits to reduce the number
of packet bits.

## GPIO Mode

In GPIO mode, the test packet is scanned in/out with the GPIO pins in 112 cycles. The
GPIO pins used are as follows:
* Mode select: `io_in[1:0] = 2'b01`
* Scan reset: `io_in[2]`
* Scan clock: `io_in[3]`
* Scan enable: `io_in[4]`
* Load SRAM result into register: `io_in[5]`
* CSB for all SRAM: `io_in[6]`
* Scan input: `io_in[7]`
* Scan output: `io_out[8]`


## LA Mode

In LA mode, the test packet is directly written from the output of the 128-bit LA.
* Mode select: `io_in[1:0] = 2'b00`
* Control register clock: `la_data_in[127]`
* Load control register: `la_data_in[125]`
* Load SRAM result into register: `la_data_in[124]`
* CSB for all SRAM: `la_data_in[123]`

## Wishbone Mode 

The wishbone mode tests the memories through the wishbone interface. The interface is used to provide data packet to the memories based on the address map of each memory.
* Mode select: `io_in[1:0] = 2'b10`
* CSB for all SRAM:  `wbs_cyc_i, wbs_stb_i and wbs_adr_i`
* Data for all SRAM: `wbs_dat_i`
* Write enable for all SRAM: `wbs_we_i`


# Authors
Muhammad Hadir Khan <mkhan33@ucsc.edu>
Jesse Cirimeli-Low <jcirimel@ucsc.edu>
Amogh Lonkar <alonkar@ucsc.edu>
Bugra Onal <bonal@ucsc.edu>
Samuel Crow <sacrow@ucsc.edu>
Matthew Guthaus <mrg@ucsc.edu>
",,DEV,0.93,,,,,,0,3,1,DEV
410093660,R_kgDOGHGIXA,Main,kanar1234/Main,0,kanar1234,https://github.com/kanar1234/Main,OTHER,All projects/programming problems,0,2021-09-24 20:29:03+00:00,2024-11-16 07:31:56+00:00,2024-11-16 07:31:53+00:00,,784,0,0,C,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['kanar1234'],,"Welcome to my GitHub repository! This repository, maintained by Alan Szeto, contains a collection of my projects developed during my time at the University of California, Santa Cruz (UCSC), as well as various practice problems I have tackled to keep my coding skills sharp.

Contents

UCSC Projects

Here you will find a series of projects that I completed as part of my coursework at UCSC. These projects cover a range of topics and programming languages, showcasing my learning journey and development as a programmer.

Practice Problems

In this section, I have included various coding challenges and problems that I solved to maintain and enhance my problem-solving abilities. These problems span different difficulty levels and come from various sources.

How to Use This Repository

Explore the Projects: Each project folder contains a README file that provides an overview, installation instructions, and usage details. Feel free to browse through them to understand the context and implementation of each project.

Review Practice Problems: The practice problems are organized by topic or source. Each problem folder includes the problem statement and my solution.

Technologies Used
Programming Languages: Python, Java, C++, JavaScript
",,EDU,0.8,,,,,,0,1,0.7,EDU
584057166,R_kgDOItABTg,CSE-20,shwetaJones/CSE-20,0,shwetaJones,https://github.com/shwetaJones/CSE-20,EDU,Beginning Programming in Python,0,2023-01-01 06:03:18+00:00,2023-01-01 06:04:16+00:00,2023-01-01 06:12:27+00:00,,25,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['shwetaJones'],,"# CSE-20

## Beginning Programming in Python 

### University: University of California, Santa Cruz

### Professor: Professor Larissa A. Munishkina

Assignment 1: Hello

    ""hello.py"": Prints out a message with specific information provided by the user
    
Assignment 2: Guess

    ""guess.py"": Interactive game that asks user to guess a number between 1 to 10
    
Assignment 3: Calculator 

    ""calculator.py"": Performs a certain operation based on user specifications

Assignment 4: Hangman

    ""hangman.py"": Mimics the game Hangman
    ""dictionary.txt"": File containing all the words the game can use
    
    
Assignment 5: TicTacToe

    ""tictac.py"": Mimics the game Tic Tac Toe, this file is the main file 
    ""player.py"": Acts as the player and asks the user for input and updated the board accordingly 
    ""board.py"": Changes and displays the values on the board
",,EDU,0.67,,,,,,0,1,0.9,EDU
38276473,MDEwOlJlcG9zaXRvcnkzODI3NjQ3Mw==,clustered_SNe,egentry/clustered_SNe,0,egentry,https://github.com/egentry/clustered_SNe,DEV,"Riemann solver with cooling, for the Sedov and cooling phases of a supernova remnant",0,2015-06-29 23:26:57+00:00,2022-07-26 17:35:16+00:00,2019-07-22 01:49:03+00:00,,15640,4,4,Jupyter Notebook,1,1,1,1,0,0,2,0,0,2,gpl-3.0,1,0,0,public,2,2,4,master,1,"['egentry', 'chomps', 'duffell']",,"# Clustered SNe
Riemann solver with cooling, for evolving a superbubble produced by 1-1000 SNe from a single cluster
-------

Author: Eric Gentry   (gentry.e@gmail.com; egentry@ucsc.edu)   

Licensed under the GPLv3.

-------

## Main Objectives
  - Evolve a spherical symmetric blast wave, incorporating hydrodynamics and radiative cooling
    - Achieve accurate and stable results through Sedov and (thin shell) radiative phases
  - Incorporate multiple supernovae, with realistic delay times and energies
    - Include pre-SNe winds as a constant wind through a star's life
  - Measure the energy and momentum injected into the surrounding medium, to be used as feedback prescriptions in low resolution galaxy/cosmology simulations


## Getting started
- Install dependencies listed below
- Adapt `src/makefile` to reflect location of required libraries (`INC_*` and `LIB_*` variables)
- In the `src` directory, run `make all install clean`

That should get you a working executable. Once you have data to process, this repo's wiki has [instructions](https://github.com/egentry/clustered_SNe/wiki/Getting-Starting-with-the-Analysis-Package) for using the python analysis code.


## Requires
  - c++ compiler (assumes clang for OS X, gcc for Linux)
    - should (must?) be c++11 capable
    - you must use the same compiler as you used for installing Boost. If you're unsure, then use the default I set.
  - [slug2](https://bitbucket.org/krumholz/slug2)
    - slug2 must be built as a shared library (change into the `src` directory and call `make lib`; if you enabled FITS at compile time, then keep it enabled at link time. For simplicity, use `make all && make lib` in the `src` directory.)
    - A fork frozen to the version used in my simulations can be found at: https://bitbucket.org/egentry/slug2
    - requires the [GSL](https://www.gnu.org/software/gsl/), [Boost](http://www.boost.org/)
  - libuuid
  - [grackle cooling](https://bitbucket.org/grackle/grackle) (v3)
    - requires [HDF5](https://www.hdfgroup.org/HDF5/release/obtain5.html)
  - For visualization:
    - Python (tested for v3.5, mostly backwards compatible)
    - Jupyter notebook (tested for v4)
      - ipywidgets (tested for v4, v5 -- mildly broken on v6)
    - Matplotlib
    - Bokeh
    - Numpy
    - Scipy
    - Astropy
    - Pandas
    - Seaborn
    - Numba
    - SQLAlchemy
    - corner
      - Only needed if you call `BayesianFit.create_corner_plots`


## Acknowledgements
This project built upon [RT1D](https://github.com/duffell/RT1D), an open-source riemann solver from [Paul Duffell](http://duffell.org/).

This project was also includes the `sedov3.f` code of http://cococubed.asu.edu/research_pages/sedov.shtml in order to generate the analytic sedov solution for the purpose of visualization.  While it's not difficult to generate a basic sedov solution, it's difficult to do well. Using `sedov3.f` allows us to avoid worrying about the details of quad precision math ourselves.

Ben Wibking was a big help in proposing efficiency improvements and identifying bugs.
",,DEV,0.76,,,,,,0,1,0.8,DEV
34430748,MDEwOlJlcG9zaXRvcnkzNDQzMDc0OA==,CMPM146-P4,SamReha/CMPM146-P4,0,SamReha,https://github.com/SamReha/CMPM146-P4,EDU,This project implements basic unit behaviors and controls in the context of a toy RTS  in Python.,0,2015-04-23 03:10:35+00:00,2015-04-25 23:21:39+00:00,2015-04-25 23:21:39+00:00,,148,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['SamReha'],,"# CMPM146-P4
This project implements basic unit behaviors and controls in the context of a toy RTS  in Python.

This project is a solution to a homework assignment from CMPM 146: Game AI with Adam Smith (amsith@ucsc.edu). Copying materials from this repository without citation may be in violation of academic integrety policies.

# Team Members
* Samuel Reha (sreha@ucsc.edu)
* Dustin Pfeiffer
",,EDU,0.89,,,,,,0,2,0.9,EDU
204952126,MDEwOlJlcG9zaXRvcnkyMDQ5NTIxMjY=,vc_tutorials,nreid/vc_tutorials,0,nreid,https://github.com/nreid/vc_tutorials,EDU,,0,2019-08-28 14:31:18+00:00,2024-08-24 02:27:33+00:00,2019-09-26 17:29:18+00:00,,461,4,4,Shell,1,1,1,1,0,0,10,0,0,0,,1,0,0,public,10,0,4,master,1,['nreid'],,"# Variant discovery tutorials

This repository contains re-worked variant detection tutorials for UConn CBC workshop. 

## Introduction

This repository an introduction to the basics of variant calling from high-throughput, short-read sequencing data. While some limited conceptual ground will be covered in the tutorial, if you are working through it independently, it will be much more helpful if you understand the motivation for the steps in advance. A useful (if dated) review of the underlying concepts is [Nielsen et al. 2011](https://www.nature.com/articles/nrg2986) in Nature Reviews Genetics. 

Most steps have associated bash scripts tailored to the UConn CBC Xanadu cluster with appropriate headers for the [Slurm](https://slurm.schedmd.com/documentation.html) scheduler. These can be modified to run interactively or with another job scheduler.  

Commands should never be executed on the submit nodes of any HPC machine.  If working on the Xanadu cluster, you should submit each script to the scheduler as `sbatch scriptname.sh` after modifying it as appropriate.  

Basic editing of all scripts can be performed on the server with tools such as nano, vim, or emacs.  If you are new to Linux, please use [this](https://bioinformatics.uconn.edu/unix-basics) handy guide for the operating system commands.  In this tutorial, you will be working with common bioinformatic file formats, such as [FASTA](https://en.wikipedia.org/wiki/FASTA_format), [FASTQ](https://en.wikipedia.org/wiki/FASTQ_format), [SAM/BAM](https://en.wikipedia.org/wiki/SAM_(file_format)), [GFF3/GTF](https://en.wikipedia.org/wiki/General_feature_format) and [VCF](https://en.wikipedia.org/wiki/Variant_Call_Format). You can learn even more about each file format [here](https://bioinformatics.uconn.edu/resources-and-events/tutorials/file-formats-tutorial/). If you do not have a Xanadu account and are an affiliate of UConn/UCHC, you can get one **[here](https://bioinformatics.uconn.edu/contact-us/)**.   

__Structure:__

1. [ Stepwise QC, alignment, post-alignment processing ](/Part1_qc_alignment.md)

2. [ Variant Calling: bcftools ](/Part2_bcftools.md)

3. [ Part 1, but a piped example ](Part3_pipedalignment.md)

4. 
	a. [ Variant calling: Freebayes ](Part4a_freebayes.md)

	b. [ Variant calling: GATK, joint calling using gvcf ](Part4b_gatk.md)

	c. Beyond variant calling: genotype likelihoods. 

5. [ Filtering and comparing variant sets ](Part5_filtering_comparing.md) 

6. [ Variant annotation ](Part6_annotation.md)

__Proposed data:__

[NIST Genome in a Bottle](https://www.nist.gov/programs-projects/genome-bottle) chinese trio. 
Region chr20:29400000-34400000

This is an arbitrary 5mb region of the genome. It is:
- near the centromere. 
- has a couple regions with mapping problems. 
- takes only a few minutes to align. 
- 50-100x whole genome shotgun sequencing for each of 3 individuals. 
	- 2x250bp paired-end reads for the son
	- 2x150bp paired-end for the parents

For a reference, we'll use GRCh38. The specific version we'll use is [recommended by Heng Li, author of bwa](https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use) for variant calling. 

_Source:_
- [GiaB](https://www.nist.gov/programs-projects/genome-bottle)
- ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/
- [Heng Li's recommendation](https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use)
- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

__Required software tools:__

_quality control_  
- [ FastQC ](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)
- [ sickle ](https://github.com/najoshi/sickle)  

_alignment_  
- [ bwa ](http://bio-bwa.sourceforge.net/)

_sequence alignment manipulation and visualization_ 
- [ samtools ](http://www.htslib.org/doc/samtools.html)
- [ picard ](https://broadinstitute.github.io/picard/)
- [ samblaster ](https://github.com/GregoryFaust/samblaster)
- [ bamtools ](https://github.com/pezmaster31/bamtools)  
- [ igv ](https://software.broadinstitute.org/software/igv/)

_variant calling_  
- [ bcftools ](http://www.htslib.org/doc/bcftools.html)
- [ freebayes ](https://github.com/ekg/freebayes)
- [ GATK ](https://software.broadinstitute.org/gatk/)  

_other utilities_  
- [ bgzip ](http://www.htslib.org/doc/bgzip.html)
- [ tabix ](http://www.htslib.org/doc/tabix.html)
- [ bedtools ](https://bedtools.readthedocs.io/en/latest/)
",,EDU,0.63,,,,,,0,1,0.9,EDU
130754579,MDEwOlJlcG9zaXRvcnkxMzA3NTQ1Nzk=,livehd,masc-ucsc/livehd,0,masc-ucsc,https://github.com/masc-ucsc/livehd,DEV,"Live Hardware Development (LiveHD), a productive infrastructure for Synthesis and Simulation",0,2018-04-23 20:44:49+00:00,2025-03-03 23:52:40+00:00,2025-03-02 18:11:52+00:00,,119558,217,217,FIRRTL,1,1,1,1,0,1,49,0,0,11,other,1,0,0,public,49,11,217,master,1,"['renau', 'swang203', 'sakshi15108', 'wait-how', 'rafaeltp', 'HCoffman', 'jesec', 'renovate[bot]', 'qchen63', 'sloanyliu', 'maximiliantiao', 'joshuapena', 'rohan-ganpati', 'jsg831', 'rabieifk', 'alidezhihui', 'arikyueh', 'birdeclipse', 'crhilber', 'markzakharov', 'bhawandeepsingh', 'zacharypotter', 'tnetuser', 'renaidisco', 'mgkapp', 'clrighthand0', 'bokket', 'kabylkas', 'shahzaibk23', 'mjao1', 'olyad', 'okbonhahaha', 'qabylqas', 'dabader', '5surim', 'akashsridhar', 'acardara', 'milad621', 'IamtheMZI', 'stnichol', 'mithro', 'azure-pipelines[bot]', 'deny72', 'mantri03']",1,"
![LiveHD](https://masc.soe.ucsc.edu/logos/livehd5.png)

# LiveHD: Live Hardware Development

## Summary

[![CodeFactor](https://www.codefactor.io/repository/github/masc-ucsc/livehd/badge)](https://www.codefactor.io/repository/github/masc-ucsc/livehd)
[![codecov](https://codecov.io/gh/masc-ucsc/livehd/branch/master/graph/badge.svg)](https://codecov.io/gh/masc-ucsc/livehd)
[![CI](https://github.com/masc-ucsc/livehd/actions/workflows/ubuntu.yml/badge.svg)](https://github.com/masc-ucsc/livehd/actions/workflows/ubuntu.yml)


LiveHD is a ""compiler"" infrastructure for hardware design optimized for
synthesis and simulation. The goals is to enable a more productive flow where
the ASIC/FPGA designer can work with multiple hardware description languages
like CHISEL, Pyrope, or Verilog.


## Goal

LiveHD: a fast and friendly hardware development flow that you can trust

To be ""Fast"", LiveHD aims to be parallel, scalable, and incremental/live flow.

To be ""friendly"", LiveHD aims to build new models to have good error reporting.

To ""trust"", LiveHD has CI and many random tests with logic equivalence tests (LEC).

> :warning: LiveHD is beta under active development and we keep improving the
> API. Semantic versioning is a 0.+, significant API changes are expect.


## LiveHD Framework


LiveHD stands for Live Hardware Development. By live, we mean that small
changes in the design should have the synthesis and simulation results in a few
seconds.

As the goal of ""seconds,"" we do not need to perform too fine grain incremental
work. Notice that this is a different goal from having an typical incremental
synthesis, where many edges are added and removed in the order of thousands of
nodes/edges.

LiveHD is optimized for synthesis and simulation. The main components of LiveHD
includes LGraph, LNAST, integrated 3rd-party tools, code generation, and ""live""
techniques. The core of LiveHD is a graph structure called LGraph (Live Graph).
LGraph is built for fast synthesis and simulation, and interfaces other tools
like Yosys, ABC, OpenTimer, and Mockturtle. LNAST stands for language neutral
AST, which is a high-level IR on both front/back-end of LGraph. LNAST helps to
bridge different HDLs and HLS into LiveHD and is useful for HDLs/C++ code
generation.

![LiveHD overall flow](./docs/livehd.svg)

## Contribute to LiveHD

Contributors are welcome to the LiveHD project. This project is led by the
[MASC group](https://masc.soe.ucsc.edu) from UCSC.

There is a list of available [projects.md](docs/projects.md) to further improve
LiveHD. If you want to contribute or seek for MS/undergraduate thesis projects,
please contact renau@ucsc.edu to query about them.


You can also
[donate](https://secure.ucsc.edu/s/1069/bp18/interior.aspx?sid=1069&gid=1001&pgid=780&cid=1749&dids=1053)
to the LiveHD project. The funds will be used to provide food for meetings,
equipment, and support to students/faculty at UCSC working on this project.


The instructions for installation and internal LiveHD passes can be found at
[Documentation](https://masc-ucsc.github.io/docs/livehd/00-intro/)


If you are not one of the code owners, you need to create a pull request as
indicated in [CONTRIBUTING.md](docs/CONTRIBUTING.md).


# Publications
For more detailed information and paper reference, please refer to
the following publications. If you are doing research or projects corresponding
to LiveHD, please send us a notification, we are glad to add your paper.

1. [A Multi-threaded Fast Hardware Compiler for HDLs](docs/papers/livehd_cc23.pdf), Sheng-Hong Wang, Hunter Coffman, Kenneth Mayer, Sakshi Garg, and Jose Renau. International Conference on Compiler Construction (CC), February 2023.

2. [LiveHD: A Productive Live Hardware Development Flow](docs/papers/LiveHD_IEEE_Micro20.pdf), Sheng-Hong Wang, Rafael T. Possignolo, Haven Skinner, and Jose Renau, IEEE Micro Special Issue on Agile and Open-Source Hardware, July/August 2020.

3. [LiveSim: A Fast Hot Reload Simulator for HDLs](docs/papers/LiveSim_ISPASS20.pdf), Haven Skinner, Rafael T. Possignolo, Sheng-Hong Wang, and Jose Renau, International Symposium on Performance Analysis of Systems and Software (ISPASS), April 2020. **(Best Paper Nomination)**

4. [SMatch: Structural Matching for Fast Resynthesis in FPGAs](docs/papers/SMatch_DAC19.pdf), Rafael T.
   Possignolo and Jose Renau, Design¬†Automation Conference (DAC), June 2019.

5. [LiveSynth: Towards an Interactive Synthesis Flow](docs/papers/LiveSynth_DAC17.pdf), Rafael T. Possignolo, and
   Jose Renau, Design Automation Conference (DAC), June 2017.

## LGraph

6. [LGraph: A Unified Data Model and API for Productive Open-Source Hardware Design](docs/papers/LGraph_WOSET19.pdf),
   Sheng-Hong Wang, Rafael T. Possignolo, Qian Chen, Rohan Ganpati, and
   Jose Renau, Second Workshop on Open-Source EDA Technology (WOSET), November 2019.

7. [LGraph: A multi-language open-source database for VLSI](docs/papers/LGraph_WOSET18.pdf), Rafael T. Possignolo,
   Sheng-Hong Wang, Haven Skinner, and Jose Renau. First Workshop on Open-Source
   EDA Technology (WOSET), November 2018. **(Best Tool Nomination)**

## LNAST

8. [LNAST: A Language Neutral Intermediate Representation for Hardware
   Description Languages](docs/papers/LNAST_WOSET19.pdf), Sheng-Hong Wang,
   Akash Sridhar, and Jose Renau, Second Workshop on Open-Source EDA Technology
   (WOSET), 2019.


",,DEV,0.85,,,,Directory exists,,0,28,1,DEV
17152891,MDEwOlJlcG9zaXRvcnkxNzE1Mjg5MQ==,wcms-design-templates,ucsc/wcms-design-templates,0,ucsc,https://github.com/ucsc/wcms-design-templates,WEB,Styles and Javascript for official UC Santa Cruz web templates,0,2014-02-24 22:18:42+00:00,2024-09-17 02:43:52+00:00,2024-09-17 02:43:49+00:00,http://webassets.ucsc.edu,9643,4,4,XSLT,1,1,1,1,0,1,3,0,0,22,,1,0,0,public,3,22,4,master,1,"['knice', 'Herm71', 'LuckyLuke001', 'dependabot[bot]']",1,"# WCMS styles, files & templates

This repo contains the assets to manage the look and feel of the campus WCMS websites.

## Setting up for development

1. Fork and clone this repository locally
2. Install `node` and `npm` by downloading the installer from [nodejs.com](http://nodejs.org) or with Homebrew
3. Run `npm install` in the project root to install all node dependencies
4. Run `npm run preview`
   - This fetches the HTML content from [several live sites](./test/pages.js) and saves them locally for testing.
   - `img`, `link`, and `script` tags are updated to point to local resrouces.
5. Run `npm run dev`
   - This starts Laravel Mix and builds the Sass files into ucsc.css.
   - It copies the javascript and image files into their proper location in the `dist/` folder.
   - It runs a localhost server and opens the site in a browser window.

## Deployment

This repo is connected to Netlify. Each commit to the master branch (via **pull request**) will trigger a build and deploy to webassets.ucsc.edu. The WCMS sites will load the updated stylesheets from that site.

## In progress

- Testing:
  - [x] basic comparision
  - automated visual regression
- Bundling javascript dependencies
- SVG sprites for certain icons
- Design System documentation
- Generate cache-busting code for WCMS",,WEB,0.96,,,,,,0,14,1,WEB
729045264,R_kgDOK3RZEA,bimzty.github.io,bimzty/bimzty.github.io,0,bimzty,https://github.com/bimzty/bimzty.github.io,OTHER,The projects that I have done during undergraduate,0,2023-12-08 09:29:51+00:00,2024-11-26 12:13:41+00:00,2024-03-13 10:25:47+00:00,https://bimzty.github.io/,50875,1,1,Jupyter Notebook,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,1,main,1,['bimzty'],,"This webpage stores the projects that I have done during my undergraduate
## About my self:
<p align=""center"">
  <img src=""https://github.com/bimzty/bimzty.github.io/blob/main/Photos/MY%20PHOTO%20II.png"" width=""400"" />
</p>
CV: https://maipdf.com/est/a14080205457@pdf <br>
Certification: https://maipdf.com/est/d17353768550@pdf<br>
My photos: https://maifile.cn/est/d65df34e842643/pdf <be>

## Research & Learning Underway: 
1. I am evolving in the research aimed at 'Addressing Class Imbalance Issues in Financial Anti-Fraud Detection with Data Preprocessing and Machine Learning'.<br>
Specifically, I am now comparing different data augmentation methods including GAN and SMOTE, and analyzing those algorithms in principle. <br>
TutorÔºöBoon Giin Lee (World top2% Scientist) https://research.nottingham.edu.cn/en/persons/boon-giin-lee<br>
This research is expected to publish a research article.

2. I am studying two certified online courses related to Data Science. They are Introduction to Databases (https://extension.berkeley.edu/search/publicCourseSearchDetails.do?method=load&courseId=40645) by UC Berkley and Java Programming I (https://www.ucsc-extension.edu/courses/java-programming-i/) by UC Santa Cruz. My ability will be officially tested through coursework and exams and certified by UCSC. Each of these courses has 3 units of credit in the US system.

***

## 1. Research for Spike protein on SARS-CoV-2 virus (07/2023-08/2023)

Position: Data Analyst Intern <br>
Location: Shenzhen Bay Laboratory. <br>
tutor: Chaowang https://www.szbl.ac.cn/en/scientificresearch/researchteam/3372.html

I was mainly involved in identifying mutational hotspots and capturing the mutation distribution using the Gaussian process on the SARS-CoV-2 spike protein.

### Background & Explain
Mutational hotspots are the places on the protein where the mutation frequency is higher than in other places, they are the main consideration for designing experiments and vaccines. Furthermore, we would like to capture the regression patterns for this protein, for potential prediction tasks in future research.

### 1.1 Applying a weighted average proximity scoring function for identifying hotspots
Check the 'Identifying hotspots using the WAP method' Rmd file. The idea for the method comes from 'A comprehensive assessment of cancer missense mutation clustering in protein structures' by Atanas Kamburov et, al. The WAP algorithm from this article is first used for identifying clusters that are significant for mutation. I further improve the methods by adding an adaptive step for finding the optimal size of each cluster before the original process.

<p align=""center"">
  <img src=""https://github.com/bimzty/bimzty.github.io/blob/main/Photos/WAP.png "" width=""400"" />
</p>
<p align=""center"">
  <em>weighted average proximity function, consider mutation frequency between a pair of residues (N<sub>q</sub> and N<sub>r</sub>) and distance between residues (the exponential term)</em>
</p>

### 1.2 Applying K-means or DBSCAN method for identifying hotspots (collaborated with a colleague)
Check the 'Identifying hotspots using Clustering method' R file for finding hotspots. Firstly, this method performs Data Preprocessing to transform the spatial position('X',' Y',' Z') of residuals and VirusPercentage to the same scale<br> Secondly, this method examines the data distribution invariant before and after the process. <br>Thirdly, the method uses K-means to cluster different residues. <br>Fourthly, it performs the Permutation methods to examine the significance of mutation frequency for different clusters. <br>Various hypothesis tests have been conducted in this step. We finally utilize T-SNE for dimensionality reduction and visualization. 

<p align=""center"">
  <img src=""https://github.com/bimzty/bimzty.github.io/blob/main/Photos/Hotspots.png "" width=""400"" />
</p>
<p align=""center"">
  <em>The hotspots in the spike protein</em>
</p>

### 1.3 Applying the Gaussian process for capturing the mutational distribution of the protein.
Check the 'Gaussian process for mutation distribution'. Data processing here includes Log transformation and Box-Cox transformation for 'Mutation Number'. I first implemented the basic GP method with the kernel function using Matern, a generalization of RBF, and using Random search for the parameters in the kernel function. This basic model gives a tragic result. 

I thus improved the model by following methods:<br>
#### 1.3.1. Feature Engineering: 
I extracted more features from the original spatial positions ('X',' Y',' Z') and selected them. <br>Feature extraction: from ('X',' Y',' Z') to (x',' y', 'z', 'distance_to_center', 'sum_xyz', 'x^2', 'y^2', 'z^2', 'xy', 'xz', 'yz').<br> Feature attribution: SHAP (Shapley Additive explanations) and Permutation Importance measures are implemented.<br> Feature selection: Correlation-based method and SelectKBest Feature Selection were applied. I finally chose  ‚Äòx‚Äô,‚Äô y‚Äô, 'x^2',‚Äô xy,‚Äôxz‚Äô, and ‚Äò‚Äòdistance to the center‚Äô as my final features.<br>


   
#### 1.3.2. The Bayesian optimization:
The Bayesian optimization method was also learned and replaced with the Random search method. The Bayesian process for Gaussian Process kernel selection involves iteratively evaluating and updating kernel configurations based on prior beliefs and observed data. It efficiently explores the search space, exploits prior knowledge, and provides uncertainty estimates. This is better than random search because it intelligently guides the search towards promising regions, utilizes past information, builds a surrogate model, and converges to better solutions faster.
 
#### 1.3.3. Data Augmentation: 
As generally the residues with high mutation numbers are in the minority, there is an obvious imbalanced distribution of mutations in the dataset. I applied SMOTE (Synthetic Minority Over-sampling Technique) to solve the problem. The detailed program is at 'SMOTE_resample.py', the program could create synthetic samples that lie on the line segments between existing minority class samples, and help to increase the representation of the minority class and reduce the imbalance in the dataset. I utilized the learning curves to show this approach can effectively improve the result.


<p align=""center"">
  <img src=""https://github.com/bimzty/bimzty.github.io/blob/main/Photos/GP%20without%20Augmentation.png"" width=""400"" />
  <img src=""https://github.com/bimzty/bimzty.github.io/blob/main/Photos/GP%20woth%20Augmentation.png"" width=""400"" />
</p>
<p align=""center"">
  <em>The difference between regression value and real value when without augmentation (left) and with augmentation</em>
</p>

#### 1.3.4 The original model has an overfitting problem. 
Best Parameterset for Regularization have been searched and applied. I have approached this by two methods: <br>
1. Selecting a simpler kernel function or reducing the number of hyperparameters <br>
2. Use Bayesian inference to estimate the posterior distribution over the model parameters.
  
***

## 2.Machine Learning Research: Classification task for Freddie Mac loan dataset and Historical Stock Market dataset(05/2022-08/2022, 09/2023)
Location: University of Nottingham, Ningbo, china.<br>
Tutor: Saeid Pourroostaei Ardakani  https://scholar.google.com/citations?user=3OeHr8gAAAAJ

I was mainly involved in delivering literature research related to Federated learning, implemented Feature Engineering, and built and compared multiple models in both tasks.

### 2.1 Classification task for Freddie Mac loan dataset 
This research is conducted in a team, and I only demonstrate the program I wrote. Check 2.1 ‚ÄôReport of Result‚Äô for several results I conducted utilizing models built by myself or other team members.

### 2.2 Historical Stock Market Dataset
Check ""2.2 ML research for Predicting Stock Market.py"". <br>Data preprocessing: Transformation, cross-sectional standardization <br> Feature Engineering: Feature Extraction: from a financial perspective (Bollinger Bands and waveform-based methods) <br> Feature AttributionÔºö Saliency, Integrated Gradients, and Shapely Value Sampling Methods <br> Feature Selection: Static and dynamic Feature Selection Self-Attention Mechanism<br> Model: LSTM and GRU; Also tried: Linear Regression; Decision Tree; Random Forest; <br> Explore: 1. Add Self-Attention Mechanism in LSTM and GRU model to improve long term memory; <br> 2. Visualize features from Neural Networks. (using TSNE)

<p align=""center"">
  <img src=""https://github.com/bimzty/bimzty.github.io/blob/main/Photos/LSTM%20for%20predicting%20stock%20price.png "" width=""400"" />
</p>
<p align=""center"">
  <em>Price Prediction using LSTM model</em>
</p>

Check 'study attention mechanism from Google article' for relevant code I produced when learning the article 'Attention Is All You Need' by Ashish Vaswani et, al.

This year, out of interest in Time Series and Quantitive Finance, I have systematically written a more systematic and true-to-life project for this task, which you can see in the folder 'A systematic approach for Quantitive Trading' (collaborated). Instead of using a neural network, I emphasized the Support Vector Machine this time after being immersed in three related articles I included in that folder. I also utilized MovingAverageCrossStrategy (in '4_mac') for conducting strategy with events in the market being considered.

***

## 3. Internship: Mathematical modeling intern at ZHONGCE RUBBER GROUP CO., LTD.(06/2022-07/2022)

Worked as a Mathematical Modelling intern
Design a program for the Hans B. Pacejka empirical tire model based on experimental data. (collaborated with colleague)

Hans B. Pacejka model: https://en.wikipedia.org/wiki/Hans_B._Pacejka
 <br>
The program code: 1. calculate the parameters for the model<br>
2. Draw a series of analytical graphs

<p align=""center"">
  <img src=""https://github.com/bimzty/bimzty.github.io/blob/main/Photos/Tire%20Dynamic%20Analysis%20Lateral%20force%20versus%20slip%20angle..png"" width=""300"" />
</p>
<p align=""center"">
 <em>Tire Dynamic AnalysisÔºöTire lateral force versus slip angle</em>
</p>

You can check the 'H.B.Pacejka model_Sample_GUI' MATLAB Figure program alongside the MATLAB GUI of the program. 
***
## 4. Competition: Formula Student Electric, won National Third Prize (12/2020-07/2022)
Responsibility: Data Analyst and Simulation Technician<br>
The 'Track Simulation' folder consists of Python codes (collaborated with a colleague) mainly developed to stimulate racecar performance on Track. It consists of functions for: <br>
Straight road, when the car accelerates only;<br>
Corner, when the car runs the maximum speed under the condition;<br>
Brake, when the car decelerates only;

<p align=""center"">
  <img src=""https://github.com/bimzty/bimzty.github.io/blob/main/Photos/Track%20Simulations.png"" width=""300"" />
</p>
<p align=""center"">
 <em>Car performance simulation on one track</em>
</p>

***

## 5. Coursework during Undergraduate 
### 5.1 Coursework for the module Machine Learning (12/2023)
The coursework aims to make use of the machine learning techniques learned in this course to diagnose breast cancer using the Wisconsin Diagnostic Breast Cancer (WDBC) dataset. Based on the recommended model and parameters of a similar competition held by Kaggle, I mainly built 7 models and conducted a systematic approach to choosing the best one from them. 

### 5.2 Coursework for the module Introduction to Scientific Computation (09/2022-05/2023)
This course aims to introduce the concept of numerical approximation to problems that cannot be solved analytically and to develop skills in Python by implementing numerical methods. Topics included in those works are: Solving nonlinear equations (approximately) using root finding methods and analyzing their convergence; Solving linear systems of equations using direct methods and iterative techniques, including Gaussian elimination and Jacobi & Gauss-Seidel method; Approximating functions by polynomial interpolants (Lagrange polynomials), and analyzing their accuracy; Approximating derivatives and definite integrals using numerical differentiation and integration such as trapezoidal, Simpson & midpoint rule, and analyzing their convergence; Approximating ODEs using numerical methods including Euler‚Äôs method and s, higher-order RK methods.

### 5.3 Coursework for the module Statistical Models and Methods (05/2023)
The objective of the coursework is to build a predictive model for body fat content using 10 body measurement variables. we first do some exploratory analysis of the data. Secondly, we do model selection to find the best subset of variables for regression based on AIC/BIC, Mallow‚Äôs Cp, and Adjusted
R-squared criterion. Thirdly, we identify and analyze outliers and high-leverage points. Fourthly, we check the linear
model assumption by plotting the QQ-plot, residual, component residual plot, etc, and do the manipulation to a model
based on that. A comparison between our best model and the full model using test data will also be provided.

















",,OTHER,0.55,,,,,,0,1,0.7,OTHER
300552283,MDEwOlJlcG9zaXRvcnkzMDA1NTIyODM=,CPANG20,pangenome/CPANG20,0,pangenome,https://github.com/pangenome/CPANG20,EDU,computational pangenomics course 2020 edition,0,2020-10-02 08:36:14+00:00,2020-10-04 13:52:22+00:00,2020-10-04 13:52:20+00:00,,7,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,"['ekg', 'Flavia95']",1,"---
layout: post
title:  ""Pangenomics <font color='black'>[closed]</font>""
date_start:   2019-12-14
date_end:   2019-12-18
venue: virtual
description: The course will take the participants through all steps of a genome assembly and annotation project, starting with quality control, moving into genome assembly and assembly validation, and ending with structural and functional annotation. Both short and long read technologies will be discussed, as well as supporting technologies. Lectures are mixed with practicals, with a majority of the time spent doing hands-on analyses in the command terminal. Focus is on eukaryotes, although most methods will be applicable to any organism. One day is set aside to discuss the participants own projects, and work with that data, if available. 

---


<table border=""0"" align=""center"">
  <tr> 
    <td><a href=""http://elixir-italy.org""><img src=""../../../img/logo_iib.png"" height=""80""></a></td>
    <td width=""10""></td>
    </tr>
  </table>   
  <table border=""0"" align=""center"">
  <tr height=""50""></tr>
  <tr>  
  <td><a href=""https://www.cnr.it/en""><img src=""../../../img/Logo_CNR_Italy.png"" height=""80""></a></td>
  </tr>
</table>
<br>
<br>



### IMPORTANT DATES

- Deadline for applications: **30 October 2020**
- Chosen participants will be notified by: 6 November 2020
- Course date: 14-18 December 2020

### VENUE

This training will take place online. The instructors will provide you with the information you will need to connect to this event.

### FEE

The course does not include any fee.

### SELECTION

A maximum of **20** candidates will be selected based on their need for the course as emerging from the application form. Chosen participants will be notified by **6th November 2020**. Priority will be given to candidates from ELIXIR-IIB member institutes (see the list at the bottom) and ELIXIR Nodes.

Go to the *[Application Form - Registration]()*.
 
**Cancellation policy:** Attendance is limited to 20 participants.
We expect many more applications. Accepted participants commit to attend the course for its whole duration. Failure to attend training sessions is disruptive. Moreover, it blocks other candidates from participating. Therefore, a cancellation policy is in place so that only written requests presented 10 days in advance relatively to the course starting date are accepted.

### Instructors

- [**Erik Garrison**]() - University of California Santa Cruz 

### Helpers

- [**Flavia Villani**](https://github.com/Flavia95)-National Research Council, Institute of Genetics and Biophysics Adriano Buzzati-Traverso, Napoli,Italy;
- [**Simon Heumos**](https://github.com/subwaystation)-Quantitative Biology Center (QBiC), University of T√ºbingen,T√ºbingen, Germany;
- [**Andrea Guarracino**](https://github.com/AndreaGuarracino)-Centre for Molecular Bioinformatics, Department of Biology, University Of Rome Tor Vergata, Rome,Italy;

### Organizers

- **Vincenza Colonna** - ELIXIR-IT
- **Loredana Le Pera** - ELIXIR-IT (Training Coordinator Deputy) - CNR-IBPM/CNR-IBIOM
- **Allegra Via** - ELIXIR-IT (Training Coordinator) - CNR-IBPM

### Contact

For all kinds of queries about the course, please contact Erik Garrison at <erik.garrison@ucsc.edu>.

### Course Description

The course will take the participants through all steps of a pangenome construction and analysis process.
We will learn the basic principles and key algorithms of contemporary pangenomic methods based on pangenome graphs.
At the end of the course, participants will be able to design and complete their own pangenomic analyses.

Lectures will provide background for the work, but this is a practical course, with a majority of the time spent doing hands-on analyses on the command line.
To keep analysis times short and encourage interative exploration, we will tend to focus on organisms with smaller genomes, but the techniques we will use are scalable and applicable to any organism.
Participants are encouraged to bring their own data, and time will be allocated to discuss and test the application of pangenomic methods to these data.

### Target audience

This course is aimed at PhD students, postdocs or other researchers interested in learning about genome assembly and genome annotation.
It will be especially useful for researchers that have (or will have) data of their own and want run the assembly - and annotation - tools themselves.

### Learning outcomes

By the end of this course, learners will be independently able to:

- Understand how genome properties will influence a genome assembly and annotation project
- Set up a workflow for a genome assembly and annotation project
- Perform quality control of Illumina data and check data integrity
- Perform genome assembly using Illumina, PacBio, and Nanopore reads
- Compare assemblies and check them for mis-assemblies and contamination
- Perform basic annotation of transposable elements
- Perform structural annotation of prokaryote and eukaryote genomes
- Assign functional information to structurally annotated genes

### Course prerequisites

The participants will need to have some experience in Linux command line (bash), enough to start tools and find paths to files. Experience with Next Generation Sequencing data and common file formats (fasta, fastq, bam, etc.) will be very useful, but is not mandatory. 

### [Application Form - Registration]()



### Programme

<table border=""1"" width=""700"">
<tr>
   <td colspan=""4""><h3>Monday - 1 July</h3></td>
</tr>
<tr>
   <td height=""50"" width=""50"">09:00 - 10:00</td>
   <td height=""50"">Lecture</td>
   <td height=""50"">Henrik Lantz</td>
   <td height=""50"">Introduction and genome properties</td>
</tr>
<tr>
   <td height=""50"">10:00 - 11:00</td>
   <td height=""50"">Lecture</td>
   <td height=""50"">Christophe Klopp</td>
   <td height=""50"">Quality control of sequence data
	</td>
</tr>
<tr>
   <td height=""50"">11:00 - 12:00</td>
   <td height=""50"">Practical</td>
   <td height=""50"">Christophe Klopp</td>
   <td height=""50"">Quality control of sequence data
	</td>
</tr>
<tr>
   <td height=""50"">12:00 - 13:00</td>
   <td colspan=""3"" height=""50"">Lunch break</td>
</tr>
<tr>
   <td height=""50"">13:00 - 14:00</td>
   <td height=""50"">Practical</td>
   <td height=""50"">Christophe Klopp</td>
   <td height=""50"">Quality control of sequence data
	</td>
</tr>
<tr>
   <td height=""50"">14:00 - 15:00</td>
   <td height=""50"">Lecture</td>
   <td height=""50"">Tomas Larsson</td>
   <td height=""50"">Genome assembly using Illumina data
	</td>
</tr>
<tr>
   <td height=""50"">15:00 - 17:00</td>
   <td height=""50"">Practical</td>
   <td height=""50"">Tomas Larsson</td>
   <td height=""50"">Genome assembly using Illumina data
	</td>
</tr>
<tr>
   <td colspan=""4""><h3>Tuesday - 2 July</h3></td>
</tr>
<tr>
   <td height=""50"">09:00 - 10:00</td>
   <td height=""50"">Lecture</td>
   <td height=""50"">Christophe Klopp</td>
   <td height=""50"">Long read (PacBio, Nanopore) genome assembly
	</td>
</tr>
<tr>
   <td height=""50"">10:00 - 12:00</td>
   <td height=""50"">Practical</td>
   <td height=""50"">Christophe Klopp</td>
   <td height=""50"">Long read (PacBio, Nanopore) genome assembly
	</td>
</tr>
<tr>
   <td height=""50"">12:00 - 13:00</td>
   <td colspan=""3"" height=""50"">Lunch break</td>
</tr>
<tr>
   <td height=""50"">13:00 - 14:00</td>
   <td height=""50"">Lecture</td>
   <td height=""50"">Tomas Larsson</td>
   <td height=""50"">Assembly validation
	</td>
</tr>
<tr>
   <td height=""50"">14:00 - 17:00</td>
   <td height=""50"">Practical</td>
   <td height=""50"">Tomas Larsson</td>
   <td height=""50"">Assembly validation
	</td>
</tr>
<tr>
   <td colspan=""4""><h3>Wednesday - 3 July</h3></td>
</tr>
<tr>
   <td height=""50"">09:00 - 17:00</td>
   <td height=""50"">Discussion & hands-on</td>
   <td height=""50"">Participants & trainers</td>
   <td height=""50"">Discussion of the participants‚Äô own projects (also hands-on work if possible), and catch-up session of day 1 and 2 for those that did not have time to finish the practicals.
	</td>
</tr>
<tr>
   <td colspan=""4""><h3>Thursday - 4 July</h3></td>
</tr>
<tr>
   <td height=""50"">09:00 - 10:00</td>
   <td height=""50"">Lecture</td>
   <td height=""50"">Joelle Amselem</td>
   <td height=""50"">Transposable element annotation
	</td>
</tr>
<tr>
   <td height=""50"">10:00 - 12:00</td>
   <td height=""50"">Practical</td>
   <td height=""50"">Joelle Amselem</td>
   <td height=""50"">Transposable element annotation
	</td>
</tr>
<tr>
   <td height=""50"">12:00 - 13:00</td>
   <td colspan=""3"" height=""50"">Lunch break</td>
</tr>
<tr>
   <td height=""50"">13:00 - 14:00</td>
   <td height=""50"">Lecture</td>
   <td height=""50"">Jacques Dainat</td>
   <td height=""50"">Structural annotation
	</td>
</tr>
<tr>
   <td height=""50"">14:00 - 17:00</td>
   <td height=""50"">Practical</td>
   <td height=""50"">Jacques Dainat</td>
   <td height=""50"">Structural annotation
	</td>
</tr>
<tr>
   <td colspan=""4""><h3>Friday - 5 July</h3></td>
</tr>
<tr>
   <td height=""50"">09:00 - 11:00</td>
   <td height=""50"">Practical</td>
   <td height=""50"">Jacques Dainat</td>
   <td height=""50"">Structural annotation
	</td>
</tr>
<tr>
   <td height=""50"">11:00 - 12:00</td>
   <td height=""50"">Lecture</td>
   <td height=""50"">Lucile Soler</td>
   <td height=""50"">Functional annotation
	</td>
</tr>
<tr>
   <td height=""50"">12:00 - 13:00</td>
   <td colspan=""3"" height=""50"">Lunch break</td>
</tr>
<tr>
   <td height=""50"">13:00 - 14:30</td>
   <td height=""50"">Practical</td>
   <td height=""50"">Lucile Soler</td>
   <td height=""50"">Functional annotation
	</td>
</tr>
<tr>
   <td height=""50"">14:30 - 16:00</td>
   <td height=""50"">Lecture & Practical</td>
   <td height=""50"">Lucile Soler</td>
   <td height=""50"">Prokaryote annotation
	</td>
</tr>
<tr>
   <td height=""50"">16:00 - 17:00</td>
   <td colspan=""3"" height=""50"">Wrap-up and feedback session</td>
</tr>

</table>
<br>
<br>

<h3>ELIXIR-IIB member institutes</h3>
<ol>
   <li> <b>CNR, National Research Council </b> (Lead Institute)</li>
   <li> CRS4</li>
   <li> CINECA</li>
   <li> Edmund Mach Foundation, Trento</li>
   <li> ENEA</li>
   <li> Fondazione Telethon</li> 
   <li> INFN</li>
   <li> Istituto Superiore di Sanit√† (ISS)</li> 
   <li> GARR</li>
   <li> Stazione Zoologica Anton Dohrn, Napoli</li>
   <li> University of Roma ""Sapienza""</li>
   <li> University of Bari</li>
   <li> University of Bologna</li>
   <li> University of Firenze</li>
   <li> University of Milano</li>
   <li> University of Milano Bicocca</li>
   <li> University of Napoli</li>
   <li> University of Padova</li>
   <li> University of Parma</li>
   <li> University of Roma ""Tor Vergata""</li>
   <li> University of Salerno</li>
   <li> University of Torino</li>
   <li> University of Tuscia </li>
</ol>
<br>
<br>
<table border=""0"">
</table>
",,EDU,0.52,,,,,,0,4,0.7,EDU
928823596,R_kgDON1y5LA,jawadefaj,jawadefaj/jawadefaj,0,jawadefaj,https://github.com/jawadefaj/jawadefaj,OTHER,homepage readme,0,2025-02-07 09:57:17+00:00,2025-02-07 11:31:31+00:00,2025-02-07 11:31:28+00:00,,183,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['jawadefaj'],,"<div align=""center"">

# ‡¶ú‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡¶¶ 

<img src=""image/jawadefaj.png"" alt=""jawadefaj"" width=""500px"" />

### üëã Hello! I'm Jawad ‚ôã
**Ph.D. Candidate in Computational Media at UC Santa Cruz**<br/>
*Game Developer, ML Enthusiast, and AV Simulation Researcher*

</div>
",,OTHER,0.8,,,,,,0,1,0.9,OTHER
20165721,MDEwOlJlcG9zaXRvcnkyMDE2NTcyMQ==,CMPS202_H.264_decoder,xyuanlu/CMPS202_H.264_decoder,0,xyuanlu,https://github.com/xyuanlu/CMPS202_H.264_decoder,EDU,CMPS 202 final peject,0,2014-05-25 20:58:42+00:00,2023-09-12 15:10:58+00:00,2014-06-04 20:18:06+00:00,,3184,2,2,Verilog,1,1,1,1,0,0,2,0,0,0,,1,0,0,public,2,0,2,master,1,['xyuanlu'],,"********************H.264 decoder********************************
*		by Xiaoyuan Lu and Bipeng Zhang			*
*		email: xl37136@ucsc.edu bizhang@ucsc.edu 	*
*****************************************************************


*****************************************************************
Introduction:

1. We rewrote opensource H.264 decoder based on elastic system. The main work is to rewrite input and output modules with C++. Also we rewrote the corresponding head files with our own child classes. In order to link systemverilog with C++, we rewrote top level verilog test bench and after studied the example of elastic system, we wrote C++ test bench from scratch. It works well on the elastic system in MASC server.

2. For Fluid Pipeline, besides what described above, we rewrote top level systemverilog file of the central logic, designed and implemented the corresponding interfaces to ""stage""(elastic buffer). 

******************************************************************
Results:

The H.264 decoder without elastic buffer works well on elastic system. We used standard QCIF video called Akiyo_cif for our testing, the bitstream for the video is 
https://docs.google.com/file/d/0Bz8hasj5gcb4SlRpdHczUHVrWFk/edit
and this is the result that we decoded
https://docs.google.com/file/d/0Bz8hasj5gcb4bldfSDhYN3VhblE/edit

There are still some bugs in our code approaching the decoder with elastic buffer.
******************************************************************
File description:

This is the like to out Git repo
https://github.com/revoiry/CMPS202_H.264_decoder

There are two branches in our git repository. The fluid-ppl is the version without elastic buffer, which works well. The elastic branch is the version with elastic buffer, which still needs debugging.

1. The fluid-ppl branch:

bitstream: it contains a txt file which is the so-called test file. It is a 300 frames test video with hexadecimal expression.

rtl: it contains all of systemverilog files used in H.264 decoder. We rewrote H264.v and defined H264_types.v.

tests: it contains all test benches. H264_tb.cpp is the C++ test bench. H264_ttb.cpp defines vpi handles. H264_tb.v is the top level verilog test bench, in which we call functions achieved in H264_tb.cpp and H264_ttb.cpp. H264_ttb.tab is a configuration file for CVS simulator.

H264.xml: it is the system configuration file. Its name should be the same as the current folder's name.


2. The elastic branch:

most of files are the same as the ones in fluid-ppl branch.

rtl: nova.v is the centrol logic of H.264 decoder with adding control signals and input and output flops. nova_inter.v was designed to be an interface between the centrol logic and ""stage"" module. In H264.v, we combined two stages and the instance of nova_inter.v.

tests: the main structure is the same as the description in fluid-ppl. The differences are new defined class and vpi interfaces as we changed the top level centrol logic achievement for using ""stage"". 


*******************************************************************
how to use:
1. Make sure the xml file has the same name as the folder containing it.
2. Change the maximum cycles you want to run in H264_tb.cpp. In our case, to decode one frame we need almost 57300 cycles.
3. Access to the folder and input the command - rake test:H264_tb.
4. If the system has errors with no achievement of functions tb_inc_stat1() and tb_inc_stat2(). Just access to H264_ttb.cpp and comment them out. These two functions will be generated by the system once you changed some .v files in rtl floder. For now, they have no influence on our project.
5. After the system finishes its work, you could see some outputs. We printed out some input and output data, control signals and verilog output variables for debugging.
6. After you run it, there will be some folder and file generated automatically. Like csrc, simv and ucli.key.
*******************************************************************

if you have questions about our project, please feel free to contact us. Thank you for your time.
 



",,EDU,0.64,,,,,,0,1,0.95,EDU
741790610,R_kgDOLDbTkg,assignments,ucsc-cse-240/assignments,0,ucsc-cse-240,https://github.com/ucsc-cse-240/assignments,EDU,,0,2024-01-11 05:44:10+00:00,2024-11-20 16:09:24+00:00,2024-11-20 16:09:21+00:00,,2264,6,6,Jupyter Notebook,1,1,1,1,0,0,18,0,0,2,,1,0,0,public,18,2,6,main,1,"['aoue', 'lgilpin']",1,"# Assignments
These are the assignments for CSE240 at UCSC.  They are:
- [Assignment1](Assignment1/)
- [Assignment2](Assignment2/) 
- [Assignment3](Assignment3/)
- [Assignment4](Assignment4/)
- [Assignment5](Assignment5/)
  
# Installing the Autograder package
Install the autograder with `pip3 install autograder-py` on the command line.  

# Submitting and using the Autograder in CSE 240

Make sure that the autograder is installed on your local machine by
typing: `python3 -m autograder.cli`.  If you see the `--help` option:

```nil
python -m autograder.cli
The autograder CLI package contains several tools for interacting with the autograder.
The following is a non-exhaustive list of CLI tools.
Invoke each command with the `--help` option for more details.
```
## Using the autograder

The autograder command line interface (cli) is [documented](https://github.com/eriq-augustine/autograder-py).  As a
student in the class, the main commands you will use are:

-   `python3 -m autograder.run.submit`: this will submit an assignment
    for a particular class and assignment.
-   `python3 -m autograder.run.peek`: this will show you your last submission
-   `python3 -m autograder.run.history`: this will show a summary of all
    your past submission
",,EDU,0.78,,,,,,0,3,0.95,EDU
25446415,MDEwOlJlcG9zaXRvcnkyNTQ0NjQxNQ==,CMPS115-Project,delandiaz/CMPS115-Project,0,delandiaz,https://github.com/delandiaz/CMPS115-Project,EDU,UCSC CMPS115 Fall 2014 (Team Skyy),0,2014-10-20 01:47:43+00:00,2014-12-10 19:28:53+00:00,2014-12-10 19:28:53+00:00,,1496,0,0,JavaScript,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,"['clewczyk', 'muz415', 'amsadegh']",,"CMPS115-Project
===============
This is the README for Team SKYY, part of CMPS115 at UCSC. 
Team members include: Muzammel Choudhery, Alex Sadeghi, Cristian Lewczyk and Delan Diaz

Description of files/folders:

info: Contains Burnup Chart, SCRUM Board, and any other non-project related files that are necessary for the team and/or course.

README.md: README file of project, extra documentation for project
",,EDU,0.87,,,,,,0,5,0.9,EDU
249545672,MDEwOlJlcG9zaXRvcnkyNDk1NDU2NzI=,Management-Software,Jiaying330/Management-Software,0,Jiaying330,https://github.com/Jiaying330/Management-Software,DEV,,0,2020-03-23 21:15:21+00:00,2023-04-14 18:43:41+00:00,2020-05-04 01:00:33+00:00,,70711,0,0,C++,0,1,1,1,0,0,5,0,0,0,,1,0,0,public,5,0,0,master,1,"['liehuozhizun', 'Jiaying330', 'masonma37078', 'JokerTBHK', 'changpian8', 'hli122', 'YiyunZheng', 'yli302']",,"# WeAlumni Management Software

This project is managed by
<br/>&nbsp;&nbsp;&nbsp;&nbsp;Information Technology Support (Dept.)
<br/>&nbsp;&nbsp;&nbsp;&nbsp;Chinese Alumni Association
<br/>&nbsp;&nbsp;&nbsp;&nbsp;University of California, Santa Cruz

<br/>The UCSC-CAA(UC Santa Cruz - Chinese Alumni Association) owns its all copyrights.
<br/>&nbsp;&nbsp;&nbsp;&nbsp;caa@ucsc.edu
<br/>¬©2020 UC Santa Cruz - CAA. All Rights Reserved.

## Brief Introduction

<br/>&nbsp;&nbsp;&nbsp;&nbsp;WeAlumni Management Software is a Windows desktop application based on .NET Framework in C++ language.
<br/>&nbsp;&nbsp;&nbsp;&nbsp;It is designed for our staffs to manage our members' information and other daily transactions of our organization.
<br/>&nbsp;&nbsp;&nbsp;&nbsp;At this time, this software uses SQLite as the local database but will be replaced by a cloud database, in the future, to better support our staffs. 
And synchronize data with another project, WeAlumni WeChat Mini Program, to maintain the data.

<br/>&nbsp;&nbsp;&nbsp;&nbsp;If you'd like to leave a feedback or any other comments that you'd like to share, please contact either the IT manager or our organization.

## Development Team

|                            |                            |
|----------------------------|----------------------------|
| ***IT Manager/PM<br/>***   | Hang Yuan (hyuan3@ucsc.edu)|
| ***Developer***            | Xiangdong Che              |
|                            | Jiaying Hou                |
|                            | Rui Jia                    |
|                            | Haoran Li                  |
|                            | Sen Ma                     |
|                            | Yiyun Zheng                |
| ***UI Designer***          | Xuqing Yan                 |

&nbsp;&nbsp;&nbsp;&nbsp;\\*Names are ordered by last name alphabetically

## Acknowledgement

&nbsp;&nbsp;&nbsp;&nbsp;We wish to give many thanks to our volunteer testers for their generous contribution to improve our UI interfaces and user experience.
<br/>&nbsp;&nbsp;&nbsp;&nbsp;The name listed here is ordered by last name alphabetically:
<br/>&nbsp;&nbsp;&nbsp;&nbsp;None

## Version Information

&nbsp;&nbsp;&nbsp;&nbsp;v1.0 - Not released yet

## Documentation
 
&nbsp;&nbsp;&nbsp;&nbsp;None
",,DEV,0.94,,,,,,0,0,0.95,DEV
65171453,MDEwOlJlcG9zaXRvcnk2NTE3MTQ1Mw==,FinalProject5381,mmehr2/FinalProject5381,0,mmehr2,https://github.com/mmehr2/FinalProject5381,EDU,"My final project for UC Santa Cruz Silicon Valley Extension course 5381.037 (Intro to Real-Time Embedded Systems Programming) with instructor Anil Gathala, Summer 2016 term.",0,2016-08-08 04:04:00+00:00,2016-08-08 05:29:56+00:00,2016-09-26 08:00:40+00:00,,140396,0,0,C,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,master,1,['mmehr2'],,"#FinalProject5381


My final project for UC Santa Cruz Silicon Valley Extension course 5381.037 (Intro to Real-Time Embedded Systems Programming) with instructor Anil Gathala, Summer 2016 term.

##File Layout
The Example01 folder contains my project source.
Other folders are generated for all LPCOpen projects and are rarely modified, except for the file FreeRTOSConfig.h.

##Project Specifications

This final project allows us to bring together what we've learned in the course so far.
Although 3 options were given, I'll be attempting them all.

##Features (Common)

- Implement a full Smart Bulb application in FreeRTOS
- The LED on-board is the Bulb. In addition use the push button and light sensor.

###Level A: (10 marks)
- Periodically poll the light sensor
- When dark ‚Äì light up the bulb, otherwise light off the bulb
- **(Custom)** I added support for the 8 RGB colors and display them according to light intensity.

###Level B: (10 marks)
- Add a push button
- Pressing a push button -> switch on light anyway (ignore the light sensor for now)
- Pressing the push button second time -> switch off light and go back to the smart bulb mode

##Features (Option 1)
###Level C1 (Bonus: +5 marks)
- Add a seven segment display (use gpio to control each led inside 7 segment)
- This should display the level of light as seen on the light sensor
- Choose a simple scale of your choice: 0 -> dark; 8 -> max light (this scale was used).
- **(Custom)** I added a segment test sequence mode to verify proper operation of the display.

##Features (Option 2)
###Level C2 (Bonus: +5 marks)
- Add a PIR occupancy sensor (GPIO)
- Link: https://www.adafruit.com/products/189
- Make smart bulb decisions based on both light as well as occupancy
- **(Custom)** The Decimal Point on the 7-segment display lights up when the room is occupied.

##Features (Option 3)
###Level C3 (Bonus: +10 marks)
- Add a I2C/SPI based temp/hum sensor
- Whenever light state changes (on -> off; off -> on), display both temp and hum

##Features (Custom) ##
###Level C4 Quad display stick
- Add support for a multi-digit display from Adafruit (Arduino house).
- See product here: https://www.adafruit.com/product/1912
- This will use the I2C2 bus.

## Project Submission Guidelines

Submit just one zip file ‚Äì with all the project files. I should be able to import it and run succesfully.

Make a simple/quick video of the working demo from you phone and submit the video

A simple / quick document of the s/w design, a block diagram or a flow diagram showing any of the following components implemented:

- Tasks
- Message queues
- ISRs and bottom-halves
- Timers
- Semaphores, etc. used

Also show the data flow between components

Please also write a few lines in the text:

- At what reading of the light sensor your code lights up/off the bulb (thresholds)
- If doing bonus: mention the 7-segment display scale

Nothing fancy:

- Make sure your video has enough light
- The document should be ideally in doc, but can be on a white sheet and scan it.
- Deadline: Sep 1st is the deadline (11:59 pm)
- Submit everything by then
",,EDU,0.79,,,,,,0,1,0.9,EDU
698827270,R_kgDOKadCBg,Network-Topology,christy-jose01/Network-Topology,0,christy-jose01,https://github.com/christy-jose01/Network-Topology,EDU,"In this repository, I practice creating a network topology and practice some shell scripting. There are all written in Python and Bash.",0,2023-10-01 04:52:17+00:00,2023-10-01 05:26:50+00:00,2023-10-01 05:29:03+00:00,,1596,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['christy-jose01'],,"# Network-Topology
The purpose of this project was to practice creating a Network topology and some shell scripting. This project was based on my Lab from my CSE 150 course at the University of California, Santa Cruz.

## Files: 
- Christy-topo.py: This file is the python script that creates the topology of a network with 4 hosts connected to 1 switch.
- Christy Jose-script.sh: This file is the python script for the prelab that prints out all of the even numbered lines in each file in the current directory. The output has the filename followed by a colon and whatever is on that line.
- Christy Jose-lab1.pdf: This file contains the solutions to all of the question for the prelab, and all of the screenshots and answers to the lab.
",,EDU,0.62,,,,,,0,1,0.9,EDU
745793595,R_kgDOLHPoOw,GeoSlug,LemonFoxmere/GeoSlug,0,LemonFoxmere,https://github.com/LemonFoxmere/GeoSlug,DEV,"üåà‚ú® GeoSlug: Your Go-To App for Mapping Furry Friends! üêæ A pawsome blend of social media and animal spotting for the UCSC community. Join our vibrant den of animal lovers, share your adorable sightings, and explore a map brimming with furry wonders! Created with love by fellow furries, for a world where every tail wag and purr counts.",0,2024-01-20 06:41:47+00:00,2024-03-26 00:44:29+00:00,2024-06-26 06:39:31+00:00,,2245,2,2,JavaScript,1,1,1,1,0,0,0,0,0,0,gpl-3.0,1,0,0,public,0,0,2,master,1,"['LemonFoxmere', 'Brightonca']",,"# üåø GeoSlug: A Pawsome Animal Mapping Adventure! üêæ

## OwO-erview üåà
GeoSlug is a totes adorbs web app developed for UC Santa Cruz! It's a place where you can share and map animal sightings, and it's like a social platform for all of us who love our furry friends! You can upload pics of animals, spill the tea about them, and show where you spotted them on a super interactive map! üì∏üó∫Ô∏è

## Features üåü
- **Animal Sightings:** ü¶ä Snap or upload pics of animals and share your exciting encounters!
- **Interactive Map:** üó∫Ô∏è Pinpoint your sightings and explore others', purr-fect for our animal-loving community!
- **User-Friendly UI:** üíÖ Designed with love and flair for an easy-peasy and fun experience.

## Tech Magic Used üåüüíª
- **Frontend Fluff:** üñ•Ô∏è SvelteKit, 'cause we love things smooth and snazzy.
- **Backend Brilliance:** üîß Firebase for all the techy stuff behind the scenes.
- **Map Mastery:** üó∫Ô∏è Mapbox, for a map that's almost as cute as a kitten.
- **Languages and Scripts:** üìù JavaScript, TypeScript, HTML, SCSS, and EJS ‚Äì we speak them all!

## Our Fabulous Journey üõ§Ô∏èüåà
- We tackled frontend and backend like champs! üí™
- Poured our hearts into UI design and making everything sparkle! üé®‚ú®
- Learned a whole bunch about APIs and tech tools. Geeky and proud! üåê

## Future Paw-sibilities üîÆ
- More UI fabulosity and shiny features! ‚ú®
- Backend, but make it fashion (and optimized)! üöÄ
- Dreaming of geolocation upgrades, migration tracking, and AI animal recognition ‚Äì 'cause we're ambitious like that! ü§ñ

## Get Started with Us! üöÄ
- Frolic over to our [GitHub repository](https://github.com/LemonFoxmere/GeoSlug) for all the deets and code! üêæüë©‚Äçüíª

## License and Legal Stuff üìÑ
This project is like totally licensed under the GPL-3.0 License.

## Special Shoutouts üôèüåà
Big hugs to Lemon Foxmere and Brightonca for creating this! Inspired by UC Santa Cruz's natural beauty and our love for all creatures great and small! üå≤ü¶ù


",,DEV,0.85,,,,,,0,1,0.95,DEV
549358313,R_kgDOIL6K6Q,lkim3834,lkim3834/lkim3834,0,lkim3834,https://github.com/lkim3834/lkim3834,OTHER,,0,2022-10-11 04:10:18+00:00,2024-10-09 06:07:35+00:00,2024-10-09 06:07:32+00:00,,20,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['lkim3834'],,"### Hello there üëã, I'm Ian Kim! <img src=""https://img.icons8.com/emoji/48/000000/woman-technologyst.png"" /> 

<!--
cmd-shift-k-v to put the example on the side
Here are some ideas to get you started:

- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
- üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->
- üòÑ Pronouns: she/her/hers
- üå± Portfolio : https://lkim3834.github.io/
- üì´ How to reach me: rladlwls17@gmail.com 

Currently: Recent graduate with a Bachelor of Science in Computer Science from the University of California, Santa Cruz.
",,OTHER,0.72,,,,,,0,1,0.7,OTHER
274220301,MDEwOlJlcG9zaXRvcnkyNzQyMjAzMDE=,CASL,lsd-ucsc/CASL,0,lsd-ucsc,https://github.com/lsd-ucsc/CASL,DOCS,,0,2020-06-22 19:03:13+00:00,2024-08-08 05:24:57+00:00,2023-05-05 00:35:43+00:00,,1650,7,7,,1,1,1,0,0,0,2,0,0,0,,1,0,0,public,2,0,7,master,1,"['plredmond', 'lkuper', 'gshen42', 'tgoodwin', 'Twisol', 'wilcoxjay']",1,"# üè∞ CASL

This is a GitHub repository for the CASL research group.  CASL is pronounced like ""castle"" and stands for...

  * Consistency-Aware Solvers and Languages
  * Coordination-Avoiding Systems Lab
  * Consistent and Available Systems Lab üòâ
  * Creative, Ambitious Systems and Languages
  * CASL Ain't a Systems Lab
  * Curses! Another Separation Logic! and/or CAusal Separation Logic
  * Can't Allocate, Stack Limit
  * ...

Our name symbolizes strength, safety, longevity, and a bit of magic and whimsy.  üè∞ 
",,EDU,0.95,,,,,,0,5,1,EDU