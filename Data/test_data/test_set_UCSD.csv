id,node_id,name,full_name,private,owner,html_url,description,fork,created_at,updated_at,pushed_at,homepage,size,stargazers_count,watchers_count,language,has_issues,has_projects,has_downloads,has_wiki,has_pages,has_discussions,forks_count,archived,disabled,open_issues_count,license,allow_forking,is_template,web_commit_signoff_required,visibility,forks,open_issues,watchers,default_branch,score,organization,readme,contributors,manual_label,prediction,release_downloads,code_of_conduct,contributing,security_policy,issue_templates,pull_request_template,subscribers_count,,,Groups/Clubs,Organizations
108087423,MDEwOlJlcG9zaXRvcnkxMDgwODc0MjM=,data_visualization,aaronpenne/data_visualization,0,aaronpenne,https://github.com/aaronpenne/data_visualization,"A collection of my data visualizations, mostly in Python.",0,2017-10-24 06:42:06+00:00,2025-01-10 00:49:57+00:00,2022-08-02 05:30:22+00:00,,318124,374,374,Python,1,1,1,1,0,0,71,0,0,1,mit,1,0,0,public,71,1,374,master,1,,,"['aaronpenne', 'natelowry']",0,0.69,0,,,,,,22,,,,
731892810,R_kgDOK5_MSg,Awesome-LLM-3D,ActiveVisionLab/Awesome-LLM-3D,0,ActiveVisionLab,https://github.com/ActiveVisionLab/Awesome-LLM-3D,Awesome-LLM-3D: a curated list of Multi-modal Large Language Model in 3D world  Resources,0,2023-12-15 06:02:44+00:00,2025-03-08 04:20:27+00:00,2025-02-14 21:23:48+00:00,,14607,1498,1498,,1,1,1,1,0,0,90,0,0,2,mit,1,0,0,public,90,2,1498,avl-branch,1,1,,"['Hannibal046', 'xianzhengma', 'yashbhalgat', 'Sumedhn97', 'AaronWhy', 'SinclairCoder', 'sinwang20', 'patrick-tssn', 'romilbhardwaj', 'pchalasani', 'junshengzhou', 'guyShilo', 'lemanschik', 'Cyril-JZ', 'jeff3071', 'ianblenke', 'dannymcy', 'YunzeMan', 'Germany321', 'Xin-Jing', 'merrymercy', 'jedyang97', 'jackmpcollins', 'zxlzr', 'AnjieCheng', 'zgzxy001', 'jeasinema', 'rese1f', 'TangYuan96', 'russhustle', 'L0Z1K', 'SartajBhuvaji', 'RunpeiDong', 'rossng', 'rogeriochaves', 'RogerHYang', 'notmahi', 'mohabfekry', 'izhx', 'yangcaoai', 'Ivan-Tang-3D', 'zhangshaolei1998', 'aryankhanna475', 'bryzgalovdm', 'crazyofapple', 'gowithwind', 'hejingcao', 'multitude00999', 'scottsuk0306', 'ywyue', 'zd11024', 'bestpredicts', 'aarnphm', 'anmorgan24', 'Samir55', 'Barqawiz', 'Macoron', 'Buzz-Beater', 'huybery', 'buoyancy99', 'ChanLiang', 'ZCMax', 'Parkchanjun', 'luciusssss', 'Cuzyoung', 'GaneshBannur', 'ZzZZCHS', 'LoganJoe', 'HuangOwen', 'typpo', 'jasonacox', 'jasonwcfan', 'AmberLJC', 'kaisugi', 'keyvank', 'lgrammel', '0xDing', 'alphadl', 'mmabrouk']",0,0.77,0,,,,,,54,,,,
623585174,R_kgDOJSsnlg,diffusion-rig,adobe-research/diffusion-rig,0,adobe-research,https://github.com/adobe-research/diffusion-rig,Code Release for DiffusionRig (CVPR 2023),0,2023-04-04 17:03:54+00:00,2025-02-24 06:54:50+00:00,2023-10-13 19:44:24+00:00,https://diffusionrig.github.io,43796,276,276,Python,1,1,1,0,0,0,19,0,0,13,other,1,1,0,public,19,13,276,main,1,1,,['zh-ding'],1,0.65,0,,,,,,12,,,,
183026431,MDEwOlJlcG9zaXRvcnkxODMwMjY0MzE=,University-of-California-San-Diego-Big-Data-Specialization,AlessandroCorradini/University-of-California-San-Diego-Big-Data-Specialization,0,AlessandroCorradini,https://github.com/AlessandroCorradini/University-of-California-San-Diego-Big-Data-Specialization,Repository for the Big Data Specialization from University of California San Diego on Coursera,0,2019-04-23 13:56:25+00:00,2025-03-02 16:51:10+00:00,2022-09-06 14:15:50+00:00,,56580,243,243,Jupyter Notebook,1,1,1,1,0,0,240,0,0,16,,1,0,0,public,240,16,243,master,1,,"# Big Data Specialization from University of California San Diego

## Overview
[Big Data Specialization from University of California San Diego](https://www.coursera.org/specializations/big-data) is an introductory learning path for the Big Data world.  

This specialization covers:

- Big Data essential concepts
- Hadoop and MapReduce
- NoSQL and MongoDB
- Graph Databases and Neo4j
- Big Data Analytics and Apache Spark, Hive, Pig

## Courses in this Program

1) [Introduction to Big Data](https://github.com/AlessandroCorradini/University-of-California-San-Diego-Big-Data-Specialization/tree/master/01%20-%20Introduction%20to%20Big%20Data) 
2) [Big Data Modeling and Management Systems](https://github.com/AlessandroCorradini/University-of-California-San-Diego-Big-Data-Specialization/tree/master/02%20-%20Big%20Data%20Modelling%20and%20Management%20Systems)
3) [Big Data Integration and Processing](https://github.com/AlessandroCorradini/University-of-California-San-Diego-Big-Data-Specialization/tree/master/03%20-%20Big%20Data%20Integration%20and%20Processing)
4) [Machine Learning With Big Data](https://github.com/AlessandroCorradini/University-of-California-San-Diego-Big-Data-Specialization/tree/master/04%20-%20Machine%20Learning%20with%20Big%20Data)
5) [Graph Analytics for Big Data](https://github.com/AlessandroCorradini/University-of-California-San-Diego-Big-Data-Specialization/tree/master/05%20-%20Graph%20Analytics%20for%20Big%20Data)
6) [Capstone](https://github.com/AlessandroCorradini/University-of-California-San-Diego-Big-Data-Specialization/tree/master/06%20-%20Big%20Data%20-%20Capstone%20Project)

## Certificate of Completion
You can see the [Certificate of Completion](https://github.com/AlessandroCorradini/Certificates/blob/master/Coursera%20-%20Big%20Data%20Specialization%20Certificate%20-%20UC%20San%20Diego.pdf) and other certificates in my [Certificates Repo](https://github.com/AlessandroCorradini/Certificates) that contains all my certificates obtained through my journey as a self-made Data Science and better developer.

<br/>

### ⚠️ Disclaimer ⚠️
**Please, don't fork or copy this repository.**

**Big Data Specialization from University of California San Diego, is a very easy and straight forward path. You can complete it with a minimal effort.**
","['AlessandroCorradini', 'skvrahul', 'nmnjn']",1,0.66,0,,,,,,7,,,,
111592136,MDEwOlJlcG9zaXRvcnkxMTE1OTIxMzY=,EEGNet,aliasvishnu/EEGNet,0,aliasvishnu,https://github.com/aliasvishnu/EEGNet,[Old version] PyTorch implementation of EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces - https://arxiv.org/pdf/1611.08024.pdf,0,2017-11-21 19:27:37+00:00,2025-03-08 09:35:53+00:00,2019-07-10 22:35:35+00:00,,173,299,299,Jupyter Notebook,1,1,1,1,1,0,68,1,0,5,mit,1,0,0,public,68,5,299,master,1,,,['aliasvishnu'],1,0.64,0,,,,,,9,,,,
128416044,MDEwOlJlcG9zaXRvcnkxMjg0MTYwNDQ=,awesome-object-detection,amusi/awesome-object-detection,0,amusi,https://github.com/amusi/awesome-object-detection,Awesome Object Detection based on handong1587 github: https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html,0,2018-04-06 15:58:50+00:00,2025-03-07 22:40:51+00:00,2022-12-17 04:28:58+00:00,,79,7448,7448,,1,1,1,1,0,0,1943,0,0,6,,1,0,0,public,1943,6,7448,master,1,,"# object-detection

[TOC]

This is a list of awesome articles about object detection. If you want to read the paper according to time, you can refer to [Date](Date.md).

- R-CNN
- Fast R-CNN
- Faster R-CNN
- Mask R-CNN
- Light-Head R-CNN
- Cascade R-CNN
- SPP-Net
- YOLO
- YOLOv2
- YOLOv3
- YOLT
- SSD
- DSSD
- FSSD
- ESSD
- MDSSD
- Pelee
- Fire SSD
- R-FCN
- FPN
- DSOD
- RetinaNet
- MegDet
- RefineNet
- DetNet
- SSOD
- CornerNet
- M2Det
- 3D Object Detection
- ZSD（Zero-Shot Object Detection）
- OSD（One-Shot object Detection）
- Weakly Supervised Object Detection
- Softer-NMS
- 2018
- 2019
- Other

Based on handong1587's github: https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html

# Survey

**Imbalance Problems in Object Detection: A Review**

- intro: under review at TPAMI
- arXiv: <https://arxiv.org/abs/1909.00169>

**Recent Advances in Deep Learning for Object Detection**

- intro: From 2013 (OverFeat) to 2019 (DetNAS)
- arXiv: <https://arxiv.org/abs/1908.03673>

**A Survey of Deep Learning-based Object Detection**

- intro：From Fast R-CNN to NAS-FPN

- arXiv：<https://arxiv.org/abs/1907.09408>

**Object Detection in 20 Years: A Survey**

- intro：This work has been submitted to the IEEE TPAMI for possible publication
- arXiv：<https://arxiv.org/abs/1905.05055>

**《Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks》**

- intro: awesome


- arXiv: https://arxiv.org/abs/1809.03193

**《Deep Learning for Generic Object Detection: A Survey》**

- intro: Submitted to IJCV 2018
- arXiv: https://arxiv.org/abs/1809.02165

# Papers&Codes

## R-CNN

**Rich feature hierarchies for accurate object detection and semantic segmentation**

- intro: R-CNN
- arxiv: <http://arxiv.org/abs/1311.2524>
- supp: <http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf>
- slides: <http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf>
- slides: <http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf>
- github: <https://github.com/rbgirshick/rcnn>
- notes: <http://zhangliliang.com/2014/07/23/paper-note-rcnn/>
- caffe-pr(""Make R-CNN the Caffe detection example""): <https://github.com/BVLC/caffe/pull/482>

## Fast R-CNN

**Fast R-CNN**

- arxiv: <http://arxiv.org/abs/1504.08083>
- slides: <http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf>
- github: <https://github.com/rbgirshick/fast-rcnn>
- github(COCO-branch): <https://github.com/rbgirshick/fast-rcnn/tree/coco>
- webcam demo: <https://github.com/rbgirshick/fast-rcnn/pull/29>
- notes: <http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/>
- notes: <http://blog.csdn.net/linj_m/article/details/48930179>
- github(""Fast R-CNN in MXNet""): <https://github.com/precedenceguo/mx-rcnn>
- github: <https://github.com/mahyarnajibi/fast-rcnn-torch>
- github: <https://github.com/apple2373/chainer-simple-fast-rnn>
- github: <https://github.com/zplizzi/tensorflow-fast-rcnn>

**A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection**

- intro: CVPR 2017
- arxiv: <https://arxiv.org/abs/1704.03414>
- paper: <http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf>
- github(Caffe): <https://github.com/xiaolonw/adversarial-frcnn>

## Faster R-CNN

**Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks**

- intro: NIPS 2015
- arxiv: <http://arxiv.org/abs/1506.01497>
- gitxiv: <http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region>
- slides: <http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf>
- github(official, Matlab): <https://github.com/ShaoqingRen/faster_rcnn>
- github(Caffe): <https://github.com/rbgirshick/py-faster-rcnn>
- github(MXNet): <https://github.com/msracver/Deformable-ConvNets/tree/master/faster_rcnn>
- github(PyTorch--recommend): <https://github.com//jwyang/faster-rcnn.pytorch>
- github: <https://github.com/mitmul/chainer-faster-rcnn>
- github(Torch):: <https://github.com/andreaskoepf/faster-rcnn.torch>
- github(Torch):: <https://github.com/ruotianluo/Faster-RCNN-Densecap-torch>
- github(TensorFlow): <https://github.com/smallcorgi/Faster-RCNN_TF>
- github(TensorFlow): <https://github.com/CharlesShang/TFFRCNN>
- github(C++ demo): <https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus>
- github(Keras): <https://github.com/yhenon/keras-frcnn>
- github: <https://github.com/Eniac-Xie/faster-rcnn-resnet>
- github(C++): <https://github.com/D-X-Y/caffe-faster-rcnn/tree/dev>

**R-CNN minus R**

- intro: BMVC 2015
- arxiv: <http://arxiv.org/abs/1506.06981>

**Faster R-CNN in MXNet with distributed implementation and data parallelization**

- github: <https://github.com/dmlc/mxnet/tree/master/example/rcnn>

**Contextual Priming and Feedback for Faster R-CNN**

- intro: ECCV 2016. Carnegie Mellon University
- paper: <http://abhinavsh.info/context_priming_feedback.pdf>
- poster: <http://www.eccv2016.org/files/posters/P-1A-20.pdf>

**An Implementation of Faster RCNN with Study for Region Sampling**

- intro: Technical Report, 3 pages. CMU
- arxiv: <https://arxiv.org/abs/1702.02138>
- github: <https://github.com/endernewton/tf-faster-rcnn>
- github: https://github.com/ruotianluo/pytorch-faster-rcnn

**Interpretable R-CNN**

- intro: North Carolina State University & Alibaba
- keywords: AND-OR Graph (AOG)
- arxiv: <https://arxiv.org/abs/1711.05226>

**Domain Adaptive Faster R-CNN for Object Detection in the Wild**

- intro: CVPR 2018. ETH Zurich & ESAT/PSI
- arxiv: <https://arxiv.org/abs/1803.03243>

## Mask R-CNN

- arxiv: <http://arxiv.org/abs/1703.06870>
- github(Keras): https://github.com/matterport/Mask_RCNN
- github(Caffe2): https://github.com/facebookresearch/Detectron
- github(Pytorch): <https://github.com/wannabeOG/Mask-RCNN>
- github(MXNet): https://github.com/TuSimple/mx-maskrcnn
- github(Chainer): https://github.com/DeNA/Chainer_Mask_R-CNN

## Light-Head R-CNN

**Light-Head R-CNN: In Defense of Two-Stage Object Detector**

- intro: Tsinghua University & Megvii Inc
- arxiv: <https://arxiv.org/abs/1711.07264>
- github(offical): https://github.com/zengarden/light_head_rcnn
- github: <https://github.com/terrychenism/Deformable-ConvNets/blob/master/rfcn/symbols/resnet_v1_101_rfcn_light.py#L784>

## Cascade R-CNN

**Cascade R-CNN: Delving into High Quality Object Detection**

- arxiv: <https://arxiv.org/abs/1712.00726>
- github: <https://github.com/zhaoweicai/cascade-rcnn>

## SPP-Net

**Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition**

- intro: ECCV 2014 / TPAMI 2015
- arxiv: <http://arxiv.org/abs/1406.4729>
- github: <https://github.com/ShaoqingRen/SPP_net>
- notes: <http://zhangliliang.com/2014/09/13/paper-note-sppnet/>

**DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection**

- intro: PAMI 2016
- intro: an extension of R-CNN. box pre-training, cascade on region proposals, deformation layers and context representations
- project page: <http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html>
- arxiv: <http://arxiv.org/abs/1412.5661>

**Object Detectors Emerge in Deep Scene CNNs**

- intro: ICLR 2015
- arxiv: <http://arxiv.org/abs/1412.6856>
- paper: <https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf>
- paper: <https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf>
- slides: <http://places.csail.mit.edu/slide_iclr2015.pdf>

**segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection**

- intro: CVPR 2015
- project(code+data): <https://www.cs.toronto.edu/~yukun/segdeepm.html>
- arxiv: <https://arxiv.org/abs/1502.04275>
- github: <https://github.com/YknZhu/segDeepM>

**Object Detection Networks on Convolutional Feature Maps**

- intro: TPAMI 2015
- keywords: NoC
- arxiv: <http://arxiv.org/abs/1504.06066>

**Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction**

- arxiv: <http://arxiv.org/abs/1504.03293>
- slides: <http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf>
- github: <https://github.com/YutingZhang/fgs-obj>

**DeepBox: Learning Objectness with Convolutional Networks**

- keywords: DeepBox
- arxiv: <http://arxiv.org/abs/1505.02146>
- github: <https://github.com/weichengkuo/DeepBox>

## YOLO

**You Only Look Once: Unified, Real-Time Object Detection**

[![img](https://camo.githubusercontent.com/e69d4118b20a42de4e23b9549f9a6ec6dbbb0814/687474703a2f2f706a7265646469652e636f6d2f6d656469612f66696c65732f6461726b6e65742d626c61636b2d736d616c6c2e706e67)](https://camo.githubusercontent.com/e69d4118b20a42de4e23b9549f9a6ec6dbbb0814/687474703a2f2f706a7265646469652e636f6d2f6d656469612f66696c65732f6461726b6e65742d626c61636b2d736d616c6c2e706e67)

- arxiv: <http://arxiv.org/abs/1506.02640>
- code: <https://pjreddie.com/darknet/yolov1/>
- github: <https://github.com/pjreddie/darknet>
- blog: <https://pjreddie.com/darknet/yolov1/>
- slides: <https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p>
- reddit: <https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/>
- github: <https://github.com/gliese581gg/YOLO_tensorflow>
- github: <https://github.com/xingwangsfu/caffe-yolo>
- github: <https://github.com/frankzhangrui/Darknet-Yolo>
- github: <https://github.com/BriSkyHekun/py-darknet-yolo>
- github: <https://github.com/tommy-qichang/yolo.torch>
- github: <https://github.com/frischzenger/yolo-windows>
- github: <https://github.com/AlexeyAB/yolo-windows>
- github: <https://github.com/nilboy/tensorflow-yolo>

**darkflow - translate darknet to tensorflow. Load trained weights, retrain/fine-tune them using tensorflow, export constant graph def to C++**

- blog: <https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp>
- github: <https://github.com/thtrieu/darkflow>

**Start Training YOLO with Our Own Data**

[![img](https://camo.githubusercontent.com/2f99b692dd7ce47d7832385f3e8a6654e680d92a/687474703a2f2f6775616e6768616e2e696e666f2f626c6f672f656e2f77702d636f6e74656e742f75706c6f6164732f323031352f31322f696d616765732d34302e6a7067)](https://camo.githubusercontent.com/2f99b692dd7ce47d7832385f3e8a6654e680d92a/687474703a2f2f6775616e6768616e2e696e666f2f626c6f672f656e2f77702d636f6e74656e742f75706c6f6164732f323031352f31322f696d616765732d34302e6a7067)

- intro: train with customized data and class numbers/labels. Linux / Windows version for darknet.
- blog: <http://guanghan.info/blog/en/my-works/train-yolo/>
- github: <https://github.com/Guanghan/darknet>

**YOLO: Core ML versus MPSNNGraph**

- intro: Tiny YOLO for iOS implemented using CoreML but also using the new MPS graph API.
- blog: <http://machinethink.net/blog/yolo-coreml-versus-mps-graph/>
- github: <https://github.com/hollance/YOLO-CoreML-MPSNNGraph>

**TensorFlow YOLO object detection on Android**

- intro: Real-time object detection on Android using the YOLO network with TensorFlow
- github: <https://github.com/natanielruiz/android-yolo>

**Computer Vision in iOS – Object Detection**

- blog: <https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/>
- github:<https://github.com/r4ghu/iOS-CoreML-Yolo>

## YOLOv2

**YOLO9000: Better, Faster, Stronger**

- arxiv: <https://arxiv.org/abs/1612.08242>
- code: <http://pjreddie.com/yolo9000/>    https://pjreddie.com/darknet/yolov2/
- github(Chainer): <https://github.com/leetenki/YOLOv2>
- github(Keras): <https://github.com/allanzelener/YAD2K>
- github(PyTorch): <https://github.com/longcw/yolo2-pytorch>
- github(Tensorflow): <https://github.com/hizhangp/yolo_tensorflow>
- github(Windows): <https://github.com/AlexeyAB/darknet>
- github: <https://github.com/choasUp/caffe-yolo9000>
- github: <https://github.com/philipperemy/yolo-9000>
- github(TensorFlow): <https://github.com/KOD-Chen/YOLOv2-Tensorflow>
- github(Keras): <https://github.com/yhcc/yolo2>
- github(Keras): <https://github.com/experiencor/keras-yolo2>
- github(TensorFlow): <https://github.com/WojciechMormul/yolo2>

**darknet_scripts**

- intro: Auxilary scripts to work with (YOLO) darknet deep learning famework. AKA -> How to generate YOLO anchors?
- github: <https://github.com/Jumabek/darknet_scripts>

**Yolo_mark: GUI for marking bounded boxes of objects in images for training Yolo v2**

- github: <https://github.com/AlexeyAB/Yolo_mark>

**LightNet: Bringing pjreddie's DarkNet out of the shadows**

<https://github.com//explosion/lightnet>

**YOLO v2 Bounding Box Tool**

- intro: Bounding box labeler tool to generate the training data in the format YOLO v2 requires.
- github: <https://github.com/Cartucho/yolo-boundingbox-labeler-GUI>

**Loss Rank Mining: A General Hard Example Mining Method for Real-time Detectors**

- intro: **LRM** is the first hard example mining strategy which could fit YOLOv2 perfectly and make it better applied in series of real scenarios where both real-time rates and accurate detection are strongly demanded.
- arxiv: https://arxiv.org/abs/1804.04606

**Object detection at 200 Frames Per Second**

- intro: faster than Tiny-Yolo-v2
- arxiv: https://arxiv.org/abs/1805.06361

**Event-based Convolutional Networks for Object Detection in Neuromorphic Cameras**

- intro: YOLE--Object Detection in Neuromorphic Cameras
- arxiv:https://arxiv.org/abs/1805.07931

**OmniDetector: With Neural Networks to Bounding Boxes**

- intro: a person detector on n fish-eye images of indoor scenes（NIPS 2018）
- arxiv:https://arxiv.org/abs/1805.08503
- datasets:https://gitlab.com/omnidetector/omnidetector

## YOLOv3

**YOLOv3: An Incremental Improvement**

- arxiv:https://arxiv.org/abs/1804.02767
- paper:https://pjreddie.com/media/files/papers/YOLOv3.pdf
- code: <https://pjreddie.com/darknet/yolo/>
- github(Official):https://github.com/pjreddie/darknet
- github:https://github.com/mystic123/tensorflow-yolo-v3
- github:https://github.com/experiencor/keras-yolo3
- github:https://github.com/qqwweee/keras-yolo3
- github:https://github.com/marvis/pytorch-yolo3
- github:https://github.com/ayooshkathuria/pytorch-yolo-v3
- github:https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch
- github:https://github.com/eriklindernoren/PyTorch-YOLOv3
- github:https://github.com/ultralytics/yolov3
- github:https://github.com/BobLiu20/YOLOv3_PyTorch
- github:https://github.com/andy-yun/pytorch-0.4-yolov3
- github:https://github.com/DeNA/PyTorch_YOLOv3

## YOLT

**You Only Look Twice: Rapid Multi-Scale Object Detection In Satellite Imagery**

- intro: Small Object Detection


- arxiv:https://arxiv.org/abs/1805.09512
- github:https://github.com/avanetten/yolt

## SSD

**SSD: Single Shot MultiBox Detector**

[![img](https://camo.githubusercontent.com/ad9b147ed3a5f48ffb7c3540711c15aa04ce49c6/687474703a2f2f7777772e63732e756e632e6564752f7e776c69752f7061706572732f7373642e706e67)](https://camo.githubusercontent.com/ad9b147ed3a5f48ffb7c3540711c15aa04ce49c6/687474703a2f2f7777772e63732e756e632e6564752f7e776c69752f7061706572732f7373642e706e67)

- intro: ECCV 2016 Oral
- arxiv: <http://arxiv.org/abs/1512.02325>
- paper: <http://www.cs.unc.edu/~wliu/papers/ssd.pdf>
- slides: [http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf)
- github(Official): <https://github.com/weiliu89/caffe/tree/ssd>
- video: <http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973>
- github: <https://github.com/zhreshold/mxnet-ssd>
- github: <https://github.com/zhreshold/mxnet-ssd.cpp>
- github: <https://github.com/rykov8/ssd_keras>
- github: <https://github.com/balancap/SSD-Tensorflow>
- github: <https://github.com/amdegroot/ssd.pytorch>
- github(Caffe): <https://github.com/chuanqi305/MobileNet-SSD>

**What's the diffience in performance between this new code you pushed and the previous code? #327**

<https://github.com/weiliu89/caffe/issues/327>

## DSSD

**DSSD : Deconvolutional Single Shot Detector**

- intro: UNC Chapel Hill & Amazon Inc
- arxiv: <https://arxiv.org/abs/1701.06659>
- github: <https://github.com/chengyangfu/caffe/tree/dssd>
- github: <https://github.com/MTCloudVision/mxnet-dssd>
- demo: <http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4>

**Enhancement of SSD by concatenating feature maps for object detection**

- intro: rainbow SSD (R-SSD)
- arxiv: <https://arxiv.org/abs/1705.09587>

**Context-aware Single-Shot Detector**

- keywords: CSSD, DiCSSD, DeCSSD, effective receptive fields (ERFs), theoretical receptive fields (TRFs)
- arxiv: <https://arxiv.org/abs/1707.08682>

**Feature-Fused SSD: Fast Detection for Small Objects**

<https://arxiv.org/abs/1709.05054>

## FSSD

**FSSD: Feature Fusion Single Shot Multibox Detector**

<https://arxiv.org/abs/1712.00960>

**Weaving Multi-scale Context for Single Shot Detector**

- intro: WeaveNet
- keywords: fuse multi-scale information
- arxiv: <https://arxiv.org/abs/1712.03149>

## ESSD

**Extend the shallow part of Single Shot MultiBox Detector via Convolutional Neural Network**

<https://arxiv.org/abs/1801.05918>

**Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection**

<https://arxiv.org/abs/1802.06488>

## MDSSD

**MDSSD: Multi-scale Deconvolutional Single Shot Detector for small objects**

- arxiv: https://arxiv.org/abs/1805.07009

## Pelee

**Pelee: A Real-Time Object Detection System on Mobile Devices**

https://github.com/Robert-JunWang/Pelee

- intro: (ICLR 2018 workshop track)


- arxiv: https://arxiv.org/abs/1804.06882
- github: https://github.com/Robert-JunWang/Pelee

## Fire SSD

**Fire SSD: Wide Fire Modules based Single Shot Detector on Edge Device**

- intro:low cost, fast speed and high mAP on  factor edge computing devices


- arxiv:https://arxiv.org/abs/1806.05363

## R-FCN

**R-FCN: Object Detection via Region-based Fully Convolutional Networks**

- arxiv: <http://arxiv.org/abs/1605.06409>
- github: <https://github.com/daijifeng001/R-FCN>
- github(MXNet): <https://github.com/msracver/Deformable-ConvNets/tree/master/rfcn>
- github: <https://github.com/Orpine/py-R-FCN>
- github: <https://github.com/PureDiors/pytorch_RFCN>
- github: <https://github.com/bharatsingh430/py-R-FCN-multiGPU>
- github: <https://github.com/xdever/RFCN-tensorflow>

**R-FCN-3000 at 30fps: Decoupling Detection and Classification**

<https://arxiv.org/abs/1712.01802>

**Recycle deep features for better object detection**

- arxiv: <http://arxiv.org/abs/1607.05066>

## FPN

**Feature Pyramid Networks for Object Detection**

- intro: Facebook AI Research
- arxiv: <https://arxiv.org/abs/1612.03144>

**Action-Driven Object Detection with Top-Down Visual Attentions**

- arxiv: <https://arxiv.org/abs/1612.06704>

**Beyond Skip Connections: Top-Down Modulation for Object Detection**

- intro: CMU & UC Berkeley & Google Research
- arxiv: <https://arxiv.org/abs/1612.06851>

**Wide-Residual-Inception Networks for Real-time Object Detection**

- intro: Inha University
- arxiv: <https://arxiv.org/abs/1702.01243>

**Attentional Network for Visual Object Detection**

- intro: University of Maryland & Mitsubishi Electric Research Laboratories
- arxiv: <https://arxiv.org/abs/1702.01478>

**Learning Chained Deep Features and Classifiers for Cascade in Object Detection**

- keykwords: CC-Net
- intro: chained cascade network (CC-Net). 81.1% mAP on PASCAL VOC 2007
- arxiv: <https://arxiv.org/abs/1702.07054>

**DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling**

- intro: ICCV 2017 (poster)
- arxiv: <https://arxiv.org/abs/1703.10295>

**Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries**

- intro: CVPR 2017
- arxiv: <https://arxiv.org/abs/1704.03944>

**Spatial Memory for Context Reasoning in Object Detection**

- arxiv: <https://arxiv.org/abs/1704.04224>

**Accurate Single Stage Detector Using Recurrent Rolling Convolution**

- intro: CVPR 2017. SenseTime
- keywords: Recurrent Rolling Convolution (RRC)
- arxiv: <https://arxiv.org/abs/1704.05776>
- github: <https://github.com/xiaohaoChen/rrc_detection>

**Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection**

<https://arxiv.org/abs/1704.05775>

**LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems**

- intro: Embedded Vision Workshop in CVPR. UC San Diego & Qualcomm Inc
- arxiv: <https://arxiv.org/abs/1705.05922>

**Point Linking Network for Object Detection**

- intro: Point Linking Network (PLN)
- arxiv: <https://arxiv.org/abs/1706.03646>

**Perceptual Generative Adversarial Networks for Small Object Detection**

<https://arxiv.org/abs/1706.05274>

**Few-shot Object Detection**

<https://arxiv.org/abs/1706.08249>

**Yes-Net: An effective Detector Based on Global Information**

<https://arxiv.org/abs/1706.09180>

**SMC Faster R-CNN: Toward a scene-specialized multi-object detector**

<https://arxiv.org/abs/1706.10217>

**Towards lightweight convolutional neural networks for object detection**

<https://arxiv.org/abs/1707.01395>

**RON: Reverse Connection with Objectness Prior Networks for Object Detection**

- intro: CVPR 2017
- arxiv: <https://arxiv.org/abs/1707.01691>
- github: <https://github.com/taokong/RON>

**Mimicking Very Efficient Network for Object Detection**

- intro: CVPR 2017. SenseTime & Beihang University
- paper: <http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf>

**Residual Features and Unified Prediction Network for Single Stage Detection**

<https://arxiv.org/abs/1707.05031>

**Deformable Part-based Fully Convolutional Network for Object Detection**

- intro: BMVC 2017 (oral). Sorbonne Universités & CEDRIC
- arxiv: <https://arxiv.org/abs/1707.06175>

**Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors**

- intro: ICCV 2017
- arxiv: <https://arxiv.org/abs/1707.06399>

**Recurrent Scale Approximation for Object Detection in CNN**

- intro: ICCV 2017
- keywords: Recurrent Scale Approximation (RSA)
- arxiv: <https://arxiv.org/abs/1707.09531>
- github: <https://github.com/sciencefans/RSA-for-object-detection>

## DSOD

**DSOD: Learning Deeply Supervised Object Detectors from Scratch**

![img](https://user-images.githubusercontent.com/3794909/28934967-718c9302-78b5-11e7-89ee-8b514e53e23c.png)

- intro: ICCV 2017. Fudan University & Tsinghua University & Intel Labs China
- arxiv: <https://arxiv.org/abs/1708.01241>
- github: <https://github.com/szq0214/DSOD>
- github:https://github.com/Windaway/DSOD-Tensorflow
- github:https://github.com/chenyuntc/dsod.pytorch

**Learning Object Detectors from Scratch with Gated Recurrent Feature Pyramids**

- arxiv:https://arxiv.org/abs/1712.00886
- github:https://github.com/szq0214/GRP-DSOD

**Tiny-DSOD: Lightweight Object Detection for Resource-Restricted Usages**

- intro: BMVC 2018
- arXiv: https://arxiv.org/abs/1807.11013

**Object Detection from Scratch with Deep Supervision**

- intro: This is an extended version of DSOD
- arXiv: https://arxiv.org/abs/1809.09294

## RetinaNet

**Focal Loss for Dense Object Detection**

- intro: ICCV 2017 Best student paper award. Facebook AI Research
- keywords: RetinaNet
- arxiv: <https://arxiv.org/abs/1708.02002>

**CoupleNet: Coupling Global Structure with Local Parts for Object Detection**

- intro: ICCV 2017
- arxiv: <https://arxiv.org/abs/1708.02863>

**Incremental Learning of Object Detectors without Catastrophic Forgetting**

- intro: ICCV 2017. Inria
- arxiv: <https://arxiv.org/abs/1708.06977>

**Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection**

<https://arxiv.org/abs/1709.04347>

**StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection**

<https://arxiv.org/abs/1709.05788>

**Dynamic Zoom-in Network for Fast Object Detection in Large Images**

<https://arxiv.org/abs/1711.05187>

**Zero-Annotation Object Detection with Web Knowledge Transfer**

- intro: NTU, Singapore & Amazon
- keywords: multi-instance multi-label domain adaption learning framework
- arxiv: <https://arxiv.org/abs/1711.05954>

## MegDet

**MegDet: A Large Mini-Batch Object Detector**

- intro: Peking University & Tsinghua University & Megvii Inc
- arxiv: <https://arxiv.org/abs/1711.07240>

**Receptive Field Block Net for Accurate and Fast Object Detection**

- intro: RFBNet
- arxiv: <https://arxiv.org/abs/1711.07767>
- github: <https://github.com//ruinmessi/RFBNet>

**An Analysis of Scale Invariance in Object Detection - SNIP**

- arxiv: <https://arxiv.org/abs/1711.08189>
- github: <https://github.com/bharatsingh430/snip>

**Feature Selective Networks for Object Detection**

<https://arxiv.org/abs/1711.08879>

**Learning a Rotation Invariant Detector with Rotatable Bounding Box**

- arxiv: <https://arxiv.org/abs/1711.09405>
- github: <https://github.com/liulei01/DRBox>

**Scalable Object Detection for Stylized Objects**

- intro: Microsoft AI & Research Munich
- arxiv: <https://arxiv.org/abs/1711.09822>

**Learning Object Detectors from Scratch with Gated Recurrent Feature Pyramids**

- arxiv: <https://arxiv.org/abs/1712.00886>
- github: <https://github.com/szq0214/GRP-DSOD>

**Deep Regionlets for Object Detection**

- keywords: region selection network, gating network
- arxiv: <https://arxiv.org/abs/1712.02408>

**Training and Testing Object Detectors with Virtual Images**

- intro: IEEE/CAA Journal of Automatica Sinica
- arxiv: <https://arxiv.org/abs/1712.08470>

**Large-Scale Object Discovery and Detector Adaptation from Unlabeled Video**

- keywords: object mining, object tracking, unsupervised object discovery by appearance-based clustering, self-supervised detector adaptation
- arxiv: <https://arxiv.org/abs/1712.08832>

**Spot the Difference by Object Detection**

- intro: Tsinghua University & JD Group
- arxiv: <https://arxiv.org/abs/1801.01051>

**Localization-Aware Active Learning for Object Detection**

- arxiv: <https://arxiv.org/abs/1801.05124>

**Object Detection with Mask-based Feature Encoding**

- arxiv: <https://arxiv.org/abs/1802.03934>

**LSTD: A Low-Shot Transfer Detector for Object Detection**

- intro: AAAI 2018
- arxiv: <https://arxiv.org/abs/1803.01529>

**Pseudo Mask Augmented Object Detection**

<https://arxiv.org/abs/1803.05858>

**Revisiting RCNN: On Awakening the Classification Power of Faster RCNN**

<https://arxiv.org/abs/1803.06799>

**Learning Region Features for Object Detection**

- intro: Peking University & MSRA
- arxiv: <https://arxiv.org/abs/1803.07066>

**Single-Shot Bidirectional Pyramid Networks for High-Quality Object Detection**

- intro: Singapore Management University & Zhejiang University
- arxiv: <https://arxiv.org/abs/1803.08208>

**Object Detection for Comics using Manga109 Annotations**

- intro: University of Tokyo & National Institute of Informatics, Japan
- arxiv: <https://arxiv.org/abs/1803.08670>

**Task-Driven Super Resolution: Object Detection in Low-resolution Images**

- arxiv: <https://arxiv.org/abs/1803.11316>

**Transferring Common-Sense Knowledge for Object Detection**

- arxiv: <https://arxiv.org/abs/1804.01077>

**Multi-scale Location-aware Kernel Representation for Object Detection**

- intro: CVPR 2018
- arxiv: <https://arxiv.org/abs/1804.00428>
- github: <https://github.com/Hwang64/MLKP>


**Loss Rank Mining: A General Hard Example Mining Method for Real-time Detectors**

- intro: National University of Defense Technology
- arxiv: https://arxiv.org/abs/1804.04606

**Robust Physical Adversarial Attack on Faster R-CNN Object Detector**

- arxiv: https://arxiv.org/abs/1804.05810

## RefineNet

**Single-Shot Refinement Neural Network for Object Detection**

- intro: CVPR 2018

- arxiv: <https://arxiv.org/abs/1711.06897>
- github: <https://github.com/sfzhang15/RefineDet>
- github: https://github.com/lzx1413/PytorchSSD
- github: https://github.com/ddlee96/RefineDet_mxnet
- github: https://github.com/MTCloudVision/RefineDet-Mxnet

## DetNet

**DetNet: A Backbone network for Object Detection**

- intro: Tsinghua University & Face++
- arxiv: https://arxiv.org/abs/1804.06215


## SSOD

**Self-supervisory Signals for Object Discovery and Detection**

- Google Brain
- arxiv:https://arxiv.org/abs/1806.03370

## CornerNet

**CornerNet: Detecting Objects as Paired Keypoints**

- intro: ECCV 2018
- arXiv: https://arxiv.org/abs/1808.01244
- github: <https://github.com/umich-vl/CornerNet>

## M2Det

**M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network**

- intro: AAAI 2019
- arXiv: https://arxiv.org/abs/1811.04533
- github: https://github.com/qijiezhao/M2Det

## 3D Object Detection

**3D Backbone Network for 3D Object Detection**

- arXiv: https://arxiv.org/abs/1901.08373

**LMNet: Real-time Multiclass Object Detection on CPU using 3D LiDARs**

- arxiv: https://arxiv.org/abs/1805.04902
- github: https://github.com/CPFL/Autoware/tree/feature/cnn_lidar_detection


## ZSD（Zero-Shot Object Detection）

**Zero-Shot Detection**

- intro: Australian National University
- keywords: YOLO
- arxiv: <https://arxiv.org/abs/1803.07113>

**Zero-Shot Object Detection**

- arxiv: https://arxiv.org/abs/1804.04340

**Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts**

- arxiv: https://arxiv.org/abs/1803.06049

**Zero-Shot Object Detection by Hybrid Region Embedding**

- arxiv: https://arxiv.org/abs/1805.06157

## OSD（One-Shot Object Detection）

**Comparison Network for One-Shot Conditional Object Detection**

- arXiv: https://arxiv.org/abs/1904.02317

**One-Shot Object Detection**

RepMet: Representative-based metric learning for classification and one-shot object detection

- intro: IBM Research AI
- arxiv:https://arxiv.org/abs/1806.04728
- github: TODO

## Weakly Supervised Object Detection

**Weakly Supervised Object Detection in Artworks**

- intro: ECCV 2018 Workshop Computer Vision for Art Analysis
- arXiv: https://arxiv.org/abs/1810.02569
- Datasets: https://wsoda.telecom-paristech.fr/downloads/dataset/IconArt_v1.zip

**Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation**

- intro: CVPR 2018
- arXiv: https://arxiv.org/abs/1803.11365
- homepage: https://naoto0804.github.io/cross_domain_detection/
- paper: http://openaccess.thecvf.com/content_cvpr_2018/html/Inoue_Cross-Domain_Weakly-Supervised_Object_CVPR_2018_paper.html
- github: https://github.com/naoto0804/cross-domain-detection

## Softer-NMS

**《Softer-NMS: Rethinking Bounding Box Regression for Accurate Object Detection》**

- intro: CMU & Face++
- arXiv: https://arxiv.org/abs/1809.08545
- github: https://github.com/yihui-he/softer-NMS

## 2019

**Feature Selective Anchor-Free Module for Single-Shot Object Detection**

- intro: CVPR 2019

- arXiv: https://arxiv.org/abs/1903.00621

**Object Detection based on Region Decomposition and Assembly**

- intro: AAAI 2019

- arXiv: https://arxiv.org/abs/1901.08225

**Bottom-up Object Detection by Grouping Extreme and Center Points**

- intro: one stage 43.2% on COCO test-dev
- arXiv: https://arxiv.org/abs/1901.08043
- github: https://github.com/xingyizhou/ExtremeNet

**ORSIm Detector: A Novel Object Detection Framework in Optical Remote Sensing Imagery Using Spatial-Frequency Channel Features**

- intro: IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

- arXiv: https://arxiv.org/abs/1901.07925

**Consistent Optimization for Single-Shot Object Detection**

- intro: improves RetinaNet from 39.1 AP to 40.1 AP on COCO datase

- arXiv: https://arxiv.org/abs/1901.06563

**Learning Pairwise Relationship for Multi-object Detection in Crowded Scenes**

- arXiv: https://arxiv.org/abs/1901.03796

**RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free**

- arXiv: https://arxiv.org/abs/1901.03353
- github: https://github.com/chengyangfu/retinamask

**Region Proposal by Guided Anchoring**

- intro: CUHK - SenseTime Joint Lab
- arXiv: https://arxiv.org/abs/1901.03278

**Scale-Aware Trident Networks for Object Detection**

- intro: mAP of **48.4** on the COCO dataset
- arXiv: https://arxiv.org/abs/1901.01892

## 2018

**Large-Scale Object Detection of Images from Network Cameras in Variable Ambient Lighting Conditions**

- arXiv: https://arxiv.org/abs/1812.11901

**Strong-Weak Distribution Alignment for Adaptive Object Detection**

- arXiv: https://arxiv.org/abs/1812.04798

**AutoFocus: Efficient Multi-Scale Inference**

- intro: AutoFocus obtains an **mAP of 47.9%** (68.3% at 50% overlap) on the **COCO test-dev** set while processing **6.4 images per second on a Titan X (Pascal) GPU** 
- arXiv: https://arxiv.org/abs/1812.01600

**NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object Detection**

- intro: Google Could
- arXiv: https://arxiv.org/abs/1812.00124

**SPLAT: Semantic Pixel-Level Adaptation Transforms for Detection**

- intro: UC Berkeley
- arXiv: https://arxiv.org/abs/1812.00929

**Grid R-CNN**

- intro: SenseTime
- arXiv: https://arxiv.org/abs/1811.12030

**Deformable ConvNets v2: More Deformable, Better Results**

- intro: Microsoft Research Asia

- arXiv: https://arxiv.org/abs/1811.11168

**Anchor Box Optimization for Object Detection**

- intro: Microsoft Research
- arXiv: https://arxiv.org/abs/1812.00469

**Efficient Coarse-to-Fine Non-Local Module for the Detection of Small Objects**

- intro: https://arxiv.org/abs/1811.12152

**NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object Detection**

- arXiv: https://arxiv.org/abs/1812.00124

**Learning RoI Transformer for Detecting Oriented Objects in Aerial Images**

- arXiv: https://arxiv.org/abs/1812.00155

**Integrated Object Detection and Tracking with Tracklet-Conditioned Detection**

- intro: Microsoft Research Asia
- arXiv: https://arxiv.org/abs/1811.11167

**Deep Regionlets: Blended Representation and Deep Learning for Generic Object Detection**

- arXiv: https://arxiv.org/abs/1811.11318

 **Gradient Harmonized Single-stage Detector**

- intro: AAAI 2019
- arXiv: https://arxiv.org/abs/1811.05181

**CFENet: Object Detection with Comprehensive Feature Enhancement Module**

- intro: ACCV 2018
- github: https://github.com/qijiezhao/CFENet

**DeRPN: Taking a further step toward more general object detection**

- intro: AAAI 2019
- arXiv: https://arxiv.org/abs/1811.06700
- github: https://github.com/HCIILAB/DeRPN

**Hybrid Knowledge Routed Modules for Large-scale Object Detection**

- intro: Sun Yat-Sen University & Huawei Noah’s Ark Lab
- arXiv: https://arxiv.org/abs/1810.12681
- github: https://github.com/chanyn/HKRM

**《Receptive Field Block Net for Accurate and Fast Object Detection》**

- intro: ECCV 2018
- arXiv: [https://arxiv.org/abs/1711.07767](https://arxiv.org/abs/1711.07767)
- github: [https://github.com/ruinmessi/RFBNet](https://github.com/ruinmessi/RFBNet)

**Deep Feature Pyramid Reconfiguration for Object Detection**

- intro: ECCV 2018
- arXiv: https://arxiv.org/abs/1808.07993

**Unsupervised Hard Example Mining from Videos for Improved Object Detection**

- intro: ECCV 2018
- arXiv: https://arxiv.org/abs/1808.04285

**Acquisition of Localization Confidence for Accurate Object Detection**

- intro: ECCV 2018
- arXiv: https://arxiv.org/abs/1807.11590
- github: https://github.com/vacancy/PreciseRoIPooling

**Toward Scale-Invariance and Position-Sensitive Region Proposal Networks**

- intro: ECCV 2018
- arXiv: https://arxiv.org/abs/1807.09528

**MetaAnchor: Learning to Detect Objects with Customized Anchors**

- arxiv: https://arxiv.org/abs/1807.00980

**Relation Network for Object Detection**

- intro: CVPR 2018
- arxiv: https://arxiv.org/abs/1711.11575
- github:https://github.com/msracver/Relation-Networks-for-Object-Detection

**Quantization Mimic: Towards Very Tiny CNN for Object Detection**

- Tsinghua University1 & The Chinese University of Hong Kong2 &SenseTime3
- arxiv: https://arxiv.org/abs/1805.02152

**Learning Rich Features for Image Manipulation Detection**

- intro: CVPR 2018 Camera Ready
- arxiv: https://arxiv.org/abs/1805.04953

**SNIPER: Efficient Multi-Scale Training**

- arxiv:https://arxiv.org/abs/1805.09300
- github:https://github.com/mahyarnajibi/SNIPER

**Soft Sampling for Robust Object Detection**

- intro: the robustness of object detection under the presence of missing annotations
- arxiv:https://arxiv.org/abs/1806.06986

**Cost-effective Object Detection: Active Sample Mining with Switchable Selection Criteria**

- intro: TNNLS 2018
- arxiv:https://arxiv.org/abs/1807.00147
- code: http://kezewang.com/codes/ASM_ver1.zip

## Other

**R3-Net: A Deep Network for Multi-oriented Vehicle Detection in Aerial Images and Videos**

- arxiv: https://arxiv.org/abs/1808.05560
- youtube: https://youtu.be/xCYD-tYudN0

# Detection Toolbox

- [Detectron(FAIR)](https://github.com/facebookresearch/Detectron): Detectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including [Mask R-CNN](https://arxiv.org/abs/1703.06870). It is written in Python and powered by the [Caffe2](https://github.com/caffe2/caffe2) deep learning framework.
- [Detectron2](https://github.com/facebookresearch/detectron2): Detectron2 is FAIR's next-generation research platform for object detection and segmentation.
- [maskrcnn-benchmark(FAIR)](https://github.com/facebookresearch/maskrcnn-benchmark): Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch.
- [mmdetection(SenseTime&CUHK)](https://github.com/open-mmlab/mmdetection): mmdetection is an open source object detection toolbox based on PyTorch. It is a part of the open-mmlab project developed by [Multimedia Laboratory, CUHK](http://mmlab.ie.cuhk.edu.hk/).
","['amusi', 'wkevin', 'haroldo-ok', 'skrish13', 'ethanhe42', 'ZhangXinNan']",0,0.64,0,,,,,,434,,,guruucsd,ucsd-ets
4989765,MDEwOlJlcG9zaXRvcnk0OTg5NzY1,archivesspace,archivesspace/archivesspace,0,archivesspace,https://github.com/archivesspace/archivesspace,"ArchivesSpace, the archives management tool",0,2012-07-11 14:00:20+00:00,2025-03-06 15:54:09+00:00,2025-03-07 13:25:49+00:00,http://archivesspace.org/,290051,363,363,Ruby,1,1,1,0,1,0,232,0,0,85,other,1,0,0,public,232,85,363,master,1,1,"ArchivesSpace 
--------------------

[![Translation status](https://hosted.weblate.org/widgets/archivesspace-interfaces/-/svg-badge.svg)](https://hosted.weblate.org/engage/archivesspace-interfaces/)


Built for archives by archivists, ArchivesSpace is the open source archives information management application for managing and providing web access to archives, manuscripts and digital objects.

[Build ArchivesSpace release package](https://github.com/archivesspace/archivesspace/workflows/Build%20ArchivesSpace%20release%20package/badge.svg?branch=master)

* [archivesspace.org](http://archivesspace.org)
* [User Documentation](https://archivesspace.atlassian.net/wiki/spaces/ADC/pages/917045261/ArchivesSpace+Help+Center) (member benefit)
* [Technical Documentation](https://docs.archivesspace.org/)
* [API](http://archivesspace.github.io/archivesspace/api)
* [Wiki](http://wiki.archivesspace.org)
* [Issue Tracker](http://development.archivesspace.org)

The latest technical documentation is managed in a separate GitHub repository [ArchivesSpace tech-docs](https://github.com/archivesspace/tech-docs) and is published at
(https://docs.archivesspace.org/).

# License

ArchivesSpace is released under the [Educational Community License, version 2.0](https://opensource.org/license/ecl-2-0/). See the [COPYING](COPYING) file for more information.

# Credits

ArchivesSpace (https://archivesspace.org) is community-supported software whose ongoing development is led, managed, implemented and funded by a diverse community of users, developers, and administrators all working together to achieve the goal of helping archives and cultural heritage institutions better manage and provide access to their collections.

ArchivesSpace 1.0 (released in 2013) was developed by [Hudson Molonglo](http://www.hudsonmolonglo.com) in partnership with the New York University Libraries, UC San Diego Libraries, and University of Illinois Urbana-Champaign Library and with
funding from the Andrew W. Mellon Foundation, organizational support from Lyrasis, and contributions from diverse persons in the archives community.

# Membership

ArchivesSpace is free to download and use, but has a membership model to promote sustainability and the continuing development of the application and to support those who use it. Learn more about [ArchivesSpace membership and its benefits](https://archivesspace.org/community/member-benefits) on our website. Reach out to the program team at ArchivesSpaceHome@lyrasis.org if you are interested in becoming an ArchivesSpace member.

# Reporting an Issue

All users are welcome to submit bug reports and requests for new features for consideration by our member community. Instructions are available on our wiki.

* [How to Report a Bug](https://archivesspace.atlassian.net/wiki/spaces/ADC/pages/19202056/How+to+Report+a+Bug)
* [How to Request a New Feature](https://archivesspace.atlassian.net/wiki/spaces/ADC/pages/19202060/How+to+Request+a+New+Feature)
","['marktriggs', 'payten', 'lmcglohon', 'quoideneuf', 'cfitz', 'lorawoodford', 'avatar382', 'jambun', 'mark-cooper', 'bobbi-SMR', 'brianzelip', 'thimios', 'cdibella', 'donaldjosephsmith', 'anarchivist', 'nebulon42', 'andrew-morrison', 'Blake-', 'pobocks', 'Perticus', 'alexduryee', 'weblate', 'brialparker', 'gwiedeman', 'fordmadox', 'sdm7g', 'abbistani', 'crugas', 'jdcrouch', 'michael-lts', 'jdshaw', 'nbut1er', 'AustinTSchaffer', 'markowitzj-si', 'sephirothkod', 'Vchouliaras', 'ktamaral', 'walkerdb', 'tanguac', 'ph448', 'summer-cook', 'stitching-coder', 'trevorthornton', 'cposton', 'ives1227', 'tdilauro', 'eshadatta', 'orangewolf', 'mredar', 'eightBitter', 'jazairi', 'sarontt', 'martinlovell', 'jlj5aj', 'kardeiz', 'lfarrell', 'tingletech', 'reeset', 'rshanrath', 'macasaurusrex', 'dsteelma-umd', 'bdwestbrook', 'fidothe', 'joshk', 'DonRichards', 'MichaelRBond', 'mistydemeo', 'ssciolla', 'SeanGaia', 'smoore4moma', 'shorock', 'dependabot[bot]', 'dvhassel', 'GarrettArm', 'knjko', 'luismart', 'uetuluk', 'vscripty', 'eckardm', 'waffle-iron', 'kspurgin', 'jonathangreen', 'jeremyf', 'jasloe', 'helrond', 'FredReiss', 'clayoster', 'cvonkleist', 'broosa', 'bradspry', 'Atul9', 'aprilrieger']",1,0.81,21032,"# ArchivesSpace Code of Conduct

The ArchivesSpace Code of Conduct covers our behavior as members of the ArchivesSpace Community in any forum, including the ArchivesSpace listservs, Slack channels, wiki, GitHub, remote and in-person forums, webinars and meetings, and private correspondence related to ArchivesSpace. ArchivesSpace is an inclusive, friendly and safe community, committed to openness and transparency in all interactions and activities.

To review the full ArchivesSpave Code of Conduct and guidelines to report violations visit https://archivesspace.org/archivesspace-code-of-conduct 
",,"# ArchivesSpace Security Issues Policy

ArchivesSpace is an open source application that depends on a large number of
third party libraries and systems. While ArchivesSpace is developed using coding best practices and is routinely scanned for security issues, it is possible for ArchivesSpace or some components of ArchivesSpace to contain security vulnerabilities that would 
allow unexpected or dangerous behavior to be triggered. This policy explains how to report security issues.

## Reporting vulnerabilities and other security issues

Please email reports about any security-related issues you find to
`archivesspacehome@lyrasis.org`. Your email will be acknowledged as soon
as possible.

Please use a descriptive subject line for your report email. After the initial
reply to your report, ArchivesSpace developers will keep you informed of
the progress being made towards a fix. If you are able to patch the issue locally please consider contributing the code so that the larger community can benefit.

Please include the following information along in your report:

* Your name and affiliation (if any).
* A description of the technical details of the vulnerabilities. It is very
  important to let us know how we can reproduce your findings.
* An explanation of who can exploit this vulnerability and what they gain when
  doing so -- write an attack scenario, if you can. This will help us evaluate your report
  quickly, especially if the issue is complex.
* Whether this vulnerability is already public or known to third parties. If it is, please
  provide details.
* If the vulnerability was discovered by a specific scan or scanning tool, please include a copy of the report when possible.

## Supported versions

Please note that when security issues are found, only the latest version of the application will be patched. Upgrade to the latest release of ArchivesSpace to ensure having all security updates.
",Directory exists,"<!--- Provide a general summary of your changes in the Title above -->

## Description
<!--- Describe your changes in detail -->
<!--- Why is this change required? What problem does it solve? -->

## Related JIRA Ticket or GitHub Issue
<!--- Please link to the JIRA Ticket or GitHub Issue here: -->

## How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->

## Screenshots (if appropriate):

## Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [ ] My code follows the code style of this project.
- [ ] My change requires a change to the documentation.
- [ ] I have read the **CONTRIBUTING** document.
- [ ] I have authority to submit this code.
- [ ] I have added tests to cover my changes.
- [ ] All new and existing tests passed.
",73,,,,
170016616,MDEwOlJlcG9zaXRvcnkxNzAwMTY2MTY=,Awesome-Autonomous-Driving,autonomousdrivingkr/Awesome-Autonomous-Driving,0,autonomousdrivingkr,https://github.com/autonomousdrivingkr/Awesome-Autonomous-Driving,,0,2019-02-10 19:21:45+00:00,2025-03-03 17:17:45+00:00,2023-09-28 19:25:40+00:00,,129,574,574,,1,1,1,1,0,0,150,0,0,1,,1,0,0,public,150,1,574,master,1,,,"['autonomousdrivingkr', 'captainzone', 'JustWon', 'harderthan', 'bhyim516', 'jihoonl', 'layumi']",0,0.73,0,,,,,,35,,,,
174594172,MDEwOlJlcG9zaXRvcnkxNzQ1OTQxNzI=,CAN_Reverse_Engineering,brent-stone/CAN_Reverse_Engineering,0,brent-stone,https://github.com/brent-stone/CAN_Reverse_Engineering,Automated Payload Reverse Engineering Pipeline for the Controller Area Network (CAN) protocol,0,2019-03-08 19:10:03+00:00,2025-03-04 14:41:45+00:00,2024-04-27 22:40:25+00:00,,29002,406,406,Python,1,1,1,1,0,0,66,0,0,5,gpl-3.0,1,0,0,public,66,5,406,master,1,,"# Automated CAN Payload Reverse Engineering

## NOTICE
> The views expressed in this document and code are those of the author and do not reflect the official policy or position of the United States Air Force, the United States Army, the United States Department of Defense or the United States Government. This material is declared a work of the U.S. Government and is not subject to copyright protection in the United States. Approval for public disclosure of this code was approved by the 88th Air Base Wing Public Affairs on 08 March 2019 under case number 88ABW-2019-0910. Unclassified disclosure of the dissertation was approved on 03 January 2019 under case number 88ABW-2019-0024.
-----------------------------------------------------------------------------------------

This project houses Python and R scripts intended to facilitate the automated reverse engineering of Controller Area Network (CAN) payloads observed from passenger vehicles. This code was originally developed by Dr. Brent Stone at the Air Force Institute of Technology in pursuit of a Doctor of Philosophy in Computer Science. Please see the included dissertation titled ""Enabling Auditing and Intrusion Detection for Proprietary Controller Area Networks"" for details about the methods used. Please open an issue letting me know if you find any typos, bad grammar, your copyrighted images you want removed, or other issues!

Special thank you to Dave Blundell, co-author of the Car Hacker's Handbook, and the Open Garages community for technical advice and serving as a sounding board.

## Tips and Advice
These scripts won't run immediately when cloning this repo. Hopefully these tips will save you time and frustration saying ""WHY WONT THESE THINGS WORK!?!?!"" Please ask questions by posting in the [Open Garages Google group](https://groups.google.com/forum/#!forum/open-garages). These scripts were developed and tested using Python 3.6. Please make sure you have the Numpy, Pandas, & scikit-learn packages available to your Python Interpreter.


The files are organized with an example CAN data sample and three folders. Each folder is a self-contained set of interdependent Python classes or R scripts for examining CAN data in the format shown in the example loggerProgram0.log. Different file formats can be used by adjusting PreProcessor.py accordingly.

* Folder 1: **Pipeline**
  * Simply copy loggerProgram0.log into this folder and run main.py.
  * This is the most basic implementation of the pipeline described in the dissertation. Over 80% of the code is referenced from main.py. Follow the calls made in main.py to see how the data are sequentially processed and saved to disk.
  * The remaining 20% is unused portions of code which were left in place to either serve as a reference for different ways of doing things in Python or interesting experiments which were worth preserving (like the Smith-Waterman search).

* Folder 2: **Pipeline_multi-file**
  * This is the most complete and robust implementation of the concepts presented in the dissertation; however, the code is also more complicated to enable automated processing of many CAN data samples at one time. If you aren't already very comfortable with Python and Pandas, make sure you understand how the scripts in the **Pipeline** folder work before attempting to go through this expanded version of the code.

  * This folder includes the same classes from **Pipeline**. However, **SOME BUGS WERE FIXED HERE** but **NOT** in the classes saved in **Pipeline**. If a generous soul wants to transplant the fixes back into **Pipeline**, I will happily merge the fork.

  * Make sure you read the comments about the expected folder structure!

* Folder 3: **R Scripts**
  * The R scripts require the [rEDM](https://CRAN.R-project.org/package=rEDM) package. Look for commands_list.txt for a sequential series of R commands. For more information about EDM, see U.C. San Diego's Sugihara Lab homepage: https://deepeco.ucsd.edu/.

  * The folders ""city"" and ""home"" include .csv files of engine RPM, brake pressure, and vehicle speed time series during different driving conditions. Each folder includes a ""commands_list_####.txt"" file for copy-paste R commands to analyze this data using the rEDM package.

  * .Rda files and .pdf graphical output are examples of output using the R commands and provided .csv data.
 

[APRIL 2020 UPDATE]
Will Freeman added support for command line arguments and can-utils log format pre-processing.
Usage is:

Example use with can-utils log format
python Main.py -c inputFile.log

python Main.py --can-utils inputFile.log

Example use with original format
python Main.py originalFormat.log

Example use with ./loggerProgram0.log
python Main.py
  
## Script specific information by folder
### Pipeline
**Input**: CAN data in the format demonstrated in loggerProgram0.log
* **Main.py**
  1. **Purpose**: This script links and calls all remaining scripts in this folder. It handles some ‘global’ variables used for modifying the flow of data between scripts as well as any files output to the local hard disk.
* **PreProcessor.py**
  1. **Purpose**: This script is responsible for reading in .log files and converting them to a runtime data structure known as a Pandas Data Frame. Some ‘data cleaning’ is also performed by this script. The output is a dictionary data structure containing ArbID runtime objects based on the class defined in **ArbID.py**. **J1979.py** is called to attempt to identify and extract data in the Data Frame related to the SAE J1979 standard. J1979 is a public communications standard so this data does not need to be specially analyzed by the following scripts.
* **LexicalAnalysis.py**
  1. **Purpose**: This script is responsible for making an educated guess about the time series data present in the Data Frame and ArbID dictionary created by **PreProcessor.py**. Individual time series are recorded using a dictionary of Signal runtime objects based on the class defined in **Signal.py**.
* **SemanticAnalysis.py**
  1. **Purpose**: This script generates a correlation matrix of Signal time series produced by **LexicalAnalysis.py**. That correlation matrix is then used to cluster Signal time series using an open source implementation of a Hierarchical Clustering algorithm.
* **Plotter.py**
  1. **Purpose**: This script uses an open source plotting library to produce visualizations of the groups of Signal time series and J1979 time series produced by the previous scripts.

**Output**: This series of scripts produces an array of output depending on the global variables defined in **Main.py**. This output may include the following:
*	‘Pickle’ files of the runtime dictionary and Data Frame objects using the open source Pickle library for Python. These files simply speed up repeated execution of the Python scripts when the same .log file is used for input to **Main.py**.
* Comma separated value (.csv) plain text files of the correlation matrix between time series data present in the .log file.
* Graphics of scatter-plots of the time series present in the .log file.
* A graphic of the dendrogram produced during Hierarchical Clustering in **SemanticAnalysis.py**. A dendrogram is a well-documented method for visualizing the results of Hierarchical Clustering algorithms.


### Pipeline_multi-file
**Input**: CAN data in the format demonstrated in loggerProgram0.log. 
* **Main.py** and the other identically named scripts from **Pipeline** have been updated to allow the scripts to automatically import and process multiple .log files.
* **FileBoi.py**
  1. **Purpose**: This is a series of functions which handle the logistics of searching for and reading in data from multiple .log files.
* **Sample.py**
  1. **Purpose**: Much of the functionality present in **Main.py** in **Pipeline** has been moved into this script. This works in conjunction with **FileBoi.py** to handle the logistics of working with multiple .log files.
* **SampleStats.py**
  1. **Purpose**: This script produces and records a series of basic statistics about a particular .log file.
* **Validator.py**
  1. **Purpose**: This script performs a common machine learning validation technique called a ‘train-test split’ to quantify the consistency of the output of **LexicalAnalysis.py** and **SemanticAnalysis.py**. This was used in conjunction with **SampleStats.py** to produce quantifiable findings for research papers and the dissertation.
**Output**: The output of **Pipeline_multi-file** is the same as **Pipeline** but organized according to the file structure used to store the set of .log files used as input. **SampleStats.py** and **Validator.py** also produce some additional statistical metrics regarding each .log file.

### R
**Input**: Plain-text .csv files containing time series data such as those included in this folder. 
* **commands_list.txt, commands_list_city.txt, commands_list_home.txt**
  1. **Purpose**: This is a list of R commands for the publically available rEDM package. The intent is to perform analysis of the time series according to the rEDM user guide. Each version is highly similar and customized only to point to a different .csv file for input and .pdf file to visualize the output.


**Output**:
* .Rda files
  1. **Purpose**: These are machine readable files for storing R Data Frame objects to disk. All of these files were generated using the operations listed in commands_list.txt, commands_list_city.txt, commands_list_home.txt, and the provided .csv files.
* .pdf files
  1. **Purpose**: These are visualizations of the output of the R commands using the provided .csv files.
","['brent-stone', 'frillweeman', 'JoshuaArking', 'Loris1123', 'mitchdetailed']",0,0.7,0,,,,,,39,,,,
67053564,MDEwOlJlcG9zaXRvcnk2NzA1MzU2NA==,Brick,BrickSchema/Brick,0,BrickSchema,https://github.com/BrickSchema/Brick,Uniform metadata schema for buildings,0,2016-08-31 16:11:29+00:00,2025-03-06 15:37:34+00:00,2025-03-05 19:42:15+00:00,http://brickschema.org/,16115,315,315,Python,1,1,1,1,0,0,81,0,0,90,bsd-3-clause,1,0,0,public,81,90,315,master,1,1,"# Brick

[![Build Status](https://github.com/BrickSchema/Brick/workflows/Build/badge.svg)](https://github.com/BrickSchema/Brick/actions)
[![Python 3.6](https://img.shields.io/badge/python-3.6+-blue.svg)](https://www.python.org/downloads/release/python-360/)

Brick is an open-source, BSD-licensed development effort to create a uniform schema for representing metadata in buildings. Brick has three components:

* An RDF class hierarchy describing the various building subsystems and the entities and equipment therein
* A minimal, principled set of relationships for connecting these entities together into a directed graph representing a building
* A method of encapsulation for composing complex components from a set of lower-level ones

The official Brick website, [http://brickschema.org/](http://brickschema.org/), contains documentation and other information about the Brick schema.

This repository tracks the main schema development of Brick.


## Discussion

Discussion takes place primarily on the Brick User Forum: [https://groups.google.com/forum/#!forum/brickschema](https://groups.google.com/forum/#!forum/brickschema)

## Questions and Issues

If you have an issue with Brick's coverage, utility or usability, or any other Brick-related question:

1. First check the [Brick user forum](https://groups.google.com/forum/#!forum/brickschema) and the [Brick issue tracker](https://github.com/BuildSysUniformMetadata/Brick/issues)
   to check if anyone has asked your question already.
2. If you find a previously submitted issue that closely mirrors your own, feel free to jump in on the conversation. Otherwise, please file a new issue or submit a new thread on the forum.

## Examples

The `examples/` directory contains executable code samples with extensive documentation that introduce Brick concepts and idioms.

- `example1`: getting familiar with RDFlib, namespaces, Brick models and when and when not to import the Brick ontology definition
- `simple_apartment`: uses Python to programmatically build a Brick model of a small apartment
- `g36`: contains Brick implementations of several figures from ASHRAE Guideline 36

## Versioning

Brick uses a semantic versioning scheme for its version numbers: `major.minor.patch`. The [releases page](https://github.com/BrickSchema/Brick/releases) contains links to each published Brick release for easy download.

We target a minor version release (e.g. `1.1`, `1.2`, `1.3`) roughly every 6 months. Minor releases will contain largely backwards-compatible extensions and new features to the ontology. Due to the significance of these changes, minor releases will be developed in their own branch; PRs for those releases will be merged into the minor version branch, and then ultimately merged into the main branch when the minor release is published.

Patch releases (e.g. `1.2.1`, `1.2.2`) contain smaller, incremental, backwards-compatible changes to the ontology. Commits and PRs for the next patch release will be merged directly into `master`. Every evening, a `nightly` build is produced containing the latest commits. **There may be bugs or errors in the nightly release**, however these bugs will be removed by the time a patch release is published.

## How To Contribute

See [CONTRIBUTING.md](https://github.com/BrickSchema/Brick/blob/master/CONTRIBUTING.md)

## Tests

Tests go in the `tests/` directory and should be implemented using [pytest](https://pytest.readthedocs.io/en/latest/getting-started.html#getstarted).
[`tests/test_inference.py`](https://github.com/BrickSchema/Brick/blob/master/tests/test_inference.py) is a good example.

Run tests by executing `pytest` or `make test` in the top-level directory of this repository.
* Before running `pytest` the Brick.ttl file needs to be created using either `make` or `python generate_brick.py`.

## Python Framework

Rather than getting lost in the Sisyphean bikeshedding of how to format everything as YAML, we're
just using Python dictionaries so we don't have to worry about any (well, not that much) parsing logic.

For now, the code is the documentation. Look at `bricksrc/equipment.py`, `bricksrc/point.py`, etc. for examples and how to add to each of the class hierarchies.

## Other Tools

### Version Comparison

We can track the different classes between versions. The below scripts produces comparison files.
- `python tools/compare_versions/compare_versions.py --oldbrick 1.0.3 https://brickschema.org/schema/1.0.3/Brick.ttl --newbrick 1.1.0 ./Brick.ttl`

It will produce three files inside `history/{old_version}-{new_version}`.
- `added_classes.txt`: A list of new classes introduced in the current version compared to the previous version.
- `removed_classes.txt`: A list of old classes removed in the current version compared to the previous version.
- `possible_mapping.json`: A map of candidate classes that can replace removed classes. Keys are removed classes and the values are candidate correspondants in the new vesion.


---

*The Brick Development Team was initiated to produce a unified metadata schema for resources in buildings in early 2016. The team consists of members from IBM; Carnegie Mellon University; University of California, Berkeley; University of California, Los Angeles; University of California, San Diego; University of Virginia and University of Southern Denmark.*
","['gtfierro', 'jbkoh', 'connorjcantrell', 'shreyasnagare', 'KiithNabaal', 'steveraysteveray', 'metesaka', 'Joern-Ploennigs', 'carlosduarteroa', 'ektrah', 'anil72007', 'jbulow', 'epaulson', 'jbulow-se', 'xyzisinus', 'blip2', 'GeorgFerdinandSchneider', 'corymosiman12', 'wcrd', 'hammar', 'dependabot[bot]', 'Gamecock', 'JoelBender', 'hicksjacobp', 'filiphl', 'david-waterworth', 'danjhugo', 'shatdal']",1,0.77,5191,,"# Contributing to Brick

This document is a set of guidelines and resources for contributing to the Brick effort.
Improvements and changes to this document are always welcome via [Pull Request](https://github.com/BrickSchema/Brick/pulls)

## Asking Questions

If you have a question about Brick or its related tools, it is recommended to make a post in the [Brick User Forum](https://groups.google.com/forum/#!forum/brickschema) or under the [Brick GitHub Issue Tracker](https://github.com/BrickSchema/Brick/issues). If you have a question about the website, please file the question either in the User Forum or on the [Brick Website Issue Tracker](https://github.com/BrickSchema/Brick/issues).

Please conduct a brief search to see if someone has asked your question already; if they have, feel free to jump into the conversation. Otherwise, please file a new issue or make a new post on the forum.

## Reporting Bugs and Issues

Reporting of bugs and issues should be done on the [Brick GitHub Issue Tracker](https://github.com/BrickSchema/Brick/issues). The purview of ""bugs and issues"" includes (but is not limited to):

- missing, incomplete or incorrect definitions of Brick classes
- errors, mistakes or inconsistencies in the Brick ontology definition

Bug reports are most helpful when they fully explain the problem and include as many details as possible.
Some suggestions:

- **Use a clear and descriptive title** for the issue that identifies the problem
- **Include as many details as possible** about the problem, including any relevant Brick/SPARQL queries, RDF triples, segments of Turtle files, Python code, etc
- **Describe the observed and expected behavior**: for example, what query did you run, what were the results, and what did you expect the results to be? What definition exists and what definition would you expect?
- **Describe the exact steps to reproducing the problem** where it is appropriate: did you execute a query and
- Make sure you are using the most recent version of the Brick repository

## Proposing Changes to Brick

The content, structure and extent of Brick is determined by its community, so suggestions for how to improve Brick are always welcome and will be taken under consideration.
Proposed changes to Brick are tracked on the [Brick GitHub Issue Tracker](https://github.com/BrickSchema/Brick/issues).

Effective proposals should fully explain the motivation and scope of the proposed changes, and should have at least an initial impression of the nature of the implementation.
The more detail, the better!

## Submitting Changes to Brick

Changes to Brick are performed through [Pull Requests](https://github.com/BrickSchema/Brick/pulls).
It is recommended that you become familiar with how to [fork a repository](https://help.github.com/en/articles/fork-a-repo) and [create a pull request](https://help.github.com/en/articles/creating-a-pull-request-from-a-fork).

### Setting up Development Environment

Brick requires Python >= 3.6. We recommend using [virtual environments](https://docs.python.org/3/library/venv.html) to manage dependencies. We use [pre-commit hooks](https://pre-commit.com/) to automatically run code formatters and style checkers when you commit.

1. Check out the Brick repository (or your own fork of it)

```bash
git clone https://github.com/BrickSchema/Brick
cd Brick
```

2. Use Git submodules to pull in the RealEstateCore ontology

```bash
git submodule update --init
```

3. Install the virtual environment and set up dependencies

```bash
# creates virtual environment
python3 -m venv venv

# activates virtual environment; do this every time you develop on Brick
source venv/bin/activate

# install dependencies
pip install -r requirements.txt

# install pre-commit hooks
pre-commit install
```

4. Run tests to make sure the build is not broken

```bash
make test
```

4. Whenever you commit, the `pre-commit` script will run the Black code formatting tool and the flake8 style checker. It will automatically format the code where it can, and indicate when there is a style error. **The tools will not commit unformatted code**; if you see a ""Failed"" message, please fix the style and re-commit the code. An example of what this looks like is below; the failed flake8 check results in a short error report at the bottom.

```
gabe@arkestra:~/src/Brick$ git commit -m 'adding changes to Alarm hierarchy'
[WARNING] Unstaged files detected.
[INFO] Stashing unstaged files to /home/gabe/.cache/pre-commit/patch1581700010.
Check Yaml...........................................(no files to check)Skipped
Fix End of Files.........................................................Passed
Trim Trailing Whitespace.................................................Passed
black....................................................................Passed
flake8...................................................................Failed
- hook id: flake8
- exit code: 1

bricksrc/alarm.py:85:78: E231 missing whitespace after ','

[INFO] Restored changes from /home/gabe/.cache/pre-commit/patch1581700010.
```

### Extending the Class Hierarchy

The Brick class hierarchy is defined across several files in `bricksrc/`, named according to the Brick class that roots the hierarchy defined in the file.

Brick point class definitions should be placed in one of the following files:
- `bricksrc/command.py` for subclasses of the Brick `Command` class
- `bricksrc/sensor.py` for subclasses of the Brick `Sensor` class
- `bricksrc/setpoint.py` for subclasses of the Brick `Setpoint` class
- `bricksrc/status.py` for subclasses of the Brick `Status` class

Brick `Equipment` definitions should be placed in the `bricksrc/equipment.py` file.

Brick `Location` definitions should be placed in the `bricksrc/location.py` file.

Brick `Parameter` definitions should be placed in the `bricksrc/parameter.py` file.

Brick `Quantity` definitions should be placed in the `bricksrc/quantities.py` file.

Brick `Substance` definitions should be placed in the `bricksrc/substances.py` file.

Brick class definitions are written using a nested Python dictionary structure.
Observe the example below from `bricksrc/sensor.py`:

```python
{
    ""Sensor"": {
        ""tags"": [ TAG.Sensor ],
        ""subclasses"": {
            ""Air_Grains_Sensor"": {
                ""tags"": [ TAG.Sensor, TAG.Air, TAG.Grains ],
                ""substances"": [ [ BRICK.measures, BRICK.Air ], [ BRICK.measures, BRICK.Grains ], ],
                ""subclasses"": {
                    ""Outside_Air_Grains_Sensor"": {
                        ""tags"": [ TAG.Outside, TAG.Air, TAG.Grains, TAG.Sensor ],
                    },
                    ""Return_Air_Grains_Sensor"": {
                        ""tags"": [ TAG.Return, TAG.Air, TAG.Grains, TAG.Sensor ],
                    }
                }
            }
        }
    }
}
```

The core class definition structure is a dictionary whose keys are Brick class names and whose values are dictionaries containing the class properties.
Class names should be alpha-numeric strings consisting of capitalized words separated by the `_` character; class names can not contain spaces.
The class property dictionary can have the following keys:

- `tags`: a list of Brick tags; an entity who has all of these tags will be inferred as an instance of the class
- `parents`: a list of Brick *classes* that are parent classes of the current class; this lets us form the class lattice with less duplication
- `subclasses`: a dictionary whose keys+values are class names and definitions; this recursively follows the same structure
- `substances`: a nested list of Brick Substance classes. Each list item should be of the form `[BRICK.measures, BRICK.<substance name>]` where `<substance name>` is replaced with the substance that is measured. Substances are defined in `bricksrc/substances.py`

To provide a textual definition for a Brick class (which will be linked to its definition via the `SKOS.definition` property), add the class name and its definition to `bricksrc/definitions.csv`.
Each entry has 3 columns: the full Brick URI for the class name (with the Brick namespace prefix), a textual definition (if this contains commas, then make sure the definition is enclosed in quotes), and an optional citation or additional resource.
Entries should be placed into the file alphabetically to facilitate maintenance.

For example, here is the row providing the textual definition of a `brick:Thermostat`:

```
https://brickschema.org/schema/Brick#Thermostat,An automatic control device used to maintain temperature at a fixed or adjustable setpoint.,
```

### Managing Tags

Tags provide an alternative way of instantiating classes; Brick can infer classifications from the set of tags applied to an entity with the `brick.hasTag` relationship.
Each subclass's tags should contain *at least* the tags of its parent class; currently, the set of tags for a class must be explicitly annotated.

### Defining Brick Relationships

Brick relationships are defined in `bricksrc/properties.py`.
The `properties` dictionary contains these definitions and follows a similar structure to the Brick class definitions: keys are property names; values are dictionaries of properties of the relationships.

The following keys are expected for each relationship:

- `A`: value is a list of OWL property classes defining how this relationships behaves. `OWL.AsymmetricProperty` and `OWL.IrreflexiveProperty` are the most common
- `SKOS.definition`: value is an RDFlib Literal containing a textual definition of the relationship
- `OWL.inverseOf` (optional): value is a string representing the name of the inverse relationship. If a relationship has a defined inverse, the inverse relationship should also appear as a top-level relationship in the `properties` dictionary.
- `RDFS.range` (optional): value is a Brick class whose instances can be object of this relationship
- `RDFS.domain` (optional): value is a Brick class whose instances can be subject of this relationship

*Subproperties are under development; this document will be updated with instructions for defining subproperties when development has finished*

### Defining Entity Properties

Brick entity properties are defined in `bricksrc/entity_properties.py`.
There are two dictionaries in this file. The `entity_properties` dictionary defines the names of properties and subproperties that relate Brick entities to their property values. These should all have `SKOS.definition` entries and an `RDFS.range` property which is a SHACL shape.

The second dictionary, `shape_properties`, defines the SHACL shapes describing the actual entity property values. Shapes fall into one of 3 forms:
- `units` and `datatype`: lists the appropriate engineering units and the allowed datatype of the value
- `values`: an enumeration of possible values of the property
- `properties`: custom properties that don't fall under the above. So far this is just used for the `AggregationShape`

As a general design pattern (but not prescribed rule), entity properties should *not* begin with ""has""; simply name the property (e.g. `hasCoolingCapacity` should be `coolingCapacity`).

### Development Environment

The Brick schema is generated by a set of Python files located in this repository.
To set up the development environment

0. Make sure you have Python3 installed on your development system
1. Fork the [Brick repository](https://github.com/BrickSchema/Brick) and clone it to your development system
2. [Set up a virtual environment](https://docs.python.org/3/library/venv.html) in the repository folder
    ```bash
    # making the virtual environment
    python3 -m venv brickvenv
    ```
3. Install the Python3 dependencies using pip:
    ```bash
    # enter the virtual environment
    . brickvenv/bin/activate
    pip install -r requirements.txt
    ```
    * You do not need to install the `docker` dependency if your system does not support it. Use of docker enables use of Allegrograph, which can speed up some of the Brick compilation processes.
    * If C++ compiler is not installed you may get an error installing the Levenshtein-search.  It is not strickly necessary to build and test brick.  You can safely ignore this error.
4. Make the desired changes **to the Python files**. Do not edit the `Brick.ttl` or `Brick+extensions.ttl` files directly.
5. Compile the `Brick.ttl` and `Brick_expanded.ttl` files by executing the `generate_brick.py` script using Python
    ```bash
    python generate_brick.py
    ```
    It may be the case that additional work is needed to generate the Brick file. If your system supports it, it is
    recommended to use the Makefile to build Brick:
    ```bash
    make
    ```
6. Commit your changes and push to your fork
7. Submit the pull request
",,,,28,,,,
255867718,MDEwOlJlcG9zaXRvcnkyNTU4Njc3MTg=,AIChip_Paper_List,BRTResearch/AIChip_Paper_List,0,BRTResearch,https://github.com/BRTResearch/AIChip_Paper_List,,0,2020-04-15 09:27:42+00:00,2025-03-03 09:00:59+00:00,2021-01-13 01:48:25+00:00,,4590,602,602,,1,1,1,1,1,0,117,0,0,3,,1,0,0,public,117,3,602,master,1,1,,"['sjtujnf', 'lliuBR', 'ghzgt', 'basicmi', 'PsychArch']",0,0.64,0,,,,,,38,,,,
74525361,MDEwOlJlcG9zaXRvcnk3NDUyNTM2MQ==,python-for-data-analysis,cuttlefishh/python-for-data-analysis,0,cuttlefishh,https://github.com/cuttlefishh/python-for-data-analysis,An introduction to data science using Python and Pandas with Jupyter notebooks,0,2016-11-23 00:31:52+00:00,2025-03-02 17:24:26+00:00,2020-10-02 15:06:39+00:00,,27801,862,862,Jupyter Notebook,1,1,1,1,1,0,334,0,0,3,mit,1,0,0,public,334,3,862,master,1,,,['cuttlefishh'],0,0.71,0,,,,,,69,,,,
7654975,MDEwOlJlcG9zaXRvcnk3NjU0OTc1,cytoscape,cytoscape/cytoscape,0,cytoscape,https://github.com/cytoscape/cytoscape,Cytoscape: an open source platform for network analysis and visualization,0,2013-01-16 22:12:17+00:00,2025-03-07 12:09:53+00:00,2024-11-18 19:55:26+00:00,http://www.cytoscape.org/,809,599,599,Shell,0,1,1,1,1,0,142,0,0,1,,1,0,0,public,142,1,599,develop,1,1,,"['keiono', 'bdemchak', 'dotasek', 'AlexanderPico', 'BrettJSettle', 'AdamStuart', 'yihangx', 'jrrmzz', 'chrtannus', 'jingjingbic', 'mikekucera', 'scootermorris', 'cmzmasek', 'IgorRodchenkov', 'kozo2']",1,0.7,1949575,,,,,,83,,,,
174434121,MDEwOlJlcG9zaXRvcnkxNzQ0MzQxMjE=,partnet_dataset,daerduoCarey/partnet_dataset,0,daerduoCarey,https://github.com/daerduoCarey/partnet_dataset,PartNet Dataset Official Release Repo,0,2019-03-07 23:06:01+00:00,2025-03-07 22:41:11+00:00,2023-02-02 21:04:41+00:00,https://cs.stanford.edu/~kaichun/partnet/,2588,371,371,Python,1,1,1,1,0,0,45,0,0,11,other,1,0,0,public,45,11,371,master,1,,,"['daerduoCarey', 'Dustinpro']",1,0.7,0,,,,,,14,,,,
49368478,MDEwOlJlcG9zaXRvcnk0OTM2ODQ3OA==,compilers-targeting-c,dbohdan/compilers-targeting-c,0,dbohdan,https://github.com/dbohdan/compilers-targeting-c,A list of compilers that can generate C code,0,2016-01-10 13:46:07+00:00,2025-03-06 06:15:18+00:00,2023-11-16 09:08:24+00:00,,67,726,726,JavaScript,1,0,1,0,0,0,49,0,0,11,,1,0,0,public,49,11,726,master,1,,,"['dbohdan', 'seanjensengrey', 'jubnzv', 'Solarspot', 'ImmutableOctet', 'DanielMazurkiewicz', 'athas', 'bencz', 'avodonosov', 'aep', 'nadako', 'dom96', 'ionelmc', 'JaDogg', 'justinethier', 'lassik', 'pfusik', 'Neopallium', 'kingland', 'sylvain-josserand', 'tommy-carlier', 'hellerve', 'Vurv78', 'shai-almog']",0,0.7,0,,,,,,41,,,,
71583602,MDEwOlJlcG9zaXRvcnk3MTU4MzYwMg==,cs-video-courses,Developer-Y/cs-video-courses,0,Developer-Y,https://github.com/Developer-Y/cs-video-courses,List of Computer Science courses with video lectures.,0,2016-10-21 17:02:11+00:00,2025-03-08 09:01:58+00:00,2025-03-07 17:52:13+00:00,,841,68259,68259,,1,1,1,1,0,0,9236,0,0,2,,1,0,0,public,9236,2,68259,master,1,,,"['Developer-Y', 'butter1125', 'DateBro', 'PeskyPotato', 'Alaharon123', 'Hyle33ies', 'inf3cti0n95', 'maxprogrammer007', 'mundher', 'bivashpandey', 'bhuthesh', 'MasoudKaviani', 'anishathalye', 'spekulatius', 'solomonbstoner', 'Suraj7879', 'm229abd', 'qkenn', 'tentena', 'unfode', 'P7h', 'R-Mahmoudi', 'richwill28', 'RohitSgh', 'sashedher', 'JZZQuant', 'ShashankP19', 'Ghost93', 'shreyasdeotare', '0xsomnus', 'trisolaris233', 'ProgrammingPete', 'ppisa', 'paulosalvatore', 'pvcraven', 'guybrush', 'Omar-Yasser', 'osyvokon', 'nishanths', 'Nishank-25', 'emperor-jimmu', 'rishabhb-git', 'phscloq', 'love25nov', 'krnets', 'eternalfool', 'elarabyelaidy19', 'sootysec', 'ayoubazaouyat', 'yugborana', 'tan-i-ham', 'awxiaoxian2020', 'wiiskii', 'wcrasta', 'Utayaki', 'hxt365', 'taylorty', 'tejasurya', 'Sushants-Git', 'dripdropdr', 'soum-c', 'norswap', 'hridaydutta123', 'gusnaughton', 'GianAndreaSechi', 'GabrielRogd', 'floe', 'FabienTregan', 'danielrbradley', 'Chirag-Bansal', 'alebcay', 'subarudad', 'bbhart', 'lamberta', 'BikramHalder', 'BemwaMalak', 'ayushpandey830', 'aleichtm', 'Atcold', 'cage433', 'wp-lai', 'ei-blue', 'naeemshaikh90', 'htarsoo', 'm4salah', 'mperreux', 'mterwill', 'MaaniGhaffari', 'miangraham', 'laithshadeed', '0x42697262', 'kairat-beep', 'joel-porquet', 'JVKdouk', 'jeffin07', 'jeffheaton', 'jeevan0920', 'xtangle', 'tientaidev', 'IureRosa', 'eltociear', 'hrithik254']",0,50,0,,"# Contribution Guidelines

- Recently the quality of MOOCs has diminished, therefore only MOOCs with comprehensive lecture material which cover a subject/topic in ample detail will be added. For example, MOOC on Computer Networks or Machine Learning with 4-5 hours may not be able to cover all topics in sufficient detail and thus should be avoided.
- One philosophy used in this list while integrating MOOCs is that links should directly point to videos for viewing/downloading than registration and waiting for the next session. If videos are directly accessible through the platform/youtube or any other source, please use the direct source. This is a list of video courses, not a list of MOOCs.
- Courses within a section are roughly sorted in terms of level i.e. undergraduate courses followed by upper-level undergraduate, followed by graduate courses. As courses are from multiple Universities, sorting is not perfect and only an approximation. For example, while adding a new undergraduate course on Algorithms, please feel free to add it along with other Algorithms courses than after graduate courses.
",,,,1864,,,foundation-interface,UCSD-Historical-Enrollment-Data
73707729,MDEwOlJlcG9zaXRvcnk3MzcwNzcyOQ==,technical-interviews,Developer-Y/technical-interviews,0,Developer-Y,https://github.com/Developer-Y/technical-interviews,,0,2016-11-14 13:34:32+00:00,2025-03-01 22:06:20+00:00,2024-02-07 11:29:50+00:00,,27,1110,1110,,1,1,1,1,0,0,304,0,0,4,,1,0,0,public,304,4,1110,master,1,,,"['Developer-Y', 'arjunkumar09', 'bikashdaga', 'letscode-17', 'msumeet']",0,0.6,0,,,,,,61,,,,
85842387,MDEwOlJlcG9zaXRvcnk4NTg0MjM4Nw==,keras-yolo2,experiencor/keras-yolo2,0,experiencor,https://github.com/experiencor/keras-yolo2,Easy training on custom dataset. Various backends (MobileNet and SqueezeNet) supported. A YOLO demo to detect raccoon run entirely in brower is accessible at https://git.io/vF7vI (not on Windows).,0,2017-03-22 15:08:29+00:00,2025-03-04 03:37:58+00:00,2023-03-24 22:26:55+00:00,,55194,1735,1735,Jupyter Notebook,1,1,1,1,0,0,783,0,0,202,mit,1,0,0,public,783,202,1735,master,1,,,"['experiencor', 'pure-water', 'alanjschoen', 'hobson', 'luislofer89', 'sirotenko', 'NPetsky', 'mrshu', 'usatenko']",0,0.63,0,,,,,,66,,,,
78594383,MDEwOlJlcG9zaXRvcnk3ODU5NDM4Mw==,ResNeXt,facebookresearch/ResNeXt,0,facebookresearch,https://github.com/facebookresearch/ResNeXt,Implementation of a classification framework from the paper Aggregated Residual Transformations for Deep Neural Networks,0,2017-01-11 02:20:25+00:00,2025-03-04 09:02:21+00:00,2020-01-14 19:58:44+00:00,,32,1914,1914,Lua,1,1,1,0,0,0,290,1,0,10,other,1,0,0,public,290,10,1914,master,1,1,"# ResNeXt: Aggregated Residual Transformations for Deep Neural Networks

By [Saining Xie](http://vcl.ucsd.edu/~sxie), [Ross Girshick](http://www.rossgirshick.info/), [Piotr Dollár](https://pdollar.github.io/), [Zhuowen Tu](http://pages.ucsd.edu/~ztu/), [Kaiming He](http://kaiminghe.com)

UC San Diego, Facebook AI Research

### Table of Contents
0. [Introduction](#introduction)
0. [Citation](#citation)
0. [Requirements and Dependencies](#requirements-and-dependencies)
0. [Training](#training)
0. [ImageNet Pretrained Models](#imagenet-pretrained-models)
0. [Third-party re-implementations](#third-party-re-implementations)

#### News
* Congrats to the ILSVRC 2017 classification challenge winner [WMW](http://image-net.org/challenges/LSVRC/2017/results).
ResNeXt is the foundation of their new SENet architecture (a **ResNeXt-152 (64 x 4d)** with the Squeeze-and-Excitation module)!
* Check out Figure 6 in the new [Memory-Efficient Implementation of DenseNets](https://arxiv.org/pdf/1707.06990.pdf) paper for a comparision between ResNeXts and DenseNets. <sub>（*DenseNet cosine is DenseNet trained with cosine learning rate schedule.*）</sub>
<p align=""center"">
<img src=""http://vcl.ucsd.edu/resnext/resnextvsdensenet.png"" width=""480"">
</p>


### Introduction

This repository contains a [Torch](http://torch.ch) implementation for the [ResNeXt](https://arxiv.org/abs/1611.05431) algorithm for image classification. The code is based on [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch).

[ResNeXt](https://arxiv.org/abs/1611.05431) is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.


![teaser](http://vcl.ucsd.edu/resnext/teaser.png)
##### Figure: Training curves on ImageNet-1K. (Left): ResNet/ResNeXt-50 with the same complexity (~4.1 billion FLOPs, ~25 million parameters); (Right): ResNet/ResNeXt-101 with the same complexity (~7.8 billion FLOPs, ~44 million parameters).
-----

### Citation
If you use ResNeXt in your research, please cite the paper:
```
@article{Xie2016,
  title={Aggregated Residual Transformations for Deep Neural Networks},
  author={Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},
  journal={arXiv preprint arXiv:1611.05431},
  year={2016}
}
```

### Requirements and Dependencies
See the fb.resnet.torch [installation instructions](https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md) for a step-by-step guide.
- Install [Torch](http://torch.ch/docs/getting-started.html) on a machine with CUDA GPU
- Install [cuDNN v4 or v5](https://developer.nvidia.com/cudnn) and the Torch [cuDNN bindings](https://github.com/soumith/cudnn.torch/tree/R4)
- Download the [ImageNet](http://image-net.org/download-images) dataset and [move validation images](https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset) to labeled subfolders

### Training

Please follow [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch) for the general usage of the code, including [how](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained) to use pretrained ResNeXt models for your own task.

There are two new hyperparameters need to be specified to determine the bottleneck template:

**-baseWidth** and **-cardinality**

### 1x Complexity Configurations Reference Table

| baseWidth | cardinality |
|---------- | ----------- |
| 64        | 1           |
| 40        | 2           |
| 24        | 4           |
| 14        | 8           |
| 4         | 32          |


To train ResNeXt-50 (32x4d) on 8 GPUs for ImageNet:
```bash
th main.lua -dataset imagenet -bottleneckType resnext_C -depth 50 -baseWidth 4 -cardinality 32 -batchSize 256 -nGPU 8 -nThreads 8 -shareGradInput true -data [imagenet-folder]
```

To reproduce CIFAR results (e.g. ResNeXt 16x64d for cifar10) on 8 GPUs:
```bash
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 128 -nGPU 8 -nThreads 8 -shareGradInput true
```
To get comparable results using 2/4 GPUs, you should change the batch size and the corresponding learning rate:
```bash
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 64 -nGPU 4 -LR 0.05 -nThreads 8 -shareGradInput true
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 32 -nGPU 2 -LR 0.025 -nThreads 8 -shareGradInput true
```
Note: CIFAR datasets will be automatically downloaded and processed for the first time. Note that in the arXiv paper CIFAR results are based on pre-activated bottleneck blocks and a batch size of 256. We found that better CIFAR test acurracy can be achieved using original bottleneck blocks and a batch size of 128.

### ImageNet Pretrained Models
ImageNet pretrained models are licensed under CC BY-NC 4.0.

[![CC BY-NC 4.0](https://i.creativecommons.org/l/by-nc/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc/4.0/)

#### Single-crop (224x224) validation error rate
| Network             | GFLOPS | Top-1 Error |  Download   |
| ------------------- | ------ | ----------- | ------------|
| ResNet-50 (1x64d)   |  ~4.1  |  23.9        | [Original ResNet-50](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained)       |
| ResNeXt-50 (32x4d)  |  ~4.1  |  22.2        | [Download (191MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_50_32x4d.t7)       |
| ResNet-101 (1x64d)  |  ~7.8  |  22.0        | [Original ResNet-101](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained)      |
| ResNeXt-101 (32x4d) |  ~7.8  |  21.2        | [Download (338MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_101_32x4d.t7)      |
| ResNeXt-101 (64x4d) |  ~15.6 |  20.4        | [Download (638MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_101_64x4d.t7)       |

### Third-party re-implementations

Besides our torch implementation, we recommend to see also the following third-party re-implementations and extensions:

1. Training code in PyTorch [code](https://github.com/prlz77/ResNeXt.pytorch)
1. Converting ImageNet pretrained model to PyTorch model and source. [code](https://github.com/clcarwin/convert_torch_to_pytorch)
1. Training code in MXNet and pretrained ImageNet models [code](https://github.com/dmlc/mxnet/tree/master/example/image-classification#imagenet-1k)
1. Caffe prototxt, pretrained ImageNet models (with ResNeXt-152), curves [code](https://github.com/cypw/ResNeXt-1)[code](https://github.com/terrychenism/ResNeXt)
","['s9xie', 'KaimingHe', 'wanyenlo']",1,0.73,0,"# Code of Conduct

Facebook has adopted a Code of Conduct that we expect project participants to adhere to.
Please read the [full text](https://code.fb.com/codeofconduct/)
so that you can understand what actions will and will not be tolerated.","# Contributing to ResNeXt
We want to make contributing to this project as easy and transparent as
possible.


## Pull Requests
We actively welcome your pull requests.

1. Fork the repo and create your branch from `master`.
2. If you haven't already, complete the Contributor License Agreement (""CLA"").

## Contributor License Agreement (""CLA"")
In order to accept your pull request, we need you to submit a CLA. You only need
to do this once to work on any of Facebook's open source projects.

Complete your CLA here: <https://code.facebook.com/cla>

## Issues
We use GitHub issues to track public bugs. Please ensure your description is
clear and has sufficient instructions to be able to reproduce the issue.

## Coding Style  
* 3 spaces for indentation rather than tabs
* 80 character line length

## License
By contributing to ResNeXt, you agree that your contributions will be licensed
under its [BSD license](https://github.com/facebookresearch/ResNeXt/blob/master/LICENSE).
",,,,72,,,,
4797147,MDEwOlJlcG9zaXRvcnk0Nzk3MTQ3,FastPFOR,fast-pack/FastPFOR,0,fast-pack,https://github.com/fast-pack/FastPFOR,The FastPFOR C++ library: Fast integer compression,0,2012-06-26 15:50:06+00:00,2025-03-06 12:18:04+00:00,2025-02-26 22:32:04+00:00,,5818,906,906,C++,1,1,1,1,0,1,126,0,0,15,apache-2.0,1,0,0,public,126,15,906,master,1,1,,"['lemire', 'seb711', 'pps83', 'xndai', 'amallia', 'ncave', 'nyurik', 'michellemay', 'mpetri', 'elshize', 'deafwolf', 'cbsmith', 'maximecaron', 'kruus', 'pdamme', 'rayburgemeestre', 'galo2099', 'kou', 'xcorail', 'hurricane1026', 'kimikage', 'orz--', 'romange', 'searchivarius', 'yujinqiu']",0,0.66,0,,,,,,42,,,,
50352391,MDEwOlJlcG9zaXRvcnk1MDM1MjM5MQ==,Neural-Networks-on-Silicon,fengbintu/Neural-Networks-on-Silicon,0,fengbintu,https://github.com/fengbintu/Neural-Networks-on-Silicon,This is originally a collection of papers on neural network accelerators. Now it's more like my selection of research on deep learning and computer architecture.,0,2016-01-25 13:31:31+00:00,2025-03-07 00:55:03+00:00,2025-01-06 14:57:13+00:00,,817,1909,1909,,1,1,1,1,0,1,384,0,0,0,,1,0,0,public,384,0,1909,master,1,,,"['fengbintu', 'kentaroy47', 'waterbearbee', 'Aayush-Ankit', 'abhishektyaagi', 'qinkunbao', 'sung-kim']",0,0.54,0,,,,,,300,,,,
209488196,MDEwOlJlcG9zaXRvcnkyMDk0ODgxOTY=,awesome-video-anomaly-detection,fjchange/awesome-video-anomaly-detection,0,fjchange,https://github.com/fjchange/awesome-video-anomaly-detection,"Papers for Video Anomaly Detection, released codes collection, Performance Comparision.",0,2019-09-19 07:19:34+00:00,2025-03-06 12:59:19+00:00,2022-09-20 09:40:04+00:00,,138,617,617,,1,1,1,1,0,0,104,0,0,3,,1,0,0,public,104,3,617,master,1,,,"['fjchange', 'ckjeter', 'demonzyj56', 'yuguangnudt']",0,0.78,0,,,,,,30,,,,
593814016,R_kgDOI2TiAA,graphologue,foundation-interface/graphologue,0,foundation-interface,https://github.com/foundation-interface/graphologue,"Use LLM to stream diagrams, instead of tokens, in real-time! (UIST 2023 Paper)",0,2023-01-26 22:24:29+00:00,2025-03-02 16:22:53+00:00,2024-04-14 00:27:49+00:00,https://graphologue.app,3182,286,286,TypeScript,1,1,1,1,0,0,43,0,0,0,mit,1,0,0,public,43,0,286,public,1,1,"# <img src=""./public/logo512.png"" width=""48"" style=""vertical-align: middle;"" alt=""G""></img>raphologue: Exploring LLM Responses with Interactive Diagrams

Graphologue transforms Large Language Model (LLM, such as GPT-4) responses into interactive diagrams in real-time with _Inline Annotation_.

![](./media/teaser.png)

[**Live Demo**](https://graphologue.app/) (An OpenAI API key is needed.) | [**UIST 2023 Paper**](https://doi.org/10.1145/3586183.3606737) | [**Website**](https://creativity.ucsd.edu/)

## Development

1. Install all the dependencies.

```bash
npm install
```

2. Create an `.env` file at the project ROOT (where the `.env.example` file is) to put the OpenAI API key. The file should look like:

```
REACT_APP_OPENAI_API_KEY=sk-...
```

3. Start the local server.

```bash
npm start
```

## UIST 2023 Paper

**Graphologue: Exploring Large Language Model Responses with Interactive Diagrams**<br />
Peiling Jiang*, Jude Rayan*, Steven P. Dow, Haijun Xia _(\* Both authors contributed equally to this research.)_

**Please cite this paper if you used the code or prompts in this repository.**

> Peiling Jiang, Jude Rayan, Steven P. Dow, and Haijun Xia. 2023. Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. In The 36th Annual ACM Symposium on User Interface Software and Technology (UIST ’23), October 29-November 1, 2023, San Francisco, CA, USA. ACM, New York, NY, USA, 20 pages. https://doi.org/10.1145/3586183.3606737

```bibtex
@inproceedings{jiang2023graphologue,
  author = {Jiang, Peiling and Rayan, Jude and Dow, Steven P. and Xia, Haijun},
  title = {Graphologue: Exploring Large Language Model Responses with Interactive Diagrams},
  year = {2023},
  isbn = {9798400701320},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3586183.3606737},
  doi = {10.1145/3586183.3606737},
  booktitle = {The 36th Annual ACM Symposium on User Interface Software and Technology},
  keywords = {Large Language Model, Natural Language Interface, Visualization},
  location = {San Francisco, CA, USA},
  series = {UIST '23}
}
```

Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented ability to synthesize text responses to diverse user questions. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can interact with the diagrams to flexibly adjust the graphical presentation and to submit context-specific prompts to obtain more information. Utilizing diagrams, Graphologue enables graphical, non-linear dialogues between humans and LLMs, facilitating information exploration, organization, and comprehension.

## Acknowledgements

This work would not be possible without the selfless and heartwarming support of all the members of the Creativity Lab at UC San Diego. Our deepest gratitude extends to Fuling Sun, who was crucial in facilitating a productive drive toward the successful completion of this project. We would also like to thank Sangho Suh, Bryan Min, Matthew Beaudouin-Lafon, and Jane E for helping with the video figure production, and William Duan, Vidya Madhavan, Tony Meng, Xiaoshuo Yao, and Juliet (Lingye) Zhuang for assistance with the technical evaluation, as well as Brian Hempel and Devamardeep Hayatpur for proofreading the paper draft. We thank anonymous reviewers for their constructive and insightful reviews. NSF grant #2009003 provided financial support.
","['peilingjiang', 'jude-rayan']",1,0.83,0,,,,,,10,,,,
221641068,MDEwOlJlcG9zaXRvcnkyMjE2NDEwNjg=,Medical_NLP,FreedomIntelligence/Medical_NLP,0,FreedomIntelligence,https://github.com/FreedomIntelligence/Medical_NLP,"Medical NLP Competition, dataset, large models, paper",0,2019-11-14 07:50:52+00:00,2025-03-07 08:02:07+00:00,2024-12-06 13:24:07+00:00,,621,2237,2237,,1,1,1,1,0,0,418,0,0,0,,1,0,0,public,418,0,2237,master,1,1,"# Medical_NLP

医疗NLP领域  评测/比赛，数据集，论文和预训练模型资源汇总。

Summary of medical NLP evaluations/competitions, datasets, papers and pre-trained models.

<p>
  <a href=""https://github.com/FreedomIntelligence/Medical_NLP""><img src=https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg ></a>
  <a href=""https://github.com/FreedomIntelligence/Medical_NLP""><img src=https://img.shields.io/github/forks/FreedomIntelligence/Medical_NLP.svg?style=social ></a>
  <a href=""https://github.com/FreedomIntelligence/Medical_NLP""><img src=https://img.shields.io/github/stars/FreedomIntelligence/Medical_NLP.svg?style=social ></a>
  <a href=""https://github.com/FreedomIntelligence/Medical_NLP""><img src=https://img.shields.io/github/watchers/FreedomIntelligence/Medical_NLP.svg?style=social ></a>
</p>

<!--
[中文版本](https://github.com/FreedomIntelligence/Chinese_medical_NLP)  [English_version](https://github.com/FreedomIntelligence/Chinese_medical_NLP/blob/master/English_vision.md)
-->

#### News

- 🟡*2024/11/14* 新增 [`4. VLM数据集`](#4-VLM数据集)、[`5.3 医疗VLM`](#53-医疗VLM)、[`5.4 医疗VLM Benchmark`](#54-医疗VLM-Benchmark)，**后续将重点维护 Medical VLM 方向相关资源汇总**，repo由[Rongsheng Wang](https://github.com/WangRongsheng)维护。
- 🟡*2024/11/14之前* 由于[Cris Lee](https://github.com/lrs1353281004)2021年离开医疗NLP领域，此repo现由[Xidong Wang](https://github.com/wangxidong06), [Ziyue Lin](https://github.com/RobinLin2002), [Jing Tang](https://github.com/vaew)继续维护。

#### Contents

* [1. 评测](#1-评测)
  * [1.1  中文医疗基准测评：CMB / CMExam / PromptCBLUE](#11--中文医疗基准测评cmb--cmexam--promptcblue)
  * [1.2  英文医疗基准测评:](#12--英文医疗基准测评)
* [2. 比赛](#2-比赛)
  * [2.1 正在进行的比赛](#21-正在进行的比赛)
  * [2.2 已经结束的比赛](#22-已经结束的比赛)
* [3. LLM 数据集](#3-LLM数据集)
  * [3.1 中文](#31-中文)
  * [3.2 英文](#32-英文)
* [4. VLM 数据集](#4-VLM数据集) 🔥
* [5. 开源预训练模型](#5-开源预训练模型)
  * [5.1 医疗PLM](#51-医疗PLM)
  * [5.2 医疗LLM](#52-医疗LLM)
  * [5.3 医疗VLM](#53-医疗VLM) 🔥
  * [5.4 医疗VLM Benchmark](#54-医疗VLM-Benchmark) 🔥
* [6. 相关论文](#6-相关论文)
  * [6.1 后ChatGPT时代 可能有帮助的论文](#61-后chatgpt时代-可能有帮助的论文)
  * [6.2 综述类文章](#62-综述类文章)
  * [6.3 特定任务文章](#63-特定任务文章)
  * [6.4 会议索引](#64-会议索引)
* [7. 开源工具包](#7-开源工具包)
* [8. 工业级产品解决方案](#8-工业级产品解决方案)
* [9. blog分享](#9-blog分享)
* [10. 友情链接](#10-友情链接) 



## 1. 评测

### 1.1  中文医疗基准测评：CMB / CMExam / PromptCBLUE 

* CMB

  * 地址：https://github.com/FreedomIntelligence/CMB
    ![](https://img.shields.io/github/stars/FreedomIntelligence/CMB)
  * 来源：各个临床医学工种各阶段考试；临床复杂病例问诊
* CMExam

  * 地址：https://github.com/williamliujl/CMExam
    ![](https://img.shields.io/github/stars/williamliujl/CMExam)
  * 来源：执业医师资格考试往年题
* PromptCBLUE
  * 地址：https://github.com/michael-wzhu/PromptCBLUE
    ![](https://img.shields.io/github/stars/michael-wzhu/PromptCBLUE)
  * 来源：CBLUE
* PromptCBLUE
  * 地址：https://github.com/CBLUEbenchmark/CBLUE
    ![](https://img.shields.io/github/stars/CBLUEbenchmark/CBLUE)
  * 来源：CHIP会议往届的学术评测比赛和阿里夸克医疗搜索业务的数据集组成
* MedBench
  * 地址：https://arxiv.org/abs/2312.12806
  * 来源：包含来自执医考试和报告的40,041个问题，覆盖各个专科。

### 1.2  英文医疗基准测评: 

* MultiMedBench

  * 简介：是一种源自Google的大型多模态生成模型


<div align=""right"">
    <b><a href=""#Contents"">↥ back to top</a></b>
</div>


## 2. 比赛

### 2.1 正在进行的比赛

* 医学搜索Query相关性判断
  
  * 地址：https://tianchi.aliyun.com/competition/entrance/532001/introduction
  * 来源：阿里天池


### 2.2 已经结束的比赛

#### 2.2.1 英文比赛

* BioNLP Workshop 2023 共享任务

  * 地址：https://aclweb.org/aclwiki/BioNLP_Workshop#SHARED_TASKS_2023
  * 来源：BioNLP Workshop
* MedVidQA 2023

  * 地址：https://medvidqa.github.io/index.html
  * 来源：美国国立卫生研究院
* MEDIQA-2021

  * 地址：https://sites.google.com/view/mediqa2021
  * 来源：NAACL-BioNLP 2021 workshop
* ICLR-2021-医疗对话生成与自动诊断国际竞赛

  * 地址：https://mlpcp21.github.io/pages/challenge
  * 来源：ICLR 2021 workshop

#### 2.2.2 中文比赛

* 影像学NLP —— 医学影像诊断报告生成

  * 地址：https://gaiic.caai.cn/ai2023/
  * 来源：2023全球人工智能技术创新大赛 赛道一
* 非标准化疾病诉求的简单分诊挑战赛2.0

  * 地址：http://challenge.xfyun.cn/topic/info?type=disease-claims-2022&ch=ds22-dw-sq03
  * 来源：科大讯飞
* 第八届中国健康信息处理大会(CHIP2022)测评任务

  * 地址：http://cips-chip.org.cn/
  * 来源：CHIP2022
* 科大讯飞-医疗实体及关系识别挑战赛

  * 地址：http://www.fudan-disc.com/sharedtask/imcs21/index.html
  * 来源：科大讯飞
    
* “肝”柔相济，大模型开创肝病医患交互服务新格局

  * 地址：http://www.fudan-disc.com/sharedtask/imcs21/index.html](https://www.dcic-china.com/competitions/10090
  * 来源：数字中国建设峰会组委会


<div align=""right"">
    <b><a href=""#Contents"">↥ back to top</a></b>
</div>

## 3. LLM数据集

### 3.1 中文

* Huatuo-26M

  * 地址：https://github.com/FreedomIntelligence/Huatuo-26M
    ![](https://img.shields.io/github/stars/FreedomIntelligence/Huatuo-26M)
  * 简介：Huatuo-26M 是迄今为止最大的中医问答数据集。
* 中文医疗对话数据集

  * 地址：https://github.com/Toyhom/Chinese-medical-dialogue-data
    ![](https://img.shields.io/github/stars/Toyhom/Chinese-medical-dialogue-data)
  * 简介：包含六个科室的医学问答数据
* CBLUE

  * 地址：https://github.com/CBLUEbenchmark/CBLUE
    ![](https://img.shields.io/github/stars/CBLUEbenchmark/CBLUE)
  * 简介：涵盖了医学文本信息抽取（实体识别、关系抽取）
* cMedQA2 (108K)

  * 地址：https://github.com/zhangsheng93/cMedQA2
    ![](https://img.shields.io/github/stars/zhangsheng93/cMedQA2)
  * 简介：中文医药方面的问答数据集，超过10万条
* xywy-KG(294K三元组)

  * 地址：https://github.com/baiyang2464/chatbot-base-on-Knowledge-Graph
    ![](https://img.shields.io/github/stars/baiyang2464/chatbot-base-on-Knowledge-Graph)
  * 简介：44.1K实体 294.1K 三元组
* 39Health-KG (210K三元组)
  * 地址：https://github.com/zhihao-chen/QASystemOnMedicalGraph
    ![](https://img.shields.io/github/stars/zhihao-chen/QASystemOnMedicalGraph)
  * 简介：包括15项信息，其中7类实体，约3.7万实体，21万实体关系。
* Medical-Dialogue-System
  * 地址：https://github.com/UCSD-AI4H/Medical-Dialogue-System
    ![](https://img.shields.io/github/stars/UCSD-AI4H/Medical-Dialogue-System)
  * MedDialog 数据集（中文）包含医生和病人之间的对话（中文）。该数据集有 110 万条对话和 400 万条语句。数据还在不断增长，未来有更多的对话将被添加进来。
* Chinese medical dialogue data
  * 地址：https://github.com/Toyhom/Chinese-medical-dialogue-data
    ![](https://img.shields.io/github/stars/Toyhom/Chinese-medical-dialogue-data)
  * 该数据集含有包括男科，儿科，妇产科，内科，外科，肿瘤科在内的六个不同科室总计792099条数据。

* Yidu-S4K

  * 地址：http://openkg.cn/dataset/yidu-s4k
  * 简介：命名实体识别, 实体及属性抽取
* Yidu-N7K

  * 地址：http://openkg.cn/dataset/yidu-n7k
  * 简介：临床语标准化
* 中文医药方面的问答数据集

  * 地址：https://github.com/zhangsheng93/cMedQA2
  * 简介：医疗问答
* 中文医患问答对话数据

  * 地址：https://github.com/UCSD-AI4H/Medical-Dialogue-System
  * 简介：医疗问答
* CPubMed-KG (4.4M三元组)

  * 地址：https://cpubmed.openi.org.cn/graph/wiki
  * 简介：中华医学会高质量全文期刊数据
* 中文医学知识图谱 CMeKG (1M三元组)

  * 地址：http://cmekg.pcl.ac.cn/
  * 简介：CMeKG（Chinese Medical Knowledge  Graph）
* CHIP历年测评 (官方测评)
  * 地址：http://cips-chip.org.cn/2022/callforeval ;  http://www.cips-chip.org.cn/2021/ ;   http://cips-chip.org.cn/2020/
  * 简介：CHIP历年测评 (官方测评)
* 瑞金医院糖尿病数据集 (糖尿病)
  * 地址：https://tianchi.aliyun.com/competition/entrance/231687/information
  * 简介：瑞金医院糖尿病数据集 (糖尿病)
* 天池新冠肺炎问句匹配比赛 (新冠)
  * 地址：https://tianchi.aliyun.com/competition/entrance/231776/information
  * 简介：本次大赛数据包括：脱敏之后的医疗问题数据对和标注数据。

### 3.2 英文

- MedMentions

  * 地址：https://github.com/chanzuckerberg/MedMentions
    ![](https://img.shields.io/github/stars/chanzuckerberg/MedMentions)
  * 简介：基于Pubmed摘要的生物医学实体链接数据集
- webMedQA
  - 地址：https://github.com/hejunqing/webMedQA
    ![](https://img.shields.io/github/stars/hejunqing/webMedQA)
  - 简介：医疗问答
- COMETA

  * 地址：https://www.siphs.org/
  * 简介：社交媒体中的医疗实体链接数据。发表于EMNLP2020
- PubMedQA

  * 地址：https://arxiv.org/abs/1909.06146
  * 简介：基于Pubmed提取的医学问答数据集
- MediQA
  - 地址：https://sites.google.com/view/mediqa2021
  - 简介：文本概括
- ChatDoctor Dataset-1
  - 地址：https://drive.google.com/file/d/1lyfqIwlLSClhgrCutWuEe_IACNq6XNUt/view?usp=sharing
  - 简介：来自 HealthCareMagic.com 的 10 万条病人与医生之间的真实对话
- ChatDoctor Dataset-2
  - 地址：https://drive.google.com/file/d/1ZKbqgYqWc7DJHs3N9TQYQVPdDQmZaClA/view?usp=sharing
  - 简介：来自 icliniq.com 的 10k 条病人与医生之间的真实对话
- BioInstruct
  - 地址：https://github.com/bio-nlp/BioInstruct
  - 简介： 超过25,000条为生物医学任务量身定制的指令，包括但不限于问答（QA）、信息提取（IE）和文本生成
- Visual Med-Alpaca Data
  - 地址：https://github.com/cambridgeltl/visual-med-alpaca/tree/main/data
  - 简介：用于Visual Med-Alpaca训练的数据，源自 [BigBio](https://huggingface.co/bigbio), [ROCO](https://github.com/razorx89/roco-dataset) and [GPT-3.5-Turbo](https://chat.openai.com/chat)
- CheXpert Plus
  - 地址：https://github.com/Stanford-AIMI/chexpert-plus
  - 简介： 放射学领域公开发布的最大文本数据集，共有 3600 万个文本tokens，均配有 DICOM 格式的高质量图像，以及涵盖各种临床和社会群体的大量图像和患者元数据，以及许多病理标签和 RadGraph注释
  
<div align=""right"">
    <b><a href=""#Contents"">↥ back to top</a></b>
</div>

## 4. VLM数据集

<!--
参考：
https://github.com/lab-rasool/Awesome-Medical-VLMs-and-Datasets
https://github.com/openmedlab/Awesome-Medical-Dataset
多模态病理数据集：https://github.com/FreedomIntelligence/Medical_NLP/blob/master/images/pathology_datasets.jpg
-->

|Dataset|Paper|Github|Keywords|
|:-|:-|:-|:-|
|MedTrinity-25M|[link](https://arxiv.org/abs/2408.02900)|[link](https://github.com/UCSC-VLAA/MedTrinity-25M)|`25 million images`、`10 modalities`、`65 diseases`、`VQA`、`EN`|
|LLaVA-Med|[link](https://arxiv.org/abs/2306.00890)|[link](https://github.com/microsoft/LLaVA-Med)|`630k images`、`VQA`、`EN`|
|Chinese-LLaVA-Med|-|[link](https://github.com/BUAADreamer/Chinese-LLaVA-Med)|`60k images`、`VQA`、`ZH`|
|HuatuoGPT-Vision|[link](https://arxiv.org/abs/2406.19280)|[link](https://github.com/FreedomIntelligence/HuatuoGPT-Vision)|`647k images`、`VQA`、`EN`|
|MedVidQA|[link](https://www.nature.com/articles/s41597-023-02036-y)|[link](https://github.com/deepaknlp/MedVidQACL)|`7k videos`、`VQA`、`EN`|
|ChiMed-VL|[link](https://arxiv.org/abs/2310.17956)|[link](https://github.com/williamliujl/Qilin-Med-VL)|`1M images`、`VQA`、`EN`、`ZH`|
|RadFM|[link](http://arxiv.org/abs/2308.02463)|[link](https://github.com/chaoyi-wu/RadFM)|`16M images`、`5000 diseases`、`VQA`、`EN`、`2D/3D`|
|BiomedParseData|[link](https://arxiv.org/abs/2405.12971)|[link](https://huggingface.co/datasets/microsoft/BiomedParseData)|`6.8 million image-mask-description`、`45 biomedical image segmentation datasets`、`9 modalities`、`EN`、`2D`|
|OmniMedVQA|[link](https://arxiv.org/abs/2402.09181)|[link](https://github.com/OpenGVLab/Multi-Modality-Arena)|`118,010 images`、`12 modalities`、`2D`、`20 human anatomical regions`|
|PreCT|[link](https://arxiv.org/abs/2410.09890)|[link](https://github.com/Luffy03/Large-Scale-Medical)|`160K volumes`、`42M slices`、`3D`、`CT`|
|GMAI-VL-5.5M|[link](https://arxiv.org/abs/2411.14522v1)|[link](https://github.com/uni-medical/GMAI-VL)|`5.5m image and text`、`219 specialized medical imaging datasets`、`2D`、`VQA`|
|SA-Med2D-20M|[link](https://arxiv.org/abs/2311.11969)|[link](https://github.com/OpenGVLab/SAM-Med2D)|`4.6 million 2D medical images and 19.7 million corresponding masks`、`2D`、`EN`|
|IMIS-Bench|[link](https://arxiv.org/pdf/2411.12814)|[link](https://github.com/uni-medical/IMIS-Bench)|`6.4 million images, 273.4 million masks (56 masks per image), 14 imaging modalities, and 204 segmentation targets`、`EN`|

<div align=""right"">
    <b><a href=""#Contents"">↥ back to top</a></b>
</div>

## 5. 开源预训练模型

### 5.1 医疗PLM

- BioBERT：

  * 地址：https://github.com/naver/biobert-pretrained 
    ![](https://img.shields.io/github/stars/naver/biobert-pretrained )
  * 简介：BioBERT是一种生物医学领域的语言表示模型，专门用于生物医学文本挖掘任务，如生物医学命名实体识别、关系提取、问答等。

* BlueBERT：

  * 地址：https://github.com/ncbi-nlp/BLUE_Benchmark
    ![](https://img.shields.io/github/stars/ncbi-nlp/BLUE_Benchmark)
  * 简介：BLUE基准包括5个不同的生物医学文本挖掘任务和10个语料库。BLUE基准依赖于预先存在的数据集，因为它们已被BioNLP社区广泛用作共享任务。这些任务涵盖了各种文本类型(生物医学文献和临床笔记)、数据集大小和难度，更重要的是，突出了常见的生物医学文本挖掘挑战。

* BioFLAIR：

  * 地址：https://github.com/flairNLP/flair
    ![](https://img.shields.io/github/stars/flairNLP/flair)
  * 简介：Flair是一个强大的NLP库，能将最先进的自然语言处理(NLP)模型应用于文本，例如命名实体识别(NER)，情感分析，词性标记(PoS)，对[生物医学数据](https://github.com/flairNLP/flair/blob/master/resources/docs/HUNFLAIR.md)的特殊支持，语义消歧和分类，支持快速增长的语言数量。Flair 还是一个文本嵌入库，一个基于PyTorch的自然语言处理框架。

* COVID-Twitter-BERT：

  * 地址：https://github.com/digitalepidemiologylab/covid-twitter-bert
    ![](https://img.shields.io/github/stars/digitalepidemiologylab/covid-twitter-bert)
  * 简介：COVID-Twitter-BERT（简称CT-BERT）是一种基于Transformer的模型，它在关于COVID-19主题的大量推特消息上进行了预训练。v2模型是在9700万条推文（12亿训练样本）上训练的。

* bio-lm (Biomedical and Clinical Language Models)

  * 地址：https://github.com/facebookresearch/bio-lm
    ![](https://img.shields.io/github/stars/facebookresearch/bio-lm)
  * 简介：这项工作评估了用于生物医学和临床自然语言处理任务的许多模型，并训练了一些性能更佳的新模型。

* BioALBERT

  * 地址：https://github.com/usmaann/BioALBERT
    ![](https://img.shields.io/github/stars/usmaann/BioALBERT)
  * 简介：这是一种针对大型领域特定(生物医学)语料库训练的生物医学语言表示模型，专为生物医学文本挖掘任务而设计。


### 5.2 医疗LLM 


#### 5.2.1 多语言医疗大模型

* ApolloMoE：
  * 地址：https://github.com/FreedomIntelligence/ApolloMoE
    ![](https://img.shields.io/github/stars/FreedomIntelligence/ApolloMoE)
  * 简介：通过语言家族专家的混合，有效地实现 50 种语言医学LLM的民主化
    
* Apollo：
  * 地址：https://github.com/FreedomIntelligence/Apollo
    ![](https://img.shields.io/github/stars/FreedomIntelligence/Apollo)
  * 简介：轻量级多语言医学LLM，将医疗人工智能普及到 60亿人群

* MMedLM：
  * 地址：https://github.com/MAGIC-AI4Med/MMedLM
    ![](https://img.shields.io/github/stars/MAGIC-AI4Med/MMedLM)
  * 简介：第一个开源的多语言医学语言模型

  
#### 5.2.2 中文医疗大语言模型

* BenTsao：
  * 地址：https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese
    ![](https://img.shields.io/github/stars/SCIR-HI/Huatuo-Llama-Med-Chinese)
  * 简介：BenTsao以LLaMA-7B为基础，经过中文医学指令精调/指令微调得到。研究人员通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果。
* BianQue：
  * 地址：https://github.com/scutcyr/BianQue
    ![](https://img.shields.io/github/stars/scutcyr/BianQue)
  * 简介：一个经过指令与多轮问询对话联合微调的医疗对话大模型，以ClueAI/ChatYuan-large-v2作为底座，使用中文医疗问答指令与多轮问询对话混合数据集进行微调。
* SoulChat：
  * 地址：https://github.com/scutcyr/SoulChat
    ![](https://img.shields.io/github/stars/scutcyr/SoulChat)
  * 简介：灵心以ChatGLM-6B作为初始化模型，经过百万规模心理咨询领域中文长文本指令与多轮共情对话数据联合指令微调，提升模型的共情能力、引导用户倾诉能力以及提供合理建议的能力。
* DoctorGLM：
  * 地址：https://github.com/xionghonglin/DoctorGLM
    ![](https://img.shields.io/github/stars/xionghonglin/DoctorGLM)
  * 简介：一个基于 ChatGLM-6B的中文问诊大模型。该模型通过中文医疗对话数据集进行微调，实现了包括lora、p-tuningv2等微调及部署。
* HuatuoGPT：
  * 地址：https://github.com/FreedomIntelligence/HuatuoGPT
    ![](https://img.shields.io/github/stars/FreedomIntelligence/HuatuoGPT)
  * 简介：华佗GPT是一个经过中文医学指令精调/指令微调(Instruct-tuning)得到的一个GPT-like模型。该模型是专门为医疗咨询设计的中文LLM，它的训练数据包含了从ChatGPT处蒸馏得到的数据和来自医生的真实数据，在训练过程中加入了RLHF的反馈。
* HuatuoGPT-II：
  * 地址：https://github.com/FreedomIntelligence/HuatuoGPT-II
    ![](https://img.shields.io/github/stars/FreedomIntelligence/HuatuoGPT-II)
  * 简介：华佗GPT2采用了创新的领域适应方法，大大提高了其医学知识和对话能力。它在多个医疗基准测试中表现出了一流的性能，尤其是在专家评估和新医学执业资格考试中超越了 GPT-4。

#### 5.2.3 英文医疗大语言模型

* GatorTron：
  * 地址：https://github.com/uf-hobi-informatics-lab/GatorTron
    ![](https://img.shields.io/github/stars/uf-hobi-informatics-lab/GatorTron)
  * 简介：一个医疗健康领域的早期大模型，致力于研究使用非结构化的电子健康病例的系统是如何从有数十亿参数的医疗大模型中获益。
* Codex-Med：
  * 地址：https://github.com/vlievin/medical-reasoning
    ![](https://img.shields.io/github/stars/uf-hobi-informatics-lab/GatorTron)
  * 简介：致力于研究GPT-3.5模型回答和推理实际医疗问题的能力。使用了医疗测试数据集USMLE和MedMCQA， 医疗阅读理解数据集PubMedQA。
* Galactica：
  * 地址：https://galactica.org/
  * 简介：Galactica致力于解决科学领域的信息过载问题，储存合并了包括医疗医疗健康领域在内的科学知识。Galactica在大型论文语料库，参考文献的基础上训练而成，尝试发现不同领域研究之间的潜在关系。
* DeID-GPT：
  * 地址：https://github.com/yhydhx/ChatGPT-API
    ![](https://img.shields.io/github/stars/yhydhx/ChatGPT-API)
  * 简介：一个创新的的支持GPT4的去识别框架，可以自动识别和删除识别信息。
* ChatDoctor：
  * 地址：https://github.com/Kent0n-Li/ChatDoctor
    ![](https://img.shields.io/github/stars/Kent0n-Li/ChatDoctor)
  * 简介：一个利用医疗领域基础知识，基于LLaMA进行微调得到的医疗对话模型。
* MedAlpaca：
  * 地址：https://github.com/kbressem/medAlpaca
    ![](https://img.shields.io/github/stars/kbressem/medAlpaca)
  * 简介：MedAlpaca采用了一种开源策略，致力于解决医疗系统中的隐私问题。该模型基于70亿和130亿参数量的LLaMa构建。
* PMC-LLaMA：
  * 地址：https://github.com/chaoyi-wu/PMC-LLaMA
    ![](https://img.shields.io/github/stars/chaoyi-wu/PMC-LLaMA)
  * 简介： PMC-LLaMA是一个开源语言模型，通过对LLaMA-7B在总计480万篇生物医学学术论文上进行调质，进一步灌输医学知识，以增强其在医学领域的能力。
* Visual Med-Alpaca：
  * 地址：https://github.com/cambridgeltl/visual-med-alpaca
    ![](https://img.shields.io/github/stars/cambridgeltl/visual-med-alpaca)
  * 简介： Visual Med-Alpaca是一个开源的、参数高效的生物医学基础模型，可以与医学的“视觉专家”集成，用于多模式生物医学任务。该模型基于LLaMa-7B架构构建，使用由GPT-3.5-Turbo和人类专家共同策划的指令集进行训练。
* GatorTronGPT：
  * 地址：https://github.com/uf-hobi-informatics-lab/GatorTronGPT
    ![](https://img.shields.io/github/stars/uf-hobi-informatics-lab/GatorTronGPT)
  * 简介：GatorTronGPT 是一个医疗生成大语言模型。该模型基于GPT-3构建，含有50亿或200亿参数。该模型使用了含有2770亿单词的，由临床和英语文本组成的庞大语料库。
* MedAGI：
  * 地址：https://github.com/JoshuaChou2018/MedAGI
    ![](https://img.shields.io/github/stars/JoshuaChou2018/MedAGI)
  * 简介：MedAGI一个范例，以最低的成本将领域特定的医疗语言模型统一起来，为实现医疗通用人工智能提供了一条可能的途径。
* LLaVA-Med：
  * 地址：https://github.com/microsoft/LLaVA-Med
    ![](https://img.shields.io/github/stars/microsoft/LLaVA-Med)
  * 简介：LLaVA- med使用通用领域LLaVA进行初始化，然后以课程学习方式进行持续训练(首先是生物医学概念对齐，然后是全面的指令调整)。  
* Med-Flamingo：
  * 地址：https://github.com/snap-stanford/med-flamingo
    ![](https://img.shields.io/github/stars/snap-stanford/med-flamingo)
  * 简介：Med-Flamingo是一个视觉语言模型，专门设计用于处理包含图像和文本的交错多模态数据。以Flamingo为基础，Med-Flamingo通过对不同医学学科的多种多模式知识来源进行预训练，进一步增强了在这些医学领域的能力。

### 5.3 医疗VLM

|Model|Paper|Github|
|:-|:-|:-|
|MedVInT|[link](https://arxiv.org/abs/2305.10415)|[link](https://github.com/xiaoman-zhang/PMC-VQA)|
|Med-Flamingo|[link](https://arxiv.org/abs/2307.15189)|[link](https://github.com/snap-stanford/med-flamingo)|
|LLaVA-Med|[link](https://arxiv.org/abs/2306.00890)|[link](https://github.com/microsoft/LLaVA-Med)|
|Qilin-Med-VL|[link](https://arxiv.org/abs/2310.17956)|[link](https://github.com/williamliujl/Qilin-Med-VL)|
|RadFM|[link](http://arxiv.org/abs/2308.02463)|[link](https://github.com/chaoyi-wu/RadFM)|
|MedDr|[link](https://arxiv.org/abs/2404.15127)|[link](https://github.com/sunanhe/MedDr)|
|HuatuoGPT-Vision|[link](https://arxiv.org/abs/2406.19280)|[link](https://github.com/FreedomIntelligence/HuatuoGPT-Vision)|
|BiomedGPT|[link](https://arxiv.org/abs/2305.17100)|[link](https://github.com/taokz/BiomedGPT)|
|Med-MoE|[link](https://arxiv.org/abs/2404.10237v3)|[link](https://github.com/jiangsongtao/Med-MoE)|
|R-LLaVA|[link](https://arxiv.org/abs/2410.20327)|-|
|Med-2E3|[link](https://arxiv.org/abs/2411.12783)|-|
|GMAI-VL|[link](https://arxiv.org/abs/2411.14522v1)|[link](https://github.com/uni-medical/GMAI-VL)|

### 5.4 医疗VLM Benchmark

|Benchmark|Paper|Github|
|:-|:-|:-|
|GMAI-MMBench|[link](https://arxiv.org/abs/2408.03361)|[link](https://github.com/uni-medical/GMAI-MMBench)|
|OmniMedVQA|[link](https://arxiv.org/abs/2402.09181)|[link](https://github.com/OpenGVLab/Multi-Modality-Arena?tab=readme-ov-file#omnimedvqa-a-new-large-scale-comprehensive-evaluation-benchmark-for-medical-lvlm)|
|MMMU|[link](https://arxiv.org/abs/2311.16502)|[link](https://github.com/MMMU-Benchmark/MMMU)|
|MultiMedEval|[link](https://openreview.net/pdf?id=inACgoTK0O)|[link](https://github.com/corentin-ryr/MultiMedEval)|
|WorldMedQA-V|[link](https://www.arxiv.org/abs/2410.12722)|-|

<div align=""right"">
    <b><a href=""#Contents"">↥ back to top</a></b>
</div>

## 6. 相关论文

### 6.1 后ChatGPT时代 可能有帮助的论文

1. 大型语言模型编码临床知识  论文地址：https://arxiv.org/abs/2212.13138

2. ChatGPT在USMLE上的表现：使用大型语言模型进行 AI 辅助医学教育的潜力  论文地址：https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000198

3. 对 ChatGPT 的医疗建议进行（图灵）测试  论文地址：https://arxiv.org/abs/2301.10035

4. Toolformer：语言模型可以自学使用工具  论文地址：https://arxiv.org/abs/2302.04761

5. 检查你的事实并再试一次：利用外部知识和自动反馈改进大型语言模型  论文地址：https://arxiv.org/abs/2302.12813

6. GPT-4 在医学挑战问题上的能力  论文地址：https://arxiv.org/abs/2303.13375


### 6.2 综述类文章

1. 生物医学领域的预训练语言模型：系统调查   [论文地址 ](https://arxiv.org/abs/2110.05006) 
2. 医疗保健深度学习指南  [论文地址 ](https://www.nature.com/articles/s41591-018-0316-z)   nature medicine发表的综述  
3. 医疗保健领域大语言模型综述    [论文地址 ](https://arxiv.org/abs/2310.05694) 

### 6.3 特定任务文章

**电子病历相关文章**

1. Transfer Learning from Medical Literature for Section Prediction in Electronic Health Records   [论文地址](https://www.aclweb.org/anthology/D19-1492/)
2. MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records [论文地址](http://arxiv-download.xixiaoyao.cn/pdf/2102.02340.pdf) 

**医学关系抽取**

1. Leveraging Dependency Forest for Neural Medical Relation Extraction [论文地址](https://www.aclweb.org/anthology/D19-1020/) 

**医学知识图谱**

1. Learning a Health Knowledge Graph from Electronic Medical Records [论文地址](https://www.nature.com/articles/s41598-017-05778-z)

**辅助诊断**

1. Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence [论文地址](https://www.nature.com/articles/s41591-018-0335-9) 

**医疗实体Linking（标准化）**

1. Medical Entity Linking using Triplet Network  [论文地址](https://www.aclweb.org/anthology/W19-1912/)
2. A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization   [论文地址](https://www.aclweb.org/anthology/2020.acl-main.748.pdf)
3. Deep Neural Models for Medical Concept Normalization in User-Generated Texts  [论文地址](https://www.aclweb.org/anthology/P19-2055.pdf)

### 6.4 会议索引

**ACL2020医学领域相关论文列表**

1. A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization [论文地址](https://www.aclweb.org/anthology/2020.acl-main.748/)
2. Biomedical Entity Representations with Synonym Marginalization [论文地址](https://www.aclweb.org/anthology/2020.acl-main.335/)
3. Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain [论文地址](https://www.aclweb.org/anthology/2020.acl-main.613/)
4. MIE: A Medical Information Extractor towards Medical Dialogues [论文地址](https://www.aclweb.org/anthology/2020.acl-main.576/)
5. Rationalizing Medical Relation Prediction from Corpus-level Statistics  [论文地址](https://www.aclweb.org/anthology/2020.acl-main.719/) 

**AAAI2020 医学NLP相关论文列表**

1. On the Generation of Medical Question-Answer Pairs [论文地址](https://arxiv.org/pdf/1811.00681.pdf)
2. LATTE: Latent Type Modeling for Biomedical Entity Linking  [论文地址](https://arxiv.org/pdf/1911.09787.pdf)
3. Learning Conceptual-Contextual Embeddings for Medical Text [论文地址](https://arxiv.org/pdf/1908.06203.pdf)
4. Understanding Medical Conversations with Scattered Keyword Attention and Weak Supervision from Responses [论文地址](http://ir.hit.edu.cn/~car/papers/AAAI2020-Shi-medconv.pdf)
5. Simultaneously Linking Entities and Extracting Relations from Biomedical Text without Mention-level Supervision [论文地址](https://arxiv.org/pdf/1912.01070.pdf)
6. Can Embeddings Adequately Represent Medical Terminology? New Large-Scale Medical Term Similarity Datasets Have the Answer!   [论文地址](https://arxiv.org/pdf/2003.11082.pdf)

**EMNLP2020 医学NLP相关论文列表**

1. Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text [论文地址](https://www.aclweb.org/anthology/2020.emnlp-main.111.pdf)
2. MedDialog: Large-scale Medical Dialogue Datasets [论文地址](https://www.aclweb.org/anthology/2020.emnlp-main.743.pdf)
3. COMETA: A Corpus for Medical Entity Linking in the Social Media  [论文地址](https://www.aclweb.org/anthology/2020.emnlp-main.253.pdf)
4. Biomedical Event Extraction as Sequence Labeling [论文地址](https://www.aclweb.org/anthology/2020.emnlp-main.431.pdf)
5. FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction [论文地址](https://www.aclweb.org/anthology/2020.emnlp-main.165.pdf)  [论文解析:FedED:用于医学关系提取的联邦学习(基于融合蒸馏)](https://blog.csdn.net/lrs1353281004/article/details/111877017)
6. Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition [论文地址](https://arxiv.org/pdf/2010.03746.pdf)
7. A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization [论文地址](https://www.aclweb.org/anthology/2020.emnlp-main.116.pdf)
8. BioMegatron: Larger Biomedical Domain Language Model [论文地址](https://www.aclweb.org/anthology/2020.emnlp-main.379.pdf)
9. Querying Across Genres for Medical Claims in News  [论文地址](https://www.aclweb.org/anthology/2020.emnlp-main.139.pdf)

<div align=""right"">
    <b><a href=""#Contents"">↥ back to top</a></b>
</div>

## 7. 开源工具包

1. 分词工具：PKUSEG [项目地址](https://github.com/lancopku/pkuseg-python)   项目说明： 北京大学推出的多领域中文分词工具，支持选择医学领域。

<div align=""right"">
    <b><a href=""#Contents"">↥ back to top</a></b>
</div>

## 8. 工业级产品解决方案

1. [灵医智慧](https://01.baidu.com/index.html)

2. [左手医生](https://open.zuoshouyisheng.com/)

3. [医渡云研究院-医学自然语言处理](https://www.yiducloud.com.cn/academy.html)

4. [百度-医学文本结构化](https://ai.baidu.com/solution/mtp)

5. [阿里云-医学自然语言处理](https://help.aliyun.com/document_detail/179395.html)

<div align=""right"">
    <b><a href=""#Contents"">↥ back to top</a></b>
</div>

## 9. blog分享

1. [Alpaca：一个强大的开源指令跟随模型](https://crfm.stanford.edu/2023/03/13/alpaca.html) 
2. [医疗领域构建自然语言处理系统的经验教训](http://www.oreilly.com.cn/radar/?p=2083)
3. [大数据时代的医学公共数据库与数据挖掘技术简介](https://mp.weixin.qq.com/s/tA44U4bJUttnROfrzpNYcQ)
4. [从ACL 2021中看NLP在医疗领域应用的发展，附资源下载](https://mp.weixin.qq.com/s/RhcHvRWHRnYUg6u9vXoIGA)

<div align=""right"">
    <b><a href=""#Contents"">↥ back to top</a></b>
</div>

## 10. 友情链接

1.  [awesome_Chinese_medical_NLP](https://github.com/GanjinZero/awesome_Chinese_medical_NLP)
2.  [中文NLP数据集搜索](https://www.cluebenchmarks.com/dataSet_search.html)
3.  [medical-data(海量医疗相关数据)](https://github.com/beamandrew/medical-data)
4.  [天池数据集(其中包含多个医疗NLP数据集)](https://tianchi.aliyun.com/dataset)

<div align=""right"">
    <b><a href=""#Contents"">↥ back to top</a></b>
</div>

## 11. reference


```bibtex
@misc{medical_NLP_github,
  author = {Xidong Wang, Ziyue Lin and Jing Tang, Rongsheng Wang, Benyou Wang},
  title = {Medical NLP},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/FreedomIntelligence/Medical_NLP}}
}
```

[![Star History Chart](https://api.star-history.com/svg?repos=FreedomIntelligence/Medical_NLP&type=Date)](https://star-history.com/#FreedomIntelligence/Medical_NLP)
","['lrs1353281004', 'wangxidong06', 'WangRongsheng', 'RobinLin2002', 'vaew', 'pariskang', 'wabyking', 'whaleloops']",0,0.63,0,,,,,,55,,,,
591046304,R_kgDOIzqmoA,awesome-computational-social-science,gesiscss/awesome-computational-social-science,0,gesiscss,https://github.com/gesiscss/awesome-computational-social-science,A list of awesome resources for Computational Social Science,0,2023-01-19 20:03:03+00:00,2025-03-07 22:29:58+00:00,2024-11-01 19:06:13+00:00,,193,639,639,R,1,1,1,1,0,0,82,0,0,3,cc0-1.0,1,0,0,public,82,3,639,main,1,1,"# Awesome Computational Social Science [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

>An [awesome](https://github.com/sindresorhus/awesome) curated list of
>resources for Computational Social Science.  
> Inspired by [Awesome Network
> Analysis](https://github.com/briatte/awesome-network-analysis) and others.

The order of entries within categories is either alphabetically or
chronologically.  
**Please add your resources according to the respective ordering**

![logo](logo.png)


## Contents

- __[Books](#books)__
- __[Conferences](#conferences)__
- __[Education](#education)__
  - [Online courses and Material](#online-courses-and-material)
  - [Videos and Lectures](#videos-and-lectures)
  - [Workshops](#workshops)
  - [University Course Programs](#university-course-programs)
- __[Research Groups](#research-groups)__
- __[Journals](#journals)__
- __[Selected Papers](#selected-papers)__
- __[Software](#software)__
  - [R](#R)
  - [Python](#python)
  - [Tutorials](#tutorials)
- __[Miscellaneous](#miscellaneous)__
- __[Relevant Awesome Lists](#relevant-awesome-lists)__
- __[Contributing](#contributing)__


## Books

> Entries are ordered chronologically

- [Growing Artificial Societies: Social Science from the Bottom Up](https://mitpress.mit.edu/9780262550253/growing-artificial-societies/), by Joshua M. Epstein and Robert L. Axtell (1996)
- [Six Degrees: The Science of a Connected Age](https://wwnorton.com/books/9780393325423), by Duncan J. Watts (2004)
- [Networks, Crowds, and Markets: Reasoning About a Highly Connected World](https://www.cs.cornell.edu/home/kleinber/networks-book/), by David Easley and Jon Kleinberg (2010)
- [Everything is Obvious](https://www.penguinrandomhouse.com/books/187477/everything-is-obvious-by-duncan-j-watts/), by Duncan J. Watts (2011)
- [Agent_Zero: Toward Neurocognitive Foundations for Generative Social Science](https://press.princeton.edu/books/hardcover/9780691158884/agentzero), by Joshua M. Epstein (2014)
- [Social Phenomena: From Data Analysis to Models](https://doi.org/10.1007/978-3-319-14011-7), edited by Bruno Gonçalves, Nicola Perra (2015)
- [Computational Social Sciences](https://www.springer.com/series/11784/books?page=3), Springer book series (2015-2023)
- [Big Data Is Not a Monolith](https://mitpress.mit.edu/9780262529488/big-data-is-not-a-monolith/), edited by Cassidy R. Sugimoto, Hamid R. Ekbia, and Michael Mattioli (2016)
- [Bit By Bit: Social Research in the Digital Age](https://www.bitbybitbook.com/), by Matthew J. Salganik (2017)
- [Decoding the Social World: Data Science and the Unintended Consequences of Communication](https://mitpress.mit.edu/9780262037075/), by Sandra González-Bailón (2017)
- [Digital Sociology: The Reinvention of Social Research](https://www.wiley.com/en-us/Digital+Sociology:+The+Reinvention+of+Social+Research-p-9780745684789), by Noortje Marres (2017)
- [How Behavior Spreads: The Science of Complex Contagions](https://press.princeton.edu/books/hardcover/9780691175317/how-behavior-spreads), by Damon Centola (2018)
- [The Model Thinker: What You Need to Know to Make Data Work for You](https://www.basicbooks.com/titles/scott-e-page/the-model-thinker/9780465094639/), by Scott E. Page (2018)
- [What is Digital Sociology?](https://www.wiley.com/en-us/What+is+Digital+Sociology%3F-p-9781509527144), by Neil Selwyn (2019)
- [The Oxford Handbook of Networked Communication](https://global.oup.com/academic/product/the-oxford-handbook-of-networked-communication-9780190460518?cc=de&lang=en&), edited by Brooke Foucault Welles and Sandra González-Bailón (2020)
- [Research Exposed: How Empirical Social Science Gets Done in the Digital Age](https://cup.columbia.edu/book/research-exposed/9780231188777), edited by Eszter Hargittai (2020)
- [Retooling Politics: How Digital Media Are Shaping Democracy](https://doi.org/10.1017/9781108297820), by Andreas Jungherr, Gonzalo Rivero, and Daniel Gayo-Avello (2020)
- [Sociologia Digital: uma breve introdução](https://repositorio.ufba.br/bitstream/ri/32746/5/SociologiaDigitalPDF.pdf), by Leonardo Nascimento (2020)
- [A First Course in Network Science](https://cambridgeuniversitypress.github.io/FirstCourseNetworkScience/), by Filippo Menczer, Santo Fortunato, Clayton A. Davis (2020)
- [How Humans Judge Machines](https://www.judgingmachines.com/), by Cesar A. Hidalgo, Diana Orghian, Jordi Albo Canals, Filipa De Almeida, Natalia Martin (2021)
- [The Science of Science](https://www.dashunwang.com/book/the-science-of-science), by Dashun Wang and Albert-László Barabási (2021)
- [Doing Computational Social Science - A Practical Introduction](https://uk.sagepub.com/en-gb/eur/doing-computational-social-science/book266031), by John McLevey (2021)
- [Big Data and Social Science: Data Science Methods and Tools for Research and Practice, 2nd Edition](https://textbook.coleridgeinitiative.org/), by Ian Foster, Rayid Ghani, Ron S. Jarmin, Frauke Kreuter and Julia Lane (2021)
- [Text as Data: A New Framework for Machine Learning and the Social Sciences](https://press.princeton.edu/books/paperback/9780691207551/text-as-data),, by Justin Grimmer, Margaret E. Roberts, and Brandon M. Stewart (2022)
- [Computational Analysis of Communication](https://cssbook.net/), by Wouter van Atteveldt, Damian Trilling, and Carlos Arcila Calderon (2022)
- [The SAGE Handbook of Social Media Research Methods](https://uk.sagepub.com/en-gb/eur/the-sage-handbook-of-social-media-research-methods/book272098), edited by Anabel Quan-Haase and Luke Sloan (2022)
- [Handbook of Computational Social Science Volume 1 & 2](https://www.routledge.com/Handbook-of-Computational-Social-Science---Vol-1--Vol-2/Engel-Quan-Haase-Liu-Lyberg/p/book/9781032111438), edited by Uwe Engel, Anabel Quan-Haase, Sunny Xun Liu, Lars Lyberg (2022)
- [Research Handbook on Digital Sociology](https://www.e-elgar.com/shop/gbp/research-handbook-on-digital-sociology-9781789906752.html), edited by Jan Skopek (2023)
- [Handbook of Computational Social Science for Policy](https://link.springer.com/book/10.1007/978-3-031-16624-2), by Eleonora Bertoni, Matteo Fontana, Lorenzo Gabrielli, Serena Signorelli, Michele Vespe (2023)
- [Computational Thinking for Social Scientists](https://jaeyk.github.io/comp_thinking_social_science/), by Jae Yeon Kim (2023)
- [Computational Thinking and Social Science](https://uk.sagepub.com/en-gb/eur/computational-thinking-and-social-science/book268542), by Matti Nelimarkka (2023)

## Conferences

> Relevant conferences where the community (or parts thereof) meets

- [BigSurv - Big Data Meets Survey Science](https://www.bigsurv.org/)
- [CHI - ACM CHI Conference on Human Factors in Computing Systems](https://chi.acm.org/)
- [Complex Networks - International Conference on Complex Networks and their Applications](https://www.complexnetworks.org/)
- [COMPTEXT Conference](https://www.comptextconference.org/)
- [Conference on Complex Systems](https://cssociety.org/events), in particular the Computational Social Science satellite
- [CSCW - ACM Conference On Computer-Supported Cooperative Work](https://cscw.acm.org/)
- [EPSA - European Political Science Association Conference](https://epsanet.org/) (Methods division)
- [IC2S2 - The International Conference for Computational Social Science](http://ic2s2.org)
- [ICA - Annual International Communication Association Conference](https://www.icahdq.org/) (Methods Division)
- [ICWSM - International AAAI Conference on Web and Social Media](https://www.icwsm.org/)
- [NetSci - International Conference on Network Science](https://netscisociety.net/events/netsci/)
- [SSC - Social Simulation Conference](http://www.essa.eu.org/event-type/conference/)
- [WebSci - ACM Web Science Conference](https://www.websci25.org/)

[Computational Social Science Events Worldwide](https://calendar.google.com/calendar/u/0/embed?src=19jm0329h91akpv0srml6c24ec@group.calendar.google.com&ctz=Europe/Rome), Public Calendar


## Education

> Learning material/courses tailored towards Computational Social
> Science


### Online Courses and Material

> See also the [Software](#software) section for material on software tools

- [CSC2552 Topics in Computational Social Science: AI, Data, and Society](https://www.cs.toronto.edu/~ashton/csc2552) - Seminar course taught by Ashton Anderson at the University of Toronto, Canada. 
- [NLP+CSS 201 Tutorials](https://nlp-css-201-tutorials.github.io/nlp-css-201-tutorials/) - Tutorials for advanced natural language processing methods designed for computational social science research.
- [SAGE collection of teaching material for Computational Social Science](https://ocean.sagepub.com/teaching-materials-for-computational-social-science) - Large collection of various teaching material for Computational Social Science
- [SICSS Learning Materials](https://sicss.io/overview) - Open source teaching and learning resources for computational social science
- [Social and Economic Networks: Models and Analysis](https://www.coursera.org/learn/social-economic-networks) - Online course on social and economic networks taught by Matthew O. Jackson
- [Toolkit for Digital Methods](https://wiki.helsinki.fi/display/TDM/Toolkit+for+Digital+Methods+Home) - A wiki of resources for digital methods in Social Sciences
- [UCCSS: University of California Computational Social Science](https://www.youtube.com/playlist?list=PLtjBSCvWCU3rJ_XAqaOr7NoQYWY005WCI) -  Multidisciplinary, multi-perspective, and multi-method guide on how to better understand society and human behavior with modern research tool.


### Videos and Lectures

- [A gentle introduction to network science](https://www.youtube.com/watch?v=L6CqqlILBCI), by Renaud Lambiotte, University of Oxford (2018)
- [Introduction to computational social science](https://youtu.be/EF7X9wwl0q4), by Matthew J. Salganik, Princeton University (2019)
- [Python for Computational Social Science and Digital Humanities](https://youtu.be/T7qMZH25co0), by Christopher Cameron, Cornell University (2023)


### Workshops

- [BIGSSS Computational Social Science Summer Schools](https://www.bigsss-bremen.de/academic-program/summer-school-program/computational-social-science-summer-schools)
- [Edinburgh Data and Text Analysis Summer School](https://www.cdcs.ed.ac.uk/training/Data-Text-Analysis-Summer-School)
- [ESSA (European Social Simulation Association) summer school](http://www.essa.eu.org/event-type/summer-school/)
- [Essex Summer School in Social Science Data Analysis](https://essexsummerschool.com/)
- [GESIS Fall Seminar in Computational Social Science](https://www.gesis.org/en/gesis-training/what-we-offer/fall-seminar-in-computational-social-science)
- [Mediterranean School of Complex Networks](https://mediterraneanschoolcomplex.net/index.html)
- [NLP + CSS](https://sites.google.com/site/nlpandcss/): a series of workshops on natural language processing (NLP) and computational social science (CSS). 
- [The Summer Institutes in Computational Social Science](https://sicss.io/)
- [Topics in Digital and Computational Demography](https://www.demogr.mpg.de/en/career_6122/international_advanced_studies_in_demography_6682/courses_6931/), PhD level, one week course.


### University Course Programs

> Bachelor, Master, PhD programs (alphabetically by country using [ISO 3166-1 alpha 3](https://en.wikipedia.org/wiki/ISO_3166-1) codes)

#### Bachelor programs

- [BA/BSc in Data Science and Society (DSS)](https://undergraduate.ceu.edu/bachelor-degree-data-science-society), Central European University, AUT
- [B.Sc. Social Data Science](https://www.uni-wh.de/en/uwh-international/university/degree-programmes-taught-in-english/b-sc-social-data-science/), Witten/Herdecke University, DEU 
- [BSc Social Sciences with Data Science](https://www.ucl.ac.uk/prospective-students/undergraduate/degrees/social-sciences-data-science-bsc), University College London, GBR
- [Bachelor of Social Sciences in Data Science and Policy Studies](https://dsps.ssc.cuhk.edu.hk/), Chinese University of Hong Kong, HKG
- [Bachelor Computational Social Science](https://www.uva.nl/en/programmes/bachelors/computational-social-science/study-programme/study-programme.html), University of Amsterdam, NLD
- [BS in Social Data Science](https://sdsc.umd.edu/), University of Maryland, USA
- [BS in Social Data Analytics](https://soda.la.psu.edu/programs/undergraduate/), Pennsylvania State University, USA
- [BS in Computational Social Science](https://www.dins.pitt.edu/academics/bs-computational-social-science), University of Pittsburgh, USA

#### Master programs

- [Master Computational Social System](https://www.tugraz.at/en/studying-and-teaching/degree-and-certificate-programmes/masters-degree-programmes/computational-social-systems),
  TU Graz, AUT
- [Master of Science program in Social Data Science](https://networkdatascience.ceu.edu/msc-social-data-science), Central European University, AUT
- [Master of Sociology: Quantitative Analysis and Social Data Science specialisation](https://soc.kuleuven.be/fsw/toekomstigestudenten/english/mos/datascience), KU Leuven, BEL 
- [Master in Computational Social Sciences](https://www.unilu.ch/studium/studienangebot/master/kultur-und-sozialwissenschaftliche-fakultaet/lucerne-master-in-computational-social-sciences-lumacss/) at the University of Lucerne, CHE
- [MSc in Computational Social Science](https://hsspg.cuhk.edu.cn/en/msccss), Chinese University of Hong Kong (Shenzhen), CHN
- [Master of Data Science for Public Policy](https://www.hertie-school.org/en/mds), Hertie School, DEU
- [Master Social and Economic Data Science](https://www.polver.uni-konstanz.de/studium/master/master-social-and-economic-data-science/),
University of Konstanz, DEU
- [Mannheim Master in Social Data Science](https://www.uni-mannheim.de/en/academics/before-your-studies/programs/mannheim-master-in-social-data-science/), University of Mannheim, DEU
- [M.Sc. Quantitative Data Science Methods: Psychometrics, Econometrics and Machine Learning](https://uni-tuebingen.de/fakultaeten/wirtschafts-und-sozialwissenschaftliche-fakultaet/faecher/fachbereich-sozialwissenschaften/methodenzentrum/studium/msc-qds/), University of Tübingen, DEU
- ~~[Master Computational Social Systems](https://www.rwth-aachen.de/cms/root/Studium/Vor-dem-Studium/Studiengaenge/Liste-Aktuelle-Studiengaenge/Studiengangbeschreibung/~sthd/Computational-Social-Systems-M-Sc/)~~ (This course of study is being phased out), RWTH Aachen, DEU
- [Master of Science (MSc) in Social Data Science](https://studies.ku.dk/masters/social-data-science/), University of Copenhagen, DNK
- [Master in Computational Social Science](https://www.uc3m.es/master/computational-social-science), Universidad Carlos III de Madrid, ESP
- [Master of Contemporary Societies](https://www.helsinki.fi/en/degree-programmes/contemporary-societies-masters-programme), track on social data science, University of Helsinki, FIN
- [MSc&T “Data and Economics for Public Policy”](https://programmes.polytechnique.edu/en/master-all-msct-programs/data-and-economics-for-public-policy), Institut Polytechnique de Paris, FRA
- [Master in Data Science for Social Sciences](https://www.tse-fr.eu/master-data-science-social-sciences), Toulouse School of Economics, FRA
- [MSc Applied Social Data Science](https://www.lse.ac.uk/Methodology/Study/MSc-Applied-Social-Data-Science), London School of Economics and Political Science, GBR
- [MPA in Data Science for Public Policy](https://www.lse.ac.uk/school-of-public-policy/study/mpaindatascience), London School of Economics and Political Science, GBR
- [MSc Social and Geographic Data Science](https://www.ucl.ac.uk/prospective-students/graduate/taught-degrees/social-and-geographic-data-science-msc), University College London, GBR
- [MSc Data Science and Public Policy](https://www.ucl.ac.uk/prospective-students/graduate/taught-degrees/data-science-and-public-policy-political-science-msc), University College London, GBR
- [MSc Human and Social Data Science](https://www.sussex.ac.uk/study/masters/courses/human-and-social-data-science-msc), University of Sussex, GBR
- [MSc Social Data Science](https://www.exeter.ac.uk/study/postgraduate/courses/politics/socialdatasciencemsc/), University of Exeter, GBR
- [MSc Social Data Science](https://www.oii.ox.ac.uk/study/msc-in-social-data-science/), University of Oxford, GBR
- [Masters in Computational Social Science](https://sola.iitj.ac.in/postgraduate-program/), Indian Institute of Technology (IIT) Jodhpur, IND
- [Master Politics and Data Science](https://www.ucd.ie/connected_politics/studywithus/), University College Dublin, IRE
- [MSc Social Data Science](https://hub.ucd.ie/usis/!W_HU_MENU.P_PUBLISH?p_tag=PROG&MAJR=W559), University College Dublin, IRE
- [MSc/PG Diploma Applied Social Data Science](https://www.tcd.ie/Political_Science/programmes/postgraduate/pg-dip-applied-social-data-science/), Trinity College Dublin, IRE
- [Master Data Science for Economics](https://dse.cdl.unimi.it/en), University of Milan, ITA
- [Master in Social Data Science](https://sds.sociologia.unimib.it/), University of Milano-Bicocca, ITA
- [Master in Social Sciences for a Digital Society](https://vu.nl/en/education/master/social-sciences-for-a-digital-society), Vrije Universiteit Amsterdam, NLD
- [Master's Programme Computational Social Science](https://liu.se/en/education/program/f7mcd), Linköping University, SWE
- [Master Computational Social Science](https://gsssh.ku.edu.tr/en/departments/computational-social-sciences/),
Koç University, TUR
- [Master of Computational Social Science (MaCSS)](https://macss.berkeley.edu/), University of California Berkeley, USA
- [M.S. in Computational Social Science](https://css.ucsd.edu/masters/index.html), University of California San Diego, USA
- [M.A. in Computational Social Science](https://macss.uchicago.edu/), University of Chicago, USA
- [M.S. in Computational Analysis & Public Policy](https://capp.uchicago.edu/), University of Chicago, USA
- [Master of Science in Data Analytics & Computational Social Science](https://www.umass.edu/social-sciences/academics/data-analytics-computational-social-science/ms-dacss), University of Massachusetts Amherst, USA
- [Master of Arts in Interdisciplinary Studies: Computational Social Science Concentration](https://mais.gmu.edu/programs/la-mais-isin-css), George Mason University, USA
- [Master of Science in Data Science for Public Policy](https://mccourt.georgetown.edu/master-of-science-in-data-science-for-public-policy/), Georgetown University, USA
- [MA in Quantitative Methods in the Social Sciences: Data Science Focus](https://www.qmss.columbia.edu/), Columbia University, USA
- [Master of Science in Public Policy and Data Science](https://priceschool.usc.edu/mppds/), University of Southern California, USA
- [Master's Degree Applied Urban Science and Informatics](https://cusp.nyu.edu/masters-degree/), New York University, USA
- [Master of Science in Survey and Data Science](https://surveydatascience.isr.umich.edu/survey-and-data-science-masters-degree-program), University of Michigan, USA
- [Master of Science in Social Policy + Data Analytics for Social Policy Certificate](https://www.sp2.upenn.edu/program/master-of-science-in-social-policy-data-analytics-for-social-policy-certificate/), University of Pennsylvania, USA

#### PhD programs

- [PhD in Computational Social Science](https://hsspg.cuhk.edu.cn/en/mphilphdcss), Chinese University of Hong Kong (Shenzhen), CHN
- [DPhil Social Data Science](https://www.oii.ox.ac.uk/study/dphil-in-social-data-science/), University of Oxford, GBR
- [PhD Quantitative and Computational Social Science](https://www.ucd.ie/spire/study/prospectivephdstudents/phdquantitativeandcomputationalsocialscience/), University College Dublin, IRE
- [PhD in Information](https://www.si.umich.edu/programs/phd-information), University of Michigan, Ann Arbor, USA
- [Complex Networks and Systems track of the PhD in Informatics](https://informatics.indiana.edu/programs/phd-informatics/complex-networks-and-systems.html), Indiana University, Bloomington, USA
- [Network Science PhD Program](https://www.networkscienceinstitute.org/phd), Northeastern University, Boston, USA
- [PhD in Information Science](https://www.colorado.edu/cmci/infoscience/phd#admissions_requirements-2457), University of Colorado, Boulder, USA
- [PhD Program in Social & Engineering Systems (SES)](https://idss.mit.edu/academics/ses_doc/),  Massachusetts Institute of Technology, Cambridge, USA
- [PhD in Information Sciences](https://ischool.illinois.edu/degrees-programs/graduate/phd-information-sciences),  University of Illinois, Champaign, USA
- [Doctor of Philosophy in Information Studies (PhD)](https://ischool.umd.edu/academics/phd-information-studies/), University of Maryland, College Park, USA
- [Ph.D. in Information Science](https://www.ischool.berkeley.edu/programs/phd), University of California, Berkeley, USA
- [Doctoral Program in Communication](https://communication.ucdavis.edu/doctoral-program), University of California, Davis, USA
- [Ph.D. Specialization in Computational Social Science](https://css.ucsd.edu/phd-specialization/index.html), University of California, San Diego, USA
- [Ph.D. in Information and Media](https://comartsci.msu.edu/academics/academic-departments/default/graduate/phd-information-and-media), Michigan State University, East Lansing, USA
- [Ph.D. in Information Technology Management](https://broad.msu.edu/phd/information-technology-management/), Michigan State University, East Lansing, USA
- [PhD in Media, Technology, and Society (MTS)](https://mts.northwestern.edu/), Northwestern University, Evanston, USA
- [Dual Ph.D. Program in Computer Science and Communication: Technology and Social Behavior (TSB)](https://tsb.northwestern.edu/), Northwestern University, Evanston, USA
- [PhD Program at the Kellogg School of Management](https://www.kellogg.northwestern.edu/doctoral.aspx), Northwestern University, Evanston, USA
- [PhD in Computational Social Science](https://science.gmu.edu/academics/departments-units/computational-data-sciences/computational-social-science-phd), George Mason University, Fairfax, USA
- [Ph.D. in Communication and Information Sciences](https://www.ics.hawaii.edu/academics/graduate-programs/ph-d-in-cis/), University of Hawaii at Manoa, Honolulu, USA
- [Information Science Ph.D. program](https://infosci.cornell.edu/phd), Cornell University, Ithaca, USA
- [Doctoral Program in Industrial & Systems Engineering (ISE)](https://viterbigradadmission.usc.edu/doctoral/phddegrees/phd-industrial-systems-engineering/), University of Southern California, Los Angeles, USA
- [Communication (PhD)](https://annenberg.usc.edu/academics/communication-phd), University of Southern California, Los Angeles, USA
- [Ph.D. Program in Communication, Information, and Media](https://comminfo.rutgers.edu/graduate-programs/phd-program), Rutgers University, New Brunswick, USA
- [Ph.D. Specialization in Data Science](https://datascience.columbia.edu/education/programs/ph-d-with-a-specialization-in-data-science/), Columbia University, New York, USA
- [Doctorate in Communication](https://www.asc.upenn.edu/graduate), University of Pennsylvania, Philadelphia, USA
- [PhD in Societal Computing](https://sc.cs.cmu.edu/program/index.html), Carnegie Mellon University, Pittsburgh, USA
- [Doctorate in Informaiton Science](https://ischool.uw.edu/programs/phd), University of Washington, Seattle, USA
- [PhD degree in Management Science and Engineering (MS&E)](https://iriss.stanford.edu/research-centers/computational-social-science), Stanford University, USA
- [PhD in Information](https://ischool.arizona.edu/phd-information), University of Arizona, Tucson, USA

## Research Groups

> Ordered alphabetically by country and city (using [ISO 3166-1 alpha 3](https://en.wikipedia.org/wiki/ISO_3166-1) country codes)

- [Data Science and AI Lab](https://nyuad.nyu.edu/en/research/faculty-labs-and-projects/data-science-and-ai-lab.html), Abu Dhabi, ARE
- [Computational Social Science Lab](https://www.sydney.edu.au/arts/our-research/centres-institutes-and-groups/sydney-social-sciences-and-humanities-advanced-research-centre/research/computational-social-science-lab.html), University of Sydney, AUS
- [CSS Lab TU Graz](https://www.tugraz.at/institute/isds/research/research-groups/computational-social-science-lab-css-lab/), Graz, AUT
- [Digital Humanities Lab at UFBA](https://www.labhd.ufba.br/), Salvador, BRA
- [Computational Social Science Lab](http://csslab.cs.toronto.edu/), University of Toronto, Toronto, CAN
- [Professorship of Computational Social Science (COSS)](https://coss.ethz.ch), ETH Zürich, CHE 
- [Social Networks Lab](https://sn.ethz.ch/), Zürich, CHE
- [Computational Communication Collaboratory](https://computational-communication.com/), Nanjing, CHN
- [Institute of Computational Social Science](https://hss.cuhk.edu.cn/en/page/1006), CUHK-Shenzhen, CHN
- [CSS Lab RWTH Aachen](https://www.css-lab.rwth-aachen.de/), Aachen, DEU
- [Data Science Lab, Hertie School](https://www.hertie-school.org/en/datasciencelab), Berlin, DEU
- [CSS Department at GESIS](https://www.gesis.org/en/institute/departments/computational-social-science), Cologne, DEU
- [Chair of Data Science in the Economic and Social Sciences](https://www.bwl.uni-mannheim.de/en/information-systems/chairs/prof-dr-strohmaier/), Mannheim, DEU
- [Computational Social Science and Big Data TUM Munich](https://www.hfp.tum.de/css/startseite/), Munich, DEU
- [Department of Digital and Computational Demography](https://www.demogr.mpg.de/en/research_6120/digital_and_computational_demography_zagheni_11666/), Rostock, DEU
- [Copenhagen Center for Social Data Science (SODAS)](https://sodas.ku.dk), Copenhagen, DNK
- [NEtwoRks, Data, and Society (NERDS)](https://nerds.itu.dk/), Copenhagen, DNK
- [Computational Social Science Group](https://css.cs.ut.ee/index.html), Tartu, EST
- [Institute for Cross-Disciplinary Physics and Complex Systems (IFISC)](https://ifisc.uib-csic.es/en/research/dynamics-and-collective-phenomena-social-systems/), Palma, ESP
- [Centre for Social Data Science](https://www.helsinki.fi/en/networks/centre-social-data-science), Helsinki, FIN
- [Social Data Science Lab at Cardiff University](http://socialdatalab.net/), Cardiff, GBR
- [Centre for Data, Culture & Society](https://www.cdcs.ed.ac.uk/), Edinburgh, GBR
- [Oxford Internet Institute](https://www.oii.ox.ac.uk/about/), Oxford, GBR
- [Social Data Institute](https://www.ucl.ac.uk/social-data/home/social-data-institute), University College London, GBR
- [Web Mining Lab](http://weblab.com.cityu.edu.hk/blog/), City University of Hong Kong, Hong Kong, HKG
- [Computational Social Sciences and Law Lab](https://www.cityu.edu.hk/cpal/lab_cssl.htm), City University of Hong Kong, Hong Kong, HKG
- [RC2S2 Research Center for Computational Social Science](https://rc2s2.elte.hu/en/home/), Eötvös Loránd University, Budapest, HUN
- [Connected_Politics Lab](https://www.ucd.ie/connected_politics/), Dublin, IRL
- [Computational Social Science Centre (CSSC)](https://centri.unibo.it/computational-social-science), Bologna, ITA
- [Behave Lab](https://behavelab.org/), Milan, ITA
- [Center of Data Science and Complexity for Society (CDCS)](https://cdcs.di.uniroma1.it/index.php), Sapienza University, Rome, ITA
- [Center for Computational Social Science and Human Dynamics (C2S2)](https://c2s2.unitn.it), University of Trento and Bruno Kessler Foundation, Trento, ITA
- [Mobile and Social Computing Lab (MobS Lab)](https://ict.fbk.eu/units/mobs/), Bruno Kessler Foundation, Trento, ITA
- [CENTAI Institute](https://centai.eu), Turin, ITA
- [Center for Computational Social Science](http://ccss.kobe-u.ac.jp/en/index.html), Kobe University, Kobe, JPN
- [Computational Social Science Lab](https://www.colorlessgreen.info/), Tokyo Institute of Technology, Tokyo, JPN
- [Computational Communication Science Amsterdam](https://ccs.amsterdam), NLD
- [Social and Behavioural Data Science Centre](https://sobedsc.uva.nl/), Amsterdam, NLD
- [UNU-MERIT Computational Innovation Lab on Crises, Transformation and Sustainable Development](https://www.merit.unu.edu/cil/), Maastricht, NLD
- [ODISSEI (Open Data Infrastructure for Social Science and Economic Innovations)](https://odissei-data.nl/en/), Rotterdam, NLD
- [Political Data Science (PODS) Research Group](https://www.sv.uio.no/isv/english/research/groups/political-data-science/), Oslo, NOR
- [UU-InfoLab (Uppsala University Information Laboratory)](https://uuinfolab.github.io/), Uppsala, SWE
- [Center for Computational Social Sciences](https://ccss.ku.edu.tr/), Koç University, Istanbul, TUR
- [Viral Lab (VRLLab)](https://varollab.com/), Sabanci University, Istanbul, TUR
- [Communication Data and Network Analytics Lab](https://cdna.survey.sinica.edu.tw/), Academia Sinica, Taipei, TWN
- [Observatory on Social Media](https://osome.iu.edu/), Indiana University, Bloomington, USA
- [Center for Complex Networks and Systems Research](https://cnets.indiana.edu/), Indiana University, Bloomington, USA
- [Soda (Social Data and AI) Lab](https://soda-labo.github.io/), Indiana University, Bloomington, USA
- [Lazerlab](https://lazerlab.net/), Northeastern University, Boston, USA
- [Laboratory for the Modeling of Biological and Socio-Technical Systems (MOBS Lab)](https://www.mobs-lab.org/), Northeastern University, Boston, USA
- [Network Science Institute (NetSI)](https://www.networkscienceinstitute.org/), Northeastern University, Boston, USA
- [Al-Adala Lab](https://www.theadalab.com/home), University of Colorado, Boulder, USA
- [Colorado Laboratory for Users, Media, and Networks (COLUMN)](https://columnlab.github.io/), University of Colorado, Boulder, USA
- [MIT Institute for Data, Systems, and Society (IDSS)](https://idss.mit.edu/),  Massachusetts Institute of Technology, Cambridge, USA
- [Center for Spatial Data Science (CSDS)](https://spatial.uchicago.edu/), University of Chicago, Chicago, USA
- [Knowledge Lab](https://www.knowledgelab.org/), University of Chicago, Chicago, USA
- [Social Data Science Center (SoDa)](https://socialdatascience.umd.edu/), University of Maryland, College Park, USA
- [MESO (Modeling Emergent Social Order) Lab]( https://www.mesoorderlab.com/), Ohio State University, Columbus, USA
- [Computational Communication Research Lab (C^2 or C-square)](https://c2.ucdavis.edu/), University of California, Davis, USA
- [Quello Center](https://quello.msu.edu/), Michigan State University, East Lansing, USA
- [Computational Journalism Lab](https://cj-lab.org/), Northwestern University, Evanston, USA
- [Delta Lab](https://delta.northwestern.edu/), Northwestern University, Evanston, USA
- [Lab on Innovation, Networks, and Knowledge (LINK)](https://link.soc.northwestern.edu/), Northwestern University, Evanston, USA
- [People, Space, and Algorithms (PSA) Research Group](https://www.psagroup.org/), Northwestern University, Evanston, USA
- [Science of Networks in Communities (SONIC) Lab](https://sonic.northwestern.edu/), Northwestern University, Evanston, USA
- [Center for AI in Society(CAIS)](https://cais.usc.edu/), University of Southern California, Los Angeles, USA
- [Computational Social Science Laboratory](https://dornsife.usc.edu/bci/computational-social-science-laboratory/), University of Southern California, Los Angeles, USA
- [HUmans | MAchines | Networks | Society Lab (HUMANS)](http://www.emilio.ferrara.name/), University of Southern California, Los Angeles, USA
- [Computational Social Science Institute at UMass](https://www.cssi.umass.edu), Massachusetts Amherst, USA
- [Computational Social Science Lab](https://csslab.rutgers.edu/), Rutgers University, New Brunswick, USA
- [Working Group on Computational Social Science](https://datascience.columbia.edu/research/groups/computational-social-science/), Columbia University, New York, USA
- [Center for Social Media and Politics (CSMaP)](https://csmapnyu.org/), New York Univiersity, New York, USA
- [Center for Information Networks and Democracy(CIND)](https://www.asc.upenn.edu/research/centers/center-for-information-networks-and-democracy), University of Pennsylvania, Philadelphia, USA
- [Computational Social Science Lab (CSSLab)](https://css.seas.upenn.edu/), University of Pennsylvania, Philadelphia, USA
- [Network Dynamics Group](https://ndg.asc.upenn.edu/), University of Pennsylvania, Philadelphia, USA
- [Center for Computational Analysis of Social and Organizational Systems (CASOS)](http://www.casos.cs.cmu.edu/index.php), Carnegie Mellon University, Pittsburgh, USA
- [PITT Initiative on Computational Social Science](https://pittcss.github.io/),  University of Pittsburgh, Pittsburgh, USA
- [Santa Fe Institute (SFI)](https://www.santafe.edu/), Santa Fe, USA
- [Community Data Science Collective](https://wiki.communitydata.science/Main_Page), University of Washington, Seattle, USA
- [IRiSS Center for Computational Social Science](https://iriss.stanford.edu/research-centers/computational-social-science), Stanford University, USA
- [Center for Social Data Analytics (C-SoDA)](https://soda.la.psu.edu/), Pennsylvania State University, University Park, USA
- [Computational Social Science Lab](https://www.cla.purdue.edu/academic/polsci/research/labs/computational-social-science-lab/index.html), Purdue University, West Lafayette, USA


## Journals

> Ordered alphabetically

- [Big Data & Society](https://journals.sagepub.com/home/bds)
- [Computational Communication Research](https://computationalcommunication.org/ccr)
- [Computational Economics](https://www.springer.com/journal/10614)
- [EPJ Data Science](https://epjdatascience.springeropen.com/)
- [Frontiers in Big Data](https://www.frontiersin.org/journals/big-data)
- [Information, Communication & Society](https://www.tandfonline.com/journals/rics20)
- [Journal of Artificial Societies and Social Simulation](https://www.jasss.org/JASSS.html)
- [Journal of Computational Social Science](https://www.springer.com/journal/42001)
- [Journal of Quantitative Description: Digital Media](https://journalqd.org)
- [Journal of Social Computing](https://www.sciopen.com/journal/2688-5255)
- [Nature Human Behavior](https://www.nature.com/nathumbehav/)
- [New Media & Society](https://journals.sagepub.com/home/nms)
- [Social Media and Society](https://journals.sagepub.com/home/sms)
- [Social Science Computer Review](https://journals.sagepub.com/home/ssc)


## Selected Papers

> Important papers for/about the field, not specific research. Ordered chronologically.

- [From Factors to Actors: Computational Sociology and Agent-Based Modeling](https://doi.org/10.1146/annurev.soc.28.110601.141117) by Michael W. Macy and Robert Willer (2002)
- [Life in the network: the coming age of computational social science](https://doi.org/10.1126%2Fscience.1167742) by David Lazer et al. (2009)
- [Critical Questions for Big Data](https://doi.org/10.1080/1369118X.2012.678878) by Dana Boyd and Kate Crawford (2012)
- [A 61-million-person experiment in social influence and political mobilization](https://doi.org/10.1038/nature11421) by Robert M. Bond et al. (2012)
- [Manifesto of computational social science](https://doi.org/10.1140/epjst/e2012-01697-8) by R. Conte, N. Gilbert, G. Bonelli, C. Cioffi-Revilla, G. Deffuant, J. Kertesz, V. Loreto, S. Moat, J. -P. Nadal, A. Sanchez, A. Nowak, A. Flache, M. San Miguel & D. Helbing (2012)
- [Digital Footprints: Opportunities and Challenges for Online Social Research](https://doi.org/10.1146/annurev-soc-071913-043145) by Scott A. Golder and Michael W. Macy (2014)
- [Social media for large studies of
  behavior](https://doi.org/10.1126/science.346.6213.1063) by Derek Ruths and Jürgen Pfeffer (2014)
- [Sociology in the Era of Big Data: The Ascent of Forensic Social Science](https://doi.org/10.1007/s12108-015-9291-8) by Daniel A. McFarland, Kevin Lewis & Amir Goldberg (2016)
- [Installing computational social science: Facing the challenges of new information and communication technologies in social science](http://journals.sagepub.com/doi/10.1177/2059799115622763) by Raphael H. Heiberger & Jan R. Riebling (2016)
- [Computational Social Science Methodology, Anyone?](https://doi.org/10.1027/1614-2241/a000127) by Joop J. Hox (2017)
- [The empiricist’s challenge: Asking meaningful questions in political science in the age of big data](https://doi.org/10.1080/19331681.2017.1312187) by Andreas Jungherr and Yannis Theocharis (2017)
- [Computational Social Science ≠ Computer Science + Social Data](https://dl.acm.org/doi/10.1145/3132698) by Hanna Wallach (2018)
- [When Communication Meets Computation: Opportunities, Challenges, and Pitfalls in Computational Communication Science](https://doi.org/10.1080/19312458.2018.1458084) by Wouter van Atteveldt and Tai-Quan Peng (2018)
- [Analytical sociology and computational social science](https://doi.org/10.1007/s42001-017-0006-5) by Keuschnigg, M., Lovsjö, N. & Hedström, P. (2018)
- [Computation and the Sociological Imagination](https://doi.org/10.1177/1536504219883850) by James Evans and Jacob G. Foster (2019)
- [Machine Learning for Sociology](https://doi.org/10.1146/annurev-soc-073117-041106) by Mario Molina and Filiz Garip (2019)
- [Social data: Biases, methodological pitfalls, and ethical boundaries](https://www.frontiersin.org/articles/10.3389/fdata.2019.00013/full) by Alexandra Olteanu, Carlos Castillo, Fernando Diaz and Emre Kıcıman (2019)
- [Computational social science: Obstacles and opportunities](https://www.science.org/doi/abs/10.1126/science.aaz8170) by David Lazer et al. (2020) ([open access version](https://dspace.mit.edu/bitstream/handle/1721.1/130299/Computational%20social%20science-%20Obstacles%20and%20opportunities.pdf?sequence=1))
- [Computational Social Science and the Study of Political Communication](https://doi.org/10.1080/10584609.2020.1833121) by Yannis Theocharis and Andreas Jungherr (2020)
- [Computational Social Science and Sociology](https://doi.org/10.1146/annurev-soc-121919-054621) by Achim Edelmann, Tom Wolff, Danielle Montagne and Christopher A. Bail (2020)
- [Machine Learning for Social Science: An Agnostic Approach](https://doi.org/10.1146/annurev-polisci-053119-015921) by Justin Grimmer, Margaret E. Roberts, and Brandon M. Stewart (2021)
- [Measuring algorithmically infused societies](https://doi.org/10.1038/s41586-021-03666-1) by Claudia Wagner, Markus Strohmaier, Alexandra Olteanu, Emre Kıcıman, Noshir Contractor & Tina Eliassi-Rad (2021)
- [The data revolution in social science needs qualitative research](https://doi.org/10.1038/s41562-022-01333-7) by Nikolitsa Grigoropoulou & Mario L. Small (2022)


## Software

> Focus on accessible introduction into computational tools, preferably open
> source material


### R

- [Awesome R](https://github.com/qinwf/awesome-R) for general resources in R


### Python

- [Awesome Python](https://github.com/vinta/awesome-python) (other lists: [1](https://github.com/kirang89/pycrumbs), [2](https://github.com/svaksha/pythonidae), [3](https://github.com/trekhleb/learn-python)) for general resources in Python


### Tutorials

- [APIs for Social Scientists](https://bookdown.org/paul/apis_for_social_scientists/)
- [Introduction to Computational Social Science in R](https://bookdown.org/markhoff/css/)
- [Introduction to Computational Social Science Methods with Python](https://github.com/gesiscss/css_methods_python)
- [Quanteda Tutorials for Quantitative Text Analysis in R](https://tutorials.quanteda.io)
- [R Course Material for Communication Science](https://github.com/ccs-amsterdam/r-course-material)


## Miscellaneous

> Resources that do not fit into other categories

- [Google Group Computational Social Science Network](https://groups.google.com/g/CSSNET)
- [Podcast about Computational Communication Science](https://anchor.fm/ccs-pod)
- [RatSWD publication ""Big data in social, behavioural, and economic sciences: Data access and research data management (Including an expert opinion on ""Web scraping in independent academic research"")""](https://www.konsortswd.de/en/latest/publication/big-data-in-social-behavioural-and-economic-sciences-data-access-and-research-data-management/)
- [reddit community ""CompSocial""](https://www.reddit.com/r/CompSocial/)


## Relevant Awesome Lists

- [Awesome Causality](https://github.com/napsternxg/awesome-causality)
- [Awesome Community Detection](https://github.com/benedekrozemberczki/awesome-community-detection)
- [Awesome Data Science with Python](https://github.com/r0f1/datascience) ([another](https://github.com/krzjoa/awesome-python-data-science))
- [Awesome Data Science](https://github.com/academic/awesome-datascience)
- [Awesome Data Visualization](https://github.com/javierluraschi/awesome-dataviz)
- [Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning)
- [Awesome Digital Humanities](https://github.com/dh-tech/awesome-digital-humanities)
- [Awesome Jupyter](https://github.com/markusschanta/awesome-jupyter)
- [Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)
- [Awesome MySQL](https://github.com/shlomi-noach/awesome-mysql)
- [Awesome Network Analysis](https://github.com/briatte/awesome-network-analysis)
- [Awesome NLP](https://github.com/keon/awesome-nlp) ([another one](https://github.com/edobashira/speech-language-processing))
- [Awesome Notebooks](https://github.com/jupyter-naas/awesome-notebooks)
- [Awesome Open Science](https://github.com/silky/awesome-open-science)
- [Awesome Python](https://github.com/vinta/awesome-python) (other lists: [1](https://github.com/kirang89/pycrumbs), [2](https://github.com/svaksha/pythonidae), [3](https://github.com/trekhleb/learn-python))
- [Awesome Quarto](https://github.com/mcanouil/awesome-quarto)
- [Awesome R](https://github.com/qinwf/awesome-R)
- [Awesome Research Software Registries](https://github.com/NLeSC/awesome-research-software-registries)
- [Awesome Scholarly Data Analysis](https://github.com/napsternxg/awesome-scholarly-data-analysis)


-----


## Contributing

Contributions welcome! Read the [contribution guidelines](contributing.md) first.
","['schochastics', 'chainsawriot', 'pitmonticone', 'jchgu', 'akbaritabar', 'cllei12', 'wanLo', 'leofn', 'arnim', 'jobreu', 'TLouf', 'sandrofsousa', 'cschwem2er', 'dilettagoglia', 'Indiiigo', 'justinchuntingho', 'yang3kc', 'msom', 'matnel', 'Michael98Liu', 'Holzhauer', 'stefan-mueller', 'clauwag']",0,0.63,0,,,,,,34,,,,
162770838,MDEwOlJlcG9zaXRvcnkxNjI3NzA4Mzg=,C-3-Framework,gjy3035/C-3-Framework,0,gjy3035,https://github.com/gjy3035/C-3-Framework,An open-source PyTorch code for crowd counting,0,2018-12-22 01:02:21+00:00,2025-03-05 15:55:52+00:00,2024-03-30 03:30:40+00:00,,13880,711,711,Jupyter Notebook,1,1,1,1,0,0,201,0,0,75,mit,1,0,0,public,201,75,711,python3.x,1,,,"['gjy3035', 'zengxin1020', 'PetitBai', 'DTennant', 'homurajiang', 'sxf118']",0,0.78,0,,,,,,22,,,,
199501522,MDEwOlJlcG9zaXRvcnkxOTk1MDE1MjI=,gmtsar,gmtsar/gmtsar,0,gmtsar,https://github.com/gmtsar/gmtsar,GMTSAR,0,2019-07-29 17:52:31+00:00,2025-02-26 07:44:35+00:00,2025-02-04 13:36:59+00:00,http://topex.ucsd.edu/gmtsar,18664,289,289,C,1,1,1,1,0,1,99,0,0,125,gpl-3.0,1,0,0,public,99,125,289,master,1,1,,"['Xiaohua-Eric-Xu', 'dsandwell', 'PaulWessel', 'xiaopengtong', 'bjmarfito', 'kguns32', 'avalentino', 'ikselven', 'calefmt', 'SteffanDavies', 'dunyuliu', 'ericlindsey', 'jeanlucmargot', 'pinotree', 'rtburns-jpl', 'vintingb', 'kmaterna']",1,0.75,0,,,,Directory exists,,26,,,,
22789533,MDEwOlJlcG9zaXRvcnkyMjc4OTUzMw==,gpgpu-sim_distribution,gpgpu-sim/gpgpu-sim_distribution,0,gpgpu-sim,https://github.com/gpgpu-sim/gpgpu-sim_distribution,"GPGPU-Sim provides a detailed simulation model of contemporary NVIDIA GPUs running CUDA and/or OpenCL workloads.  It includes support for features such as TensorCores and CUDA Dynamic Parallelism as well as a performance visualization tool, AerialVisoin, and an integrated energy model, GPUWattch.",0,2014-08-09 15:55:39+00:00,2025-03-07 08:45:13+00:00,2025-02-15 15:54:23+00:00,,35132,1241,1241,C++,1,1,1,1,0,0,541,0,0,156,other,1,0,0,public,541,156,1241,dev,1,1,"Welcome to GPGPU-Sim, a cycle-level simulator modeling contemporary graphics
processing units (GPUs) running GPU computing workloads written in CUDA or
OpenCL. Also included in GPGPU-Sim is a performance visualization tool called
AerialVision and a configurable and extensible power model called AccelWattch.
GPGPU-Sim and AccelWattch have been rigorously validated with performance and
power measurements of real hardware GPUs.

This version of GPGPU-Sim has been tested with a subset of CUDA version 4.2,
5.0, 5.5, 6.0, 7.5, 8.0, 9.0, 9.1, 10, and 11

Please see the copyright notice in the file COPYRIGHT distributed with this
release in the same directory as this file.

GPGPU-Sim 4.0 is compatible with Accel-Sim simulation framework. With the support 
of Accel-Sim, GPGPU-Sim 4.0 can run NVIDIA SASS traces (trace-based simulation) 
generated by NVIDIA's dynamic binary instrumentation tool (NVBit). For more information 
about Accel-Sim, see [https://accel-sim.github.io/](https://accel-sim.github.io/)

If you use GPGPU-Sim 4.0 in your research, please cite:

Mahmoud Khairy, Zhesheng Shen, Tor M. Aamodt, Timothy G Rogers.
Accel-Sim: An Extensible Simulation Framework for Validated GPU Modeling.
In proceedings of the 47th IEEE/ACM International Symposium on Computer Architecture (ISCA),
May 29 - June 3, 2020.

If you use CuDNN or PyTorch support (execution-driven simulation), checkpointing or our new debugging tool for functional 
simulation errors in GPGPU-Sim for your research, please cite:

Jonathan Lew, Deval Shah, Suchita Pati, Shaylin Cattell, Mengchi Zhang, Amruth Sandhupatla, 
Christopher Ng, Negar Goli, Matthew D. Sinclair, Timothy G. Rogers, Tor M. Aamodt
Analyzing Machine Learning Workloads Using a Detailed GPU Simulator, arXiv:1811.08933,
https://arxiv.org/abs/1811.08933

If you use the Tensor Core model in GPGPU-Sim or GPGPU-Sim's CUTLASS Library 
for your research please cite:

Md Aamir Raihan, Negar Goli, Tor Aamodt,
Modeling Deep Learning Accelerator Enabled GPUs, arXiv:1811.08309, 
https://arxiv.org/abs/1811.08309

If you use the AccelWattch power model in your research, please cite:

Vijay Kandiah, Scott Peverelle, Mahmoud Khairy, Junrui Pan, Amogh Manjunath, Timothy G. Rogers, Tor M. Aamodt, and Nikos Hardavellas. 2021.
AccelWattch: A Power Modeling Framework for Modern GPUs. In MICRO54: 54th Annual IEEE/ACM International Symposium on Microarchitecture
(MICRO ’21), October 18–22, 2021, Virtual Event, Greece.

If you use the support for CUDA dynamic parallelism in your research, please cite:

Jin Wang and Sudhakar Yalamanchili, Characterization and Analysis of Dynamic 
Parallelism in Unstructured GPU Applications, 2014 IEEE International Symposium 
on Workload Characterization (IISWC), November 2014.

If you use figures plotted using AerialVision in your publications, please cite:

Aaron Ariel, Wilson W. L. Fung, Andrew Turner, Tor M. Aamodt, Visualizing
Complex Dynamics in Many-Core Accelerator Architectures, In Proceedings of the
IEEE International Symposium on Performance Analysis of Systems and Software
(ISPASS), pp. 164-174, White Plains, NY, March 28-30, 2010.

This file contains instructions on installing, building and running GPGPU-Sim.
Detailed documentation on what GPGPU-Sim models, how to configure it, and a
guide to the source code can be found here: <http://gpgpu-sim.org/manual/>.
Instructions for building doxygen source code documentation are included below.

Previous versions of GPGPU-Sim (3.2.0 to 4.1.0) included the [GPUWattch Energy model](http://gpgpu-sim.org/gpuwattch/) which has been replaced by AccelWattch version 1.0 in GPGPU-Sim version 4.2.0. AccelWattch supports modern GPUs and is validated against a NVIDIA Volta QV100 GPU. Detailed documentation on AccelWattch can be found here: [AccelWattch Overview](https://github.com/VijayKandiah/accel-sim-framework#accelwattch-overview) and [AccelWattch MICRO'21 Artifact Manual](https://github.com/VijayKandiah/accel-sim-framework/blob/release/AccelWattch.md).

If you have questions, please sign up for the google groups page (see
gpgpu-sim.org), but note that use of this simulator does not imply any level of
support. Questions answered on a best effort basis.

To submit a bug report, go here: http://www.gpgpu-sim.org/bugs/

See Section 2 ""INSTALLING, BUILDING and RUNNING GPGPU-Sim"" below to get started.

See file CHANGES for updates in this and earlier versions.

# CONTRIBUTIONS and HISTORY

## GPGPU-Sim

GPGPU-Sim was created by Tor Aamodt's research group at the University of
British Columbia. Many have directly contributed to development of GPGPU-Sim
including: Tor Aamodt, Wilson W.L. Fung, Ali Bakhoda, George Yuan, Ivan Sham,
Henry Wong, Henry Tran, Andrew Turner, Aaron Ariel, Inderpret Singh, Tim
Rogers, Jimmy Kwa, Andrew Boktor, Ayub Gubran Tayler Hetherington and others.

GPGPU-Sim models the features of a modern graphics processor that are relevant
to non-graphics applications. The first version of GPGPU-Sim was used in a
MICRO'07 paper and follow-on ACM TACO paper on dynamic warp formation. That
version of GPGPU-Sim used the SimpleScalar PISA instruction set for functional
simulation, and various configuration files indicating which loops should be
spawned as kernels on the GPU, along with reconvergence points required for
SIMT execution to provide a programming model simlar to CUDA/OpenCL. Creating
benchmarks for the original GPGPU-Sim simulator was a very time consuming
process and the validity of code generation for CPU run on a GPU was questioned
by some. These issues motivated the development an interface for directly
running CUDA applications to leverage the growing number of applications being
developed to use CUDA. We subsequently added support for OpenCL and removed
all SimpleScalar code.

The interconnection network is simulated using the booksim simulator developed
by Bill Dally's research group at Stanford.

To produce output that matches the output from running the same CUDA program on
the GPU, we have implemented several PTX instructions using the CUDA Math
library (part of the CUDA toolkit). Code to interface with the CUDA Math
library is contained in cuda-math.h, which also includes several structures
derived from vector_types.h (one of the CUDA header files).

## AccelWattch Power Model

AccelWattch (introduced in GPGPU-Sim 4.2.0) was developed by researchers at 
Northwestern University, Purdue University, and the University of British Columbia. 
Contributors to AccelWattch include Nikos Hardavellas's research group at Northwestern University: 
Vijay Kandiah; Tor Aamodt's research group at the University of British Columbia: Scott Peverelle; 
and Timothy Rogers's research group at Purdue University: Mahmoud Khairy, Junrui Pan, and Amogh Manjunath. 

AccelWattch leverages McPAT, which was developed by Sheng Li et al. at the
University of Notre Dame, Hewlett-Packard Labs, Seoul National University, and
the University of California, San Diego. The McPAT paper can be found at
http://www.hpl.hp.com/research/mcpat/micro09.pdf.


# INSTALLING, BUILDING and RUNNING GPGPU-Sim

Assuming all dependencies required by GPGPU-Sim are installed on your system,
to build GPGPU-Sim all you need to do is add the following line to your
~/.bashrc file (assuming the CUDA Toolkit was installed in /usr/local/cuda):

```
  export CUDA_INSTALL_PATH=/usr/local/cuda
```

then type

```
  bash
  source setup_environment
  make
```

If the above fails, see ""Step 1"" and ""Step 2"" below.

If the above worked, see ""Step 3"" below, which explains how to run a CUDA
benchmark on GPGPU-Sim.

## Step 1: Dependencies

GPGPU-Sim was developed on SUSE Linux (this release was tested with SUSE
version 11.3) and has been used on several other Linux platforms (both 32-bit
and 64-bit systems). In principle, GPGPU-Sim should work with any linux
distribution as long as the following software dependencies are satisfied.

Download and install the CUDA Toolkit. It is recommended to use version 3.1 for
normal PTX simulation and version 4.0 for cuobjdump support and/or to use
PTXPlus (Harware instruction set support). Note that it is possible to have
multiple versions of the CUDA toolkit installed on a single system -- just
install them in different directories and set your CUDA_INSTALL_PATH
environment variable to point to the version you want to use.

[Optional] If you want to run OpenCL on the simulator, download and install
NVIDIA's OpenCL driver from <http://developer.nvidia.com/opencl>. Update your
PATH and LD_LIBRARY_PATH as indicated by the NVIDIA install scripts. Note that
you will need to use the lib64 directory if you are using a 64-bit machine. We
have tested OpenCL on GPGPU-Sim using NVIDIA driver version 256.40
<http://developer.download.nvidia.com/compute/cuda/3_1/drivers/devdriver_3.1_linux_64_256.40.run>
This version of GPGPU-Sim has been updated to support more recent versions of
the NVIDIA drivers (tested on version 295.20).

GPGPU-Sim dependencies:
- gcc
- g++
- make
- makedepend
- xutils
- bison
- flex
- zlib
- CUDA Toolkit

GPGPU-Sim documentation dependencies:
- doxygen
- graphvi

AerialVision dependencies:
- python-pmw
- python-ply
- python-numpy
- libpng12-dev
- python-matplotlib

We used gcc/g++ version 4.5.1, bison version 2.4.1, and flex version 2.5.35.

If you are using Ubuntu, the following commands will install all required
dependencies besides the CUDA Toolkit.

GPGPU-Sim dependencies:

	sudo apt-get install build-essential xutils-dev bison zlib1g-dev flex libglu1-mesa-dev

GPGPU-Sim documentation dependencies:

	sudo apt-get install doxygen graphviz

AerialVision dependencies:

	sudo apt-get install python-pmw python-ply python-numpy libpng12-dev python-matplotlib

CUDA SDK dependencies:

	sudo apt-get install libxi-dev libxmu-dev libglut3-dev

If you are running applications which use NVIDIA libraries such as cuDNN and 
cuBLAS, install them too.

Finally, ensure CUDA_INSTALL_PATH is set to the location where you installed
the CUDA Toolkit (e.g., /usr/local/cuda) and that \$CUDA_INSTALL_PATH/bin is in
your PATH. You probably want to modify your .bashrc file to incude the
following (this assumes the CUDA Toolkit was installed in /usr/local/cuda):

	export CUDA_INSTALL_PATH=/usr/local/cuda
	export PATH=$CUDA_INSTALL_PATH/bin

If running applications which use cuDNN or cuBLAS:

	export CUDNN_PATH=<Path To cuDNN Directory>
	export LD_LIBRARY_PATH=$CUDA_INSTALL_PATH/lib64:$CUDA_INSTALL_PATH/lib:$CUDNN_PATH/lib64

	

## Step 2: Build

To build the simulator, you first need to configure how you want it to be
built. From the root directory of the simulator, type the following commands in
a bash shell (you can check you are using a bash shell by running the command
""echo \$SHELL"", which should print ""/bin/bash""):

source setup_environment <build_type>

replace <build_type> with debug or release. Use release if you need faster
simulation and debug if you need to run the simulator in gdb. If nothing is
specified, release will be used by default.

Now you are ready to build the simulator, just run

	make


After make is done, the simulator would be ready to use. To clean the build,
run

	make clean

To build the doxygen generated documentations, run

	make docs

To clean the docs run

	make cleandocs


The documentation resides at doc/doxygen/html.

To run Pytorch applications with the simulator, install the modified Pytorch library as well by following instructions [here](https://github.com/gpgpu-sim/pytorch-gpgpu-sim).

## Step 3: Run

Before we run, we need to make sure the application's executable file is dynamically linked to CUDA runtime library. This can be done during compilation of your program by introducing the nvcc flag ""-lcudart"" in makefile (quotes should be excluded).

To confirm the same, type the follwoing command:

`ldd <your_application_name>`

You should see that your application is using libcudart.so file in GPGPUSim directory. If the application is a Pytorch application, `<your_application_name>` should be `$PYTORCH_BIN`, which should be set during the Pytorch installation.

If running applications which use cuDNN or cuBLAS:

* Modify the Makefile or the compilation command of the application to change 
   all the dynamic links to static ones, for example:
	* `-L$(CUDA_PATH)/lib64 -lcublas` to
	  `-L$(CUDA_PATH)/lib64 -lcublas_static`

	* `-L$(CUDNN_PATH)/lib64 -lcudnn` to
	  `-L$(CUDNN_PATH)/lib64 -lcudnn_static`

* Modify the Makefile or the compilation command such that the following 
   flags are used by the nvcc compiler:
	`-gencode arch=compute_61,code=compute_61`

   (the number 61 refers to the SM version. You would need to set it based 
   on the GPGPU-Sim config `-gpgpu-ptx-force-max-capability` you use)

Copy the contents of configs/QuadroFX5800/ or configs/GTX480/ to your
application's working directory. These files configure the microarchitecture
models to resemble the respective GPGPU architectures.

To use ptxplus (native ISA) change the following options in the configuration
file to ""1"" (Note: you need CUDA version 4.0) as follows:

	-gpgpu_ptx_use_cuobjdump 1
	-gpgpu_ptx_convert_to_ptxplus 1

Now To run a CUDA application on the simulator, simply execute

	source setup_environment <build_type>

Use the same <build_type> you used while building the simulator. Then just
launch the executable as you would if it was to run on the hardware. By
running `source setup_environment <build_type>` you change your LD_LIBRARY_PATH
to point to GPGPU-Sim's instead of CUDA or OpenCL runtime so that you do NOT
need to re-compile your application simply to run it on GPGPU-Sim.

To revert back to running on the hardware, remove GPGPU-Sim from your
LD_LIBRARY_PATH environment variable.

The following GPGPU-Sim configuration options are used to enable AccelWattch

	-power_simulation_enabled 1 (1=Enabled, 0=Not enabled)
	-power_simulation_mode 0 (0=AccelWattch_SASS_SIM or AccelWattch_PTX_SIM, 1=AccelWattch_SASS_HW, 2=AccelWattch_SASS_HYBRID)
	-accelwattch_xml_file <filename>.xml

The AccelWattch XML configuration file name is set to accelwattch_sass_sim.xml by default and is
currently provided for SM7_QV100, SM7_TITANV, SM75_RTX2060_S, and SM6_TITANX. 
Note that all these AccelWattch XML configuration files are tuned only for SM7_QV100. Please refer to
<https://github.com/VijayKandiah/accel-sim-framework#accelwattch-overview> for more information.

Running OpenCL applications is identical to running CUDA applications. However,
OpenCL applications need to communicate with the NVIDIA driver in order to
build OpenCL at runtime. GPGPU-Sim supports offloading this compilation to a
remote machine. The hostname of this machine can be specified using the
environment variable OPENCL_REMOTE_GPU_HOST. This variable should also be set
through the setup_environment script. If you are offloading to a remote machine,
you might want to setup passwordless ssh login to that machine in order to
avoid having too retype your password for every execution of an OpenCL
application.

If you need to run the set of applications in the NVIDIA CUDA SDK code
samples then you will need to download, install and build the SDK.

The CUDA applications from the ISPASS 2009 paper mentioned above are
distributed separately on github under the repo ispass2009-benchmarks.
The README.ISPASS-2009 file distributed with the benchmarks now contains
updated instructions for running the benchmarks on GPGPU-Sim v3.x.

# (OPTIONAL) Contributing to GPGPU-Sim (ADVANCED USERS ONLY)

If you have made modifications to the simulator and wish to incorporate new
features/bugfixes from subsequent releases the following instructions may help.
They are meant only as a starting point and only recommended for users
comfortable with using source control who have experience modifying and
debugging GPGPU-Sim.

WARNING: Before following the procedure below, back up your modifications to
GPGPU-Sim. The following procedure may cause you to lose all your changes. In
general, merging code changes can require manual intervention and even in the
case where a merge proceeds automatically it may introduce errors. If many
edits have been made the merge process can be a painful manual process. Hence,
you will almost certainly want to have a copy of your code as it existed before
you followed the procedure below in case you need to start over again. You
will need to consult the documentation for git in addition to these
instructions in the case of any complications.

STOP. BACK UP YOUR CHANGES BEFORE PROCEEDING. YOU HAVE BEEN WARNED. TWICE.

To update GPGPU-Sim you need git to be installed on your system. Below we
assume that you ran the following command to get the source code of GPGPU-Sim:

```
  git clone git://dev.ece.ubc.ca/gpgpu-sim
```

Since running the above command you have made local changes and we have
published changes to GPGPU-Sim on the above git server. You have looked at the
changes we made, looking at both the new CHANGES file and probably even the
source code differences. You decide you want to incorporate our changes into
your modified version of GPGPU-Sim.

Before updating your source code, we recommend you remove any object files:

```
  make clean
```

Then, run the following command in the root directory of GPGPU-Sim:

```
  git pull
```

While git is pulling the latest changes, conflicts might arise due to changes
that you made that conflict with the latest updates. In this case, you need to
resolved those conflicts manually. You can either edit the conflicting files
directly using your favorite text editor, or you can use the following command
to open a graphical merge tool to do the merge:

```
  git mergetool
```

## Testing updated version of GPGPU-Sim

Now you should test that the merged version ""works"". This means following the
steps for building GPGPU-Sim in the _new_ README file (not this version) since
they may have changed. Assuming the code compiles without errors/warnings the
next step is to do some regression testing. At UBC we have an extensive set of
regression tests we run against our internal development branch when we make
changes. In the future we may make this set of regression tests publically
available. For now, you will want to compile the merged code and re-run all of
the applications you care about (implying these applications worked for you
before you did the merge). You want to do this before making further changes to
identify any compile time or runtime errors that occur due to the code merging
process.


# MISCELLANEOUS

## Speeding up the execution

Some applications take several hours to execute on GPGPUSim. This is because the simulator has to dump the PTX, analyze them and get resource usage statistics. This can be avoided everytime we execute the program in the following way:

1. Execute the program by enabling “-save_embedded_ptx 1” in config file, execute the code and let cuobjdump command dump all necessary files. After this process, you will get 2 new files namely:  _cuobjdump_complete_output_<some_random_name> and _1.ptx

2. Create new environment variables or include the below in your .bashrc file:
	1. export PTX_SIM_USE_PTX_FILE=_1.ptx
	2. export PTX_SIM_KERNELFILE=_1.ptx
	3. export CUOBJDUMP_SIM_FILE=_cuobjdump_complete_output_<some_random_name>

3. Disable -save_embedded_ptx flag, execute the code again. This will skip the dumping by cuobjdump and directly goes to executing the program thus saving time.


## Debugging failing GPGPU-Sim Regressions
 
Credits: Tor M Aamodt

To debug failing GPGPU-Sim regression tests you need to run them locally.  The fastest way to do this, assuming you are working with GPGPU-Sim versions more recent than the GPGPU-Sim dev branch circa March 28, 2018 (commit hash 2221d208a745a098a60b0d24c05007e92aaba092), is to install Docker.  The instructions below were tested with Docker CE version 18.03 on Ubuntu and Mac OS.  Docker will enable you to run the same set of regressions used by GPGPU-Sim when submitting a pull request to https://github.com/gpgpu-sim/gpgpu-sim_distribution and also allow you to log in and launch GPGPU-Sim in gdb so you can inspect failures.  

1. Install Docker.  On Ubuntu 14.04 and 16.04 the following instructions work:  https://docs.docker.com/install/linux/docker-ce/ubuntu/#uninstall-old-versions 

2. Clone GPGPU-Sim from your fork of GPGPU-Sim. For example:

	git clone https://github.com/<YOUR GITHUB USERNAME>/gpgpu-sim_distribution.git


3. Run the following command (this is all one line) to run the regressions in docker:
	```
	docker run --privileged -v `pwd`:/home/runner/gpgpu-sim_distribution:rw aamodt/gpgpu-sim_regress:latest /bin/bash -c ""./start_torque.sh; chown -R runner /home/runner/gpgpu-sim_distribution; su - runner -c 'source /home/runner/gpgpu-sim_distribution/setup_environment && make -j -C /home/runner/gpgpu-sim_distribution && cd /home/runner/gpgpu-sim_simulations/ && git pull && /home/runner/gpgpu-sim_simulations/util/job_launching/run_simulations.py -c /home/runner/gpgpu-sim_simulations/util/job_launching/regression_recipies/rodinia_2.0-ft/configs.gtx1080ti.yml -N regress && /home/runner/gpgpu-sim_simulations/util/job_launching/monitor_func_test.py -v -N regress'; tail -f /dev/null""
	```
	Explanation: The last part of this command, ""tail -f /dev/null"" will keep the docker container running after the regressions finish.  This enables you to log into the container to run the same tests inside gdb so you can debug.   The ""--privileged"" part enables you to use breakpoints inside gdb in a container.  The ""-v"" part maps the current directory (with the GPGPU-Sim source code you want to test) into the container. The string ""aamodt/gpgpu-sim_regress:latest"" is a tag for a container setup to run regressions which will be downloaded from docker hub.  The portion starting with /bin/bash is a set of commands run inside a bash shell inside the container.  E.g., the command start_torque.sh starts up a queue manager inside the container.  

	If the above command stops with the message ""fatal: unable to access 'https://github.com/tgrogers/gpgpu-sim_simulations.git/': Could not resolve host: github.com"" this likely means your computer sits behind a firewall which is blocking access to Google's name servers (e.g., 8.8.8.8).  To get around this you will need to modify th above command to point to your local DNS server.  Lookup your DNS server IP address which we will call <DNS_IP_ADDRESS> below.  On Ubuntu run ""ifconfig"" to lookup the network interface connecting your computer to the network.  Then run ""nmcli device show <interface name>"" to find the IP address of your DNS server.  Modify the above command to include ""--dns <DNS_IP_ADDRESS>"" after ""run"", E.g.,
	```
	docker run --dns <DNS_IP_ADDRESS> --privileged -v `pwd`:/home/runner/gpgpu-sim_distribution:rw aamodt/gpgpu-sim_regress:latest /bin/bash -c ""./start_torque.sh; chown -R runner /home/runner/gpgpu-sim_distribution; su - runner -c 'source /home/runner/gpgpu-sim_distribution/setup_environment && make -j -C /home/runner/gpgpu-sim_distribution && cd /home/runner/gpgpu-sim_simulations/ && git pull && /home/runner/gpgpu-sim_simulations/util/job_launching/run_simulations.py -c /home/runner/gpgpu-sim_simulations/util/job_launching/regression_recipies/rodinia_2.0-ft/configs.gtx1080ti.yml -N regress && /home/runner/gpgpu-sim_simulations/util/job_launching/monitor_func_test.py -v -N regress'; tail -f /dev/null""
	```

4. Find the CONTAINER ID associated with your docker container by running ""docker ps"". 

5. Log into the container by running the command:
	```
	docker exec -it <CONTAINER_ID> /bin/bash -c ""su -l runner""`
	```
	The container is running Ubuntu 16.04 and has screen, cscope and vim installed (if you find a favorite Linux tool missing, it is fairly easy to create derived containers that have additional tools).

6. Lookup the directory of the regression test you want to debug by going to the regression log file directory:
	```
	cd /home/runner/gpgpu-sim_simulations/util/job_launching/logfiles
	```

7.  The file ""failed_job_log_sim_log.regress.<DATE>.txt"" includes information about the failed test including its simulation directory.  For the following example, I'll assume the first failing test was ""hotspot-rodinia-2.0-ft-30_6_40___data_result_30_6_40_txt--GTX1080Ti"" for which the simulation directory is /home/runner/gpgpu-sim_simulations/util/job_launching/../../sim_run_4.2/hotspot-rodinia-2.0-ft/30_6_40___data_result_30_6_40_txt/GTX1080Ti/

8.  Change to the simulation directory using:
	```
	cd <simulation_directory>
	```
	E.g., `cd /home/runner/gpgpu-sim_simulations/util/job_launching/../../sim_run_4.2/hotspot-rodinia-2.0-ft/30_6_40___data_result_30_6_40_txt/GTX1080Ti/`

	This directory should contain a file called ""torque.sim"" that contains commands used to launch the simulation during regression tests.  We will modify this file to enable us to re-run the regression test in gdb.   This directory should also contain a file containing the standard output during the regression test.  This file will end in .o<number> where <number> is the torque queue manager job number.  For the running example for me this file is called ""hotspot-rodinia-2.0-ft-30_6_40___data_result_30_6_40_txt.o2"".  Open this file to determine the LD_LIBRARY_PATH settings used when launching the simulation.  Look for a line that starts ""doing: export LD_LIBRARY_PATH"" and copy the entire line starting with ""export LD_LIBRARY_PATH ...""

9. Paste the ""export LD_LIBRARY_PATH ..."" line into the bash shell to set LD_LIBRARY_PATH.  E.g.,
	```
	export LD_LIBRARY_PATH=/home/runner/gpgpu-sim_simulations/util/job_launching/../../sim_run_4.2/gpgpu-sim-builds/libcudart_gpgpu-sim_git-commit-177d02254ae38b6331b17dd6cd139b570a03c589_modified_0.so:/gpgpu-sim/usr/local/gcc-4.5.4/lib64:/gpgpu-sim/usr/local/gcc-4.5.4/lib:/gpgpu-sim/usr/local/gcc-4.5.4/lib/gcc/x86_64-unknown-linux-gnu/lib64/:/gpgpu-sim/usr/local/gcc-4.5.4/lib/gcc/x86_64-unknown-linux-gnu/4.5.4/:/usr/lib/x86_64-linux-gnu:/home/runner/gpgpu-sim_distribution/lib/gcc-4.5.4/cuda-4020/release:/gpgpu-sim/usr/local/gcc-4.5.4/lib64:/gpgpu-sim/usr/local/gcc-4.5.4/lib:/gpgpu-sim/usr/local/gcc-4.5.4/lib/gcc/x86_64-unknown-linux-gnu/lib64/:/gpgpu-sim/usr/local/gcc-4.5.4/lib/gcc/x86_64-unknown-linux-gnu/4.5.4/:/usr/lib/x86_64-linux-gnu:
	```

10. In the same shell, build the debug version of GPGPU-Sim then return to the directory above:
	```
	pushd ~/gpgpu-sim_distribution/
	source setup_environment debug
	make
	popd
	```

11. Open and edit torque.sim and preface the very last line with ""gdb --args "".  After editing the last line in torque.sim should look something like:
	```
	gdb --args /home/runner/gpgpu-sim_simulations/util/job_launching/../../benchmarks/bin/4.2/release/hotspot-rodinia-2.0-ft 30 6 40 ./data/result_30_6_40.txt
	```

12. Re-run the regression test in gdb by sourcing the torque.sim file:
	```
	. torque.sim
	```
	This will put you in at the (gdb) prompt.  Setup any breakpoints needed and run.  

","['tgrogers', 'mkhairy', 'aamodt', 'brad-mengchi', 'andrewboktor', 'wwlfung', 'JRPan', 'tayler-hetherington', 'AamirRaihan', 'CoffeeBeforeArch', 'barnes88', 'sspenst', 'jwang323', 'speverel', 'deval281shah', 'LAhmos', 'qqldd', 'ElTantawy', 'William-An', 'pigrew', 'PSuchita', 'christindbose', 'negargoli93', 'gangmul12', 'FJShen', 'negargoli', 'cesar-avalos3', 'RSpliet', 'nothingface0', 'gjulianm', 'jooybar', 'lucylufei', 'rgreen', 'shenjiangqiu', 'allencho1222', 'bftf', 'amruth-s', 'shen203', 'cng123', 'VijayKandiah', 'yoosful', 'srirajpaul', 'prdalmia', 'yuechen-sys', 'YWHyuk', 'shreyas42singh', 'rodhuega', 'fusiled', 'RedCarrottt', 'gvoskuilen', 'Connie120']",0,0.71,0,,,,,,47,,,,
467513973,R_kgDOG92ydQ,GZCTF,GZTimeWalker/GZCTF,0,GZTimeWalker,https://github.com/GZTimeWalker/GZCTF,"The GZ::CTF project, an open source CTF platform.",0,2022-03-08 13:01:42+00:00,2025-03-08 08:08:34+00:00,2025-03-08 08:08:43+00:00,https://gzctf.gzti.me/,21455,1129,1129,C#,1,1,1,1,1,1,133,0,0,5,agpl-3.0,1,0,0,public,133,5,1129,develop,1,,"<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""assets/banner.dark.svg"">
  <img alt=""Banner"" src=""assets/banner.light.svg"">
</picture>

# GZ::CTF

[![publish](https://github.com/GZTimeWalker/GZCTF/actions/workflows/ci.yml/badge.svg)](https://github.com/GZTimeWalker/GZCTF/actions/workflows/ci.yml)
![version](https://img.shields.io/github/v/release/GZTimeWalker/GZCTF?include_prereleases&label=version)
![license](https://img.shields.io/github/license/GZTimeWalker/GZCTF?color=FF5531)
[![Crowdin](https://badges.crowdin.net/gzctf/localized.svg)](https://crowdin.com/project/gzctf)

[![Telegram Group](https://img.shields.io/endpoint?color=blue&url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Fgzctf)](https://telegram.dog/gzctf)
[![QQ Group](https://img.shields.io/badge/QQ%20Group-903244818-blue)](https://jq.qq.com/?_wv=1027&k=muSqhF9x)
[![Discord](https://img.shields.io/discord/1239476909033656320?label=Discord)](https://discord.gg/dV9A6ZjVhC)

[English](./README.md), [简体中文](./README.zh.md), [日本語](./README.ja.md)

GZ::CTF is an open source CTF platform based on ASP.NET Core.

> [!IMPORTANT]
>
> **To save your effort, please read the documentation carefully before using: [https://gzctf.gzti.me/](https://gzctf.gzti.me/)**

> [!WARNING]
>
> **Upgrade and migration considerations:**
>
> 1. To upgrade the platform, simply pull the latest image and restart, and the database migration will be performed automatically.
> 2. In general, **downgrade** operations are not supported. Upgrading versions with a large time span may result in data incompatibility, so **please make sure to back up your data**.
> 3. After the upgrade, there may be new configuration items and changes in file structure. It is recommended to consult the official documentation or the community.
> 4. If you are migrating to another branch project, please pay attention to whether the database structure has changed. **The database after the change does not support rollback to the original version**.
> 5. The community and the official maintainers are not responsible for data loss, data incompatibility, and other issues. For issues with branch projects, please contact the corresponding project maintainer.

## Features 🛠️

- Create highly customizable challenges

  - Type of challenges: Static Attachment, Dynamic Attachment, Static Container, Dynamic Container

    - Static Attachment: Shared attachments, any configured flag can be accepted.
    - Dynamic Attachment: The number of flags and attachments must be at least the number of teams. Attachments and flags are distributed according to the teams.
    - Static Container: Shared container templates, no dynamic flag is issued, and any configured flag can be submitted.
    - Dynamic Container: Automatically generate and issue flags through container environment variables, and flag of each team is unique.

  - Dynamic Scores

    - Curve of scores:

      $$f(S, r, d, x) = \left \lfloor S \times \left[r  + ( 1- r) \times \exp\left( \dfrac{1 - x}{d} \right) \right] \right \rfloor $$

      Where $S$ is the original score, $r$ is the minimum score ratio, $d$ is the difficulty coefficient, and $x$ is the number of submissions. The first three parameters can be customized to satisfy most of the dynamic score requirements.

    - Bonus for first three solves:
      The platform rewards 5%, 3%, and 1% of the current score for the first three solves respectively.

  - Disable or enable challenges during the competition, and release new challenges at any time.
  - Dynamic flag sharing detection, optional flag template, leet flag

- **Teams** score timeline, scoreboard. Teams can be grouped
- Dynamic container distribution, management, and multiple port mapping methods based on **Docker or K8s**
- **Real-time** competition notification, competition events and flag submission monitoring, and log monitoring based on SignalR
- SMTP email verification, malicious registration protection based on Cloudflare Turnstile
- Ban specific user, three-level user permission management
- Optional team review, invitation code, registration email restriction
- Writeup collection, review, and batch download in the platform
- Download exported scoreboard, export all submission records
- Monitor submissions and major event logs during the competition
- Challenges traffic forwarding based on **TCP over WebSocket proxy**, configurable traffic capture
- Cluster cache based on Redis, database storage backend based on PGSQL
- Storage backend based on local disk and **object storage (MinIO, S3, etc.)**
- Customizable global configuration, platform title, record information
- Support for **dark mode**, multiple languages, and custom themes
- Customizable **website footer**, **website favicon**, and **html description** for SEO
- Support metrics and distributed tracing
- And more...

## About i18n 🌐

Currently, the platform supports multiple languages, and the translation progress is as follows:

### Translated by Community

- English (en-US): Fully supported, **default language**
- Simplified Chinese (zh-CN): Fully supported
- Traditional Chinese (zh-TW): Fully supported
- Japanese (ja-JP): Fully supported, translated by [Steve](https://github.com/hez2010)
- Indonesian (id-ID): Fully supported, translated by [Rio](https://github.com/riodrwn)
- Korean (ko-KR): Fully supported, translated by [Sy2n0](https://github.com/Sy2n0), [kimjw0427](https://github.com/kimjw0427), [LittleDev0617](https://github.com/LittleDev0617), [Jungwoong Kim](https://github.com/jungwngkim) and [blluv](https://github.com/blluv)
- Russian (ru-RU): Fully supported, translated by [FazaN](https://github.com/CyberFazaN)
- Vietnamese (vi-VN): Fully supported, translated by [Ethical Hacker Club](https://github.com/FPTU-Ethical-Hackers-Club)

### Translated by Machine and AI

- German (de-DE)
- French (fr-FR)
- Spanish (es-ES)

These translations are not perfect, and we need your help to improve them.

If you are interested in contributing to the translation, please refer to the [Crowdin project](https://crowdin.com/project/gzctf).

## Demo 🗿

![index.webp](assets/images/index.webp)
![game.list.webp](assets/images/game.list.webp)
![game.challenges.webp](assets/images/game.challenges.webp)
![game.scoreboard.webp](assets/images/game.scoreboard.webp)
![admin.settings.webp](assets/images/admin.settings.webp)
![admin.challenges.webp](assets/images/admin.challenges.webp)
![admin.challenge.info.webp](assets/images/admin.challenge.info.webp)
![admin.challenge.flags.webp](assets/images/admin.challenge.flags.webp)
![admin.game.info.webp](assets/images/admin.game.info.webp)
![admin.game.review.webp](assets/images/admin.game.review.webp)
![admin.teams.webp](assets/images/admin.teams.webp)
![admin.instances.webp](assets/images/admin.instances.webp)
![monitor.game.events.webp](assets/images/monitor.game.events.webp)
![monitor.game.submissions.webp](assets/images/monitor.game.submissions.webp)

## Contributors 👋

<a href=""https://github.com/GZTimeWalker/GZCTF/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=GZTimeWalker/GZCTF"" />
</a>

## CTF hosted with GZ::CTF 🏆

Some event organizers have already chosen GZCTF and successfully completed their competitions. Their trust, support, and timely feedback are the primary driving force behind the continuous improvement of GZCTF.

### International Events

- [**San Diego CTF 2024: Organized by University of California, San Diego**](https://ctftime.org/event/2325)
- [**R3CTF 2024: Organized by r3kapig**](https://ctftime.org/event/2273)
- [**TCP1P CTF 2024: Exploring Nusantara's Digital Realm**](https://ctftime.org/event/2256/)

### Other Events

- **THUCTF 2022: Tsinghua University Network Security Technology Challenge**
- **ZJUCTF 2022/2023/2024: Zhejiang University CTF**
- **SUSCTF 2022/2023/2024: Southeast University Tiger Crouching, Dragon Coiling Cup Network Security Challenge**
- **DIDCTF 2022/2023/2024: Gansu Political and Legal University CTF**
- **W4terCTF [2023](https://github.com/W4terDr0p/W4terCTF-2023)/2024: Information Security Novice Competition of Sun Yat-sen University**
- **ZJNUCTF [2023](https://github.com/A1natas/zjnuctf-school-contest-2023)/[2024](https://github.com/A1natas/zjnuctf-school-contest-2024): Zhejiang Normal University CTF**
- **Woodpecker: The First Network Security Practice Competition of Shandong University of Science and Technology**
- **NPUCTF 2022: Northwestern Polytechnical University CTF**
- **SkyNICO Network Space Security Tri-school Competition (Xiamen University of Technology, Fujian Normal University, Qilu University of Technology)**
- **Hunan Police Academy Network Security Attack and Defense Competition**
- **TongjiCTF 2023: The Fifth Network Security Competition of Tongji University**
- **CatCTF 2023/2024: Network Security Competition of Tongji University (Elementary Level)**
- **CTBUCTF 2023: The First Network Security Competition of Chongqing Technology and Business University**
- **NPUCTF 2023 - The First Security Experimental Skills Competition of Northwestern Polytechnical University**
- **XZCTF 2023: The First Network Security Novice Competition of Zhejiang Normal University Xingzhi College**
- **ORGCTF 2023: Gongcheng Cup Freshman Competition of Harbin Engineering University**
- **SHCTF 2023: ""Shanhe"" Network Security Skills Challenge**
- **Tianjin University of Science and Technology 2023 College Student Maker Training Camp Network Security Group Selection**
- **HYNUCTF 2023: Xuantian Network Security Laboratory Recruitment Competition of Hunan Hengyang Normal University**
- **NYNUCTF S4: Recruitment Competition of Xuantian Network Security Laboratory of Nanyang Normal University**
- **The First Network Security Freshman Challenge of Shangqiu Normal University**
- **SVUCTF-WINTER-2023: Suzhou Vocational University 2023 Winter Freshman Competition**
- **BIEM CTF 2024：Beijing Institute Of Economics And Management - The first BIEM ""Xin'an Cup"" CTF competition**
- **BUAACTF 2024: Beihang University CTF**
- **The first ""Qu STAR"" network security skills competition of Qufu Normal University**
- **DinoCTF: The 4th & 5th Information Security Competition of Chengdu University of Technology**
- **RedrockCTF 2024：Information Security Novice Competition of Chongqing University Of Posts And Telecommunications**
- **WAXFCTF 2024：Chongqing Vocational Institute of Safety Technology ""Pioneer Cup"" Cybersecurity Skills Competition**

_These list is not in any particular order, and PRs are welcome for additions._

## Special Thanks ❤️‍🔥

Thanks to NanoApe, the organizer of THUCTF 2022, for providing sponsorship and conducting Alibaba Cloud public network stress testing. This helped validate the service stability of the GZCTF standalone instance (16c90g) under the pressure of thousands of concurrent requests and 1.34 million requests in three minutes.

## Stars ✨

[![Stargazers over time](https://starchart.cc/GZTimeWalker/GZCTF.svg?variant=adaptive)](https://starchart.cc/GZTimeWalker/GZCTF)
","['GZTimeWalker', 'hez2010', 'GrakePch', 'chenjunyu19', 'Hanmur', 'KpwnZ', 'dependabot[bot]', 'kengwang', 'deepsource-autofix[bot]', 'idawnlight', 'cyc4188', 'Konano', 'YanWQ-monad', 'Ad-Bean', 'TonyCrane', 'kdxcxs', 'mcyydscc', 'xiongnemo', 'Lil-Ran', 'weyung', 'ElaBosak233', 'Zeroc0077', 'wjhwjhn', 'happybear1234', 'didiaojun', 'xfoxfu', 'Beatrueman', 'xiongsp', 'W1ndys', 'EwdRe', 'Ra1nbowNeko', 'goodlunatic', 'onlykood', 'mnixry', 'AethoceSora', 'HK-SHAO', 'chanios', 'BoyChai', 'Abyssun', '13m0n4de']",0,0.69,0,,,,"blank_issues_enabled: false
contact_links:
  - name: Read the docs 阅读文档
    url: https://gzctf.gzti.me/
    about: If you haven't read the docs yet, please check it out first. 如果你还没有阅读过文档，请先查阅。
  - name: Community support 社区支持
    url: https://github.com/GZTimeWalker/GZCTF/discussions
    about: Please ask and answer questions in the user group or GitHub Discussions. 请在用户群或 GitHub Discussions 中提问交流。
",,7,,,,
247581239,MDEwOlJlcG9zaXRvcnkyNDc1ODEyMzk=,SAPIEN,haosulab/SAPIEN,0,haosulab,https://github.com/haosulab/SAPIEN,SAPIEN Embodied AI Platform,0,2020-03-16 01:11:40+00:00,2025-03-07 02:19:13+00:00,2025-03-03 04:51:16+00:00,https://sapien.ucsd.edu/,347915,504,504,C++,1,1,1,1,0,1,45,0,0,39,mit,1,0,0,public,45,39,504,master,1,1,,"['fbxiang', 'yzqin', 'jetd1', 'Jiayuan-Gu', 'angli66', 'kent0318', 'MRzNone', 'Colin97']",1,0.8,967,,,,Directory exists,,18,,,,
124716861,MDEwOlJlcG9zaXRvcnkxMjQ3MTY4NjE=,Research-Internships-for-Undergraduates,himahuja/Research-Internships-for-Undergraduates,0,himahuja,https://github.com/himahuja/Research-Internships-for-Undergraduates,List of Research Internships for Undergraduate Students,0,2018-03-11 02:34:38+00:00,2025-03-06 11:00:31+00:00,2025-01-23 18:19:45+00:00,,194,4283,4283,,1,1,1,1,0,0,767,0,0,11,,1,0,0,public,767,11,4283,master,1,,"# Research Internships for Undergraduates

## Looking for someone to manage this repository in exchange for mentorship. [Apply here](https://forms.gle/BhCrhXE8ZYsG7zw9A)

#### This is a non-exhaustive list of opportunities available to Undergraduate students. Many of these positions are only focussed towards Indian students.

I am making this list as an attempt to fill the void of any such open source list. Your contribution is vital to keep the list up-to-date for future students to use. Hence it's a humble request to each user to contribute any new opportunity you discover and update the old ones. Without updation, the web-links will break, and the list will become futile! I'll make sure that any such modification in the `README.md` raised as an issue or submitted as a pull request be timely reviewed and added.

The following are some global research internship positions:

CANADA

  * ~~[MITACS Globalink](https://www.mitacs.ca/en/programs/globalink/globalink-research-internship), for research positions in Canada.~~ (NO LONGER AVAILABLE)
  * [University of Alberta Research Experience (UARE)](https://www.ualberta.ca/admissions-programs/visiting-student-and-internship-programs/research-internships/ualberta-research-experience/index.html)

US
  * [LPI Summer Intern Program in Planetary Science](https://www.lpi.usra.edu/lpiintern/application/), for both national and international students.
  * [NASA - CalTech Summer Undergraduate Research Fellowship (SURF)](https://www.jpl.nasa.gov/edu/intern/apply/caltech-summer-undergraduate-research-fellowship/), for research positions at Jet Propulsion Laboratory, California Institute of Technology.
  * [Santa Fe Institute Summer Research Experience](https://www.santafe.edu/engage/learn/programs/undergraduate-complexity-research)
  * [Robotics Institute for Summer Scholars](https://riss.ri.cmu.edu/), CMU 
  * [Data Science for Social Good Fellowship](https://www.dssgfellowship.org/), CMU 
  * [QRLSSP Summer Program](https://mcmsc.asu.edu/institutes/qrlssp), Arizona State University: Only for US Citizens / Permanent Residents
  * [Summer Undergraduate Research fellowship program](https://www.rockefeller.edu/education-and-training/surf/), Rockfeller Institute
  * [CalTech SURF Program](https://sfp.caltech.edu/undergraduate-research/programs/surf/application_information)
  * [Space Astronomy Summer Program, Space Telescope Science Institute (STScI)](http://www.stsci.edu/opportunities/space-astronomy-summer-program)
  * ~~[Data Visualization Programming Summer Student Internship in New York City](https://simonsfoundation.wd1.myworkdayjobs.com/en-US/simonsfoundationcareers/job/162-Fifth-Avenue/Data-Visualization-Intern--SCC_R0000579)~~, Flatiron Institute. (NEW LINK NEEDED)
  * [Summer Research Program, Princeton University](https://undergraduateresearch.princeton.edu/programs/summer-programs?field_princeton_status_eligibili_value=Non-Princeton+undergrads&field_class_year_eligibility_value=Juniors&field_division_value=Engineering)
  * International Student Research Internship Program - [McKelvey School of Engineering](https://engineering.wustl.edu/academics/undergraduate-research/international-student-research-internship-program.html)
  * Luddy School of Informatics, Computing, and Engineering - [Global Talent Attraction Program (GTAP)](https://luddy.indiana.edu/research/student-research/fellowship.html)
  * [BioChemCoRe](https://biochemcore.ucsd.edu/) (Biology and Chemistry Computational Research), Amaro Lab, UC San Diego 
  * [Global Research Experience in Advanced Technologies (GREAT) Program, UC Davis](https://great.ucdavis.edu/)
  * [ICT Summer Research Program](https://ict.usc.edu/academics/internships/application/)
  * [UC Berkeley Amgen Scholars program](https://amgenscholars.berkeley.edu/)
  * [Summer Undergraduate Program in Engineering Research at Berkeley (SUPERB) (for Information Technology students](https://eecs.berkeley.edu/resources/undergrads/research/superb)
  * [SUMMER RESEARCH OPPORTUNITIES PROGRAM(SROP), Purdue University](https://www.purdue.edu/gradschool/diversity/programs/summer-research-opportunities-program/)
  * [Summer Undergraduate Research Fellowship (SURF), Stanford University](https://engineering.stanford.edu/students-academics/equity-and-inclusion-initiatives/prospective-graduate-programs/summer)
  * [IPAM UCLA Program](http://www.ipam.ucla.edu/programs/student-research-programs/)

SWITZERLAND

  * [Research fellowship at ETH Zurich](https://www.inf.ethz.ch/studies/summer-research-fellowship.html)  - Application closed
  * [CERN Summer Student Program](https://careers.cern/summer)  
  * [CERN OpenLab for Computer Science Undergraduates](https://openlab.cern/education)
  * [E3, EPFL (Switzerland)](https://eee.epfl.ch/)
  * [Summer at EPFL](https://summer.epfl.ch/)
  * [Swissnex Program](https://swissnex.org/india/thinkswiss/)

GERMANY
  * [Max Planck Institute for Gravitational Physics](https://www.aei.mpg.de/student-internships), for all students. 
  * [Max Planck Institute for Software Systems](https://apply.mpi-sws.org/register/internship/)
  * [Warwick Statistics Internship Scheme](https://warwick.ac.uk/fac/sci/statistics/research/internships/), University of Warwick
  * [DAAD WISE Scholarship](https://www2.daad.de/deutschland/stipendium/datenbank/en/21148-scholarship-database/?detail=50015295)
  * [HZDR Dresden, Summer student program (Germany)](https://www.hzdr.de/db/Cms?pOid=34387&pNid=2519)
  * [MaxSIP (Max Planck Institute Summer Internship)](https://imprs-ls.opencampus.net/en/maxsip_application_info)
  * [UROP International](http://www.rwth-aachen.de/cms/root/Forschung/Angebote-fuer-Forschende/Angebote-fuer-Studierende/UROP/UROP-INternational/~wnr/Informationen-fuer-Studierende/?lidx=1), RWTH Aachen

UK

  * [DSSGx UK Summer Fellowship](https://warwick.ac.uk/research/data-science/warwick-data/dssgx/), University of Warwick
  * [Visual Geometry Group](https://www.robots.ox.ac.uk/~vgg/)

INDIA

  * [ICTS LONG TERM VISITING STUDENTS PROGRAM](https://www.icts.res.in/academic/long-term-visiting-student-program) for both International and Indian Students. Aims to provide opportunities to students of science, mathematics and engineering to spend a longer period of time (1 or 2 semesters).
  * [ICTS SN BHATT Memorial Excellence Fellowship](https://www.icts.res.in/academic/summer-research-program) for Undergraduate students of science, mathematics and engineering, who are in their third, fourth or fifth year of the program are eligible to apply. 
  * [INSA-IASc-NASI SRFP](https://www.ias.ac.in) for undergraduate and postgraduate students in Chemistry, Earth and Planetary sciences, Engineering including computer sciences, Life sciences, mathematics and physics. The fellowship period is 56 days and is quite prestigious for anyone who wishes to go into the field of research.

SOUTH KOREA

  * [SPIKE @ UNIST](https://spike.unist.ac.kr:10449/02_learn/learn03.php), Summer Program for Internship and Korean Experience
  * [SPIKE@UNIST (South Korea)](https://spike.unist.ac.kr:10449/02_learn/learn03.php)
  * [GIST Global Internship Program (South Korea)](https://www.gist.ac.kr/en/html/sub07/0702.html)
  * [Research internship @ Yonsei University](https://summer.yonsei.ac.kr/home/program/internship02.asp)
  * [Yonsei University](https://summer.yonsei.ac.kr/home/program/internship02.asp)

TAIWAN

  * [NTHU Summer Internship Program](http://eng-en.web.nthu.edu.tw/files/14-1130-129169,r1447-1.php)
  * [TIGP-IIP](https://tigpsip.apps.sinica.edu.tw/index.php), Academia Sincia, Taiwan
  * [ITRI Global Internship program](https://www.itri.org.tw/english/ListStyle.aspx?DisplayStyle=05&SiteID=1&MmmID=617731531432246346)
  * [TEEP@Asia (Taiwan)](https://teep.studyintaiwan.org/programs/Engineering)
  * [TEEP Internship, Taiwan](https://www.roc-taiwan.org/in_en/post/2749.html)

JAPAN
  * [University of Tokyo](http://www.amgenscholars.com/japan-program), Amgen Scholarship Japan (only Bio Tech and Bio-Chem Programs)
  * [OIST Research Internship Program](https://admissions.oist.jp/oist-research-internship-program-description), Okinawa Institute of Science and Technology 
  * [NIMS (Japan)](https://www.nims.go.jp/eng/hr-development/internship.html)
  * [Hennge Global Internship Program](https://hennge.com/global/gip.html), for remote internship in a Japan-based tech start-up.
  * [Internet Initiative Japan Internship] (https://www.iijlab.net/en/career/internship.html), for on-site tech internship in Japan.

AUSTRIA

  * [ISternship Summer Student Program](https://phd.pages.ist.ac.at/isternship/), IST Austria
  
HONG KONG 
  * [HKU, Computer Science Department](https://www.cs.hku.hk/rintern/)
  * [IVISP, HKUST](https://pg.ust.hk/ivisp) (only for senior year undergraduates + postgraduates)
  * [SURP, CUHK](http://www.summer.cuhk.edu.hk/surp/)
  * [SURP (Hongkong)](http://www.summer.cuhk.edu.hk/surp/?fbclid=IwAR0-H6g4x7UetRxFQkcnK95zvgjkp81TjgCZlBgv-NjrRSxWiOxy84TZuhw)

AUSTRALIA

  * [UNSW Civil and Environmental Engineering Research Internship](https://www.engineering.unsw.edu.au/civil-engineering/study-with-us/international-exchange/research-internship-to-unsw-for-international-students) 
  * [UNSW Research Internship (Australia)](https://www.unsw.edu.au/science/student-life-resources/student-opportunities/research-integrated-learning)

ISRAEL

  * [Summer Intern at the Kupcinet Getz International Summer Science School of the Weizmann Institute of Science](https://www.weizmann.ac.il/feinberg/admissions/kupcinet-getz-international-summer-school/about-program-0)

SPAIN

 * [Summer Training Program,CNIO](https://www.cnio.es/en/education-and-career-development/career-development-programmes/undergraduate-students/)
 * [ Computer Vision Center Internship Program](https://www.cvc.uab.es/internship/)

FRANCE

  * [OECD Internship Programme (France)](https://www.oecd.org/careers/internship-programme/)

SAUDI ARABIA

  * [KAUST (Saudi Arabia)](https://vsrp.kaust.edu.sa/)

United Arab Emirates

  * [Research internship program for AI](https://mbzuai.ac.ae/ugrip/)

Turkey

  * [Koç Üniversitesi Research Program](https://vprd.ku.edu.tr/kusrp/)
  * [Sabancı Üniversitesi PURE Summer Research Program](https://pure.sabanciuniv.edu/)

Indonesia
  * [Institut Teknologi Sepuluh Nopember Lab-Based Internship](https://www.its.ac.id/international/experiencing-its/prospective-student/admission/internship/)

Thailand
  * [Faculty of Engineering Kasetsart University internship/training/research programs](https://en.eng.ku.ac.th/?page_id=67)
  * [Faculty of Engineering King Mongkut University of North Bangkok Exchange/internship programs](https://www.eng.kmutnb.ac.th/ciep/?page_id=2550)

RUSSIA

  * HSE University, Moscow [Computer Science Internships](https://cs.hse.ru/en/internships/)

MULTIPLE COUNTRIES

  * [IPAM UCLA, RIPS Program](http://www.ipam.ucla.edu/programs/student-research-programs/)
  * [Amgen Scholars Program](https://amgenscholars.com/)
  * [UX Research Internship, Red Hat](https://us-redhat.icims.com/jobs/83084/remote-us-nc/job)
  * [Allen Institute for AI, Research and Engineering Internships](https://allenai.org/internships)


The following are some research internships for Indian students:

1. ~~S.N. Bose Scholarship sponsored by IUSSTF~~, it's been closed for almost two years now.
2. [ICTS Summer research program](https://www.icts.res.in/academic/summer-research-program), by ICTS-TIFR.
3. [DAAD Wise Scholarship](https://www.daad.de/go/en/stipa50015295), for research positions in Germany
4. [Shastri Research Student Fellowship](https://www.shastriinstitute.org/shastri-research-student-fellowship) by Shastri Indo-Canadian Institute
5. [Viterbi India Program](https://iusstf.org/iusstf-viterbi-program), sponsored by IUSSTF and USC Viterbi
6. [Khorana Program for Scholars](https://www.iusstf.org/program/khorana-program-for-scholars), sponsored by IUSSTF
8. [Charpak Global Scholarship](https://www.inde.campusfrance.org/charpak-lab-scholarship), funded by French Embassy in India
9. [Indian Student Internship Program at NTHU](http://oga.nthu.edu.tw/news.php?id=233&lang=en)
10. [IBM Blue Mix](https://researcher.watson.ibm.com/researcher/view_group_subpage.php?id=8101) Research Internship
11. [IBM Extreme Blue](http://www-07.ibm.com/employment/in/students/extreme-blue/index.html) (limited to IITs and NITs)
12. [LIGO IndiGo](http://jobs.gw-indigo.org/tiki-index.php?page=LIGO-IndIGO+Summer+Students+Program) at CalTech, funded by US NSF
13. [IIITD Summer Internship](https://www.iiitd.ac.in/placement/internships), IIIT, New Delhi
14. [Summer Research Fellowship Programme 2021](https://webjapps.ias.ac.in/fellowship2022/index.html), IISc, IAS, INS
15. [Quantitative Research Summer Internship](https://websim.worldquantchallenge.com/en/cms/wqc/summerprograms/india/), WorldQuant India
16. [Internship At CMI](https://www.cmi.ac.in/admissions/internships.php), Chennai Mathematical Institute
17. [Shastri Student Internship Project](https://www.shastriinstitute.org/Shastri_Student_Internship_Project), Eligible universities [here](https://www.shastriinstitute.org/member-council)
18. [Cisco Summer internship](https://jobs.cisco.com/jobs/ProjectDetail/Software-Engineer-Bachelor-s-Intern-United-States/1295250?source=Pitt+CSC&tags=CDC+SnNG+students-and-new-graduate-programs)
19. [Hackerrank SDE intern](https://breakinghierarchy.com/hackerrank-sde-intern/)
20. [SSERD Research Intern](https://www.sserd.org/internship/) for research work in topics like Space Settlement, Astrophysics, Space Mission Design etc.
21. [IISER Kolkata Summer Student Research Programme](https://www.iiserkol.ac.in/~summer.research/), IISER Kolkata
22. [IITM Summer Fellowship Programme](https://sfp.iitm.ac.in), IIT Madras
23. [IITD GIPEDI](https://web.iitd.ac.in/~subrat/SummerInternshipRules.htm), IIT Delhi
24. [IITB Research Internship Award](http://www.iitb.ac.in/en/education/research-internship), IIT Bombay
25. [IITK SURGE Program](http://surge.iitk.ac.in/about.html), IIT Kanpur
26. [SPARK](http://spark.iitr.ac.in/), IIT Roorkee
27. [SRIP](https://srip.iitgn.ac.in/info/), IIT Gandhinagar
28. [IIT Ropar Summer Internship](https://onlineportal.iitrpr.ac.in/sia-21)[ [Information]](https://www.iitrpr.ac.in/sites/default/files/Advertisement%20for%20Summer%20Internship%202021.pdf), IIT Ropar
29. [Bhaba Atomic Research Centre](http://www.barc.gov.in/student/)
30. [ISI Kolkata Summer Internship](https://www.isical.ac.in/~rcbose/internship/index.html)
31. [IIT (ISM) Dhanbad SRIP Programme](https://www.iitism.ac.in/deans/research/SRIP.php)
32. [IIIT Delhi](https://www.iiitd.ac.in/placement/summer-internships)
33. [IIIT HYDERABAD](https://ihub-data.iiit.ac.in/programs/events/shristi-23/)
33. [Summer Program, CeNSE IISc Bangalore](http://www.cense.iisc.ac.in/content/summer-program)
34. [Prof. G.S. Ramaswamy Summer Internship, CSIR-SERC Chennai](https://serc.res.in/professor-gs-ramaswamy-internship-undergraduate-dual-degree-students), for civil/mechanical engineering students
","['himahuja', 'vanshaj18', 'Rohit-Kundu', 'thesauravkarmakar', 'Khushm', 'sthakurr', 'Nandan-N', 'KhalidAlnujaidi', 'donkeydoughnuts', 'subhankar01', 'shreshtha48', 'sabhya19', 'codeshruti', 'Nilesh2000', 'JaydevSR', 'ChirantanGanguly', 'srivastav-ayush', 'KesharwaniArpita', 'ReanSchwarzer1', 'Abhijeet399', 'AaronWatters', 'prakharrathi25', 'silver919', 'PurneswarPrasad', 'CODESURGEN', 'saksham291', 'sritasngh', 'sarosijbose', 'shyammarjit', 'gambani-simran', 'soumyaa1804', 'UJJWAL2001', 'utkarshtambe10', 'VedantParanjape', 'Yashsaini20', 'YashviKommidi', 'hsnaved', 'masif2002', 'akankshasingh25', 'AniLeo-01', 'codetronaut', 'Arjit3', 'iamarpitpatidar', 'Ashish-Hallur', 'sinAshish', 'DishankJ', 'fandreuz', 'GoodnessEzeh', 'hiteshcmonga', 'munozariasjm', 'Komal7209', 'deutranium', 'MetaphorC', 'Nishaghoul', 'parthxtripathi']",0,0.61,0,,,,,,134,,,soc-ucsd,
164433070,MDEwOlJlcG9zaXRvcnkxNjQ0MzMwNzA=,AI-System-School,HuaizhengZhang/AI-System-School,0,HuaizhengZhang,https://github.com/HuaizhengZhang/AI-System-School,"🚀 Awesome System for Machine Learning ⚡️ AI System Papers and Industry Practice. ⚡️ System for Machine Learning, LLM (Large Language Model), GenAI (Generative AI). 🍻 OSDI, NSDI, SIGCOMM, SoCC, MLSys, etc. 🗃️ Llama3, Mistral, etc. 🧑‍💻 Video Tutorials. ",0,2019-01-07 12:59:19+00:00,2025-03-08 07:58:03+00:00,2024-08-14 05:12:47+00:00,https://huaizheng.xyz/,910,2820,2820,,1,1,1,1,0,0,321,0,0,12,mit,1,0,0,public,321,12,2820,master,1,,,"['HuaizhengZhang', 'huangyz0918', 'jasperzhong', 'xwzheng1020', 'BDHU', 'anancds', 'swagshaw', 'stjepanjurekovic', 'saeid93', 'gaocegege', 'adam-narozniak', 'MengShen0709', 'AtomicVar', 'tp-nan', 'Paras-96', 'weimingwill', 'terrytangyuan', 'YuanmingLeee', 'wang-zerui', 'zyang37', 'binmakeswell']",0,0.64,0,,,,,"  <!-- Thanks for your pull request -->
  
  - Title [[Paper]](link) [[GitHub]](link)
  - Author (*conference(journal) year*)
  - Summary: 
",127,,,mlpc-ucsd,
277251921,MDEwOlJlcG9zaXRvcnkyNzcyNTE5MjE=,recommendation_model,huangjunheng/recommendation_model,0,huangjunheng,https://github.com/huangjunheng/recommendation_model,"练习下用pytorch来复现下经典的推荐系统模型, 如MF, FM, DeepConn, MMOE, PLE, DeepFM, NFM, DCN, AFM, AutoInt, ONN, FiBiNET, DCN-v2, AFN, DCAP等",0,2020-07-05 07:24:19+00:00,2025-03-04 05:33:49+00:00,2022-03-14 04:21:13+00:00,,23395,585,585,Python,1,1,1,1,0,0,125,0,0,7,,1,0,0,public,125,7,585,master,1,,,['huangjunheng'],0,0.7,0,,,,,,4,,,,
670475494,R_kgDOJ_ak5g,awesome-llm-powered-agent,hyp1231/awesome-llm-powered-agent,0,hyp1231,https://github.com/hyp1231/awesome-llm-powered-agent,Awesome things about LLM-powered agents. Papers / Repos / Blogs / ...,0,2023-07-25 06:23:18+00:00,2025-03-08 06:42:09+00:00,2025-02-24 17:57:18+00:00,,415,1889,1889,,1,0,1,0,0,0,151,0,0,2,mit,1,0,0,public,151,2,1889,main,1,,,"['hyp1231', 'zehuichen123', 'zhiyuanhubj', 'night-chen', 'boyuanzheng010', 'icecream-and-tea', 'baltaci-r', 'Rosie72770', 'jeasinema', 'IranQin', 'heroding77', 'conglu1997', 'davorrunje', 'shobrook', 'LeoYML', 'Nicolas99-9', 'brickee', 'Persdre', 'Andy-jqa', 'ryoungj', 'RishiHazra', 'samholt', 'AgentLaboratory', 'skzhang1', 'liushunyu', 'WxxShirley', 'XuhuiZhou', 'entslscheia', 'ZubinGou', 'acbull', 'koalazf99', 'luciolcv', 'shenao-zhang', 'xinyadu']",1,0.67,0,,,,,,52,,,,
488021171,R_kgDOHRacsw,torchhd,hyperdimensional-computing/torchhd,0,hyperdimensional-computing,https://github.com/hyperdimensional-computing/torchhd,Torchhd is a Python library for Hyperdimensional Computing and Vector Symbolic Architectures,0,2022-05-02 23:32:45+00:00,2025-03-03 00:26:47+00:00,2025-02-08 17:37:31+00:00,https://torchhd.readthedocs.io,56644,301,301,Python,1,0,1,0,0,0,28,0,0,11,mit,1,0,0,public,28,11,301,main,1,1,"<p align=""center"">
    <a href=""https://github.com/hyperdimensional-computing/torchhd/blob/main/LICENSE""><img alt=""GitHub license"" src=""https://img.shields.io/badge/license-MIT-orange.svg?style=flat"" /></a>
    <a href=""https://pypi.org/project/torch-hd/""><img alt=""pypi version"" src=""https://img.shields.io/pypi/v/torch-hd.svg?style=flat&color=orange"" /></a>
    <a href=""https://anaconda.org/torchhd/torchhd""><img alt=""conda version"" src=""https://img.shields.io/conda/v/torchhd/torchhd?label=conda&style=flat&color=orange"" /></a>
    <a href=""https://github.com/hyperdimensional-computing/torchhd/actions/workflows/test.yml?query=branch%3Amain""><img alt=""tests status"" src=""https://img.shields.io/github/actions/workflow/status/hyperdimensional-computing/torchhd/test.yml?branch=main&label=tests&style=flat"" /></a>
    <img alt=""PRs Welcome"" src=""https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat"" />
</p>

<div align=""center"">
    <a href=""https://github.com/hyperdimensional-computing/torchhd"">
        <img width=""380px""  alt=""Torchhd logo"" src=""https://raw.githubusercontent.com/hyperdimensional-computing/torchhd/main/docs/images/torchhd-logo.svg"" />
    </a>
</div>

# Torchhd

Torchhd is a Python library for _Hyperdimensional Computing_ (also known as _Vector Symbolic Architectures_).

- **Easy-to-use:** Torchhd makes it painless to develop a wide range of Hyperdimensional Computing (HDC) applications and algorithms. For someone new to the field, we provide Pythonic abstractions and examples to get you started fast. For the experienced researchers, we made the library modular by design, giving you endless flexibility to prototype new ideas in no-time.
- **Performant:** The library is build on top of the high-performance [PyTorch](https://pytorch.org/) library, giving you optimized tensor execution without the headaches. Moreover, PyTorch makes it effortless to accelerate your code on a GPU.

## Installation

Torchhd is hosted on [PyPi](https://pypi.org/project/torch-hd/) and [Anaconda](https://anaconda.org/torchhd/torchhd). First, install PyTorch using their [installation instructions](https://pytorch.org/get-started/locally/). Then, use one of the following commands to install Torchhd:

```bash
pip install torch-hd
```

```bash
conda install -c torchhd torchhd
```

## Documentation

You can find documentation for Torchhd [on the website](https://torchhd.readthedocs.io).

Check out the [Getting Started](https://torchhd.readthedocs.io/en/stable/getting_started.html) page for a quick overview.

The API documentation is divided into several sections:

- [`torchhd`](https://torchhd.readthedocs.io/en/stable/torchhd.html)
- [`torchhd.embeddings`](https://torchhd.readthedocs.io/en/stable/embeddings.html)
- [`torchhd.structures`](https://torchhd.readthedocs.io/en/stable/structures.html)
- [`torchhd.models`](https://torchhd.readthedocs.io/en/stable/models.html)
- [`torchhd.memory`](https://torchhd.readthedocs.io/en/stable/memory.html)
- [`torchhd.datasets`](https://torchhd.readthedocs.io/en/stable/datasets.html)

You can improve the documentation by sending pull requests to this repository.

## Examples

We have several examples [in the repository](https://github.com/hyperdimensional-computing/torchhd/tree/main/examples). Here is a simple one to get you started:

```python
import torch, torchhd

d = 10000  # number of dimensions

# create the hypervectors for each symbol
keys = torchhd.random(3, d)
country, capital, currency = keys

usa, mex = torchhd.random(2, d)  # United States and Mexico
wdc, mxc = torchhd.random(2, d)  # Washington D.C. and Mexico City
usd, mxn = torchhd.random(2, d)  # US Dollar and Mexican Peso

# create country representations
us_values = torch.stack([usa, wdc, usd])
us = torchhd.hash_table(keys, us_values)

mx_values = torch.stack([mex, mxc, mxn])
mx = torchhd.hash_table(keys, mx_values)

# combine all the associated information
mx_us = torchhd.bind(torchhd.inverse(us), mx)

# query for the dollar of mexico
usd_of_mex = torchhd.bind(mx_us, usd)

memory = torch.cat([keys, us_values, mx_values], dim=0)
torchhd.cosine_similarity(usd_of_mex, memory)
# tensor([-0.0062,  0.0123, -0.0057, -0.0019, -0.0084, -0.0078,  0.0102,  0.0057,  0.3292])
# The hypervector for the Mexican Peso is the most similar.
```

This example is from the paper [What We Mean When We Say ""What's the Dollar of Mexico?"": Prototypes and Mapping in Concept Space](https://redwood.berkeley.edu/wp-content/uploads/2020/05/kanerva2010what.pdf) by Kanerva. It first creates hypervectors for all the symbols that are used in the computation, i.e., the variables for `country`, `capital`, and `currency` and their values for both countries. These hypervectors are then combined to make a single hypervector for each country using a hash table structure. A hash table encodes key-value pairs as: `k1 * v1 + k2 * v2 + ... + kn * vn`. The hash tables are then bound together to form their combined representation which is finally queried by binding with the Dollar hypervector to obtain the approximate Mexican Peso hypervector. The similarity output shows that the Mexican Peso hypervector is indeed the most similar one.

## Supported HDC/VSA models

Currently, the library supports the following HDC/VSA models:

- [Multiply-Add-Permute (MAP)](https://torchhd.readthedocs.io/en/stable/generated/torchhd.MAPTensor.html)
- [Binary Spatter Codes (BSC)](https://torchhd.readthedocs.io/en/stable/generated/torchhd.BSCTensor.html)
- [Holographic Reduced Representations (HRR)](https://torchhd.readthedocs.io/en/stable/generated/torchhd.HRRTensor.html)
- [Fourier Holographic Reduced Representations (FHRR)](https://torchhd.readthedocs.io/en/stable/generated/torchhd.FHRRTensor.html)
- [Binary Sparse Block Codes (B-SBC)](https://torchhd.readthedocs.io/en/stable/generated/torchhd.BSBCTensor.html)
- [Modular Composite Representation (MCR)](https://torchhd.readthedocs.io/en/stable/generated/torchhd.MCRTensor.html)
- [Vector-Derived Transformation Binding (VTB)](https://torchhd.readthedocs.io/en/stable/generated/torchhd.VTBTensor.html)

We welcome anyone to help with contributing more models to the library!

## About

Initial development of Torchhd was performed by [Mike Heddes](https://www.mikeheddes.nl/) and [Igor Nunes](https://sites.uci.edu/inunes/) as part of their research in Hyperdimensional Computing at the University of California, Irvine. The library was extended with significant contributions from Pere Vergés and Dheyay Desai. Torchhd later merged with a project by Rishikanth Chandrasekaran, who worked on similar problems as part of his research at the University of California, San Diego.

## Contributing

We are always looking for people that want to contribute to the library. If you are considering contributing for the first time we acknowledge that this can be daunting, but fear not! You can look through the [open issues](https://github.com/hyperdimensional-computing/torchhd/issues) for inspiration on the kind of problems you can work on. If you are a researcher and want to contribute your work to the library, feel free to open a new issue so we can discuss the best strategy for integrating your work.

### Documentation

To build the documentation locally do the following:

1. Use `pip install -r docs/requirements.txt` to install the required packages.
2. Use `sphinx-build -b html docs build` to generate the html documentation in the `/build` directory.

To create a clean build, remove the `/build` and `/docs/generated` directories.

### Creating a New Release

1. Increment the version number in [version.py](https://github.com/hyperdimensional-computing/torchhd/blob/main/torchhd/version.py) using [semantic versioning](https://semver.org).
2. Create a new GitHub release. Set the tag according to [PEP 440](https://peps.python.org/pep-0440/), e.g., v1.5.2, and provide a clear description of the changes. You can use GitHub's ""auto-generate release notes"" button. Look at previous releases for examples.
3. A GitHub release triggers a GitHub action that builds the library and publishes it to PyPi and Conda in addition to the documentation website.

### Running tests

To run the unit tests located in [`torchhd/tests`](https://github.com/hyperdimensional-computing/torchhd/tree/main/torchhd/tests) do the following:

1. Use `pip install -r dev-requirements.txt` to install the required development packages.
2. Then run the tests using just `pytest`.

Optionally, to measure the code coverage use `coverage run -m --omit=""torchhd/tests/**"" pytest` to create the coverage report. You can then view this report with `coverage report`.

### License

This library is [MIT licensed](https://github.com/hyperdimensional-computing/torchhd/blob/main/LICENSE).

To add the license to all source files, first install [`licenseheaders`](https://github.com/johann-petrak/licenseheaders) and then use `licenseheaders -t ./LICENSE -d ./torchhd`.

## Cite

Consider citing [our paper](https://jmlr.org/papers/v24/23-0300.html) published in the Journal of Machine Learning Research (JMLR) if you use Torchhd in your work:

```
@article{JMLR:v24:23-0300,
  author  = {Heddes, Mike and Nunes, Igor and Vergés, Pere and Kleyko, Denis and Abraham, Danny and Givargis, Tony and Nicolau, Alexandru and Veidenbaum, Alex},
  title   = {Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {255},
  pages   = {1--10},
  url     = {http://jmlr.org/papers/v24/23-0300.html}
}
```
","['mikeheddes', 'dheyay', 'github-actions[bot]', 'denkle', 'milad2073', 'igordeoliveiranunes', 'pereverges', '21leejenny', 'Zeldax64', 'Didanny', 'eltociear', 'Orienfish', 'aglinxinyuan', 'scken', 'pverges']",1,0.67,0,,,,,,15,,,,
247428714,MDEwOlJlcG9zaXRvcnkyNDc0Mjg3MTQ=,DeepEMD,icoz69/DeepEMD,0,icoz69,https://github.com/icoz69/DeepEMD,"Code for paper ""DeepEMD: Few-Shot Image Classification with Differentiable Earth Mover's Distance and Structured Classifiers"", CVPR2020",0,2020-03-15 08:31:53+00:00,2025-03-05 10:37:26+00:00,2022-11-15 03:47:27+00:00,,225,586,586,Python,1,1,1,1,0,0,83,0,0,15,mit,1,0,0,public,83,15,586,master,1,,"# DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning

PyTorch implementation of 

[DeepEMD: Few-Shot Image Classification with Differentiable Earth Mover's Distance and Structured Classifiers](https://arxiv.org/pdf/2003.06777v2.pdf)"" (CVPR 2020 oral [(oral video)](https://www.youtube.com/watch?v=X2ZUZYy_GtY) ) 

and

""[DeepEMD v2: Differentiable Earth Mover's Distance for Few-Shot Learning](https://arxiv.org/pdf/2003.06777.pdf)"" (TPAMI Extension). 

**DeepEMD achieves new state-of-the-art performance on five few-shot learning benchmarks with significant advantages (up to 7%). The result is obtained without using any extra data for training or testing (tranductive setting).**

Check [few-shot classification leaderboard](https://few-shot.yyliu.net/miniimagenet.html).

If you use the code in this repo for your work, please cite the following bib entries:

    @InProceedings{Zhang_2020_CVPR,
    author = {Zhang, Chi and Cai, Yujun and Lin, Guosheng and Shen, Chunhua},
    title = {DeepEMD: Few-Shot Image Classification With Differentiable Earth Mover's Distance and Structured Classifiers},
    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
    }
and

    @misc{zhang2020deepemdv2,
        title={DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning},
        author={Chi Zhang and Yujun Cai and Guosheng Lin and Chunhua Shen},
        year={2020},
        eprint={2003.06777v3},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
    }

If you have any question regarding the paper, please send a email to `chi007[at]e[dot]ntu[dot]edu[dot]sg`.

## Abstract

Deep learning has proved to be very effective in learning with a large amount of labelled data. Few-shot learning in contrast attempts to learn with only a few labelled data. In this work, we develop methods for few-shot image classification from a new perspective of optimal matching between image regions. We employ the Earth Mover's Distance (EMD) as a metric to compute a structural distance between dense image representations to determine image relevance. The EMD generates the optimal matching flows between structural elements that have the minimum matching cost, which is used to calculate the image distance for classification. To generate the important weights of elements in the EMD formulation, we design a cross-reference mechanism, which can effectively alleviate the adverse impact caused by the cluttered background and large intra-class appearance variations. To handle k-shot classification, we propose to learn a structured fully connected layer that can directly classify dense image representations with the proposed EMD. Based on the implicit function theorem, the EMD can be inserted as a layer into the network for end-to-end training. Our extensive experiments validate the effectiveness of our algorithm which outperforms state-of-the-art methods by a significant margin on four widely used few-shot classification benchmarks, namely, miniImageNet, tieredImageNet, Fewshot-CIFAR100 (FC100) and Caltech-UCSD Birds-200-2011 (CUB).

<img src='architecture.png' width='1280' height='520'>

## Few-shot classification Results

Experimental results on few-shot learning datasets with ResNet-12 backbone. We report average results with 5,000 randomly sampled episodes for 1-shot evaluation and 600 episodes for k-shot evaluation



**MiniImageNet Dataset**

|  Setups  | 1-Shot 5-Way | 5-Shot 5-Way |   
|:--------:|:------------:|:------------:|
| Previous SOTA |     64.12    |     80.51    |
| **DeepEMD-FCN** |     **66.50**    |     **82.41**    |
|  **DeepEMD-Grid**  |     **67.83**    |     **83.14**    | 
| **DeepEMD-Sampling** |     **68.77**    |     **84.13**    | 

**TieredImageNet Dataset**

|  Setups  | 1-Shot 5-Way | 5-Shot 5-Way |   
|:--------:|:------------:|:------------:|
| Previous SOTA |     68.50    |     84.28    |
| **DeepEMD-FCN** |     **72.65**    |     **86.03**    |
|  **DeepEMD-Grid**  |     **73.13**    |     **87.08**    | 
| **DeepEMD-Sampling** |     **74.29**    |     **86.98**    | 


## Prerequisites

The following packages are required to run the scripts:

- [PyTorch >= version 1.1](https://pytorch.org)

- [QPTH](https://github.com/locuslab/qpth)

- [CVXPY](https://www.cvxpy.org/)

- [OpenCV-python](https://pypi.org/project/opencv-python/)

- [tensorboard](https://www.tensorflow.org/tensorboard)
## Dataset
Please click the Google Drive [link](https://drive.google.com/drive/folders/1sXJgi9pXo8i3Jj1nk08Sxo6x7dAQjf9u?usp=sharing) or [Baidu Drive (uk3o)](https://pan.baidu.com/s/17hbnrRhM1acpcjR41P3J0A) for downloading the 
following datasets, or running the downloading bash scripts in folder `datasets/` to download.


### MiniImageNet Dataset

It contains 100 classes with 600 images in each class, which are built upon the ImageNet dataset. The 100 classes are divided into 64, 16, 20 for meta-training, meta-validation and meta-testing, respectively.

### TieredImageNet Dataset
TieredImageNet is also a subset of ImageNet, which includes 608 classes from 34 super-classes. Compared with  miniImageNet, the splits of meta-training(20), meta-validation(6) and meta-testing(8) are set according to the super-classes to enlarge the domain difference between  training and testing phase. The dataset also include more images for training and evaluation (779,165 images in total).

### CUB Dataset
CUB was originally proposed for fine-grained bird classification, which contains 11,788 images from 200 classes. We follow the splits in [FEAT](https://github.com/Sha-Lab/FEAT) that 200 classes are divided into 100, 50 and 50 for meta-training, meta-validation and meta-testing, respectively.

### FC100 Dataset
FC100 is a few-shot classification dataset built on CIFAR100. We follow the split division proposed in [TADAM](https://papers.nips.cc/paper/7352-tadam-task-dependent-adaptive-metric-for-improved-few-shot-learning.pdf), where 36 super-classes were divided into 12 (including 60 classes), 4 (including 20 classes), 4 (including 20 classes), for meta-training, meta-validation and meta-testing, respectively, and each class contains 600 images.

### CIFAR-FS dataset (not in paper)
CIFAR-FS was also built upon CIFAR100,proposed in [here](https://arxiv.org/pdf/1805.08136.pdf). It contains 64, 16, 20 classes for training, validation and testing.




## Important Arguments
We list some important arguments of our networks.
 
**Model Selection Arguments**
- `deepemd`:  choices=['fcn', 'grid', 'sampling']

- `pretrain_dir`:  dir of the pre-trained model

- `model_dir`:  dir of the testing model in `eval.py`

**Solver Selection Arguments**
- `solver`:  choices=['opencv', 'qpth']

- `form`: two ways to use the QPTH for solving EMD, choices=['QP', 'L2']. see codes for detail

- `l2_strength`: the weight for omitting the quadratic term in the form 'L2'

**DeepEMD-FCN Related Arguments**
- `feature_pyramid`: feature pyramid applied to FCN, e.g. `'2,3'`, default: None

**DeepEMD-Grid Related Arguments**
- `patch_list`: the size of grids at every image-pyramid level, e.g. '2,3' means a pyramid with 2×2 and 3×3 structure.
- `patch_ratio`: scale the patch region to incorporate context around the patch 

**DeepEMD-Sampling Related Arguments**
- `num_patch`: the number of sampled patches

**SFC Related Arguments** 
- `sfc_lr`: learning rate for finetuning SFC
- `sfc_wd`: weight decay strength for finetuning SFC
- `sfc_update_step`: number of finetuning step for SFC
- `sfc_bs`: batch size for finetuning SFC


## Some general tips
**Get started.**
The training of our model has two stages, the model pre-training stage (`train_pretrain.py`) and 
the episodic meta-training stage (`train_meta.py`). You may also directly test the trained models by running `eval.py`.
Before running these scripts, please set the dataset directory (`-data_dir`) and pre-trained model directory (`-pretrain_dir`) in the arguments
or directly change the default directories in the code.  

**Solver selection.** We provide two solvers to solve the LP problem in the EMD layer, OpenCV and QPTH. 
OpenCV is much faster than QPTH, therefore you can use OpenCV for validation and QPTH for training. You may also use OpenCV for training, which is much faster and saves memory,
 but this omits the graidents through the constraints and compromises performance. 

**About GPU memory.** There are many arguments that influence the GPU momory. 
You may choose to adjust these arguments to make a blance between GPU memory and performance.

- `solver`: `OpenCV` requires much less GPU memories than `QPTH`. 

- `query`: The number of query images in each class. Play the role of batch size in a task.

- `num_patch`: The number of sampling patches in DeepEMD-Sampling.
 
 - `patch_list`: A list that indicates the grid size at different levels of image-pyramid  in DeepEMD-Grid.

For DeepEMD-Sampling and DeepEMD-Grid, you may choose to train with a small number of patches 
but test with a big number of patches to achieve improved performance. 




## Testing scripts for DeepEMD


Test DeepEMD-FCN with a trained model for 1-shot 5-way task on the miniImageNet dataset:

    $ python eval.py  -deepemd fcn -gpu 0,1,2,3
    
Test DeepEMD-FCN with a trained model for 5-shot 5-way task on the miniImageNet dataset:

    $ python eval.py  -deepemd fcn -shot 5 -test_episode 600 -gpu 0,1,2,3

Test DeepEMD-Gird-Pyramid (2,3) with a trained model for 1-shot 5-way task on the miniImageNet dataset:

    $ python eval.py  -deepemd grid -patch_list 2,3  -gpu 0,1,2,3

Test DeepEMD-Sampling (9-patch) with a trained model for 1-shot 5-way task on the miniImageNet dataset:

    $ python eval.py  -deepemd sampling -num_patch 9   -gpu 0,1,2,3


## Training scripts for DeepEMD
Pre-train the models on the miniImagenet

    $ python train_pretrain.py -dataset miniimagenet -gpu 0,1,2,3
    

Train DeepEMD-FCN with a pre-trained model for 1-shot 5-way task on the miniImageNet dataset:

    #use opencv solver (about 8GB memory)
    $ python train_meta.py -deepemd fcn -shot 1 -way 5 -solver opencv -gpu 0,1,2,3
    
    #use QPTH solver (about 32GB memory)
    $ python train_meta.py -deepemd fcn -shot 1 -way 5 -solver qpth -gpu 0,1,2,3

Train DeepEMD-Gird-Pyramid (2,3) with a pre-trained model for 1-shot 5-way task on the miniImageNet dataset:
 
    #use opencv solver (about 45GB memory)
    $ python train_meta.py  -deepemd grid -patch_list 2,3 -shot 1 -way 5 -solver opencv -gpu 0,1,2,3
        
Train DeepEMD-Sampling (9 patchs) with a pre-trained model for 1-shot 5-way task on the miniImageNet dataset:
 
    #use opencv solver (about 32GB memory)
    $ python train_meta.py  -deepemd sampling -patch_list 9 -shot 1 -way 5 -solver opencv -gpu 0,1,2,3
        
            
## Download  Models


[Pre-trained Models](https://drive.google.com/file/d/1Prn7_41NVrZbnePAlSiKjD21Jlz0LKJM/view?usp=sharing)
(or run `bash download_pretrain_model.sh`)

[Meta-trained Models](https://drive.google.com/file/d/1lGcNHMRnBrjODDmt647RzMJ5cLCd4pmv/view?usp=sharing)
(or run `bash download_trained_model.sh`)

## Acknowledgment
Our project references the codes in the following repos.
- [FEAT](https://github.com/Sha-Lab/FEAT)

- [MTL](https://github.com/yaoyao-liu/meta-transfer-learning)



",['icoz69'],0,0.66,0,,,,,,19,,,,
471173914,R_kgDOHBWLGg,NeRFusion,jetd1/NeRFusion,0,jetd1,https://github.com/jetd1/NeRFusion,,0,2022-03-17 23:27:10+00:00,2024-12-10 13:05:14+00:00,2024-08-14 20:07:25+00:00,,1757,266,266,Python,1,1,1,1,0,0,5,0,0,11,mit,1,0,0,public,5,11,266,main,1,,"# NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction (CVPR 2022 Oral)

[Project Sites](https://jetd1.github.io/NeRFusion-Web/)
 | [Paper](https://arxiv.org/abs/2203.11283) |
Primary contact: [Xiaoshuai Zhang](https://jetd1.github.io/NeRFusion-Web/)

## Note

This `dev` branch is currently **under development**. We will finish and merge this into `main` in a few days. This is a re-development of the original NeRFusion code based heavily on [nerf_pl](https://github.com/kwea123/nerf_pl), [NeuralRecon](https://github.com/zju3dv/NeuralRecon), [MVSNeRF](https://github.com/apchenstu/mvsnerf). We thank the authors for sharing their code. The model released in this repo is optimized for large-scale scenes further compared to the CVPR submission. A changelist will be provided.


## Introduction

<img src=""./assets/teaser.png"" />

While NeRF has shown great success for neural reconstruction and rendering, its limited MLP capacity and long per-scene optimization times make it challenging to model large-scale indoor scenes. In contrast, classical 3D reconstruction methods can handle large-scale scenes but do not produce realistic renderings. We propose NeRFusion, a method that combines the advantages of NeRF and TSDF-based fusion techniques to achieve efficient large-scale reconstruction and photo-realistic rendering. We process the input image sequence to predict per-frame local radiance fields via direct network inference. These are then fused using a novel recurrent neural network that incrementally reconstructs a global, sparse scene representation in real-time at 22 fps. This volume can be further fine-tuned to boost rendering quality. We demonstrate that NeRFusion achieves state-of-the-art quality on both large-scale indoor and small-scale object scenes, with substantially faster reconstruction speed than NeRF and other recent methods.

<img src=""./assets/pipeline.png"" />

## Reference
Please cite our paper if you are interested   
 <strong>NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction</strong>.  &nbsp;&nbsp;&nbsp; 
```
@article{zhang2022nerfusion,
  author    = {Zhang, Xiaoshuai and Bi, Sai and Sunkavalli, Kalyan and Su, Hao and Xu, Zexiang},
  title     = {NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction},
  journal   = {CVPR},
  year      = {2022},
}
```


## Installation

### Requirements
All the codes are tested in the following environment:
* Linux (Ubuntu 20.04 or above)
* 32GB RAM (in order to load full size images)
* NVIDIA GPU with Compute Compatibility >= 75 and VRAM >= 6GB, CUDA >= 11.3

### Dependencies
* Python>=3.8 (installation via [anaconda](https://www.anaconda.com/distribution/) is recommended, use `conda create -n ngp_pl python=3.8` to create a conda environment and activate it by `conda activate ngp_pl`)
* Python libraries
    * Install `pytorch>=1.11.0` by `pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113`
    * Install `torch-scatter` following their [instruction](https://github.com/rusty1s/pytorch_scatter#installation)
    * Install `tinycudann` following their [instruction](https://github.com/NVlabs/tiny-cuda-nn#requirements) (compilation and pytorch extension)
    * Install `apex` following their [instruction](https://github.com/NVIDIA/apex#linux)
    * Install `torchsparse` following their [instruction](https://github.com/mit-han-lab/torchsparse#installation)
    * Install core requirements by `pip install -r requirements.txt`

* Cuda extension: Upgrade `pip` to >= 22.1 and run `pip install models/csrc/` (please run this each time you `pull` the code)

## Data Preparation
We follow the same data organization as the original NeRF, which expects camera parameters to be provided in a `transforms.json` file. We also support data from NSVF, NeRF++, colmap and ScanNet.

### Custom Sequence
You can test our pre-trained model on custom sequences captured under casual settings. To do so, the data should be organized in the original NeRF-style:

```
data
├── transforms.json
├── images
│   ├── 0000.jpg
    ├── 0001.jpg
    ├── ...
```

If a video is all you have (no camera parameters). You should install `ffmpeg` and `colmap`. Then follow the instructions as introduced in [instant-ngp](https://github.com/NVlabs/instant-ngp/blob/master/scripts/colmap2nerf.py) to generate the `transformas.json`.

## Inference using Pre-trained Network
```bash
python train.py --dataset_name scannet --root_dir DIR_TO_SCANNET_SCENE0000_01 --exp_name EXP_NAME --ckpt_path PATH_TO_G_CKPT
```
Please find the pre-trained weights for networks [here](https://drive.google.com/file/d/1YjwO1Q2CAn7tdnwVzDgL_iEH_m7cSiHW/view?usp=sharing).

### Per-Scene Optimization
Note: currently this script trains model from scratch. We are updating generalized pipeline.
```bash
python train.py --dataset_name DATASET_NAME --root_dir DIR_TO_SCANNET_SCENE --exp_name EXP_NAME
```

You can test using our [sample data](https://drive.google.com/file/d/1vy5whVQbMcyKTK5W0LJsTlDgCS7wGih7/view?usp=sharing) on ScanNet. You can also try evaluation using our [sample checkpoint](https://drive.google.com/file/d/1wHSPMSGhy1TVSWCYttz2JDNUTMTeI9w0/view?usp=sharing) on ScanNet:
```bash
python train.py --dataset_name scannet --root_dir DIR_TO_SCANNET_SCENE0000_01 --exp_name EXP_NAME --val_only --ckpt_path PATH_TO_SCANNET_SCENE0000_01_CKPT
```

## Training Procedure

Please download and organize the datasets in the following manner:
```
├──data/
    ├──DTU/
    ├──google_scanned_objects/
    ├──ScanNet/
```

For google scanned objects, we used [renderings](https://drive.google.com/file/d/1w1Cs0yztH6kE3JIz7mdggvPGCwIKkVi2/view?usp=sharing) from IBRNet. Download with:

```
gdown https://drive.google.com/uc?id=1w1Cs0yztH6kE3JIz7mdggvPGCwIKkVi2
unzip google_scanned_objects_renderings.zip
```

For DTU and ScanNet, please use the official toolkits for downloading and processing of the data, and unpack the root directory to the `data` folder mentioned above. Train with:

```bash
python train.py --train_root_dir DIR_TO_DATA --exp_name EXP_NAME
```

See `opt.py` for more options.


## Performance

We applied optimization on large-scale scenes in this code base, and the performance may not exactly match all numbers in the paper. Our test results with this code base is reported here. For generalized no per-scene optimization setting, we achieve 23.35/0.844/0.333 on ScanNet eight scenes, 26.23/0.925/0.169 on DTU, and 24.21/0.888/0.129 on NeRF Synthetic. For per-scene optimization setting, we achieve 27.78/0.917/0.199 on ScanNet eight scenes, 31.76/0.961/0.118 on DTU, and 29.88/0.949/0.099 on NeRF Synthetic.


## Acknowledgement
Our repo is developed based on [nerf_pl](https://github.com/kwea123/nerf_pl), [NeuralRecon](https://github.com/zju3dv/NeuralRecon) and [MVSNeRF](https://github.com/apchenstu/mvsnerf). Please also consider citing the corresponding papers. 

The project is conducted collaboratively between Adobe Research and University of California, San Diego. 

## LICENSE

The code is released under MIT License.
",['jetd1'],1,0.78,0,,,,,,50,,,,
641551644,R_kgDOJj1NHA,Research-for-UG-Students,karanwxliaa/Research-for-UG-Students,0,karanwxliaa,https://github.com/karanwxliaa/Research-for-UG-Students,Research programs for Undergraduate students,0,2023-05-16 17:55:44+00:00,2025-03-08 06:28:46+00:00,2025-02-20 10:22:42+00:00,,69,654,654,,1,1,1,1,0,0,76,0,0,0,,1,0,0,public,76,0,654,main,1,,"
# Research-for-UG-Students
⭐ the repo so you don't lose it later :)

## Welcome to the list of research programs and internships available worldwide. <br>
(All the headings are hyperlinks to the official websites of the research programs)

#### This README provides information about various opportunities for students interested in gaining research experience <br>
Feel free to contribute to this list and share your knowledge as well!

## BULGARIA
  * [SURF@INSAIT](https://insait.ai/surf)

## ABU DHABI
  * [MBZUAI, UGRIP](https://mbzuai.ac.ae/ugrip/)

## Austria
 * [ISTernship – ISTA Summer Fellowship](https://phd.pages.ist.ac.at/isternship/)
    
## HONG KONG 
  * [HKU, Computer Science Department](https://www.cs.hku.hk/rintern/)
  * [IVISP, HKUST](https://pg.ust.hk/ivisp) (only for senior year undergraduates + postgraduates)
  * [SURP, CUHK](http://www.summer.cuhk.edu.hk/surp/)
  * [SURP (Hongkong)](http://www.summer.cuhk.edu.hk/surp/?fbclid=IwAR0-H6g4x7UetRxFQkcnK95zvgjkp81TjgCZlBgv-NjrRSxWiOxy84TZuhw)
## SINGAPORE
  * [Singapore International Pre-Graduate Award (SIPGA)](https://www.a-star.edu.sg/Scholarships/for-undergraduate-studies/singapore-international-pre-graduate-award-sipga) (3RD YEAR +)

## SWITZERLAND
  * [Research fellowship at ETH Zurich](https://www.inf.ethz.ch/studies/summer-research-fellowship.html) 
  * [CERN Summer Student Program](https://careers.cern/summer)  
  * [CERN OpenLab for Computer Science Undergraduates](https://openlab.cern/education)
  * [E3, EPFL (Switzerland)](https://eee.epfl.ch/)
  * [Summer at EPFL](https://summer.epfl.ch/)
  * [Swissnex Program](https://swissnex.org/india/thinkswiss/)

## GERMANY
  * [Max Planck Institute for Gravitational Physics](https://www.aei.mpg.de/student-internships), for all students. 
  * [Max Planck Institute for Software Systems](https://apply.mpi-sws.org/register/internship/)
  * [Warwick Statistics Internship Scheme](https://warwick.ac.uk/fac/sci/statistics/research/internships/), University of Warwick
  * [DAAD WISE Scholarship](https://www2.daad.de/deutschland/stipendium/datenbank/en/21148-scholarship-database/?detail=50015295)
  * [HZDR Dresden, Summer student program (Germany)](https://www.hzdr.de/db/Cms?pOid=34387&pNid=2519)
  * [MaxSIP (Max Planck Institute Summer Internship)](https://imprs-ls.opencampus.net/en/maxsip_application_info)
  * [UROP International](http://www.rwth-aachen.de/cms/root/Forschung/Angebote-fuer-Forschende/Angebote-fuer-Studierende/UROP/UROP-INternational/~wnr/Informationen-fuer-Studierende/?lidx=1), RWTH Aachen

## CANADA
  * [MITACS Globalink](https://www.mitacs.ca/en/programs/globalink/globalink-research-internship), for research positions in Canada.
  * [University of Alberta Research Experience (UARE)](https://www.ualberta.ca/admissions-programs/visiting-student-and-internship-programs/research-internships/ualberta-research-experience/index.html)
  * [NSERC Undergraduate Student Research Awards (USRA)](https://www.nserc-crsng.gc.ca/Students-Etudiants/UG-PC/USRA-BRPC_eng.asp)
  * [Amgen Scholars Canada Program](https://amgenscholars.ca/)
  * [Google AI Residency Program - Canada](https://sites.research.google/residency/)
  * [Canadian Space Agency (CSA) - Junior Astronaut Camp](https://www.asc-csa.gc.ca/eng/astronauts/junior-astronauts/camp.asp)

## SOUTH KOREA
  * [SPIKE @ UNIST](https://spike.unist.ac.kr:10449/02_learn/learn03.php), Summer Program for Internship and Korean Experience
  * [SPIKE@UNIST (South Korea)](https://spike.unist.ac.kr:10449/02_learn/learn03.php)
  * [GIST Global Internship Program (South Korea)](https://www.gist.ac.kr/en/html/sub07/0702.html)
  * [Research internship @ Yonsei University](https://summer.yonsei.ac.kr/home/program/internship02.asp)
  * [Yonsei University](https://summer.yonsei.ac.kr/home/program/internship02.asp)
 
## UNITED STATES 
  * [LPI Summer Intern Program in Planetary Science](https://www.lpi.usra.edu/lpiintern/application/), for both national and international students.
  * ~~[NASA - CalTech Summer Undergraduate Research Fellowship (SURF)]~~(https://www.jpl.nasa.gov/edu/intern/apply/caltech-summer-undergraduate-research-fellowship/), for research positions at Jet Propulsion Laboratory, California Institute of Technology. (Open to U.S. citizens and lawful permanent residents (LPRs).)
  * [Santa Fe Institute Summer Research Experience](https://www.santafe.edu/engage/learn/programs/undergraduate-complexity-research)
  * [Robotics Institute for Summer Scholars](https://riss.ri.cmu.edu/), CMU 
  * [Data Science for Social Good Fellowship](https://www.dssgfellowship.org/), CMU 
  * [QRLSSP Summer Program](https://qrlssp.asu.edu/summerprogram), Arizona State University: Only for US Citizens / Permanent Residents
  * [Summer Undergraduate Research fellowship program](https://www.rockefeller.edu/education-and-training/surf/), Rockfeller Institute
  * [CalTech SURF Program](https://sfp.caltech.edu/undergraduate-research/programs/surf/application_information)
  * [Space Astronomy Summer Program, Space Telescope Science Institute (STScI)](http://www.stsci.edu/opportunities/space-astronomy-summer-program)
  * ~~[Data Visualization Programming Summer Student Internship in New York City](https://simonsfoundation.wd1.myworkdayjobs.com/en-US/simonsfoundationcareers/job/162-Fifth-Avenue/Data-Visualization-Intern--SCC_R0000579)~~, Flatiron Institute. (NEW LINK NEEDED)
  * [Summer Research Program, Princeton University](https://undergraduateresearch.princeton.edu/programs/summer-programs?field_princeton_status_eligibili_value=Non-Princeton+undergrads&field_class_year_eligibility_value=Juniors&field_division_value=Engineering)
  * International Student Research Internship Program - [McKelvey School of Engineering](https://engineering.wustl.edu/academics/undergraduate-research/international-student-research-internship-program.html)
  * Luddy School of Informatics, Computing, and Engineering - [Global Talent Attraction Program (GTAP)](https://luddy.indiana.edu/research/student-research/fellowship.html)
  * [BioChemCoRe](https://biochemcore.ucsd.edu/) (Biology and Chemistry Computational Research), Amaro Lab, UC San Diego 
  * [Global Research Experience in Advanced Technologies (GREAT) Program, UC Davis](https://great.ucdavis.edu/)
  * [ICT Summer Research Program](https://ict.usc.edu/academics/internships/application/)
  * [UC Berkeley Amgen Scholars program](https://amgenscholars.berkeley.edu/)
  * [Summer Undergraduate Program in Engineering Research at Berkeley (SUPERB) (for Information Technology students](https://eecs.berkeley.edu/resources/undergrads/research/superb)
  * [SUMMER RESEARCH OPPORTUNITIES PROGRAM(SROP), Purdue University](https://www.purdue.edu/gradschool/diversity/programs/summer-research-opportunities-program/)
  * [Summer Undergraduate Research Fellowship (SURF), Stanford University](https://engineering.stanford.edu/students-academics/equity-and-inclusion-initiatives/prospective-graduate-programs/summer)
  * [IPAM UCLA Program](http://www.ipam.ucla.edu/programs/student-research-programs/)
  * [Santa Fe Institute Undergraduate Complexity Research (UCR)](https://www.santafe.edu/engage/learn/programs/undergraduate-complexity-research)
  * [REU Summer TERC Scholars Program - TERC](https://www.terc.edu/work-with-us/interships/summer-terc-scholars-program/)
  * [NSF Dristtributed Research Experiences for Undergraduates (DREU)](https://cra.jotform.com/242948236029866)

## UNITED KINGDOM
  * [Amgen Scholars UK Programme](http://www.amgenscholars.medschl.cam.ac.uk/)
  * [Lloyds Scholars Programme](https://www.lloyds-scholars.com/)
  * [Wellcome Trust Vacation Scholarships](https://wellcome.ac.uk/funding/schemes/vacation-scholarships)
  * [Nuffield Research Placements](https://www.nuffieldfoundation.org/students-teachers/research-placements)
  * [GSK Undergraduate Industrial Placement](https://www.gsk.com/en-gb/careers/early-talent-opportunities/undergraduate-industrial-placement/)
  * [Imperial College London, Undergraduate Research Opportunities Programme (UROP)](https://www.imperial.ac.uk/urop/what-is-urop/)
  * [University of Oxford - BDI Summer Internship Programme](https://www.bdi.ox.ac.uk/study/bdi-summer-studentship-programme#:~:text=The%20BDI%20will%20offer%20four,be%20based%20in%20the%20UK.) #Has begun to run regularly now.



## AUSTRALIA
  * [University of Melbourne, Australia - Vacation Research Experience Scheme (VRES)](https://ms.unimelb.edu.au/engage/vacation-scholarships/vacation-scholarships-projects)
  * [ANU, Australia - ANU Summer Research Program](https://science.anu.edu.au/summer-research-scholars-program-application)
  * [CSIRO, Australia - Vacation Scholarships](https://www.csiro.au/en/careers/scholarships-student-opportunities/undergraduate-studentships/undergraduate-vacation-studentships)


## INDIA
  * [IIT Roorkee, Summer Internship Program](https://www.iitr.ac.in/academics/internship/pages/index.html)
  * [IIT Bombay, Summer Research Program](https://www.ircc.iitb.ac.in/IRCC-Webpage/rnd/SummerResearchInternship)
  * [IIT Kharagpur, Summer of Code](https://summerofcode.in/)
  * [IIT Madras, Summer Fellowship Programme](https://sfp.iitm.ac.in/)
  * [BITS Pilani, Summer Undergraduate Research Fellowship (SURF)](https://www.bits-pilani.ac.in/pilani/summerresearchfellowship)
  * [IIIT Hyderabad, Summer Internship Program](https://www.iith.ac.in/news/2023/02/03/Summer-Undergraduate-Research-Exposure/)
  * [IIIT Delhi, Summer Research Internship Program](https://www.iiitd.ac.in/)
  * [IISc Bangalore, Summer Research Program](https://www.iisc.ac.in/)
  * [IISER Pune, Summer Student Program](https://www.iiserpune.ac.in/)
  * [IISER Kolkata, Summer Student Research Programme](https://www.iiserkol.ac.in/)
  * [IISER Mohali, Summer Research Program](https://www.iisermohali.ac.in/)
  * [IISER Bhopal, Summer Student Research Fellowship](https://www.iiserb.ac.in/)
  * [IISER Tirupati, Summer Research Programme](https://www.iisertirupati.ac.in/)
  * [IISER Thiruvananthapuram, Summer Visiting Programme](https://www.iisertvm.ac.in/)
  * [BITS Goa Summer Research Programme](https://www.bits-pilani.ac.in/goa/bgsrp/)
  * [Microsoft India Research Fellows Program](https://www.microsoft.com/en-us/research/academic-program/research-fellows-program-at-microsoft-research-india/)

## GERMANY
  * [RISE - Research Internships in Science and Engineering](https://www.daad.de/rise/en/)
  * [International Max Planck Research School (IMPRS) Summer Internship Program](https://www.imprs-cs.de/summer-internships)
  * [Helmholtz Centre for Infection Research (HZI) Summer Student Program](https://www.helmholtz-hzi.de/en/research/research_schools/school_of_infection_research/internships/summer_student_program/)
  * [LMU Munich, International Summer University for Women in Informatics and Computer Science (ISU)](https://www.wimi.ovgu.de/wimi_international/ISU.html)

## FRANCE
  * [Pasteur Institute, Pasteur-Roux-Cantarini Internship Program](https://www.pasteur.fr/en/education/robert-cantarini-internship-program)
  * [Institut Curie, Undergraduate Internship Program](https://training.curie.fr/article502.html)
  * [École Normale Supérieure (ENS) de Lyon, Summer Research Program](https://www.ens.psl.eu/summer-research-programme)
  * [Paris-Saclay University, Internship Program](https://www.universite-paris-saclay.fr/en/education/doctoral-studies/internship-bourses)
  * [Research Program for International Talents, École Polytechnique](https://programmes.polytechnique.edu/en/exchange-programs/research-program-for-international-talents/program-details)

## JAPAN
* [Okinawa Institute of Science and Technology, Research Internship Program](https://admissions.oist.jp/oist-research-internship-program-description)


## MULTIPLE COUNTRIES

  * [IPAM UCLA, RIPS Program](http://www.ipam.ucla.edu/programs/student-research-programs/)
  * [Amgen Scholars Program](https://amgenscholars.com/)
  * [UX Research Internship, Red Hat](https://us-redhat.icims.com/jobs/83084/remote-us-nc/job)
  * [Allen Institute for AI, Research and Engineering Internships](https://allenai.org/internships)


Other programs for Indian students:

1. ~~S.N. Bose Scholarship sponsored by IUSSTF~~, it's been closed for almost two years now.
2. [ICTS Summer research program](https://www.icts.res.in/academic/summer-research-program), by ICTS-TIFR.
3. [DAAD Wise Scholarship](https://www.daad.de/go/en/stipa50015295), for research positions in Germany
4. [Shastri Research Student Fellowship](https://www.shastriinstitute.org/shastri-research-student-fellowship) by Shastri Indo-Canadian Institute
5. [Viterbi India Program](https://iusstf.org/iusstf-viterbi-program), sponsored by IUSSTF and USC Viterbi
6. [Khorana Program for Scholars](https://www.iusstf.org/program/khorana-program-for-scholars), sponsored by IUSSTF
8. [Charpak Global Scholarship](https://www.inde.campusfrance.org/charpak-lab-scholarship), funded by French Embassy in India
9. [Indian Student Internship Program at NTHU](http://oga.nthu.edu.tw/news.php?id=233&lang=en)
10. [IBM Blue Mix](https://researcher.watson.ibm.com/researcher/view_group_subpage.php?id=8101) Research Internship
11. [IBM Extreme Blue](http://www-07.ibm.com/employment/in/students/extreme-blue/index.html) (limited to IITs and NITs)
12. [LIGO IndiGo](http://jobs.gw-indigo.org/tiki-index.php?page=LIGO-IndIGO+Summer+Students+Program) at CalTech, funded by US NSF
13. [IIITD Summer Internship](https://www.iiitd.ac.in/placement/internships), IIIT, New Delhi
14. [Summer Research Fellowship Programme 2021](https://webjapps.ias.ac.in/fellowship2022/index.html), IISc, IAS, INS
15. [Quantitative Research Summer Internship](https://websim.worldquantchallenge.com/en/cms/wqc/summerprograms/india/), WorldQuant India
16. [Internship At CMI](https://www.cmi.ac.in/admissions/internships.php), Chennai Mathematical Institute
17. [Shastri Student Internship Project](https://www.shastriinstitute.org/Shastri_Student_Internship_Project), Eligible universities [here](https://www.shastriinstitute.org/member-council)
18. [Cisco Summer internship](https://jobs.cisco.com/jobs/ProjectDetail/Software-Engineer-Bachelor-s-Intern-United-States/1295250?source=Pitt+CSC&tags=CDC+SnNG+students-and-new-graduate-programs)
19. [Hackerrank SDE intern](https://breakinghierarchy.com/hackerrank-sde-intern/)
20. [SSERD Research Intern](https://www.sserd.org/internship/) for research work in topics like Space Settlement, Astrophysics, Space Mission Design etc.
21. [IISER Kolkata Summer Student Research Programme](https://www.iiserkol.ac.in/~summer.research/), IISER Kolkata
22. [IITM Summer Fellowship Programme](https://sfp.iitm.ac.in), IIT Madras
23. [IITD GIPEDI](https://web.iitd.ac.in/~subrat/SummerInternshipRules.htm), IIT Delhi
24. [IITB Research Internship Award](http://www.iitb.ac.in/en/education/research-internship), IIT Bombay
25. [IITK SURGE Program](http://surge.iitk.ac.in/about.html), IIT Kanpur
26. [SPARK](http://spark.iitr.ac.in/), IIT Roorkee
27. [SRIP](https://srip.iitgn.ac.in/info/), IIT Gandhinagar
28. [IIT Ropar Summer Internship](https://onlineportal.iitrpr.ac.in/sia-21)[ [Information]](https://www.iitrpr.ac.in/sites/default/files/Advertisement%20for%20Summer%20Internship%202021.pdf), IIT Ropar
29. [Bhaba Atomic Research Centre](http://www.barc.gov.in/student/)
30. [ISI Kolkata Summer Internship](https://www.isical.ac.in/~rcbose/internship/index.html)
31. [IIT (ISM) Dhanbad SRIP Programme](https://www.iitism.ac.in/deans/research/SRIP.php)
32. [IIIT Delhi](https://www.iiitd.ac.in/placement/summer-internships)
33. [IIIT HYDERABAD](https://ihub-data.iiit.ac.in/programs/events/shristi-23/)
34. [Summer Program, CeNSE IISc Bangalore](http://www.cense.iisc.ac.in/content/summer-program)
36. [Nanyang Technical University,Singapore (India Connect @ NTU)](https://www.ntu.edu.sg/about-us/global/india-connect-ntu)


","['karanwxliaa', 'nidhiparab', 'aravharish', 'Abhyuday-06', 'aindree-2005', 'gajjararyan', 'Devadeut', 'ekassz', 'noctkun', 'ishansurdi', 'aymuos15', 'pythongiant']",0,0.63,0,,,,,,13,,,,
132788247,MDEwOlJlcG9zaXRvcnkxMzI3ODgyNDc=,pp4fpgas,KastnerRG/pp4fpgas,0,KastnerRG,https://github.com/KastnerRG/pp4fpgas,Parallel Programming for FPGAs -- An open-source high-level synthesis book,0,2018-05-09 17:03:03+00:00,2025-03-06 18:24:07+00:00,2025-01-13 17:33:09+00:00,http://hls.ucsd.edu/,94073,814,814,TeX,1,1,1,1,0,0,149,0,0,6,cc-by-4.0,1,0,0,public,149,6,814,master,1,1,,"['rck289', 'stephenneuendorffer', 'codspalaniappan95', 'arkhodamoradi', 'level2fast', 'jmduarte', 'LinnaIkae', 'qkgautier', 'jcrisologo', 'mithro', 'Mustafa3296', 'omasanori', 'anderspitman', 'davidmetz', 'jiafulow', 'NN708', 'sthornington', 'priyarora4']",1,0.76,0,,,,,,55,,,,
121909204,MDEwOlJlcG9zaXRvcnkxMjE5MDkyMDQ=,ML-for-High-Schoolers,kjaisingh/ML-for-High-Schoolers,0,kjaisingh,https://github.com/kjaisingh/ML-for-High-Schoolers,This guide details a learning path for high school students looking to explore the field of Machine Learning & Artificial Intelligence.,0,2018-02-18 01:51:40+00:00,2025-02-02 17:41:31+00:00,2024-09-27 02:04:59+00:00,,230,1006,1006,,1,1,1,1,0,0,139,0,0,1,,1,0,0,public,139,1,1006,master,1,,"# Learning Artificial Intelligence and Machine Learning as a High Schooler
<a href=""./README.md""><img alt=""English"" src=""https://img.shields.io/badge/English-lightgrey""></a>
<a href=""./README-en-cn.md""><img alt=""中文"" src=""https://img.shields.io/badge/中文-lightgrey""></a>

Hi, I'm Karan, a high school student based in Singapore. Having spent the last year exploring the field of Artificial Intelligence (AI) and Machine Learning (ML), I believe that there does not exist a learning path in this field that is built specifically **for High School students**. This is my attempt to create one.

Since I started my journey into this area, I've tried to spend a couple of hours every day understanding as much as I can, whether it be watching Youtube videos, undertaking personal projects or simply reading books. I've been guided by older peers who've had far more experience than me, but know that such guidance is not available to everyone - so this is my attempt to relay all the learnings into one concrete document.

All the information that I have compiled in this guide is intended for high schoolers wishing to excel in this up and coming field. It is intended to be followed chronologically, and unlike most guides/learning paths that I've come across, **doesn't require an understanding** of linear algebra, partial derivatives and other complex mathemathical concepts which one cannot find in their high school syllabuses. However, it does include a course which covers the fundamentals of the essential math for Machine Learning - the level of which I'd consider comparable to high school maths. If you work through this path on a regular basis, I believe that you could get to a reasonably proficient level in about three months. However, this learning path does provide content that can keep you learning for the rest of your time in high school.

So, lets get to it.

### 1. Learn the basics of programming in Python.
I strongly suggest Python as a starting point, as it's a language that ticks most boxes when it comes to being used in the AI/ML domain - not only is it extremely easy to learn, it provides libraries and frameworks for pretty much every basic algorithm known in the field. While R is useful, I find that Python is far more suitable for high school students due to its readability and learnability. Besides basic programming, for Machine Learning in particular, the libraries that are most useful are Numpy, Pandas and Matplotlib. 
- For those of you who have never coded before, I suggest going to a course provided by the University of Toronto (one of the best universities for AI/ML right now). It will take you a few weeks, but its well worth your time - most of the knowledge you gain through this course can be applied to any other programming language, the only difference being the syntax. The course is free, and can be found [here](https://www.coursera.org/learn/learn-to-program?siteID=SAyYsTvLiGQ-rs4V8qoewjp3oL7Nr.r_Fw&utm_content=10&utm_medium=partners&utm_source=linkshare&utm_campaign=SAyYsTvLiGQ#).
- For those of you who have coding experience in a language besides Python, just skim through [this tutorial](https://www.tutorialspoint.com/python/python_basic_syntax.htm) for a basic understanding of Python syntax - it shouldn't take you more than a day.
- ML and AI are built on mathematical principles like Calculus, Linear Algebra, Probability, Statistics, and Optimization - many hopeful AI practitioners (like myself) find this daunting. This course on edX [Essential Math for Machine Learning: Python Edition](https://www.edx.org/course/essential-math-for-machine-learning-python-edition-2) by Microsoft is not designed to make you a mathematician. Rather, it aims to help you learn some essential foundational concepts and the notation used to express them. The course provides a hands-on approach to working with data and applying the techniques you’ve learned in real-world problem settings. Financial aid is available for those who need it.
- Now, after you've learnt the basics of Python, you need to understand the fundamental two libraries used in the field - Numpy and Pandas, which are used primarily for data manipulation, representation and storage. Matplotlib, the third 'core' library in the area, is used to visualize this data through graphs and diagrams - but we'll get to that later. These two courses together shouldn't take more than a couple of days: [Numpy](http://cs231n.github.io/python-numpy-tutorial/) and [Pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html).
  
With this in your back pocket, Now you should be set in the core programming needed to learn Machine Learning and Artificial Intelligence.
 
### 2. Understand the fundamentals of Machine Learning.
If there's one universal course for Machine Learning, it has to be Andrew Ng's. It may seem slightly challenging for high school students, as it refers to concepts such as partial derivatives 
 - but I firmly believe that understanding these aren't required to gain tangible knowledge from the course. I found it particularly beneficial to re-watch some lectures in Weeks 3 to 5 - these topics are advanced, so it may feel a bit fast the first time you watch it. Don't be too worried if you can't fully follow the core mathematics, especially with respect to the calculus - some of this certainly requires university-level math knowledge. It is more important that you are able to follow the thought process that Prof. Ng uses when relaying his knowledge, as this enables you to gain an understanding of what is going on under the hood of Machine Learning processes.
 
I would encourage you to take notes during the course, as writing down what you learn helps ensure that you are truly understanding the information relayed. Completing the programming tutorials and exercises is not essential, as these are done in Matlab - which (in my experience) can be tricky to grasp, as it is a matrix-based language. But don't worry, we will be doing the very same (and far more advanced) algorithms in Python in just a short amount of time.
 
This free course can be found [here](https://www.coursera.org/learn/machine-learning).
 
### 3. Gain exposure to an assortment of Mchine learning algorithms, and implement them in real-world scenarios.
Implementing ML algorithms without the university-level math knowledge that powers the nuts and bolts of these algorithms sounds like a paradoxical task - however, a team from Australia set out to do just this.

Kirill Eremenko and Hadelin de Ponteves, a pair of researchers part of the 'SuperDataScience' team, are absolutely fantastic at finding relevant ways to apply simple algorithms in real life. Furthermore, they go into a suitable amount of depth to understand the functionality of the algorithm, but without the complex maths that a high school student would not be able to understand. Their course covers both Python and R, though I would not worry about R at this point - simply go through the Python tutorials. Also, if you find that they are going a bit too slow, play this course at 1.25x speed (I did that and found it much more suitable to my learning).

Their course is on Udemy, and is only offered as a paid version, though Udemy regularly has discounts of 90% or more on their courses. It can be found [here](https://www.udemy.com/machinelearning/learn/v4/overview), and is usually around $10. It covers everything from basic regression algorithms to deep neural networks, the latter of which is the core architecture used in many modern day applications like ChatGPT and AlphaFold. If you wish to explore even more advanced areas, their Deep Learning course is offered at the end of the Machine Learning for a 90% discount. 

If you're unwilling to pay for this course, you can check out Google's free Deep Learning course [here](https://www.udacity.com/course/deep-learning--ud730) or University of Michigan's free course [here](https://www.coursera.org/learn/python-machine-learning). In my opinion though, these are far from as well-rounded as the SuperDataScience team's courses.

For these courses, taking notes aren't a necessity - there are tons of 'algorithm cheat sheets' online, which offer a quick intution on how they work. [This website](https://www.analyticsvidhya.com/blog/2017/02/top-28-cheat-sheets-for-machine-learning-data-science-probability-sql-big-data/) lists a few.

### 4. Explore, explore and explore.
Now that you've covered a wide range of machine learning concepts, it's time for you to independently use this knowledge to complete some projects. I'd suggest exploring [Kaggle](https://www.kaggle.com/) and the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets.html) - find a dataset you have an interest in, and model some solutions to problems that they relate to. Play around with different algorithms and work towards optimizing performance.

Ensure that the datasets you use are simple and clean in nature - i.e. they shouldn't require too much pre-processing or domain-specific knowledge to work with. Some easy datasets off the top of my head are the Iris, Wine, Breast Cancer Wisconsin, Autism Screening, Congress Voting, Handwritten Digits MNIST and Fashion MNIST ones.

If you ever come across a road block, [Stack Overflow](https://stackoverflow.com/) is your best friend - they have an answer to almost any question that you'd have. If it doesn't, just post one - you should get replies within a couple of hours! There's nothing much more to this step - when you find that you've become comfortable with the whole modelling process from back to front, feel free to move on!

### 5. Find a niche and dive deeper.
Now you should not only have a great and broad understanding of all the basics, but also have an ability to apply it to some real-world data problems. However, it's important to understand that these basics don't span the whole world of ML/AI - rather, many of them have been known ways of tackling such data problems for years, but unfortunataly only more recently were computers powerful enough to truly leverage them in a reasonable amount of runtime. Most modern work in the area focuses on improving these in a variety of novel ways, and building systems tangential to these that leverage the underlying algorithms but improve, extent and enhance them in a variety of ways. Thus, I suggest you find an area of interest in the broader field of Machine Learning, and delve deeper into it in order to become more experienced with the state of the art of that field as it is today. You probably won't have time to become experts in all of the areas I outlined during your high school tenure, but try and conquer one or two.

Before getting into these areas, I'd recommend truly understanding what it pertains - a simple Youtube search for a high-level explanation will give you all you need. So let's get to it.
- Computer Vision: This area pertains to making computers see and understand things using a special type of neural network. Stanford publishes their course on this online [here](http://cs231n.stanford.edu/), with lectures, course notes and assignments available online. Go through this, but don't worry about the math being too complicated at times - the course is intended primarily to deepen your knowledge, which it inevitably will do. You can also look to OpenCV, a computer vision library that does a lot of the complex stuff for you. A great tutorial can be found [here](https://www.youtube.com/watch?v=Z78zbnLlPUA&list=PLQVvvaa0QuDdttJXlLtAJxJetJcqmqlQq). Once you're done with these, look at more advanced image datasets on Kaggle and UCI, or even enter some Kaggle Competitions.

- Natural Language Processing: Understanding how computers learn to speak is also a prominent topic today. Once again, Stanford offers a great course thats online and can be found [here](http://web.stanford.edu/class/cs224n/). If you don't understand some of the math concepts, don't worry, just gain an understanding of how this domain works. For implementations, you could undertake [this Udemy course](https://www.udemy.com/data-science-natural-language-processing-in-python/). However, you could alternatively go through some of well-known Machine Learner [Siraj Raval's videos](https://www.youtube.com/watch?v=9zhrxE5PQgY). One you've done these, try undertaking simple, well-known projects like building a chatbot, sentiment analysis or creating lyrics to a song - simple Youtube searches should help you out. More modern applications like ChatGPT and Claude are built on a Neural Network-based system called Large Language Models, which are primarily based on the Transformer architecture - [this course](https://www.deeplearning.ai/courses/generative-ai-with-llms/) by Andrew Ng is a good starting point.

- Reinforcement Learning: This domain focuses on how machines learn to act in a particular setting, and its most popular application is in the field of video games. Siraj Raval has a pretty good playlist on this, which can be found [here](https://www.youtube.com/watch?v=i_McNBDP9Qs&list=PL2-dafEMk2A5FZ-MnPMpp3PBtZcINKwLA). If you are looking for implementation-based tutorials specifically using a high-level package like Tensorflow, Denny Britz has a solid set of tutorials which can be found [here](https://github.com/dennybritz/reinforcement-learning). David Silver's UCL course is also great, though beginners may find it a bit tricky - it can be found [here](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html). Once you're done with these, its pretty logical to just start downloading base projects or games from online, and adding an element of AI to govern how the agents act. Simple walkthroughs can again be found via a simple Youtube search.

- Data Science & Analytics: This field is a budding domain with tons of exciting job oppurtunities, and is used extensively in most modern corporations to derive insights from the hoards of data being collected in order to inform business decisions. I suggest undertaking either SuperDataScience's [paid course](https://www.udemy.com/datascience/) or UC San Diego's Python-based [free course](https://www.edx.org/course/python-data-science-uc-san-diegox-dse200x), though you can find specific learning paths for data science with a simple Google search. You can also use the following links to learn [SQL](https://www.khanacademy.org/computing/computer-programming/sql) and [Matplotlib](https://www.youtube.com/watch?v=q7Bo_J8x_dw), which are tangential languages used for a lot of modern data analytics. The advantage in learning this at a student level is employability - I have numerous friends in high school who've been offered data science internships, as the insights gained work in this discipline can instantly be monetized by companies. Data-driven decision making is really the only form of decision making in today's corporate world.

- There are also areas like Boltzmann Machines (used for recommendation systems), Adversial Networks (AI improving AI) and Genetic Algorithms (improving a solution to a problem in a way similar to natural evolution), but in my opinion, the combination of their niche applicability and requiring more advanced levels of math make them less desirable as a starting point. Do feel free to explore these if you have a particular passion for one of them, though they also aren't as well documented as the other areas, which may make mastering them slightly more tricky.

 ### BONUS (though still extremely important): Broaden your AI/ML horizons.
If you want to work in this field in the long run, its crucial to understand from a more holistic perspective - by this I mean learning about groundbreaking discoveries, the discource around how it should be applied and its general implications on society. You should start doing things listed in this section as soon as you have the necessary understanding of how the technology works - I believe a good starting point is as you begin Section 4 of this learning path. This kind of information may not specifically help with implementing algorithms for data problems, but for a technology that is so integral to today's world, it really helps shape a more robust understanding of its role, true potential and limits.

There's a few things that a high schooler can do to deepen their general understanding of the field and make them more knowledgeable:
- Start reading research papers: I'd like to emphasize these really aren't as challenging as they sound. While the math that governs modern techniques may be very advanced, simply gaining exposure to what's going on in the front lines of the industry is never a bad thing. If you ever come across one you don't understand, just put it down - there are more than enough alternatives to keep you busy. [This website](https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html) offers a host of great papers to start with, though after you finish those, [this](http://www.jmlr.org/papers/) offers a more lengthy list - simply read ones you're interested in or related to your area of 'expertise' from Section 5. It's helpful to keep a small diary of learnings from each paper. If you are unable to truly understand many of these reserach papers, try going through [this guide](https://github.com/kjaisingh/Artificial-Intelligence-Research-Papers) I wrote which provides more digestible breakdowns of some recent innovations. [This Youtube channel](https://www.youtube.com/@TwoMinutePapers) also has a host of more introductory explanations of papers, each covered in just two minutes.
- Follow pioneers: People like Andrew Ng, Ian Goodfellow and Yann LeCunn are regularly interviewed, providing the perspective of the 'founders' of what we know as AI & ML today. This [Youtube channel](https://www.youtube.com/user/Maaaarth/videos) gathers the best of these talks, and compiles them into a central resource - watch one a night, and I guarantee that you'll feel like an expert within weeks.
- Stay up-to-date with the field: Wired is one of the best platforms for anyone interested in tech. It publishes multiple AI-related stories every day (though doesn't everyone these days?), which can be found [here](https://www.wired.com/tag/artificial-intelligence/). It's a quick and engaging way to understand the trends of the time. Alternatively, subsribe to TechCrunch's Facebook Messenger bot - it often has interested AI-related articles, and prompts you with information every day.
- Understand the implications: There's no better way to do this than listening to TED talks. Their speakers are extremely knowledgeable in the field, and there is an increasing emphasis on AI in their speeches. A collection of videos can be found [here](https://www.youtube.com/user/TEDtalksDirector/videos).
- The Philosophy: AI has its supporters and its opposers. The philosophy behind it, however, is intriguing. My favourite books that explore this area, and are suitable for high school students, include 'How to Create a Mind' by Ray Kurzweil and ['Life 3.0'](http://s3.amazonaws.com/arena-attachments/1446178/cffa5ebc74cee2b1edf58fa9a5bbcb1c.pdf?1511265314) by Max Tegmark - do give them a shot. They look at the more long-term trajectory of AI, which may not feel as relevant on a day-to-day basis, but helps understand the wider context of the technology as a whole.
- Contributing: If you're the kind of person who likes to learn from others experience, check out avenues of discourse such as this [Facebook group](https://www.facebook.com/groups/DeepNetGroup/), where people regularly post insightful articles and papers relating to advances in the area. Alternatively, for more casual conversations, check out subreddits on AI [like this one](https://www.reddit.com/r/artificial/). 
- Delve into the math: Yes, you do need university level math fundamentals for these, but if you're a strong math student, there's nothing stopping you from taking some online courses. Microsoft has a [free course](https://www.edx.org/course/essential-math-for-machine-learning-python-edition-2) that I've heard good things about, and requires just high school-level maths. This [Quora thread](https://www.quora.com/How-do-I-learn-mathematics-for-machine-learning) also has some great resources that you should check out. [3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw/playlists) is a famous name in the community too, as his Youtube videos are fantastic for learning the maths (primarily linear algebra and calculus) behind some of the more complicated concepts.

## Conclusion
I've heard far too many people tell me that learning Machine Learning and Artificial Intelligence is too much of a stretch for a high schooler to not write this - with a well-paved learning path, it can be studied by anyone. And with that, I wish everyone the best of luck in undertaking this learning path. 

If you have additions or possible improvements to this guide, feel free to make a PR to this repository. And for feedback, collaborations or just general queries, feel free to write to me @ [kj.jaisingh@gmail.com](mailto:kj.jaisingh@gmail.com).

<h2>Contributors</h2>

<a href=""https://github.com/kjaisingh/ML-for-High-Schoolers/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=kjaisingh/ML-for-High-Schoolers"" />
</a>

<br>
","['kjaisingh', 'Sam-SSTU', 'mdvsh', 'xiaolai', 'Xue-Alex']",0,0.57,0,,,,,,47,,,,
37458505,MDEwOlJlcG9zaXRvcnkzNzQ1ODUwNQ==,awesome-random-forest,kjw0612/awesome-random-forest,0,kjw0612,https://github.com/kjw0612/awesome-random-forest,Random Forest - a curated list of resources regarding random forest,0,2015-06-15 10:23:25+00:00,2025-03-01 06:14:55+00:00,2023-11-16 18:51:31+00:00,http://jiwonkim.org/awesome-random-forest,484,1190,1190,,1,1,1,1,1,0,336,0,0,3,,1,0,0,public,336,3,1190,master,1,,"# Awesome Random Forest

Random Forest - a curated list of resources regarding tree-based methods and more, including but not limited to random forest, bagging and boosting.

## Contributing
Please feel free to [pull requests](https://github.com/kjw0612/awesome-random-forest/pulls).

The project is not actively maintained.

[![Join the chat at https://gitter.im/kjw0612/awesome-random-forest](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/kjw0612/awesome-random-forest?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

![randomforest](https://31.media.tumblr.com/79670eabe93cdd448c15f5bcb198d0fb/tumblr_inline_n8e398YbKv1s04rc3.png)

## Table of Contents

 - [Codes] (#codes)
 - [Theory](#theory)
   - [Lectures](#lectures)
   - [Books](#books)
   - [Papers] (#papers)
     - [Analysis / Understanding] (#analysis--understanding)
     - [Model variants] (#model-variants)
   - [Thesis] (#thesis)
 - [Applications] (#applications)
   - [Image Classification] (#image-classification)
   - [Object Detection] (#object-detection)
   - [Object Tracking] (#object-tracking)
   - [Edge Detection] (#edge-detection)
   - [Semantic Segmentation] (#semantic-segmentation)
   - [Human / Hand Pose Estimation] (#human--hand-pose-estimation)
   - [3D Localization] (#3d-localization)
   - [Low-Level Vision] (#low-level-vision)
   - [Facial Expression Recognition] (#facial-expression-recognition)
   - [Interpretability, regularization, compression pruning and feature selection](#Interpretability, regularization, compression pruning and feature selection)
   

## Codes
* Matlab
  * [Piotr Dollar's toolbox] (http://vision.ucsd.edu/~pdollar/toolbox/doc/)
  * [Andrej Karpathy's toolbox] (https://github.com/karpathy/Random-Forest-Matlab)
  * [M5PrimeLab by Gints Jekabsons] (http://www.cs.rtu.lv/jekabsons/regression.html)
* R
  * [Breiman and Cutler's random forests] (http://cran.r-project.org/web/packages/randomForest/)
  * [Hothorn et al.'s party package with `cforest` function](http://cran.r-project.org/web/packages/party/)
* C/C++
  * [Sherwood library] (http://research.microsoft.com/en-us/downloads/52d5b9c3-a638-42a1-94a5-d549e2251728/)
  * [Regression tree package by Pierre Geurts] (http://www.montefiore.ulg.ac.be/~geurts/Software.html)
  * [ranger: A Fast Implementation of Random Forests] (https://github.com/imbs-hl/ranger)
* Python
  * [Scikit-learn] (http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)
* JavaScript
  * [Forestjs] (https://github.com/karpathy/forestjs)
* Go (golang)
  * [CloudForest] (https://github.com/ryanbressler/CloudForest)
   
## Theory
### Lectures
* [ICCV 2013 Tutorial : Decision Forests and Fields for Computer Vision] (http://research.microsoft.com/en-us/um/cambridge/projects/iccv2013tutorial/) by Jamie Shotton and Sebastian Nowozin
  * [Lecture 1] (http://techtalks.tv/talks/randomized-decision-forests-and-their-applications-in-computer-vision-jamie/59432/) : Randomized Decision Forests and their Applications in Computer Vision I (Decision Forest, Classification Forest, 
  * [Lecture 2] (http://techtalks.tv/talks/decision-jungles-jamie-second-half-of-above/59434/) : Randomized Decision Forests and their Applications in Computer Vision II (Regression Forest, Decision Jungle)
  * [Lecture 3] (http://techtalks.tv/talks/entropy-estimation-and-streaming-data-sebastian/59433/) : Entropy estimation and streaming data
  * [Lecture 4] (http://techtalks.tv/talks/decision-and-regression-tree-fields-sebastian/59435/) : Decision and Regression Tree Fields
* [UBC Machine Learning] (http://www.cs.ubc.ca/~nando/540-2013/lectures.html) by Nando de Freitas
  * [Lecture 8 slide] (http://www.cs.ubc.ca/~nando/540-2013/lectures/l8.pdf) , [Lecture 8 video] (https://www.youtube.com/watch?v=-dCtJjlEEgM&list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6&index=11) : Decision trees
  * [Lecture 9 slide] (http://www.cs.ubc.ca/~nando/540-2013/lectures/l9.pdf) , [Lecture 9 video] (https://www.youtube.com/watch?v=3kYujfDgmNk&list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6&index=12) : Random forests
  * [Lecture 10 video] (https://www.youtube.com/watch?v=zFGPjRPwyFw&index=13&list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6) : Random forest applications
  
### Books
* Antonio Criminisi, Jamie Shotton (2013)
  * [Decision Forests for Computer Vision and Medical Image Analysis] (http://link.springer.com/book/10.1007%2F978-1-4471-4929-3)
* Trevor Hastie, Robert Tibshirani, Jerome Friedman (2008)
  * [The Elements of Statistical Learning, (Chapter 10, 15, and 16)] (http://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf)
* Luc Devroye, Laszlo Gyorfi, Gabor Lugosi (1996) 
  * [A Probabilistic Theory of Pattern Recognition (Chapter 20, 21)](http://www.szit.bme.hu/~gyorfi/pbook.pdf)
  
### Papers
#### Analysis / Understanding
* Consistency of random forests [[Paper]](http://www.normalesup.org/~scornet/paper/article.pdf) 
 * Scornet, E., Biau, G. and Vert, J.-P. (2015). Consistency of random forests, The Annals of Statistics, in press. 
* On the asymptotics of random forests [[Paper]](http://arxiv.org/abs/1409.2090)
 * Scornet, E. (2015). On the asymptotics of random forests, Journal of Multivariate Analysis, in press.
* Random Forests In Theory and In Practice [[Paper] (http://jmlr.org/proceedings/papers/v32/denil14.pdf)]
  * Misha Denil, David Matheson, Nando de Freitas, Narrowing the Gap: Random Forests In Theory and In Practice, ICML 2014
* Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers Abraham J. Wyner, Matthew Olson, Justin Bleich, David Mease [[Paper](https://arxiv.org/abs/1504.07676)]

  
#### Model variants
* Deep Neural Decision Forests [[Paper](https://www.microsoft.com/en-us/research/publication/deep-neural-decision-forests/)]
  * Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulo, Deep Neural Decision Forests, ICCV 2015
* Canonical Correlation Forests [[Paper](http://arxiv.org/pdf/1507.05444.pdf)]
  * Tom Rainforth, and Frank Wood, Canonical Correlation Forests, arxiv 2015
* Relating Cascaded Random Forests to Deep Convolutional Neural Networks [[Paper] (http://arxiv.org/pdf/1507.07583.pdf)]
  * David L Richmond, Dagmar Kainmueller, Michael Y Yang, Eugene W Myers, and Carsten Rother, Relating Cascaded Random Forests to Deep Convolutional Neural Networks for Semantic Segmentation, arxiv 2015
* Bayesian Forests [[Paper] (http://jmlr.org/proceedings/papers/v37/matthew15.pdf)]
  * Taddy Matthew, Chun-Sheng Chen, Jun Yu, Mitch Wyle, Bayesian and Empirical Bayesian Forests, ICML 2015
* Mondrian Forests: Efficient Online Random Forests [[Paper]](http://www.gatsby.ucl.ac.uk/~balaji/mondrian_forests_nips14.pdf) [[Code]](http://www.gatsby.ucl.ac.uk/~balaji/mondrianforest/) [[Slides]](http://www.gatsby.ucl.ac.uk/~balaji/mondrian_forests_slides.pdf)
  * Balaji Lakshminarayanan, Daniel M. Roy and Yee Whye Teh, Mondrian Forests: Efficient Online Random Forests, NIPS 2014
* Extremely randomized trees P Geurts, D Ernst, L Wehenkel - Machine learning, 2006 [[Paper](http://orbi.ulg.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf)] [[Code] (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)]
* Decision Jungles [[Paper] (http://research.microsoft.com/pubs/205439/DecisionJunglesNIPS2013.pdf)]
  * Jamie Shotton, Toby Sharp, Pushmeet Kohli, Sebastian Nowozin, John Winn, and Antonio Criminisi, Decision Jungles: Compact and Rich Models for Classification, NIPS 2013
  * Laptev, Dmitry, and Joachim M. Buhmann. Transformation-invariant convolutional jungles. CVPR 2015. [[Paper](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Laptev_Transformation-Invariant_Convolutional_Jungles_2015_CVPR_paper.pdf)]
* Semi-supervised Node Splitting for Random Forest Construction [[Paper] (http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Liu_Semi-supervised_Node_Splitting_2013_CVPR_paper.pdf)]
  * Xiao Liu, Mingli Song, Dacheng Tao, Zicheng Liu, Luming Zhang, Chun Chen and Jiajun Bu, Semi-supervised Node Splitting for Random Forest Construction, CVPR 2013
* Improved Information Gain Estimates for Decision Tree Induction [[Paper] (http://www.nowozin.net/sebastian/papers/nowozin2012infogain.pdf)]
  * Sebastian Nowozin, Improved Information Gain Estimates for Decision Tree Induction, ICML 2012
* MIForests: Multiple-Instance Learning with Randomized Trees [[Paper] (http://lrs.icg.tugraz.at/pubs/leistner_eccv_10.pdf)] [[Code] (http://www.ymer.org/amir/software/milforests/)]
  * Christian Leistner, Amir Saffari, and Horst Bischof, MIForests: Multiple-Instance Learning with Randomized Trees, ECCV 2010
* Samuel Schulter, Paul Wohlhart, Christian Leistner, Amir Saffari, Peter M. Roth, Horst Bischof: Alternating Decision Forests. CVPR 2013 [Paper](http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Schulter_Alternating_Decision_Forests_2013_CVPR_paper.pdf)
* Decision Forests, Convolutional Networks and the Models in-Between [[Paper](https://arxiv.org/abs/1603.01250)]
* Random Uniform Forests Saïp Ciss [[Paper](https://hal.archives-ouvertes.fr/hal-01104340/)] [[Code R](https://cran.r-project.org/web/packages/randomUniformForest/index.html)]
* Autoencoder Trees, Ozan İrsoy, Ethem Alpaydın 2015 [[Paper](http://www.jmlr.org/proceedings/papers/v45/Irsoy15.pdf)] 
 

## Thesis
* Understanding Random Forests
 * PhD dissertation, Gilles Louppe, July 2014. Defended on October 9, 2014. 
 * [[Repository]](https://github.com/glouppe/phd-thesis) with thesis and related codes

 
## Applications

### Image classification
* ETH Zurich [[Paper-CVPR15] (http://www.iai.uni-bonn.de/~gall/download/jgall_coarse2fine_cvpr15.pdf)]
			 [[Paper-CVPR14] (http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Ristin_Incremental_Learning_of_2014_CVPR_paper.pdf)]
			 [[Paper-ECCV] (http://www.vision.ee.ethz.ch/~lbossard/bossard_eccv14_food-101.pdf)]
  * Marko Ristin, Juergen Gall, Matthieu Guillaumin, and Luc Van Gool, From Categories to Subcategories: Large-scale Image Classification with Partial Class Label Refinement, CVPR 2015
  * Marko Ristin, Matthieu Guillaumin, Juergen Gall, and Luc Van Gool, Incremental Learning of NCM Forests for Large-Scale Image Classification, CVPR 2014
  * Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool, Food-101 – Mining Discriminative Components with Random Forests, ECCV 2014
* University of Girona & University of Oxford [[Paper] (http://www.cs.huji.ac.il/~daphna/course/CoursePapers/bosch07a.pdf)]
  * Anna Bosch, Andrew Zisserman, and Xavier Munoz, Image Classification using Random Forests and Ferns, ICCV 2007

### Object Detection
* Graz University of Technology [[Paper-CVPR] (http://lrs.icg.tugraz.at/pubs/schulter_cvpr_14.pdf)] [[Paper-ICCV] (http://lrs.icg.tugraz.at/pubs/schulter_iccv_13.pdf)]
  * Samuel Schulter, Christian Leistner, Paul Wohlhart, Peter M. Roth, and Horst Bischof, Accurate Object Detection with Joint Classification-Regression Random Forests, CVPR 2014
  * Samuel Schulter, Christian Leistner, Paul Wohlhart, Peter M. Roth, and Horst Bischof, Alternating Regression Forests for Object Detection and Pose Estimation, ICCV 2013
* ETH Zurich + Microsoft Research Cambridge [[Paper] (http://www.iai.uni-bonn.de/~gall/download/jgall_houghforest_cvpr09.pdf)]
  * Juergen Gall, and Victor Lempitsky, Class-Specific Hough Forests for Object Detection, CVPR 2009

### Object Tracking
* Technische Universitat Munchen [[Paper] (http://campar.in.tum.de/pub/tanda2014cvpr/tanda2014cvpr.pdf)]
  * David Joseph Tan, and Slobodan Ilic, Multi-Forest Tracker: A Chameleon in Tracking, CVPR 2014
* ETH Zurich + Leibniz University Hannover + Stanford University [[Paper] (http://www.igp.ethz.ch/photogrammetry/publications/pdf_folder/LeaFenKuzRosSavCVPR14.pdf)]
  * Laura Leal-Taixe, Michele Fenzi, Alina Kuznetsova, Bodo Rosenhahn, and Silvio Savarese, Learning an image-based motion context for multiple people tracking, CVPR 2014
* Graz University of Technology [[Paper] (https://lrs.icg.tugraz.at/pubs/godec_iccv_11.pdf)]
  * Martin Godec, Peter M. Roth, and Horst Bischof, Hough-based Tracking of Non-Rigid Objects, ICCV 2011

### Edge Detection
* University of California, Irvine [[Paper] (http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Hallman_Oriented_Edge_Forests_2015_CVPR_paper.pdf)] [[Code] (https://github.com/samhallman/oef)]
  * Sam Hallman, and Charless C. Fowlkes, Oriented Edge Forests for Boundary Detection, CVPR 2015
* Microsoft Research [[Paper] (http://research-srv.microsoft.com/pubs/202540/DollarICCV13edges.pdf)] [[Code] (https://github.com/pdollar/edges)]
  * Piotr Dollar, and C. Lawrence Zitnick, Structured Forests for Fast Edge Detection, ICCV 2013
* Massachusetts Inst. of Technology + Microsoft Research [[Paper] (http://research.microsoft.com/en-us/um/people/larryz/cvpr13sketchtokens.pdf)] [[Code] (https://github.com/joelimlimit/SketchTokens)]
  * Joseph J. Lim, C. Lawrence Zitnick, and Piotr Dollar, Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection, CVPR 2013

### Semantic Segmentation
* Fondazione Bruno Kessler, Microsoft Research Cambridge [[Paper] (http://www.dsi.unive.it/~srotabul/files/publications/CVPR2014a.pdf)]
  * Samuel Rota Bulo, and Peter Kontschieder, Neural Decision Forests for Semantic Image Labelling, CVPR 2014
* INRIA +  Microsoft Research Cambridge [[Paper] (http://step.polymtl.ca/~rv101/MICCAI-Laplacian-Forest.pdf)]
  * Herve Lombaert, Darko Zikic, Antonio Criminisi, and Nicholas Ayache, Laplacian Forests:Semantic Image Segmentation by Guided Bagging, MICCAI 2014
* Microsoft Research Cambridge +  GE Global Research Center + University of California +  Rutgers Univeristy [[Paper] (http://research.microsoft.com/pubs/146430/criminisi_ipmi_2011c.pdf)]
  * Albert Montillo1, Jamie Shotton, John Winn, Juan Eugenio Iglesias, Dimitri Metaxas, and Antonio Criminisi, Entangled Decision Forests and their Application for Semantic Segmentation of CT Images, IPMI 2011
* University of Cambridge + Toshiba Corporate R&D Center [[Paper] (http://mi.eng.cam.ac.uk/~cipolla/publications/inproceedings/2008-CVPR-semantic-texton-forests.pdf)]
  * Jamie Shotton, Matthew Johnson, and Roberto Cipolla, Semantic Texton Forests for Image Categorization and Segmentation, CVPR 2008
  
### Human / Hand Pose Estimation
* Microsoft Research Cambridge [[Paper-CHI] (http://research.microsoft.com/pubs/238453/pn362-sharp.pdf)][[Video-CHI] (http://research.microsoft.com/pubs/238453/pn362-sharp-video.mp4)]
                               [[Paper-CVPR] (http://research.microsoft.com/pubs/162510/vm.pdf)]
  * Toby Sharp, Cem Keskin, Duncan Robertson, Jonathan Taylor, Jamie Shotton, David Kim, Christoph Rhemann, Ido Leichter, Alon Vinnikov, Yichen Wei, Daniel Freedman, Pushmeet Kohli, Eyal Krupka, Andrew Fitzgibbon, and Shahram Izadi, Accurate, Robust, and Flexible Real-time Hand Tracking, CHI 2015
  * Jonathan Taylor, Jamie Shotton, Toby Sharp, and Andrew Fitzgibbon, The Vitruvian Manifold:Inferring Dense Correspondences for One-Shot Human Pose Estimation, CVPR 2012
* Microsoft Research Haifa [[Paper] (http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Krupka_Discriminative_Ferns_Ensemble_2014_CVPR_paper.pdf)]
  * Eyal Krupka, Alon Vinnikov, Ben Klein, Aharon Bar Hillel, and Daniel Freedman, Discriminative Ferns Ensemble for Hand Pose Recognition, CVPR 2014
* Microsoft Research Asia [[Paper] (http://research.microsoft.com/en-us/people/yichenw/cvpr14_facealignment.pdf)]
  * Shaoqing Ren, Xudong Cao, Yichen Wei, and Jian Sun, Face Alignment at 3000 FPS via Regressing Local Binary Features, CVPR 2014
* Imperial College London [[Paper-CVPR-Face] (http://www.iis.ee.ic.ac.uk/icvl/doc/cvpr14_xiaowei.pdf)]
                          [[Paper-CVPR-Hand] (http://www.iis.ee.ic.ac.uk/icvl/doc/cvpr14_danny.pdf)]
						  [[Paper-ICCV] (http://www.iis.ee.ic.ac.uk/icvl/doc/ICCV13_danny.pdf)]
  * Xiaowei Zhao, Tae-Kyun Kim, and Wenhan Luo, Unified Face Analysis by Iterative Multi-Output Random Forests, CVPR 2014
  * Danhang Tang, Hyung Jin Chang, Alykhan Tejani, and Tae-Kyun Kim, Latent Regression Forest: Structured Estimation of 3D Articulated Hand Posture, CVPR 2014
  * Danhang Tang, Tsz-Ho Yu, and Tae-Kyun Kim, Real-time Articulated Hand Pose Estimation using Semi-supervised Transductive Regression Forests, ICCV 2013
* ETH Zurich + Microsoft [[Paper] (https://lirias.kuleuven.be/bitstream/123456789/398648/2/3601_open+access.pdf)]
  * Matthias Dantone, Juergen Gall, Christian Leistner, and Luc Van Gool, Human Pose Estimation using Body Parts Dependent Joint Regressors, CVPR 2013
  
### 3D localization 
* Imperial College London [[Paper] (http://www.iis.ee.ic.ac.uk/icvl/doc/ECCV2014_aly.pdf)]
  * Alykhan Tejani, Danhang Tang, Rigas Kouskouridas, and Tae-Kyun Kim, Latent-Class Hough Forests for 3D Object Detection and Pose Estimation, ECCV 2014
* Microsoft Research Cambridge + University of Illinois + Imperial College London [[Paper] (http://abnerguzman.com/publications/gkgssfi_cvpr14.pdf)]
  * Abner Guzman-Rivera, Pushmeet Kohli, Ben Glocker, Jamie Shotton, Toby Sharp, Andrew Fitzgibbon, and Shahram Izadi, Multi-Output Learning for Camera Relocalization, CVPR 2014
* Microsoft Research Cambridge [[Paper] (http://research.microsoft.com/pubs/184826/relocforests.pdf)]
  * Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon, Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images, CVPR 2013

### Low-Level vision
* Super-Resolution
  * Technicolor R&I Hannover [[Paper](https://technicolor-my.sharepoint.com/personal/jordi_salvador_technicolor_com/_layouts/15/guestaccess.aspx?guestaccesstoken=2z88Le9arMQ7tcGGYApHmdM9Pet2AqqoxMBDcu6eRbc%3d&docid=0e7f0b9ed1d0f4497829ae6b2b0deeec3)]
    * Jordi Salvador, and Eduardo Pérez-Pellitero, Naive Bayes Super-Resolution Forest, ICCV 2015
  * Graz University of Technology [[Paper] (http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schulter_Fast_and_Accurate_2015_CVPR_paper.pdf)]
    * Samuel Schulter, Christian Leistner, and Horst Bischof, Fast and Accurate Image Upscaling with Super-Resolution Forests, CVPR 2015
* Denoising 
  * Microsoft Research + iCub Facility - Istituto Italiano di Tecnologia [[Paper] (http://research.microsoft.com/pubs/217099/CVPR2014ForestFiltering.pdf)]
    * Sean Ryan Fanello, Cem Keskin, Pushmeet Kohli, Shahram Izadi, Jamie Shotton, Antonio Criminisi, Ugo Pattacini, and Tim Paek, Filter Forests for Learning Data-Dependent Convolutional Kernels, CVPR 2014

### Facial expression recognition
* Sorbonne Universites [[Paper](http://www.isir.upmc.fr/files/2015ACTI3549.pdf)]
  * Arnaud Dapogny, Kevin Bailly, and Severine Dubuisson, Pairwise Conditional Random Forests for Facial Expression Recognition, ICCV 2015
  
### Interpretability, regularization, compression pruning and feature selection
* Global Refinement of Random Forest [[Paper] (http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ren_Global_Refinement_of_2015_CVPR_paper.pdf)]
  * Shaoqing Ren, Xudong Cao, Yichen Wei, Jian Sun, Global Refinement of Random Forest, CVPR 2015
* L1-based compression of random forest models Arnaud Joly, Fran¸cois Schnitzler, Pierre Geurts and Louis Wehenkel ESANN 2012 [[Paper](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2012-43.pdf)]
* Feature-Budgeted Random Forest [[Paper] (http://jmlr.org/proceedings/papers/v37/nan15.pdf)] [[Supp](http://jmlr.org/proceedings/papers/v37/nan15-supp.pdf)]
  * Feng Nan, Joseph Wang, Venkatesh Saligrama, Feature-Budgeted Random Forest, ICML 2015 
  * Pruning Random Forests for Prediction on a Budget Feng Nan, Joseph Wang, Venkatesh Saligrama NIPS 2016 [[Paper](https://papers.nips.cc/paper/6250-pruning-random-forests-for-prediction-on-a-budget.pdf)]
* Meinshausen, Nicolai. ""Node harvest."" The Annals of Applied Statistics 4.4 (2010): 2049-2072. [[Paper](http://projecteuclid.org/download/pdfview_1/euclid.aoas/1294167809)] [[Code R](https://cran.r-project.org/web/packages/nodeHarvest/index.html)] [[Code Python](https://github.com/mbillingr/NodeHarvest)]
* Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach S. Hara, K. Hayashi, [[Paper](https://arxiv.org/abs/1606.09066)] [[Code](https://github.com/sato9hara/defragTrees)]
* Cui, Zhicheng, et al. ""Optimal action extraction for random forests and boosted trees."" ACM SIGKDD 2015. [[Paper](http://www.cse.wustl.edu/~ychen/public/OAE.pdf)]
* DART: Dropouts meet Multiple Additive Regression Trees K. V. Rashmi, Ran Gilad-Bachrach [[Paper](http://www.jmlr.org/proceedings/papers/v38/korlakaivinayak15.pdf)]
* Begon, Jean-Michel, Arnaud Joly, and Pierre Geurts. Joint learning and pruning of decision forests. (2016). [[Paper](http://orbi.ulg.ac.be/bitstream/2268/202344/1/Begon_jlpdf_abstract.pdf)]	



Maintainers - [Jiwon Kim](http://github.com/kjw0612), [Jung Kwon Lee](http://github.com/deruci)
","['kjw0612', 'deruci', 'beedotkiran', 'andland', 'samhallman', 'gitter-badger', 'wujian752']",0,0.56,0,,,,,,115,,,,
466980236,R_kgDOG9WNjA,audio-dataset,LAION-AI/audio-dataset,0,LAION-AI,https://github.com/LAION-AI/audio-dataset,Audio Dataset for training CLAP and other models,0,2022-03-07 07:03:12+00:00,2025-03-02 20:17:58+00:00,2024-02-05 21:08:13+00:00,,86188,668,668,Python,1,1,1,1,0,0,56,0,0,39,,1,0,0,public,56,39,668,main,1,1,,"['YuchenHui22314', 'lukewys', 'marianna13', 'knoriy', 'tianyu-z', 'christophschuhmann', 'turian', 'kjhenner', 'krishnakalyan3', 'TJ-Solergibert', 'dmarx', 'isaac0804', 'RetroCirce', 'rvencu']",1,0.66,0,,,,,,20,,,,
134344489,MDEwOlJlcG9zaXRvcnkxMzQzNDQ0ODk=,linux-network-performance-parameters,leandromoreira/linux-network-performance-parameters,0,leandromoreira,https://github.com/leandromoreira/linux-network-performance-parameters,Learn where some of the network sysctl variables fit into the Linux/Kernel network flow. Translations: 🇷🇺,0,2018-05-22 01:35:26+00:00,2025-03-07 18:04:06+00:00,2025-02-23 19:33:25+00:00,https://github.com/leandromoreira/linux-network-performance-parameters,223,5618,5618,,1,1,1,1,0,0,523,0,0,2,bsd-3-clause,1,0,0,public,523,2,5618,master,1,,"[🇷🇺](/README_RU.md ""Russian"")

# TOC

* [Introduction](#introduction)
* [Linux network queues overview](#linux-network-queues-overview)
* [Fitting the sysctl variables into the Linux network flow](#fitting-the-sysctl-variables-into-the-linux-network-flow)
  * Ingress - they're coming
  * Egress - they're leaving
  * How to check - perf
* [What, Why and How - network and sysctl parameters](#what-why-and-how---network-and-sysctl-parameters)
  * Ring Buffer - rx,tx
  * Interrupt Coalescence (IC) - rx-usecs, tx-usecs, rx-frames, tx-frames (hardware IRQ)
  * Interrupt Coalescing (soft IRQ) and Ingress QDisc
  * Egress QDisc - txqueuelen and default_qdisc
  * TCP Read and Write Buffers/Queues
  * Honorable mentions - TCP FSM and congestion algorithm
* [Network tools](#network-tools-for-testing-and-monitoring)
* [References](#references)

# Introduction

Sometimes people are looking for [sysctl](https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt) cargo cult values that bring high throughput and low latency with no trade-offs and work in every situation. That's not realistic, although we can say that the **newer kernel versions are very well tuned by default**. In fact, you might [hurt performance if you mess with the defaults](https://medium.com/@duhroach/the-bandwidth-delay-problem-c6a2a578b211).

This brief tutorial shows **where some of the most used and quoted sysctl/network parameters are located into the Linux network flow**, it was heavily inspired by [the illustrated guide to the Linux networking stack](https://blog.packagecloud.io/eng/2016/10/11/monitoring-tuning-linux-networking-stack-receiving-data-illustrated/) and many of [Marek Majkowski's posts](https://blog.cloudflare.com/how-to-achieve-low-latency/). 

> #### Feel free to send corrections and suggestions! :)

# Linux network queues overview

![linux network queues](/img/linux_network_flow.png ""A graphic representation of linux/kernel network main buffer / queues"")

# Fitting the sysctl variables into the Linux network flow

## Ingress - they're coming
1. Packets arrive at the NIC
2. NIC will verify `MAC` (if not on promiscuous mode) and `FCS` and decide to drop or to continue
3. NIC will [DMA packets to RAM](https://en.wikipedia.org/wiki/Direct_memory_access), in a region previously prepared (mapped) by the driver
4. NIC will enqueue references to the packets at receive [ring buffer](https://en.wikipedia.org/wiki/Circular_buffer) queue `rx` until `rx-usecs` timeout or `rx-frames`
5. NIC will raise a `hard IRQ`
6. CPU will run the `IRQ handler` that runs the driver's code
7. 7. The driver will `schedule NAPI`, clear the `hard IRQ`, and return
8. The driver raises a `soft IRQ (NET_RX_SOFTIRQ)`
9. NAPI will poll data from the receive ring buffer until `netdev_budget_usecs` timeout or `netdev_budget` and `dev_weight` packets
10. Linux will also allocate memory to `sk_buff`
11. Linux fills the metadata: protocol, interface, setmacheader, removes ethernet
12. Linux will pass the skb to the kernel stack (`netif_receive_skb`)
13. It will set the network header, clone `skb` to taps (i.e. tcpdump) and pass it to tc ingress
14. Packets are handled to a qdisc sized `netdev_max_backlog` with its algorithm defined by `default_qdisc`
15. It calls `ip_rcv` and packets are handed to IP
16. It calls netfilter (`PREROUTING`)
17. It looks at the routing table, if forwarding or local
18. If it's local it calls netfilter (`LOCAL_IN`)
19. It calls the L4 protocol (for instance `tcp_v4_rcv`)
20. It finds the right socket
21. It goes to the tcp finite state machine
22. Enqueue the packet to  the receive buffer and sized as `tcp_rmem` rules
    1. If `tcp_moderate_rcvbuf` is enabled, the kernel will auto-tune the receive buffer
23. Kernel will signalize that there is data available to apps (epoll or any polling system)
24. Application wakes up and reads the data

## Egress - they're leaving
1. Application sends message (`sendmsg` or other)
2. TCP send message allocates skb_buff
3. It enqueues skb to the socket write buffer of `tcp_wmem` size
4. Builds the TCP header (src and dst port, checksum)
5. Calls L3 handler (in this case `ipv4` on `tcp_write_xmit` and `tcp_transmit_skb`)
6. L3 (`ip_queue_xmit`) does its work: build ip header and call netfilter (`LOCAL_OUT`)
7. Calls output route action
8. Calls netfilter (`POST_ROUTING`)
9. Fragment the packet (`ip_output`)
10. Calls L2 send function (`dev_queue_xmit`)
11. Feeds the output (QDisc) queue of `txqueuelen` length using its default_qdisc algorithm.
12. The driver code enqueue the packets at the `ring buffer tx`
13. The driver will do a `soft IRQ (NET_TX_SOFTIRQ)` after `tx-usecs` timeout or `tx-frames`
14. Re-enable hard IRQ to NIC
15. Driver will map all the packets (to be sent) to some DMA'ed region
16. NIC fetches the packets (via DMA) from RAM to transmit
17. After the transmission NIC will raise a `hard IRQ` to signal its completion
18. The driver will handle this IRQ (turn it off)
19. And schedule (`soft IRQ`) the NAPI poll system 
20. NAPI will handle the receive packets signaling and free the RAM

## How to check - perf

If you want to see the network tracing within Linux you can use [perf](https://man7.org/linux/man-pages/man1/perf-trace.1.html).

```
docker run -it --rm --cap-add SYS_ADMIN --entrypoint bash ljishen/perf
apt-get update
apt-get install iputils-ping

# this is going to trace all events (not syscalls) to the subsystem net:* while performing the ping
perf trace --no-syscalls --event 'net:*' ping globo.com -c1 > /dev/null
```
![perf trace network](https://user-images.githubusercontent.com/55913/147019725-69624e67-b3ca-48b4-a823-10521d2bed83.png)


# What, Why and How - network and sysctl parameters

## Ring Buffer - rx,tx
* **What** - the driver receive/send queue a single or multiple queues with a fixed size, usually implemented as FIFO, it is located at RAM
* **Why** - buffer to smoothly accept bursts of connections without dropping them, you might need to increase these queues when you see drops or overrun, aka there are more packets coming than the kernel is able to consume them, the side effect might be increased latency.
* **How:**
  * **Check command:** `ethtool -g ethX`
  * **Change command:** `ethtool -G ethX rx value tx value`
  * **How to monitor:** `ethtool -S ethX | grep -e ""err"" -e ""drop"" -e ""over"" -e ""miss"" -e ""timeout"" -e ""reset"" -e ""restar"" -e ""collis"" -e ""over"" | grep -v ""\: 0""`
 
## Interrupt Coalescence (IC) - rx-usecs, tx-usecs, rx-frames, tx-frames (hardware IRQ)
* **What** - number of microseconds/frames to wait before raising a hardIRQ, from the NIC perspective it'll DMA data packets until this timeout/number of frames
* **Why** - reduce CPUs usage, hard IRQ, might increase throughput at cost of latency.
* **How:**
  * **Check command:** `ethtool -c ethX`
  * **Change command:** `ethtool -C ethX rx-usecs value tx-usecs value`
  * **How to monitor:** `cat /proc/interrupts` 
  
## Interrupt Coalescing (soft IRQ) and Ingress QDisc
* **What** - maximum number of microseconds in one [NAPI](https://en.wikipedia.org/wiki/New_API) polling cycle. Polling will exit when either `netdev_budget_usecs` have elapsed during the poll cycle or the number of packets processed reaches  `netdev_budget`.
* **Why** - instead of reacting to tons of softIRQ, the driver keeps polling data; keep an eye on `dropped` (# of packets that were dropped because `netdev_max_backlog` was exceeded) and `squeezed` (# of times ksoftirq ran out of `netdev_budget` or time slice with work remaining).
* **How:**
  * **Check command:** `sysctl net.core.netdev_budget_usecs`
  * **Change command:** `sysctl -w net.core.netdev_budget_usecs value`
  * **How to monitor:** `cat /proc/net/softnet_stat`; or a [better tool](https://raw.githubusercontent.com/majek/dump/master/how-to-receive-a-packet/softnet.sh)
* **What** - `netdev_budget` is the maximum number of packets taken from all interfaces in one polling cycle (NAPI poll). In one polling cycle interfaces which are registered to polling are probed in a round-robin manner. Also, a polling cycle may not exceed `netdev_budget_usecs` microseconds, even if `netdev_budget` has not been exhausted.
* **How:**
  * **Check command:** `sysctl net.core.netdev_budget`
  * **Change command:** `sysctl -w net.core.netdev_budget value`
  * **How to monitor:** `cat /proc/net/softnet_stat`; or a [better tool](https://raw.githubusercontent.com/majek/dump/master/how-to-receive-a-packet/softnet.sh)
* **What** - `dev_weight` is the maximum number of packets that kernel can handle on a NAPI interrupt, it's a Per-CPU variable. For drivers that support LRO or GRO_HW, a hardware aggregated packet is counted as one packet in this.
* **How:**
  * **Check command:** `sysctl net.core.dev_weight`
  * **Change command:** `sysctl -w net.core.dev_weight value`
  * **How to monitor:** `cat /proc/net/softnet_stat`; or a [better tool](https://raw.githubusercontent.com/majek/dump/master/how-to-receive-a-packet/softnet.sh)
* **What** - `netdev_max_backlog` is the maximum number  of  packets,  queued  on  the  INPUT side (_the ingress qdisc_), when the interface receives packets faster than kernel can process them.
* **How:**
  * **Check command:** `sysctl net.core.netdev_max_backlog`
  * **Change command:** `sysctl -w net.core.netdev_max_backlog value`
  * **How to monitor:** `cat /proc/net/softnet_stat`; or a [better tool](https://raw.githubusercontent.com/majek/dump/master/how-to-receive-a-packet/softnet.sh)
  
## Egress QDisc - txqueuelen and default_qdisc
* **What** - `txqueuelen` is the maximum number of packets, queued on the OUTPUT side.
* **Why** - a buffer/queue to face connection burst and also to apply [tc (traffic control).](http://tldp.org/HOWTO/Traffic-Control-HOWTO/intro.html)
* **How:**
  * **Check command:** `ip link show dev ethX`
  * **Change command:** `ip link set dev ethX txqueuelen N`
  * **How to monitor:** `ip -s link` 
* **What** - `default_qdisc` is the default queuing discipline to use for network devices.
* **Why** - each application has different load and need to traffic control and it is used also to fight against [bufferbloat](https://www.bufferbloat.net/projects/codel/wiki/)
* **How:**
  * **Check command:** `sysctl net.core.default_qdisc`
  * **Change command:** `sysctl -w net.core.default_qdisc value`
  * **How to monitor:**   `tc -s qdisc ls dev ethX`

## TCP Read and Write Buffers/Queues

> The policy that defines what is [memory pressure](https://web.archive.org/web/20200315112330/wwwx.cs.unc.edu/~sparkst/howto/network_tuning.php) is specified at tcp_mem and tcp_moderate_rcvbuf.

* **What** - `tcp_rmem` - min (size used under memory pressure), default (initial size), max (maximum size) - size of receive buffer used by TCP sockets.
* **Why** - the application buffer/queue to the write/send data, [understand its consequences can help a lot](https://blog.cloudflare.com/the-story-of-one-latency-spike/).
* **How:**
  * **Check command:** `sysctl net.ipv4.tcp_rmem`
  * **Change command:** `sysctl -w net.ipv4.tcp_rmem=""min default max""`; when changing default value, remember to restart your user space app (i.e. your web server, nginx, etc)
  * **How to monitor:** `cat /proc/net/sockstat`
* **What** - `tcp_wmem` - min (size used under memory pressure), default (initial size), max (maximum size) - size of send buffer used by TCP sockets.
* **How:**
  * **Check command:** `sysctl net.ipv4.tcp_wmem`
  * **Change command:** `sysctl -w net.ipv4.tcp_wmem=""min default max""`; when changing default value, remember to restart your user space app (i.e. your web server, nginx, etc)
  * **How to monitor:** `cat /proc/net/sockstat`
* **What** `tcp_moderate_rcvbuf` - If set, TCP performs receive buffer auto-tuning, attempting to automatically size the buffer.
* **How:**
  * **Check command:** `sysctl net.ipv4.tcp_moderate_rcvbuf`
  * **Change command:** `sysctl -w net.ipv4.tcp_moderate_rcvbuf value`
  * **How to monitor:** `cat /proc/net/sockstat`

## Honorable mentions - TCP FSM and congestion algorithm

> Accept and SYN Queues are governed by net.core.somaxconn and net.ipv4.tcp_max_syn_backlog. [Nowadays net.core.somaxconn caps both queue sizes.](https://blog.cloudflare.com/syn-packet-handling-in-the-wild/#queuesizelimits)

* `sysctl net.core.somaxconn` - provides an upper limit on the value of the backlog parameter passed to the [`listen()` function](https://eklitzke.org/how-tcp-sockets-work), known in userspace as `SOMAXCONN`. If you change this value, you should also change your application to a compatible value (i.e. [nginx backlog](http://nginx.org/en/docs/http/ngx_http_core_module.html#listen)).
* `cat /proc/sys/net/ipv4/tcp_fin_timeout` - this specifies the number of seconds to wait for a final FIN packet before the socket is forcibly closed.  This is strictly a violation of the TCP specification but required to prevent denial-of-service attacks.
* `cat /proc/sys/net/ipv4/tcp_available_congestion_control` - shows the available congestion control choices that are registered.
* `cat /proc/sys/net/ipv4/tcp_congestion_control` - sets the congestion control algorithm to be used for new connections.
* `cat /proc/sys/net/ipv4/tcp_max_syn_backlog` - sets the maximum number of queued connection requests which have still not received an acknowledgment from the connecting client; if this number is exceeded, the kernel will begin dropping requests.
* `cat /proc/sys/net/ipv4/tcp_syncookies` - enables/disables [syn cookies](https://en.wikipedia.org/wiki/SYN_cookies), useful for protecting against [syn flood attacks](https://www.cloudflare.com/learning/ddos/syn-flood-ddos-attack/).
* `cat /proc/sys/net/ipv4/tcp_slow_start_after_idle` - enables/disables tcp slow start.

**How to monitor:** 
* `netstat -atn | awk '/tcp/ {print $6}' | sort | uniq -c` - summary by state
* `ss -neopt state time-wait | wc -l` - counters by a specific state: `established`, `syn-sent`, `syn-recv`, `fin-wait-1`, `fin-wait-2`, `time-wait`, `closed`, `close-wait`, `last-ack`, `listening`, `closing`
* `netstat -st` - tcp stats summary
* `nstat -a` - human-friendly tcp stats summary
* `cat /proc/net/sockstat` - summarized socket stats
* `cat /proc/net/tcp` - detailed stats, see each field meaning at the [kernel docs](https://www.kernel.org/doc/Documentation/networking/proc_net_tcp.txt)
* `cat /proc/net/netstat` - `ListenOverflows` and `ListenDrops` are important fields to keep an eye on
  * `cat /proc/net/netstat | awk '(f==0) { i=1; while ( i<=NF) {n[i] = $i; i++ }; f=1; next} \
(f==1){ i=2; while ( i<=NF){ printf ""%s = %d\n"", n[i], $i; i++}; f=0} ' | grep -v ""= 0`; a [human readable `/proc/net/netstat`](https://sa-chernomor.livejournal.com/9858.html)

![tcp finite state machine](https://upload.wikimedia.org/wikipedia/commons/a/a2/Tcp_state_diagram_fixed.svg ""A graphic representation of tcp tcp finite state machine"")
Source: https://commons.wikimedia.org/wiki/File:Tcp_state_diagram_fixed_new.svg

# Network tools for testing and monitoring

* [iperf3](https://iperf.fr/) - network throughput
* [vegeta](https://github.com/tsenart/vegeta) - HTTP load testing tool
* [netdata](https://github.com/firehol/netdata) - system for distributed real-time performance and health monitoring
* [prometheus](https://prometheus.io/) + [grafana](https://grafana.com/) + [node exporter full dashboard](https://grafana.com/grafana/dashboards/1860-node-exporter-full/) - monitoring stack to graph detailed system behaviour

# References

* https://www.kernel.org/doc/Documentation/sysctl/net.txt
* https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt
* https://www.kernel.org/doc/Documentation/networking/scaling.txt
* https://www.kernel.org/doc/Documentation/networking/proc_net_tcp.txt
* https://www.kernel.org/doc/Documentation/networking/multiqueue.txt
* http://man7.org/linux/man-pages/man7/tcp.7.html
* http://man7.org/linux/man-pages/man8/tc.8.html
* http://cseweb.ucsd.edu/classes/fa09/cse124/presentations/TCPlinux_implementation.pdf
* https://netdevconf.org/1.2/papers/bbr-netdev-1.2.new.new.pdf
* https://blog.cloudflare.com/how-to-receive-a-million-packets/
* https://blog.cloudflare.com/how-to-achieve-low-latency/
* https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/
* https://www.youtube.com/watch?v=6Fl1rsxk4JQ
* https://oxnz.github.io/2016/05/03/performance-tuning-networking/
* https://www.intel.com/content/dam/www/public/us/en/documents/reference-guides/xl710-x710-performance-tuning-linux-guide.pdf
* https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf
* https://medium.com/@matteocroce/linux-and-freebsd-networking-cbadcdb15ddd
* https://blogs.technet.microsoft.com/networking/2009/08/12/where-do-resets-come-from-no-the-stork-does-not-bring-them/
* https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/multi-core-processor-based-linux-paper.pdf
* http://syuu.dokukino.com/2013/05/linux-kernel-features-for-high-speed.html
* https://www.bufferbloat.net/projects/codel/wiki/Best_practices_for_benchmarking_Codel_and_FQ_Codel/
* https://software.intel.com/en-us/articles/setting-up-intel-ethernet-flow-director
* https://courses.engr.illinois.edu/cs423/sp2014/Lectures/LinuxDriver.pdf
* https://www.coverfire.com/articles/queueing-in-the-linux-network-stack/
* http://vger.kernel.org/~davem/skb.html
* https://www.missoulapubliclibrary.org/ftp/LinuxJournal/LJ13-07.pdf
* https://opensourceforu.com/2016/10/network-performance-monitoring/
* https://www.yumpu.com/en/document/view/55400902/an-adventure-of-analysis-and-optimisation-of-the-linux-networking-stack
* https://lwn.net/Articles/616241/
* https://medium.com/@duhroach/tools-to-profile-networking-performance-3141870d5233
* https://www.lmax.com/blog/staff-blogs/2016/05/06/navigating-linux-kernel-network-stack-receive-path/
* https://fasterdata.es.net/host-tuning/linux/100g-tuning/
* http://tcpipguide.com/free/t_TCPOperationalOverviewandtheTCPFiniteStateMachineF-2.htm
* http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html
* https://people.cs.clemson.edu/~westall/853/tcpperf.pdf
* http://tldp.org/HOWTO/Traffic-Control-HOWTO/classless-qdiscs.html
* https://fasterdata.es.net/assets/Papers-and-Publications/100G-Tuning-TechEx2016.tierney.pdf
* https://www.kernel.org/doc/ols/2009/ols2009-pages-169-184.pdf
* https://devcentral.f5.com/articles/the-send-buffer-in-depth-21845
* http://packetbomb.com/understanding-throughput-and-tcp-windows/
* https://www.speedguide.net/bdp.php
* https://www.switch.ch/network/tools/tcp_throughput/
* https://www.ibm.com/support/knowledgecenter/en/SSQPD3_2.6.0/com.ibm.wllm.doc/usingethtoolrates.html
* https://blog.tsunanet.net/2011/03/out-of-socket-memory.html
* https://unix.stackexchange.com/questions/12985/how-to-check-rx-ring-max-backlog-and-max-syn-backlog-size
* https://serverfault.com/questions/498245/how-to-reduce-number-of-time-wait-processes
* https://unix.stackexchange.com/questions/419518/how-to-tell-how-much-memory-tcp-buffers-are-actually-using
* https://eklitzke.org/how-tcp-sockets-work
* https://www.linux.com/learn/intro-to-linux/2017/7/introduction-ss-command
* https://staaldraad.github.io/2017/12/20/netstat-without-netstat/
* https://loicpefferkorn.net/2016/03/linux-network-metrics-why-you-should-use-nstat-instead-of-netstat/
* http://assimilationsystems.com/2015/12/29/bufferbloat-network-best-practice/
* https://wwwx.cs.unc.edu/~sparkst/howto/network_tuning.php
* https://medium.com/@tom_84912/the-alphabet-soup-of-receive-packet-steering-rss-rps-rfs-and-arfs-c84347156d68
","['leandromoreira', 'zersh01', 'danielfm', 'eranchetz', 'jpoliv', 'shaymolcho', 'shemminger', 'andrewsjg', 'phosae']",0,0.52,0,,,,,,179,,,ucsd-ccbb,UCSD-SUMS
539727112,R_kgDOICuVCA,How-to-run,lidangzzz/How-to-run,0,lidangzzz,https://github.com/lidangzzz/How-to-run,立党零基础转码笔记,0,2022-09-21 23:57:37+00:00,2025-03-07 04:57:46+00:00,2024-05-05 19:26:22+00:00,,210,5521,5521,TypeScript,1,1,1,1,1,1,351,0,0,0,gpl-3.0,1,0,0,public,351,0,5521,main,1,,,['lidangzzz'],0,0.67,0,,,,,,80,,,UCSD-PL,
272766490,MDEwOlJlcG9zaXRvcnkyNzI3NjY0OTA=,InverseRenderingOfIndoorScene,lzqsd/InverseRenderingOfIndoorScene,0,lzqsd,https://github.com/lzqsd/InverseRenderingOfIndoorScene,,0,2020-06-16 17:06:33+00:00,2025-03-07 15:58:45+00:00,2023-04-09 00:09:09+00:00,,101,306,306,Python,1,1,1,1,0,0,36,0,0,11,mit,1,0,0,public,36,11,306,master,1,,,"['lzqsd', 'manukc']",1,0.71,0,,,,,,13,,,,
76066550,MDEwOlJlcG9zaXRvcnk3NjA2NjU1MA==,awesome-autonomous-vehicles,manfreddiaz/awesome-autonomous-vehicles,0,manfreddiaz,https://github.com/manfreddiaz/awesome-autonomous-vehicles,Curated List of Self-Driving Cars and Autonomous Vehicles Resources,0,2016-12-09 20:07:33+00:00,2025-03-08 06:30:26+00:00,2024-03-15 17:19:11+00:00,https://manfreddiaz.github.io/awesome-autonomous-vehicles/,288,2231,2231,,1,1,1,1,1,0,582,0,0,1,,1,0,0,public,582,1,2231,master,1,,,"['manfreddiaz', 'garrettdowd', 'saimj7', 'nikwl', 'johnwlambert', 'tymtam2', 'XiaotaoGuo', 'szepilot']",0,0.69,0,,,,,,165,,,,
102768461,MDEwOlJlcG9zaXRvcnkxMDI3Njg0NjE=,calcflow,matryx/calcflow,0,matryx,https://github.com/matryx/calcflow,A virtual reality tool for mathematical modeling!,0,2017-09-07 17:52:06+00:00,2025-01-12 09:33:00+00:00,2020-07-21 20:17:12+00:00,,808307,497,497,C#,1,1,1,1,0,0,54,0,0,0,other,1,0,0,public,54,0,497,master,1,1,"# Calcflow

A Virtual Reality Tool for Mathematical Modeling!

![interpolations](https://github.com/matryx/matryx-alpha-source/blob/master/assets/calcflow_gif.gif?raw=true)

The repository contains the open-sourced code to Calcflow, a powerful mathematical visualization tool designed to give students, educators, and engineers a better grasp on some of the most difficult concepts in vector calculus. Utilizing the best UI/UX theory, Calcflow is an intuitive VR interface for vector calculus and is used by scientists and engineers for both educational and commercial applications. Some core features:

* Parametric graphing utility designed to map points in 2D space into 3D by defining (x,y,z) as a function of parameters u and v. Recent functionality also includes the ability to map from 1D to 3D.
* Vector field utility
* Cross product and vector addition modules rendered in real time as a user manipulates initial vectors
* Double integral grapher

## Why Calcflow?
Today, the most common tool for complex maths visualization in the classroom is the TI-84+. Scientists, researchers, and other professionals who implement calculus in their work may rely on more complex toolkits like MATLAB. Though these tools offer broader functionality, they are similarly if not more unintuitive than their handheld counterparts. 

![particles](https://user-images.githubusercontent.com/27929626/30620415-2a4d3482-9d73-11e7-98f8-e906e83205e2.gif)

Visualization plays a crucial role in understanding, mastering, and improving upon mathematical concepts, but today's standard interfaces frustrate and alienate many individuals, creating an excessively high barrier of entry to higher level math studies. Calcflow shatters this interfacial bottleneck by enabling users to interact directly with complex equations in physical space. Users can manipulate inputs and parameters and observe changes to 3D visualizations in realtime.

## Getting Started

### Prerequisites

* Programming Language: C#
* Operating System: Windows
* VR HMD: Oculus Rift or HTC Vive


## Built With

* [Unity3D](https://unity3d.com/) - The game engine platform used

## Versioning

Currently, the version is 5.6.2f1

## Authors

* **Ethan Vander Horn** - *Lead Calcflow Developer* - [EthanVanderHorn](https://github.com/EthanVanderHorn)
* **Edward Zhou** - *Lead Calcflow Developer* - [YangZ530](https://github.com/YangZ530)
* **Jacqueline Bontigao** - *Lead UI Developer* - [Calcodeus](https://github.com/calcodeus)
* **Max Howard** - *Matryx Developer* - [astrovicis](https://github.com/astrovicis)
* **Sam Hessenauer** - *Engineering Project Leader* - [shessenauer](https://github.com/shessenauer)

## License

This project is licensed under the NANOME VR PRODUCT SUITE - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* We would like to thank John Eggers, Marc Loschen, and Kyle Lee for the initial development and support as well as all the developers who have been involved in this project. We also would like to thank the students involved in UC San Diego's Summer Academy Vector Calculus course in 2016 and 2017 for providing the instrumental feedback necessary for Calcflow's development.
* We would like to honor Dr. Jeff Remmel for his contributions and advice over the years. It is sad news for everyone on the Matryx team because he helped us start Calcflow back when we first conceived of the project. He was a prolific mathematician and a passionate believer in STEM education. See this [article](http://www.sandiegouniontribune.com/news/science/sd-me-jeffrey-remmel-20171006-story.html) for what other mathematicians at UC San Diego said about him.
","['Astrovicis', 'ethandbrand', 'mathgeek31415', 'EthanVanderHorn', 'tianhengMa', 'mukmai', 'orangeisnotred', 'YangZ530', 'LostJam', 'wkjdfx', 'shessenauer', 'calcodeus', 'kylelee4c6', 'ottomanbob', 'MojoJojoee']",1,0.79,222,,,,,,39,,,,
100725132,MDEwOlJlcG9zaXRvcnkxMDA3MjUxMzI=,EEG-Datasets,meagmohit/EEG-Datasets,0,meagmohit,https://github.com/meagmohit/EEG-Datasets,A list of all public EEG-datasets,0,2017-08-18 15:21:18+00:00,2025-03-07 16:05:49+00:00,2024-08-05 19:02:17+00:00,,160,2423,2423,,1,1,1,1,0,0,560,0,0,10,,1,0,0,public,560,10,2423,master,1,,,"['meagmohit', 'lkorczowski', 'arnodelorme', 'computerscienceiscool', 'MohammadJavadD', 'PTDZ', 'cfauchereau']",0,0.73,0,,,,,,95,,,vibansal,
49697553,MDEwOlJlcG9zaXRvcnk0OTY5NzU1Mw==,DSE210_Probability_Statistics_Python,mGalarnyk/DSE210_Probability_Statistics_Python,0,mGalarnyk,https://github.com/mGalarnyk/DSE210_Probability_Statistics_Python,Probability and Statistics Using Python Data Science Masters Course at UCSD (DSE 210),0,2016-01-15 05:05:05+00:00,2024-12-13 07:00:23+00:00,2017-08-21 19:36:39+00:00,,98232,177,177,Jupyter Notebook,1,1,1,1,0,0,128,0,0,1,,1,0,0,public,128,1,177,master,1,,,"['mGalarnyk', 'mas-dse-mgalarny']",1,0.61,0,,,,,,24,,,,
54156538,MDEwOlJlcG9zaXRvcnk1NDE1NjUzOA==,Flye,mikolmogorov/Flye,0,mikolmogorov,https://github.com/mikolmogorov/Flye,De novo assembler for single molecule sequencing reads using repeat graphs,0,2016-03-17 22:47:39+00:00,2025-03-06 15:56:30+00:00,2024-08-28 18:46:15+00:00,,36080,813,813,C,1,1,1,1,0,1,167,0,0,25,other,1,0,0,public,167,25,813,flye,1,,,"['mikolmogorov', 'jeyuan', 'druvus', 'atabeerk', 'emilhaegglund', 'hiroshisuga', 'acollens', 'asan-emirsaleh', 'jackgoza', 'jvhaarst', 'hyphaltip', 'zymergen-luke', 'zwets', 'pushkarnk', 'rmzelle', 'SESDNA', 'RaverJay', 'tmassingham-ont', 'zeeev', 'matthewwiese', 'mphschmitt', 'sebschmi', 'tayabsoomro', 'emollier']",1,0.57,0,,,,,,29,,,,
869330403,R_kgDOM9Dt4w,hart,mit-han-lab/hart,0,mit-han-lab,https://github.com/mit-han-lab/hart,HART: Efficient Visual Generation with Hybrid Autoregressive Transformer,0,2024-10-08 05:50:06+00:00,2025-03-05 07:44:40+00:00,2024-10-16 05:46:35+00:00,,1383,419,419,Python,1,1,1,1,0,0,21,0,0,15,mit,1,0,0,public,21,15,419,main,1,1,,"['kentang-mit', 'eltociear']",1,0.65,0,,,,,,6,,,,
796839092,R_kgDOL37MtA,omniserve,mit-han-lab/omniserve,0,mit-han-lab,https://github.com/mit-han-lab/omniserve,[MLSys'25] QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving; [MLSys'25] LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention,0,2024-05-06 18:16:30+00:00,2025-03-07 06:14:32+00:00,2025-03-06 03:22:14+00:00,,5764,575,575,C++,1,1,1,1,0,0,35,0,0,42,apache-2.0,1,0,0,public,35,42,575,main,1,1,"# OmniServe: Unified and Efficient Inference Engine for Large-Scale LLM Serving

**[Paper (QServe)](https://arxiv.org/abs/2405.04532) | [Paper (LServe)](https://arxiv.org/abs/2502.14866) | [Website (QServe)](https://hanlab.mit.edu/projects/qserve) | [Website (LServe)](https://hanlab.mit.edu/projects/lserve)**

OmniServe aims to revolutionize large-scale LLM serving by unifying and optimizing key advancements in both low-bit quantization and long-context processing. OmniServe integrates the innovations from [QServe](https://arxiv.org/abs/2405.04532), which boosts efficiency with W4A8KV4 quantization and reduces dequantization overheads, and [LServe](https://arxiv.org/abs/2502.14866), which accelerates long-context LLM inference through unified sparse attention and hierarchical KV cache management. OmniServe delivers a comprehensive solution for scalable and cost-effective LLM deployment. This unified system addresses the dual challenges of computational complexity and memory overhead, achieving significant speedups in both prefill and decoding stages, while also maximizing GPU throughput and minimizing infrastructure costs.

## News
- [2025/02] 🔥 **OmniServe is now publicly available!** OmniServe has integrated optimizations from both QServe and LServe into one single LLM inference framework. Experience efficient and accurate inference for both [long-context](#lserve-efficient-long-sequence-llm-serving-with-unified-sparse-attention) and [quantized](#qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving) LLMs with OmniServe now!
- [2025/02] 🏆 Both **QServe and LServe have been accepted by MLSys 2025**!
- [2024/12] 🔥 QServe has been integrated into NVIDIA [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llama/README.md#w4aint8-quantization-qserve)!
- [2024/05] 🔥 QServe is publicly released! Check our paper [here](https://arxiv.org/abs/2405.04532).


## Key Features
**OmniServe** is a unified, flexible, and efficient LLM serving system designed to support modern large language models and multi-modal language models. With configurable quantization precisions and hybrid sparse attention patterns, OmniServe integrates the strengths of [QServe](#qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving) and [LServe](#lserve-efficient-long-sequence-llm-serving-with-unified-sparse-attention), enabling efficient processing of both large-batch and long-context inputs, significantly reducing LLM serving costs while maintaining high response quality.

## Contents
- [Installation](#installation)
- [OmniServe Model Zoo](#omniserve-model-zoo)

- [QServe: ***W4A8KV4*** Quantization and System Co-design for Efficient LLM Serving](#qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving)
  - [Introduction](#introduction)
  - [Usage and Examples](#usage-and-examples)
  - [Results](#results)
    - [Accuracy Evaluation](#accuracy-evaluation)
    - [Efficiency Benchmarks](#efficiency-benchmarks)


- [LServe: Efficient Long-Sequence LLM Serving with ***Unified Sparse Attention***](#lserve-efficient-long-sequence-llm-serving-with-unified-sparse-attention)
  - [Introduction](#introduction-1)
  - [Usage and Examples](#usage-and-examples-1)
  - [Results](#results-1)
    - [Accuracy Evaluation](#accuracy-evaluation-1)
    - [Efficiency Benchmarks](#efficiency-benchmarks-1)


- [Reference](#reference)
- [Team](#team)
- [Related Projects](#related-projects)
- [Acknowledgement](#acknowledgement)


## Installation

1. Clone this repository and navigate to the corresponding folder:
```bash
git clone https://github.com/mit-han-lab/OmniServe
cd OmniServe
```

2. Install OmniServe

2.1 LLM setup tutorial

If you hope to serve text-only LLMs, please follow the tutorial below:

```bash
conda create -n OmniServe python=3.10 -y
conda activate OmniServe
pip install --upgrade pip  # enable PEP 660 support

conda install -c nvidia cuda-toolkit -y  # This is optional if you prefer to use built-in nvcc

# Install OmniServe package
pip install -e .
pip install flash-attn --no-build-isolation
```

We recommend starting an interactive python CLI interface and run `import flash_attn` to check whether FlashAttention-2 is installed successfully. If not, we recommend downloading pre-built wheels from [here](https://github.com/Dao-AILab/flash-attention/releases/tag/v2.5.8). Please notice:

- PyTorch version needs to exactly match with the version specified in the `.whl` name;
- Check out both `cxx11abiTRUE` and `cxx11abiFALSE` wheels if one of them does not work;
- It's recommended to match CUDA version specified in the `.whl` filename, but minor mismatches (e.g. 12.1 vs 12.2, or even 11.8 vs 12.2) usually do not matter.

2.2 Sparse prefilling with [Block-Sparse-Attention](https://github.com/mit-han-lab/Block-Sparse-Attention).

We provide pre-built wheels for Block-Sparse-Attention [here](https://github.com/mit-han-lab/Block-Sparse-Attention/releases). Please download and install the `.whl` file with pip according to your environment. Similar to `flash_attn`, We recommend starting an interactive python CLI interface and run `import block_sparse_attn` to check the installation. Please also notice:

- PyTorch version needs to exactly match with the version specified in the `.whl` name;
- Check out both `cxx11abiTRUE` and `cxx11abiFALSE` wheels if one of them does not work;
- It's recommended to match CUDA version specified in the `.whl` filename, but minor mismatches (e.g. 12.1 vs 12.2, or even 11.8 vs 12.2) usually do not matter.

To build Block-Sparse-Attention from source, please follow the instructions below:

```bash
git clone https://github.com/mit-han-lab/Block-Sparse-Attention.git --recursive
cd Block-Sparse-Attention

pip install packaging
pip install ninja
python setup.py install
```


<!-- 2.2 [Optional] VLM setup tutorial

QServe also supports synthetic caption generation with VILA VLMs. Please follow the [tutorial](#qserve-vlm-installation) for installation details. -->


3. Compile the CUDA kernels for OmniServe.

Please return to the OmniServe directory and execute the following commands:

```bash
pip install ninja   # Install ninja if not already

cd kernels
python setup.py install
```

4. If you want to clone our model zoo, please make sure that `git-lfs` is installed.

## OmniServe Model Zoo

We provide pre-quantized checkpoints for multiple model families. For example, for Llama-3-8B model, please run the following commands to download:

```bash
# git lfs install  # install git lfs if not already
mkdir -p qserve_checkpoints && cd qserve_checkpoints
git clone https://huggingface.co/mit-han-lab/Llama-3-8B-Instruct-QServe 
```

For other models, please refer to the detailed support list for the links to download:

| Models    | W4A8-per-channel     |  W4A8-g128       |
| --------- | ---------------------- | -------------- | 
| Llama3    | ✅   [8B](https://huggingface.co/mit-han-lab/Llama-3-8B-QServe)/70B          |  ✅  [8B](https://huggingface.co/mit-han-lab/Llama-3-8B-QServe-g128)/70B|
| Llama3-Instruct   | ✅   [8B](https://huggingface.co/mit-han-lab/Llama-3-8B-Instruct-QServe)/70B          | ✅  [8B](https://huggingface.co/mit-han-lab/Llama-3-8B-Instruct-QServe-g128)/70B              |
| Llama2    |  ✅  [7B](https://huggingface.co/mit-han-lab/Llama-2-7B-QServe)/[13B](https://huggingface.co/mit-han-lab/Llama-2-13B-QServe)/70B           | ✅ [7B](https://huggingface.co/mit-han-lab/Llama-2-7B-QServe-g128)/[13B](https://huggingface.co/mit-han-lab/Llama-2-13B-QServe-g128)/70B               
| Vicuna    | ✅  [7B](https://huggingface.co/mit-han-lab/vicuna-7b-v1.5-QServe)/[13B](https://huggingface.co/mit-han-lab/vicuna-13b-v1.5-QServe)/30B           | ✅ [7B](https://huggingface.co/mit-han-lab/vicuna-7b-v1.5-QServe-g128)/[13B](https://huggingface.co/mit-han-lab/vicuna-13b-v1.5-QServe-g128)/30B               |
| Mistral   | ✅  [7B](https://huggingface.co/mit-han-lab/Mistral-7B-v0.1-QServe)           | ✅  [7B](https://huggingface.co/mit-han-lab/Mistral-7B-v0.1-QServe-g128)               |
| Yi        | ✅    [34B](https://huggingface.co/mit-han-lab/Yi-34B-QServe)         | ✅   [34B](https://huggingface.co/mit-han-lab/Yi-34B-QServe-g128)             |
| Qwen      |✅    72B         | ✅      72B          |

For flagship datacenter GPUs such as the A100, it is recommended to use QServe-per-channel, while for inference datacenter GPUs like the L40S, QServe-per-group is the recommended approach.

If you are interested in generating the quantized checkpoints on your own, please follow the instructions in [DeepCompressor Library](https://github.com/mit-han-lab/deepcompressor/tree/lmquant-v0.0.0-deprecated) to run QoQ quantization and dump the fake-quantized models. We then provide checkpoint converter to real-quantize and pack the model into QServe format:

```bash
python checkpoint_converter.py --model-path <hf-model-path> --quant-path <fake-quant-model-path> --group-size -1 --device cpu
# <fake-quant-model-path> is a directory generated by DeepCompressor, including model.pt and scale.pt
```
We also provide a [script](./scripts/ckpt_converter/convert.sh) to run the checkpoint converter. The final model will be automatically stored under `qserve_checkpoints`. 


# QServe: ***W4A8KV4*** Quantization and System Co-design for Efficient LLM Serving

**[Paper](https://arxiv.org/abs/2405.04532) | [Website](https://hanlab.mit.edu/projects/qserve) | [DeepCompressor Library](https://github.com/mit-han-lab/deepcompressor/tree/lmquant-v0.0.0-deprecated)**

**QServe: Efficient and accurate LLM serving system** on GPUs with W4A8KV4 quantization (4-bit weights, 8-bit activations, and 4-bit KV cache). Compared with leading industry solution TensorRT-LLM, QServe achieves **1.2x-1.4x higher throughput** when serving Llama-3-8B, and **2.4x-3.5x higher throughput** when serving Qwen1.5-72B, on L40S and A100 GPUs. QServe also allows users to achieve A100-level throughput on **3x cheaper** L40S GPUs.  

QServe is suitable for **large-scale synthetic data generation** with both LLMs and VLMs. Check out our latest [QServe-VLM](#qserve-vlm) release!

![teaser](assets/qserve_figures/teaser.png)
![efficiency](assets/qserve_figures/efficiency.png)


## Introduction

Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when **dequantizing either weights or partial sums** on GPUs. To address this challenge, we introduce **QoQ**, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for **quattuor-octo-quattuor**, which represents 4-8-4 in Latin. QoQ is implemented by the **QServe** inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by **operations on low-throughput CUDA cores**. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by **1.2×** on A100, **1.4×** on L40S; and Qwen1.5-72B by **2.4×** on A100, **3.5×** on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by **3×**.

**The current release supports:**

- Blazingly fast system support for **QoQ W4A8KV4** quantization (Algorithim release: [DeepCompressor Library](https://github.com/mit-han-lab/deepcompressor/tree/lmquant-v0.0.0-deprecated));
- Pre-quantized QServe model zoo with **W4A8KV4 QoQ** for mainstream LLMs;
- **Fully PyTorch-based** runtime and user interface for LLM serving, with **TensorRT-LLM-level efficiency** and **PyTorch-level flexibility**;
- Full support for **in-flight batching** and **paged attention**;
- Efficient **fused** CUDA kernels for **W4A8**/W8A8 GEMM and **KV4**/KV8 attention;
- Easy-to-use examples on speed benchmarking and **large-scale end-to-end content generation** (with W4A8KV4, in-flight batching and paged attention).


## Usage and Examples

We support both offline benchmarking and online generation (in-flight-batching) in QServe.

1. Offline speed benchmarking (Batched input sequences, fixed context length = 1024 and generation length = 512). We take Llama-3-8B (per-channel quant) as an example here. Please make sure that you have already downloaded the QoQ-quantized QServe model. 

```bash
export MODEL_PATH=./qserve_checkpoints/Llama-3-8B-QServe # Please set the path accordingly

GLOBAL_BATCH_SIZE=128 \
python qserve_benchmark.py \
  --model $MODEL_PATH \
  --benchmarking \
  --precision w4a8kv4 \
  --group-size -1
```

If you hope to use larger batch sizes such as 256, you may need to change `NUM_GPU_PAGE_BLOCKS` to a larger value than the automatically-determined value on A100. For example:
```bash
export MODEL_PATH=./qserve_checkpoints/Llama-3-8B-QServe # Please set the path accordingly

GLOBAL_BATCH_SIZE=256 \
NUM_GPU_PAGE_BLOCKS=6400 \
python qserve_benchmark.py \
  --model $MODEL_PATH \
  --benchmarking \
  --precision w4a8kv4 \
  --group-size -1
```

2. This is an online demonstration of batched generation, showcasing in-flight batching, paged attention of W4A8KV4 QoQ LLMs. We will randomly sample a set of safety-moderated conversations from the [WildChat](https://huggingface.co/datasets/allenai/WildChat) dataset and process them efficiently through in-flight batching.
   
```bash
export MODEL_PATH=./qserve_checkpoints/Llama-3-8B-Instruct-QServe # Please set the path accordingly

python qserve_e2e_generation.py \
  --model $MODEL_PATH \
  --ifb-mode \
  --precision w4a8kv4 \
  --quant-path $MODEL_PATH \
  --group-size -1
```

3. Argument list in QServe
   
   Below are some frequently used arguments in QServe interface:

- `--model`: Path to the folder containing hf model configs. Can be the same as `--quant-path` if you directly download the models from QServe model zoo.
- `--quant-path`: Path to the folder containing quantized LLM checkpoints. 
- `--precision`: The precision for GEMM in QServe, please choose from the following values: `w4a8kv4`, `w4a8kv8`, `w4a8` (means `w4a8kv8`), `w8a8kv4`, `w8a8kv8`, `w8a8` (means `w8a8kv8`). Default: `w4a8kv4`. 
- `--group-size`: Group size for weight quantization, -1 means per-channel quantization. QServe only supports -1 or 128. Please make sure your group size matches the checkpoint.
- `--max-num-batched-tokens`: Maximum number of batched tokens per iteration. Default: 262144.
- `--max-num-seqs`: Maximum number of sequences per iteration. Default: 256. Remember to increase it if you want larger batch sizes. 
- `--ifb-mode`: Enable in-flight batching mode. Suggest to activate in e2e generation.
- `--benchmarking`: Enable speed profiling mode. Benchmark settings aligned with TensorRT-LLM.

   Environment variables in QServe:
- `GLOBAL_BATCH_SIZE`: Batch size used in offline speed benchmarking.
- `NUM_GPU_PAGE_BLOCKS`: Number of pages to be allocated on GPU. If not specified, it will be automatically determined based on available GPU memory. Note that the current automatic GPU page allocation algorithm is very conservative. It is recommended to manually set this value to a larger number if you observe that GPU memory utilization is relatively low.

4. One-line scripts:

We also provide sample scripts in QServe. 

- End to end generation: `./scripts/qserve_e2e.sh`;
- Speed benchmarking: `./scripts/qserve_benchmark/benchmark_a100.sh` or `./scripts/qserve_benchmark/benchmark_l40s.sh`.

These scripts are expected to be executed in the QServe project folder (not in the `scripts` folder). Please note that `git-lfs` is needed for downloading QServe benchmark config files from huggingface before running the benchmark scripts.


## Results

We evaluate QServe W4A8KV4 quantization on a wide range of mainstream LLMs. QServe consistently outperforms existing W4A4 or W4A8 solutions from the accuracy perspective, while providing State-of-the-Art LLM serving efficiency.

### Efficiency Benchmarks

When serving the large language models Llama-3-8B and Qwen1.5-72B on L40S and A100 GPUs, QServe demonstrates superior performance, achieving **1.2x-1.4x higher throughput** compared to the leading industry solution, TensorRT-LLM, for Llama-3-8B, and a **2.4x-3.5x higher throughput** for Qwen1.5-72B. It is also able to **deliver higher throughput** and **accomodate the same batch size** on **L40S** compared with TensorRT-LLM on **A100** for six of eight models benchmarked, effectively saving the dollar cost of LLM serving by around 3x.

Benchmarking setting: the criterion is maximum achieveable throughput on NVIDIA GPUs, and the input context length is 1024 tokens, output generation length is 512 tokens. For all systems that support paged attention, we enable this feature. In-flight batching is turned off in the efficiency benchmarks.

| L40S (48G)     | Llama-3-8B | Llama-2-7B | Mistral-7B | Llama-2-13B | Llama-30B | Yi-34B | Llama-2-70B | Qwen-1.5-72B |
|----------------|------------ | ------------|------------|-------------|-----------|--------|-------------|--------------|
| TRT-LLM-FP16    | 1326 | 444        | 1566       | 92          | OOM       | OOM    | OOM         | OOM          |
| TRT-LLM-W4A16   | 1431 | 681        | 1457       | 368         | 148       | 313    | 119         | 17           |
| TRT-LLM-W8A8    | 2634 | 1271       | 2569       | 440         | 123       | 364    | OOM         | OOM          |
| Atom-W4A4      | -- | 2120       | --          | --           | --         | --      | --           | --            |
| QuaRot-W4A4    | -- | 805        | --          | 413         | 133       | --      | --           | 15           |
| QServe-W4A8KV4 | **3656** | **2394**       | **3774**       | **1327**        | **504**       | **869**    | **286**         | **59**           |
| Throughput Increase*     | **1.39x** | **1.13x**      | **1.47x**      | **3.02x**       | **3.41x**     | **2.39x**  | **2.40x**       | **3.47x**        |

| A100 (80G)   | Llama-3-8B | Llama-2-7B | Mistral-7B | Llama-2-13B | Llama-30B | Yi-34B | Llama-2-70B | Qwen-1.5-72B |
|----------------| ------------| ------------|------------|-------------|-----------|--------|-------------|--------------|
| TRT-LLM-FP16    | 2503 | 1549       | 2371       | 488         | 80        | 145    | OOM         | OOM          |
| TRT-LLM-W4A16   | 2370 | 1549       | 2403       | 871         | 352       | 569    | 358         | 143          |
| TRT-LLM-W8A8    | 2396 | 2334       | 2427       | 1277        | 361       | 649    | 235         | 53           |
| Atom-W4A4      | -- | 1160       | --          | --           | --         | --      | --           | --            |
| QuaRot-W4A4    | -- | 1370       | --         | 289         | 267       | --      | --           | 68           |
| QServe-W4A8KV4 | **3005** | **2908**       | **2970**       | **1741**        | **749**       | **803**    | **419**         | **340**          |
| Throughput Increase*     | **1.20x** | **1.25x**      | **1.22x**      | **1.36x**       | **2.07x**     | **1.23x**  | **1.17x**       | **2.38x**        |

The absolute token generation throughputs of QServe and baseline systems (Unit: tokens/second. `--` means unsupported). All experiments were
conducted under the same device memory budget. Throughput increase of QServe is calculated with regard to the best baseline in each column. It is recommended to use QServe-per-channel on high-end datacenter GPUs like A100 and QServe-per-group is recommended on inference GPUs like L40S. 

Max throughput batch sizes used by QServe:
| Device  | Llama-3-8B | Llama-2-7B | Mistral-7B | Llama-2-13B | Llama-30B | Yi-34B | Llama-2-70B | Qwen-1.5-72B |
|----------------| ------------| ------------|------------|-------------|-----------|--------|-------------|--------------|
| L40S    | 128 | 128       | 128       | 75         | 32        | 64    | 24         | 4          |
| A100   | 256 | 190       | 256       | 128         | 64       | 196    | 96         | 32          |

We recommend direcly setting the `NUM_GPU_PAGE_BLOCKS` environmental variable to `25 * batch size`, since in our benchmarking setting we have a context length of 1024 and generation length of 512, which corresponds to 24 pages (each page contains 64 tokens). We leave some buffer by allocating one more page for each sequence.

### Accuracy Evaluation

QServe also maintains high accuracy thanks to the QoQ algorithm provided in our [DeepCompressor](https://github.com/mit-han-lab/deepcompressor/tree/lmquant-v0.0.0-deprecated) quantization library.

Below is the WikiText2 perplexity evaluated with 2048 sequence length. The lower is the better.


| Models      | Precision | Llama-3 8B | Llama-2 7B | Llama-2 13B | Llama-2 70B | Llama 7B | Llama 13B | Llama 30B | Mistral 7B | Yi 34B |
|-------------|-----------|------------|------------|-------------|-------------|----------|-----------|-----------|------------|--------|
| FP16        |              | 6.14       | 5.47       | 4.88        | 3.32        | 5.68     | 5.09      | 4.10      | 5.25       | 4.60   |
| SmoothQuant | W8A8         | 6.28       | 5.54       | 4.95        | 3.36        | 5.73     | 5.13      | 4.23      | 5.29       | 4.69   |
| GPTQ-R      | W4A16 g128   | 6.56       | 5.63       | 4.99        | 3.43        | 5.83     | 5.20      | 4.22      | 5.39       | 4.68   |
| AWQ         | W4A16 g128   | 6.54       | 5.60       | 4.97        | 3.41        | 5.78     | 5.19      | 4.21      | 5.37       | 4.67   |
| QuaRot      | W4A4         | 8.33       | 6.19       | 5.45        | 3.83        | 6.34     | 5.58      | 4.64      | 5.77       | NaN    |
| Atom        | W4A4 g128    | 7.76       | 6.12       | 5.31        | 3.73        | 6.25     | 5.52      | 4.61      | 5.76       | 4.97   |
| QoQ         | W4A8KV4      | 6.89       | 5.75       | 5.12        | 3.52        | 5.93     | 5.28      | 4.34      | 5.45       | 4.74   |
| QoQ         | W4A8KV4 g128 | 6.76       | 5.70       | 5.08        | 3.47        | 5.89     | 5.25      | 4.28      | 5.42       | 4.76   |

\* SmoothQuant is evaluated with per-tensor static KV cache quantization.


# LServe: Efficient Long-Sequence LLM Serving with ***Unified Sparse Attention***

**[Paper](https://arxiv.org/abs/2502.14866) | [Website](https://hanlab.mit.edu/projects/lserve) | [DeepCompressor Library](https://github.com/mit-han-lab/deepcompressor/tree/lmquant-v0.0.0-deprecated)**

**LServe: Efficient and accurate serving system for long-context LLMs** on GPUs with Unified Sparse Attention. Introducing hybrid sparse attention patterns into the inference pipeline, LServe surpasses state-of-the-art LLM serving systems, including vLLM and TensorRT-LLM. 


![lserve_teaser](assets/lserve_figures/teaser.png)

## Introduction
Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce **LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention**. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to **2.9×** and decoding by **1.3-2.1×** over vLLM, maintaining long-context accuracy.


## Usage and Examples

We provide sample scripts for both efficiency benchmarking and accuracy evaluation of LServe system.

1. End-to-end Generation. We provide a sample scripts demonstrating how to use LServe for e2e LLM inference. Run the following command to check it out!

```bash
bash scripts/lserve_e2e.sh
```

In this script, we simulate a simplified version of Needle-in-a-Haystack task. The expected output (needle) is ""eating a sandwich and sitting in Dolores Park on a sunny day.""


2. Speed Benchmarking. The efficiency evaluation of LServe can be easily accomplished with a push-button command:

```bash
bash scripts/lserve_benchmark/launch.sh
```

Or specify your own test configurations with the following command:

```bash
bash scripts/lserve_benchmark/benchmark.sh \
    $model_path $attn_path \
    $batch_size $prefill_len $decode_len \
    $precision $kv_quant_granularity \
    $static_sparsity $sparse_prefill_mode \
    $sparse_decode_mode $dynamic_attn_budget $dynamic_select_interval $sub_chunk_per_block \
    $gpu_id
```


3. Accuracy Evaluation. In LServe, we provide accuracy evaluation scripts for Benchmarks such as [LongBench](https://arxiv.org/abs/2308.14508) and [Needle-in-a-Haystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).

<!-- [TODO] Do we need to provide more flexible acc eval scripts with deep compressor? -->

* Needle-in-a-Haystack Evaluation. Evaluation results will be updated into `eval/needle/img`.
```bash
bash eval/scripts/needle/submit_niah.sh
```

* LongBench Evaluation. Evaluation results can be found in `eval/LongBench/pred`.
```bash
bash eval/scripts/LongBench/submit_longbench.sh
```

* We also provide slurm scripts for paralleled evaluation across multiple GPUs/nodes. Please find more examples in `eval/scripts`.

<!-- 4. [Optional] Checkpoint Preparation.

The above commands will automatically download the checkpoints we pre-built for LServe benchmarking and evaluation. To built your own checkpoints for LServe, please follow the instructions below:

* Quantize the large language model with [DeepCompressor](https://github.com/mit-han-lab/deepcompressor).

[TODO/WIP] Add a patch for per-tensor quant in public deep compressor.

* Prepare the quantized checkpoint with our [`checkpoint_converter`](./scripts/ckpt_converter/checkpoint_converter.py).

  * For LServe models, please use the following command to prepare the checkpoint with recommended configurations:
  ```bash
  python checkpoint_converter.py --model-path $MODEL_PATH --quant-path $QUANT_PATH --w-bit 8 --group-size -1 --device cpu --kv-per-tensor
  ```

* Identify streaming attention heads with [Duo Attention](https://github.com/mit-han-lab/duo-attention).

  * We provide attention patterns for prevailing models in [`attn_patterns`](./attn_patterns/). To support your own models, please utilize the [Duo Attention scripts](https://github.com/mit-han-lab/duo-attention?tab=readme-ov-file#retrieval-head-identification) for the streaming head identification process. -->




## Results

We evaluate LServe across diverse long-context benchmarks and models, demonstrating consistently superior throughput over existing LLM serving frameworks for long-sequence inference, without compromising accuracy.

### Accuracy Evaluation
We evaluated LServe across a wide range of long-context benchmarks including [LongBench](https://arxiv.org/abs/2308.14508) and [Needle-In-A-Haystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack). Some of the evaluation results on Llama-3-8B are as follows.

![lserve_acc](assets/lserve_figures/lserve-acc.png)

Please find more accuracy evaluation results in the [LServe paper](https://arxiv.org/abs/2502.14866).

### Efficiency Benchmarks

Compared with the state-of-the-art serving systems, LServe demonstrates significant and consistent efficiency improvements across different GPU platforms and model architectures. On Llama-3-8B and Minitron-4B, LServe achieves 1.5× average speedup over vLLM. For MHA-based model Llama-2-7B, LServe runs more than 2.0× faster than baselines on average.

Benchmarking setting: We evaluate the decoding throughput across different sequence lengths for each model. The measured numbers were than normalized to 1 in the following figure. Benchmarks were conducted on NVIDIA A100 80G and L40S 48G GPUs.

![lserve_speed](assets/lserve_figures/lserve-speed.png)

## Reference

If you find OmniServe/QServe/LServe useful or relevant to your research and work, please kindly cite our paper:

```
@article{lin2024qserve,
  title={QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving},
  author={Lin*, Yujun and Tang*, Haotian and Yang*, Shang and Zhang, Zhekai and Xiao, Guangxuan and Gan, Chuang and Han, Song},
  journal={arXiv preprint arXiv:2405.04532},
  year={2024}
}
```

```
@article{yang2025lserve,
  title={LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention},
  author={Yang*, Shang and Guo*, Junxian and Tang, Haotian and Hu, Qinghao and Xiao, Guangxuan and Tang, Jiaming and Lin, Yujun and Liu, Zhijian and Lu, Yao and Han, Song},
  journal={arXiv preprint arXiv:2502.14866},
  year={2025}
}
```

## Team
The OmniServe serving library is maintained by the following research team:

- [Shang Yang*](https://ys-2020.github.io), QServe and LServe system lead, MIT EECS; 
- [Haotian Tang*](http://kentang.net), QServe system lead, LServe mentor, MIT EECS;
- [Junxian Guo*](https://scholar.google.com/citations?user=3P6kczsAAAAJ&hl=zh-CN), LServe system lead, SJTU and MIT EECS; 
- [Yujun Lin](https://yujunlin.com), QServe quantization algorithm lead, MIT EECS;
- [Qinghao Hu](https://tonyhao.xyz/), system evaluation, MIT EECS;
- [Zhekai Zhang](https://hanlab.mit.edu/team/zhekai-zhang), system evaluation, MIT EECS;
- [Guangxuan Xiao](https://guangxuanx.com), algorithm evaluation, MIT EECS;
- [Jiaming Tang](https://jiamingtang.me/), algorithm evaluation, MIT EECS;
- [Zhijian Liu](https://zhijianliu.com), advisor, University of California San Diego and NVIDIA;
- [Yao Lu](https://scholar.google.com/citations?user=OI7zFmwAAAAJ&hl=en), advisor, NVIDIA;
- [Chuang Gan](https://people.csail.mit.edu/ganchuang), advisor, UMass Amherst and MIT-IBM Watson AI Lab;
- [Song Han](https://songhan.mit.edu), advisor, MIT EECS and NVIDIA.

## Related Projects

The following projects are highly related to OmniServe. Our group has developed full-stack application-algorithm-system-hardware support for efficient large models, receiving **9k+ GitHub stars** and **over 10M Huggingface community downloads**.

You are also welcome to check out [MIT HAN LAB](https://hanlab.mit.edu) for other exciting projects on **Efficient Generative AI**!

- [**Algorithm**] [DeepCompressor: Model Compression for Large Language Models and Diffusion Models](https://github.com/mit-han-lab/deepcompressor/tree/lmquant-v0.0.0-deprecated)

- [**Algorithm**] [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://github.com/mit-han-lab/llm-awq)

- [**System**] [TinyChat: Efficient and Lightweight Chatbot with AWQ](https://github.com/mit-han-lab/llm-awq/tree/main/tinychat)

- [**Application**] [VILA: On Pretraining of Visual-Language Models](https://github.com/Efficient-Large-Model/VILA)

- [**Algorithm**] [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://github.com/mit-han-lab/smoothquant)

- [**Algorithm**] [StreamingLLM: Efficient Streaming Language Models with Attention Sinks](https://github.com/mit-han-lab/streaming-llm)

- [**Hardware**] [SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning](https://arxiv.org/abs/2012.09852)


## Acknowledgement

We thank Julien Demouth, June Yang, and Dongxu Yang from NVIDIA for the helpful discussions. OmniServe is also inspired by many open-source libraries, including (but not limited to) [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [vLLM](https://github.com/vllm-project/vllm), [vLLM-SmoothQuant](https://github.com/vllm-project/vllm/pull/1112), [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [LMDeploy](https://github.com/InternLM/lmdeploy), [TorchSparse++](https://github.com/mit-han-lab/torchsparse), [GPTQ](https://arxiv.org/abs/2210.17323), [QuaRot](https://arxiv.org/abs/2404.00456) and [Atom](https://arxiv.org/abs/2310.19102). 
","['ys-2020', 'kentang-mit', 'synxlin', 'iamsiddhantsahu']",1,0.69,0,,,,,,12,,,,
289956862,MDEwOlJlcG9zaXRvcnkyODk5NTY4NjI=,torchsparse,mit-han-lab/torchsparse,0,mit-han-lab,https://github.com/mit-han-lab/torchsparse,"[MICRO'23, MLSys'22] TorchSparse: Efficient Training and Inference Framework for Sparse Convolution on GPUs.",0,2020-08-24 14:50:55+00:00,2025-03-06 13:40:50+00:00,2025-02-24 13:52:53+00:00,https://torchsparse.mit.edu,1522,1274,1274,Cuda,1,1,1,0,0,1,150,0,0,35,mit,1,0,0,public,150,35,1274,master,1,1,,"['kentang-mit', 'clee-ai', 'sandeepnmenon', 'Xiuyu-Li', 'ys-2020', 'victoryc', 'ioeddk', 'digital-idiot', 'gyes00205', 'wangg12', 'turbo0628', 'TonyLianLong', 'noahstier', 'zhijian-liu', 'resuly', 'snuffle-PX']",1,0.68,0,,,,Directory exists,,15,,,,
673712714,R_kgDOKCgKSg,BLIVA,mlpc-ucsd/BLIVA,0,mlpc-ucsd,https://github.com/mlpc-ucsd/BLIVA,(AAAI 2024) BLIVA: A Simple Multimodal LLM for Better Handling of Text-rich Visual Questions,0,2023-08-02 08:54:36+00:00,2025-02-23 20:29:29+00:00,2024-04-14 22:27:17+00:00,https://arxiv.org/abs/2308.09936,12907,254,254,Python,1,1,1,0,0,0,28,0,0,20,bsd-3-clause,1,0,0,public,28,20,254,main,1,1,,['gordonhu608'],1,0.73,0,,,,,,8,,,,
351263278,MDEwOlJlcG9zaXRvcnkzNTEyNjMyNzg=,CoaT,mlpc-ucsd/CoaT,0,mlpc-ucsd,https://github.com/mlpc-ucsd/CoaT,(ICCV 2021 Oral) CoaT: Co-Scale Conv-Attentional Image Transformers,0,2021-03-25 00:35:20+00:00,2025-01-06 12:26:25+00:00,2022-02-03 22:13:14+00:00,,7321,231,231,Jupyter Notebook,1,1,1,1,0,0,32,0,0,3,apache-2.0,1,0,0,public,32,3,231,main,1,1,,"['xwjabc', 'yix081']",1,0.79,0,,,,,,10,,,,
358478423,MDEwOlJlcG9zaXRvcnkzNTg0Nzg0MjM=,LETR,mlpc-ucsd/LETR,0,mlpc-ucsd,https://github.com/mlpc-ucsd/LETR,(CVPR 2021 Oral) LETR: Line Segment Detection Using Transformers without Edges,0,2021-04-16 04:47:35+00:00,2025-03-07 03:11:12+00:00,2024-07-05 21:22:29+00:00,,3174,218,218,Jupyter Notebook,1,1,1,1,0,0,39,0,0,22,apache-2.0,1,0,0,public,39,22,218,master,1,1,,"['yix081', 'HappyCodingA']",1,0.8,0,,,,,,4,,,,
478351853,R_kgDOHIMR7Q,TESTR,mlpc-ucsd/TESTR,0,mlpc-ucsd,https://github.com/mlpc-ucsd/TESTR,(CVPR 2022) Text Spotting Transformers,0,2022-04-06 00:56:21+00:00,2025-01-08 12:53:49+00:00,2023-01-30 01:19:10+00:00,,242,184,184,Python,1,1,1,1,0,0,22,0,0,9,apache-2.0,1,0,0,public,22,9,184,main,1,1,,['zx1239856'],1,0.75,0,,,,,,9,,,,
186043715,MDEwOlJlcG9zaXRvcnkxODYwNDM3MTU=,coding-problems,MTrajK/coding-problems,0,MTrajK,https://github.com/MTrajK/coding-problems,Solutions for various coding/algorithmic problems and many useful resources for learning algorithms and data structures,0,2019-05-10 19:29:20+00:00,2025-03-04 17:51:17+00:00,2023-08-04 12:49:08+00:00,,203,3285,3285,Python,1,1,1,1,0,0,620,0,0,1,mit,1,0,0,public,620,1,3285,master,1,,"# Coding Problems

Here you can find [solutions](#Solutions) for various coding/algorithmic problems and many useful [resources](#Learning-Resources) for learning algorithms and data structures.\
Also, this repo will be updated with new solutions and resources from time to time.

*Note that this repo is meant to be used for learning and researching purposes only and it is **not** meant to be used for production.*


## Solutions

Algorithms and data structures are not language-specific (it's true that some languages are faster, and some are easier to use), but if you are good with the logic and pseudocode, any language would be good.\
So I've decided to use [Python](https://en.wikipedia.org/wiki/Python_(programming_language)) because I think it's very close to pseudocode and it's easily readable (so it'll be easy for someone from another environment to implement the same solutions).\
As I said previously, all solutions are written in [Python](https://www.python.org/) (more precisely, [Python 3](https://docs.python.org/3)), using the [Built-in Functions](https://docs.python.org/3/library/functions.html) (print, len, range, sorted, sum, min, max, etc...) and a few modules from the [Python Standard Library](https://docs.python.org/3/library/) like:
- [math](https://docs.python.org/3/library/math.html) (used for constants like math.pi, math.inf and functions like math.ceil, math.floor, math.gcd, math.log, math.pow, math.sqrt, etc)
- [collections](https://docs.python.org/3/library/collections.html) (used for [collections.deque](https://docs.python.org/3/library/collections.html#collections.deque) when there is a need for [Stack](https://en.wikipedia.org/wiki/Stack_(abstract_data_type)) or [Queue](https://en.wikipedia.org/wiki/Queue_(abstract_data_type)) data structures)
- [heapq](https://docs.python.org/3/library/heapq.html) (used when there is a need for [Priority Queue](https://en.wikipedia.org/wiki/Priority_queue) data structure).
- [random](https://docs.python.org/3/library/random.html) (used for [nondeterministic algorithms](https://en.wikipedia.org/wiki/Nondeterministic_algorithm), like shuffling arrays ([Fisher–Yates shuffle](https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle)), sampling arrays ([Reservoir sampling](https://en.wikipedia.org/wiki/Reservoir_sampling)) and [Monte Carlo methods](https://en.wikipedia.org/wiki/Monte_Carlo_method)).

So, to execute these solutions there is no need from installing any external packages. \
Coding style and name conventions are described in the official [PEP8](https://www.python.org/dev/peps/pep-0008) page.

*Note that I'm **not** the author of these problems, they are from sites like [LeetCode](https://leetcode.com/) (you can find more than 40 sites like this in the [Training Sites](#Training-Sites) section). **Only** the solutions and explanations are mine. If you find any **bug** or incorrect implementation (or faster/better implementation) in this repo, please let me know by opening an issue or pull request.*


### Template

For easier navigation into the solutions, each file with a solution in this repo will have the following template:

```python
'''
Problem Name

Problem explanation.

Input: XXX
Output: XXX
Output explanation: XXX

=========================================
Solution 1 explanation.
    Time Complexity:    O(X)
    Space Complexity:   O(X)
Solution 2 explanation.
(some of the problems are solved in more than one way)
    Time Complexity:    O(X)
    Space Complexity:   O(X)
'''


##############
# Solution 1 #
##############

def name_of_solution_1(params):
    # description of code
    pass


##############
# Solution 2 #
##############

def name_of_solution_2(params):
    # description of code
    pass


###########
# Testing #
###########

# Test 1
# Correct result => 'result1'
test_val = 'example1'
print(name_of_solution_1(test_val))
print(name_of_solution_2(test_val))

# Test 2
# Correct result => 'result2'
test_val = 'example2'
print(name_of_solution_1(test_val))
print(name_of_solution_2(test_val))
```

*Note that here I'm using the **simplest** way of testing, printing the results using the [print](https://docs.python.org/3/library/functions.html#print) method. Why? Because I think that the bigger part of the users of this repo isn't familiar with [unit testing](https://en.wikipedia.org/wiki/Unit_testing) and I wanted this part to be intuitive. Btw, I strongly recommend using some unit testing framework for this kind of testing. The Python Standard Library contains a **great** framework for unit testing called [unittest](https://docs.python.org/3/library/unittest.html), or you can install some third-party unit testing framework like [pytest](https://docs.pytest.org/en/latest/).*

### Categories

Each solution/problem in this repo belongs to one of these categories:

1. [Arrays](https://github.com/MTrajK/coding-problems/tree/master/Arrays) - Array Manipulations, Sorting, Binary Search, Divide and Conquer, Sliding Window, etc.
2. [Linked Lists](https://github.com/MTrajK/coding-problems/tree/master/Linked%20Lists) - Linked List Searching, Pointer Manipulations, etc.
3. [Trees](https://github.com/MTrajK/coding-problems/tree/master/Trees) - Binary Search Trees, Tree Traversals: Breadth-First (Level Order) Traversal, Depth-First Traversal (Inorder, Preorder, Postorder), etc.
4. [Hashing DS](https://github.com/MTrajK/coding-problems/tree/master/Hashing%20DS) - Hashing Data Structures: Sets/HashSets and Dictionaries/HashMaps.
5. [Dynamic Programming](https://github.com/MTrajK/coding-problems/tree/master/Dynamic%20Programming) - 2D and 1D Dynamic Programming, LCS, LIS, Knapsack, etc.
6. [Strings](https://github.com/MTrajK/coding-problems/tree/master/Strings) - String Manipulations, Reversing, Encodings/Decodings, etc.
7. [Math](https://github.com/MTrajK/coding-problems/tree/master/Math) - GCD, LCM, Factorization, Geometry, Math Formulas, etc.
8. [Other](https://github.com/MTrajK/coding-problems/tree/master/Other) - Backtracking, BFS, DFS, Stacks, Queues, Deques, Priority Queues (Heaps), Matrices, etc.


## Learning Resources

The learning resources are divided into 4 categories: [Courses](#Courses), [Books](#Books), [Training Sites](#Training-Sites), [Other Resources](#Other-Resources).


### Courses

Collection of free courses from one of the best CS universities.

1. Stanford University
    - [Algorithms Specialization (Coursera)](https://www.coursera.org/specializations/algorithms)
        * [Divide and Conquer, Sorting and Searching, and Randomized Algorithms](https://www.coursera.org/learn/algorithms-divide-conquer)
        * [Graph Search, Shortest Paths, and Data Structures](https://www.coursera.org/learn/algorithms-graphs-data-structures)
        * [Greedy Algorithms, Minimum Spanning Trees, and Dynamic Programming](https://www.coursera.org/learn/algorithms-greedy)
        * [Shortest Paths Revisited, NP-Complete Problems and What To Do About Them](https://www.coursera.org/learn/algorithms-npcomplete)
    - [Introduction to Programming Contests - CS 97SI](http://web.stanford.edu/class/cs97si/)

2.  Princeton University
    - [Algorithms Part 1 (Coursera)](https://www.coursera.org/learn/algorithms-part1)
    - [Algorithms Part 2 (Coursera)](https://www.coursera.org/learn/algorithms-part2)
    - [Analysis of Algorithms (Coursera)](https://www.coursera.org/learn/analysis-of-algorithms)

3. UC San Diego
    - [Data Structures and Algorithms Specialization (Coursera)](https://www.coursera.org/specializations/data-structures-algorithms)
        * [Algorithmic Toolbox](https://www.coursera.org/learn/algorithmic-toolbox)
        * [Data Structures](https://www.coursera.org/learn/data-structures)
        * [Algorithms on Graphs](https://www.coursera.org/learn/algorithms-on-graphs)
        * [Algorithms on Strings](https://www.coursera.org/learn/algorithms-on-strings)
        * [Advanced Algorithms and Complexity](https://www.coursera.org/learn/advanced-algorithms-and-complexity)
    - [Data Structures and Performance (Coursera)](https://www.coursera.org/learn/data-structures-optimizing-performance)
    - [edX](https://www.edx.org/school/uc-san-diegox)
        * [Data Structures Fundamentals](https://www.edx.org/course/data-structures-fundamentals)
        * [Algorithmic Design and Techniques](https://www.edx.org/course/algorithmic-design-and-techniques)
        * [Graph Algorithms](https://www.edx.org/course/graph-algorithms)
        * [Data Structures: An Active Learning Approach](https://www.edx.org/course/data-structures-an-active-learning-approach)

4. MIT University
    - [Introduction to algorithms 2005](https://www.youtube.com/playlist?list=PL8B24C31197EC371C) - *[Official MIT page with resources](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/)*. Note: this course is the old 6.046J course (the new name is ***Design and analysis of algorithms***, you can find it below).
    - [Introduction to algorithms 2011 - 6.006](https://www.youtube.com/playlist?list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb) - *[Official MIT page with resources](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/)*
    - [Design and analysis of algorithms - 6.046J](https://www.youtube.com/playlist?list=PLUl4u3cNGP6317WaSNfmCvGym2ucw3oGp) - *[Official MIT page with resources](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/index.htm)*
    - [Advanced Data Structures - 6.851](https://www.youtube.com/playlist?list=PLUl4u3cNGP61hsJNdULdudlRL493b-XZf) - *[Official MIT page with resources](http://courses.csail.mit.edu/6.851/spring14/lectures/)*
    - [Advanced Algorithms 2016 - 6.854](https://www.youtube.com/playlist?list=PL6ogFv-ieghdoGKGg2Bik3Gl1glBTEu8c) - *[Official MIT page with resources](http://people.csail.mit.edu/moitra/854.html)*
    - [Programming for the Puzzled 2018 - 6.S095](https://www.youtube.com/playlist?list=PLUl4u3cNGP62QumaaZtCCjkID-NgqrleA) - *[Official MIT page with resources](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s095-programming-for-the-puzzled-january-iap-2018/)*

5. Harvard University
    - [Advanced algorithms - CS224](https://www.youtube.com/playlist?list=PL2SOU6wwxB0uP4rJgf5ayhHWgw7akUWSf)

6. UC Berkeley
    - [Data Structures - CS61B](https://inst.eecs.berkeley.edu/~cs61b/archives.html)
    - [Efficient Algorithms and Intractable Problems - CS170](https://cs170.org/) - *[YouTube videos](https://www.youtube.com/playlist?list=PLkFD6_40KJIx8lWWbE-Uk069aZ1R-W-VU)*


### Books

Several books that have made an impression on me:

1. [Grokking Algorithms by Aditya Bhargava](https://www.goodreads.com/book/show/22847284-grokking-algorithms-an-illustrated-guide-for-programmers-and-other-curio) - **The best** book for complete beginners in algorithms! I wish this book existed when I started learning algorithms.
2. [Introduction to Algorithms by CLRS](https://www.goodreads.com/book/show/6752187-introduction-to-algorithms) - This book is called the ""bible textbook of algorithms"" by many programmers.
3. [Algorithms by Robert Sedgewick & Kevin Wayne](https://www.goodreads.com/book/show/10803540-algorithms) - These authors are instructors of the previously mentioned Coursera courses: [Algorithms Part 1](https://www.coursera.org/learn/algorithms-part1) and [Algorithms Part 2](https://www.coursera.org/learn/algorithms-part2). Also, this book has an excellent and free [site](http://algs4.cs.princeton.edu) with exercises, presentations, and examples.
4. [The Algorithm Design Manual by Steven Skiena](https://www.goodreads.com/book/show/425208.The_Algorithm_Design_Manual) - The book describes many advanced topics and algorithms and it focuses on real-life practical examples. This book has [one](http://www.algorist.com) of the best sites with resources ([solution wiki](https://algorist.com/algowiki/index.php/), [algorithms and data structures](http://www.algorist.com/algorist.html), [python implementations](http://www.algorist.com/languages/Python.html)).
5. [Algorithms by S. Dasgupta, C. Papadimitriou, and U. Vazirani](https://www.goodreads.com/book/show/138563.Algorithms) - This book is an official book for algorithms and data structures classes in several famous universities.
6. [Competitive Programming 3 by Steven Halim & Felix Halim](https://www.goodreads.com/book/show/22820968-competitive-programming-3) - A great book that prepares you for competitive programming (not for complete beginners). You can learn many things and tricks about competitive programming. *But if your goal is to prepare for competitive programming then choose a faster language than Python, **C/C++** (or Java, it's faster than Python but not like C/C++).*
7. [Cracking the Coding Interview by Gayle Laakmann McDowell](https://www.goodreads.com/book/show/29350585-cracking-the-coding-interview) - A bit different from the previous books. Prepares you for coding interviews using great coding problems.


### Training Sites

If the problems from [LeetCode](https://leetcode.com/) are not enough and you need more problems like those, you can find much more on these platforms:

- [CodeChef](http://codechef.com/)
- [HackerEarth](http://hackerearth.com/)
- [CodeForces](http://codeforces.com/)
- [Topcoder](http://topcoder.com/)
- [AtCoder](https://atcoder.jp/)
- [HackerRank](http://hackerrank.com/)
- [SPOJ](http://www.spoj.com/)
- [PEG](https://wcipeg.com/)
- [Online Judge](https://onlinejudge.org/)
- [AIZU OJ](https://onlinejudge.u-aizu.ac.jp/)
- [E-Olymp](https://www.e-olymp.com/en/)
- [VJudge](https://vjudge.net/)
- [DMOJ](https://dmoj.ca/)
- [USA CO](http://www.usaco.org/)
- [POJ](http://poj.org/)
- [Project Euler](https://projecteuler.net/)
- [Rosetta Code](http://rosettacode.org/)
- [LintCode](http://www.lintcode.com/en/)
- [Kattis](https://www.kattis.com/developers)
- [CodeAbbey](http://codeabbey.com/)
- [CS Academy](https://csacademy.com/)
- [Advent of Code](https://adventofcode.com/)
- [Exercism](https://exercism.io/)
- [CodeFu](https://codefu.mk/)
- [Mendo](https://mendo.mk/Welcome.do)
- [Codewars](http://www.codewars.com/)
- [Wolfram Challenges](https://challenges.wolfram.com/)
- [Google's Coding Competitions](https://codingcompetitions.withgoogle.com/)
- [Cyber-dojo](https://cyber-dojo.org/)
- [CodingBat](http://codingbat.com/)
- [CodeKata](http://codekata.com/)
- [Daily Coding Problem](https://www.dailycodingproblem.com/)
- [Daily Interview Pro](http://dailyinterviewpro.com/)
- [AlgoDaily](https://algodaily.com/)
- [Codility](https://codility.com/)
- [CoderByte](https://coderbyte.com/)
- [AlgoExpert](https://www.algoexpert.io/)
- [CodeSignal](https://codesignal.com/)
- [Edabit](https://edabit.com/)
- [DevPost](https://devpost.com/)
- [Brilliant](http://brilliant.org/)
- [Codingame](https://www.codingame.com/)
- [CheckiO](http://www.checkio.org/)
- [Kaggle](http://kaggle.com/)
- [Rosalind](http://rosalind.info/problems/locations/)
- [workat.tech](https://workat.tech/problem-solving/practice/)


### Other Resources

1. [Geeks For Geeks](https://www.geeksforgeeks.org/) - The site which **all** interested in algorithms (no matter if beginners or experts) should know! [YouTube channel](https://www.youtube.com/channel/UC0RhatS1pyxInC00YKjjBqQ) with many useful videos.
2. [The Algorithms - Python](https://github.com/TheAlgorithms/Python) - Great GitHub repo with many algorithms written in Python ([Link](https://github.com/TheAlgorithms) from the same repo written in other programming languages).
3. [CP Algorithms](http://cp-algorithms.com/) - Great page with excellent explanations for various algorithms.
4. Visualizers:
    - [USFCA Visualization Tool](https://www.cs.usfca.edu/~galles/visualization/Algorithms.html) - Great tool for visualizing data structures and algorithms, created by the University of San Francisco.
    - [VisuAlgo](https://visualgo.net/en) - Another great tool for visualizing data structures and algorithms through animation.
    - [Algorithm Visualizer](https://algorithm-visualizer.org/) - Interactive online platform that visualizes algorithms from code. This platform is an open-source project, [here](https://github.com/algorithm-visualizer/algorithm-visualizer) you can find the source code.
5. Courses and tutorials (but not from universities like the [Courses](#Courses) section):
    - [Google - Intro to Data Structures and Algorithms](https://www.udacity.com/course/data-structures-and-algorithms-in-python--ud513) - Free course on Udacity offered by Google.
    - [HackerEarth - Tutorials and Practice](https://www.hackerearth.com/practice/) - Practice problems and learn about many algorithms and data structures needed for competitive programming.
    - [KhanAcademy - Algorithms](https://www.khanacademy.org/computing/computer-science/algorithms) - Good explanations for some basic algorithms.
    - [Tutorialspoint - Data Structures and Algorithms](https://www.tutorialspoint.com/data_structures_algorithms/index.htm) - Another platform with good explanations, also Tutorialspoint has free tutorials for almost everything related to CS!
    - [Programiz - Data Structures and Algorithms](https://www.programiz.com/dsa) - One more platform which explains the data structures and algorithms in a simple and interesting way.
    - [Hackr.io - Data Structures and Algorithms Tutorials and Courses](https://hackr.io/tutorials/learn-data-structures-algorithms) - Big collection of tutorials and courses.
    - [Scaler - Data Structures Tutorial](https://www.scaler.com/topics/data-structures/) - Interesting and interactive explanations of some basic data structures.
6. YouTube playlists with tutorials:
    - [Data Structures by mycodeschool](https://www.youtube.com/playlist?list=PL2_aWCzGMAwI3W_JlcBbtYTwiQSsOTa6P)
    - [Data Structures by HackerRank](https://www.youtube.com/playlist?list=PLI1t_8YX-Apv-UiRlnZwqqrRT8D1RhriX)
    - [Algorithms by HackerRank](https://www.youtube.com/playlist?list=PLI1t_8YX-ApvMthLj56t1Rf-Buio5Y8KL)
","['MTrajK', 'bikashdaga', 'sagar0907', 'jtisaac', 'philippeitis', 'pkacprzak', 'wyugue']",0,0.62,0,,,,,,86,,,UCSD-E4E,
38276507,MDEwOlJlcG9zaXRvcnkzODI3NjUwNw==,open-source-cs-degree,mvillaloboz/open-source-cs-degree,0,mvillaloboz,https://github.com/mvillaloboz/open-source-cs-degree,The Open Source Computer Science Degree,0,2015-06-29 23:28:11+00:00,2025-03-08 03:07:45+00:00,2022-05-16 16:42:32+00:00,,24,3552,3552,,1,1,1,1,0,0,417,0,0,9,,1,0,0,public,417,9,3552,master,1,,"# The Open-Source Computer Science Degree

Inspired by [The Open-Source Data Science Masters](https://github.com/datasciencemasters/go), this project aims to do the same for an undergraduate Computer Science degree. The following document outlines free online courses from top schools like Harvard, Stanford and MIT. The groupings by Term are meant to pace and structure the course according to a typical Computer Science track at a college or university. The focus is on the core Computer Science courses; liberal arts or ""GenEd"" courses have been omitted.

### Term 1

**Intro to Computer Science**

> [Stanford CS101](https://web.stanford.edu/class/cs101/) *(Note: teaches in JavaScript)*  
> *or*  
> [Stanford CS106a](https://itunes.apple.com/us/itunes-u/programming-methodology/id384232896?mt=10) *(Note: teaches in Java)*  
> *or*
> [Stanford CS106a](https://web.stanford.edu/class/cs106a/index.html) *(Note: teaches in Python)*  
> *or*  
> [Harvard CS50x](https://www.edx.org/course/introduction-computer-science-harvardx-cs50x)  
> *or*  
> [UC Berkeley CS61A](https://cs61a.org/)

**Discrete Mathematics for Computer Science**

> [UC Berkeley CS70](https://www.eecs70.org/)  
> *or*  
> [MIT 6.042J](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/)  
> *or*  
> [Carnegie Mellon 15-251](http://www.cs.cmu.edu/~15251/index.html)  

**Data Structures**

> [Stanford CS106B](https://web.stanford.edu/class/cs106b/)  
> *or*  
> [UC Berkeley CS61B](https://inst.eecs.berkeley.edu/~cs61b/archives.html)

**Computer Architecture**

> [Princeton Coursera](https://www.coursera.org/course/comparch)  
> *or*  
> [MIT 6.823](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-823-computer-system-architecture-fall-2005/)  
> *or*  
> [UC Berkeley CS61C](https://inst.eecs.berkeley.edu/~cs61c/archives)

### Term 2

**UX Design**

> [Udacity UD849](https://www.udacity.com/course/ux-design-for-mobile-developers--ud849) *(Note: Android platform)*

**Intro to Web Development**

> [Udacity CS253](https://www.udacity.com/course/web-development--cs253)

**Intro to Databases**

> [Stanford DB](https://cs.stanford.edu/people/widom/DB-mooc.html)  
> *or*  
> [MIT 6.830](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-830-database-systems-fall-2010/)

**Operating Systems**

> [UC Berkeley CS162 Youtube](https://www.youtube.com/playlist?list=PL3A5075EC94726781&feature=plcp) 
> *or*
> [UC Berkeley CS162 Course Pages](https://inst.eecs.berkeley.edu/~cs162/archives.html)
> *or*  
> [MIT 6.828](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-828-operating-system-engineering-fall-2012/)

**Computer Graphics**

> [MIT 6.837](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-837-computer-graphics-fall-2012/)  
> *or*  
> [UC San Diego edX](https://www.edx.org/course/computer-graphics-uc-san-diegox-cse167x)

### Term 3

**Programming Languages Theory**

> [Brown University CS173](https://cs.brown.edu/courses/cs173/2012/OnLine/)

**Algorithms**

> [Stanford Coursera](https://www.coursera.org/course/algo)  
> *or*  
> [Princeton Coursera](https://www.coursera.org/course/algs4partI)  
> *or*  
> [Udacity CS215](https://www.udacity.com/course/intro-to-algorithms--cs215)  

**System Engineering**

> [MIT 6.033](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-033-computer-system-engineering-spring-2009/index.htm)

**Introduction to Embedded Systems and Real-Time Systems**

> [UC Riverside CS120B] (http://cms.cs.ucr.edu/faculty/philip/open_source_courses/CS120B_labs.html)  
> *and/or*  
> [UC Riverside CS122A] (http://cms.cs.ucr.edu/faculty/philip/open_source_courses/CS122A_labs.html)

**Software Engineering**

> [MIT 1.124J](http://ocw.mit.edu/courses/civil-and-environmental-engineering/1-124j-foundations-of-software-engineering-fall-2000/)  
> *or*  
> [UC Berkeley CS169](https://inst.eecs.berkeley.edu/~cs169/archives.html)
> *or*
> [Harvard CS164](http://cs164.tv/2014/spring/)

**Principles of Computing**
> [Rice University Coursera](https://www.coursera.org/course/principlescomputing1)

### Term 4

**Computer Networking**

> [Stanford Networking](https://lagunita.stanford.edu/courses/Engineering/Networking-SP/SelfPaced/about)  
> *or*  
> [Udacity UD436](https://www.udacity.com/course/computer-networking--ud436)

**Mobile Software Development**

> [University of Maryland Coursera](https://www.coursera.org/course/androidpart1) *(Note: Android platform)*  
> *or*  
> [Udacity UD585](https://www.udacity.com/course/intro-to-ios-app-development-with-swift--ud585) *(Note: iOS platform)*

**Programming Languages & Compilers**

> [UC Berkeley CS164](https://inst.eecs.berkeley.edu/~cs164/archives.html)  
> *or*  
> [MIT 6.035](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-035-computer-language-engineering-sma-5502-fall-2005/)

**Artificial Intelligence**

> [MIT 6.034](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/index.htm)  
> *or*  
> [UC Berkeley CS188](https://inst.eecs.berkeley.edu/~cs188/archives.html)

**Parallel Computing**

> [Carnegie Mellon 15-418](http://15418.courses.cs.cmu.edu/spring2015/)

**Machine Learning**

> [Carnegie Mellon 10-806](http://www.cs.cmu.edu/~ninamf/courses/806/)
","['mvillaloboz', 'DLambros91', 'ujvl', 'aweary', 'EdgeJ', 'kathgironpe', 'kevinji', 'nashvail', 'paramsingh', 'gueladjo', 'kennethyang404']",0,0.69,0,,,,,,216,,,UCSD-SASLab,
722785380,R_kgDOKxTUZA,dataengineering-roadmap,natayadev/dataengineering-roadmap,0,natayadev,https://github.com/natayadev/dataengineering-roadmap,"Un repositorio más con conceptos básicos, desafíos técnicos y recursos sobre ingeniería de datos en español 🧙✨",0,2023-11-24 00:23:05+00:00,2025-03-08 01:54:54+00:00,2024-12-23 04:21:44+00:00,https://natayadev.github.io/dataengineering-roadmap/,130692,702,702,,1,1,1,1,1,1,67,0,0,0,mit,1,0,0,public,67,0,702,main,1,,"<h1 align=""center""> Roadmap 2024 - Data engineering en español</h1>
<p align=""center""> Un repositorio más con conceptos básicos, desafíos técnicos y recursos sobre ingeniería de datos en español 🧙✨</p>
<p align=""center""> ¿Te gustaría aportar al repositorio? Visitá la <a href=""CONTRIBUTING.md"">guía de contribución</a> </p>

*Nota: la siguiente ruta de aprendizaje está diseñada a **criterio personal** con la idea de facilitar el estudio de aquellos interesados en la ingeniería de datos con material libre, gratuito y en español que encontré en internet. **No es** una guía definitiva ni un curso, **es** una lista de recursos que puede ser mejorada con el tiempo con contribuciones de la comunidad.*

[📚 Libros de ingeniería de datos en inglés](books)

[📖 Patrones de Diseño para DE en inglés](https://www.dedp.online/)

<img align=""center"" alt=""Roadmap illustration"" src=""./src/ROADMAP.png"" />

## Programación

### 🚀 Fundamentos

Comenzamos con la comprensión de los conceptos fundamentales de programación y lógica. Esta sección puede ser desarrollada simultáneamente con el aprendizaje del lenguaje de programación que elijan.

- [Curso: Programación Básica de Platzi](https://platzi.com/cursos/programacion-basica/)
- [Videos: Introducción a los Algoritmos y la Programación de TodoCode](https://www.youtube.com/playlist?list=PLQxX2eiEaqbzRVxjkstjLmmn9enb0x3zi)
- [Videos: Ejercicios de Pseudocódigo de TodoCode](https://www.youtube.com/playlist?list=PLQxX2eiEaqbwHMRObsvtRSb6sA43msUJt)
- [Videos: Linea de Comandos de Datademia](https://www.youtube.com/playlist?list=PLlhVpWerA0KzfxmP5CRFvmnEkRFldCUqI)
- [Videos: Bash scripting de Fazt](https://www.youtube.com/playlist?list=PLL0TiOXBeDai--LxFT1tro1qzO4Ayg5p_)
- [Lectura: Introducción a la línea de comandos de Linux y el shell de Microsoft Learn](https://learn.microsoft.com/es-es/training/paths/shell/)

### 🐍 Lenguaje de programación

Recomiendo iniciar con Python debido a su curva de aprendizaje amigable y su prevalencia en la industria actual. No obstante, es importante destacar que el procesamiento de datos también puede realizarse con R, Java, Scala, Julia, entre otros.

- [Videos: Python desde 0 de PildorasInformáticas](https://www.youtube.com/playlist?list=PLU8oAlHdN5BlvPxziopYZRd55pdqFwkeS)
- [Curso: Computación científica con Python de FreeCodeCamp](https://www.freecodecamp.org/learn/scientific-computing-with-python/)
- [Curso: Álgebra universitaria con Python de FreeCodeCamp](https://www.freecodecamp.org/learn/college-algebra-with-python/)
- [Curso: Harvard CS50’s Introducción a la programación con Python subtítulado de FreeCodeCamp](https://www.youtube.com/watch?v=nLRL_NcnK-4&ab_channel=freeCodeCamp.org)
- [Curso: Python intermedio subtitulado de FreeCodeCamp](https://www.youtube.com/watch?v=HGOBQPFzWKo&ab_channel=freeCodeCamp.org)
- [Curso: Pandas de Kaggle](https://www.kaggle.com/learn/pandas)
- [Videos: Expresiones Regulares de Ada Lovecode](https://www.youtube.com/playlist?list=PLI7nHlOIIPOJNEgw0BHE415nQST8Ve3JN)
- [Video: Principios de la Programación Orientada a Objetos de BettaTech](https://www.youtube.com/watch?v=tTPeP5dVuA4&ab_channel=BettaTech)
- [Videos: Programación Orientada a Objetos explicada con Minecraft de Absolute](https://www.youtube.com/playlist?list=PL-9YbO84eUcfKPIbzI6-ledKGY_6_Fvcj)
- [Curso: Julia para gente con prisa de Miguel Raz](https://github.com/miguelraz/JuliaParaGenteConPrisa)

### 📊 Excel

- []()

### 🔄 Control de versiones con Git

El aprendizaje sobre el control de versiones no solo es valioso al trabajar en equipos, sino que también nos proporciona la capacidad de rastrear, comprender y gestionar los cambios realizados en nuestro proyecto y así mantener un desarrollo eficiente y colaborativo.

- [Video: ¿Qué es el control de versiones y porque es tan importante para programar? de Datademia](https://www.youtube.com/watch?v=8HSjmgeJxqg&ab_channel=Datademia)
- [Curso: Git y Github de MoureDev](https://www.youtube.com/watch?v=3GymExBkKjE&ab_channel=MoureDevbyBraisMoure)
- [Videos: Git y Github de TodoCode](https://www.youtube.com/playlist?list=PLQxX2eiEaqby-qh4raiKfYyb4T7WyHsfW)
- [Lectura: Usa Git correctamente de Attlasian](https://www.atlassian.com/es/git)
- [Juego: Learn Git Branching](https://learngitbranching.js.org/?locale=es_AR)

### 🛠️ Más herramientas

- Notebooks: [Google Collab](https://colab.google/), [Jupyter](https://jupyter.org/) o [Deepnote](https://deepnote.com/)
- Editores de texto: [VSCode](https://code.visualstudio.com/), [Spyder](https://www.spyder-ide.org/) o [Google IDX](https://idx.google.com/)

## Bases de datos

### 🚀 Fundamentos

En esta instancia toca aprender sobre las bases de datos. La elección del gestor de bases de datos a utilizar queda a tu criterio, aunque personalmente recomiendo [PostgreSQL](https://www.postgresql.org/) para datos estructurados y [MongoDB](https://www.mongodb.com/es) para datos no estructurados. Sin embargo, existen muchas otras opciones: MySQL, SQLite y demás.

- [Videos: Introducción a las bases de datos de TodoCode](https://www.youtube.com/playlist?list=PLQxX2eiEaqbwcW3dkmUqJq7B-SXHyCglf)
- [Lectura: Diferencias entre DDL, DML y DCL de TodoPostgreSQL](https://www.todopostgresql.com/diferencias-entre-ddl-dml-y-dcl/)
- [Video: Procedimientos almacenados #1 de Héctor de León](https://youtu.be/NCcc2aISGtE?si=SZpgTmkSRmYzzmRd)
- [Video: Procedimientos almacenados #2 de Héctor de León](https://youtu.be/B9yw925kdiw?si=WwndA4YEmzKS3MsN)
- [Video: MongoDB de Fazt](https://www.youtube.com/watch?v=lWMemPN9t6Q&ab_channel=Fazt)
- [Videos: MongoDB de MitoCode](https://www.youtube.com/playlist?list=PLvimn1Ins-43y_9RNEo4JIFdpA5SJCYey)
    
### 📊 SQL

También aprenderás SQL, un lenguaje de consulta para gestionar y manipular las bases de datos relacionales.
- [Videos: SQL de Data Engineering LATAM](https://www.youtube.com/playlist?list=PLdxuOh58KNA6ybDbpk4pmk5BCxluqz5cS)
- [Intro to SQL de Kaggle](https://www.kaggle.com/learn/intro-to-sql)
- [Advanced SQL de Kaggle](https://www.kaggle.com/learn/advanced-sql)

### 📐 Diseño

Ahora seguimos con conceptos más avanzados que nos servirán para diseñar bases de datos, datalake, datawarehouses, esquemas, etcétera.
- [Video: ¿Cuándo utilizar SQL y cuando NoSQL? de Héctor de León](https://www.youtube.com/watch?v=EwJlyyq2urE&list=PLWYKfSbdsjJi6lb_dZ-UrGwRCJxmjhapl&index=9&ab_channel=hdeleon.net)
- [Video: ¿Cómo se modelan las bases de datos NoSQL? de HolaMundo](https://youtu.be/Zdlude8l8w4?si=gfIqq3gknpT4nDT4)
- [Lectura: Bases de datos orientadas a grafos de Oracle](https://www.oracle.com/ar/autonomous-database/what-is-graph-database/)
- [Video: Bases de Datos de Grafos, Fundamentos y Práctica de Datahack](https://www.youtube.com/watch?v=gDiZQ60LkpE&ab_channel=Datahack)
  
## Big Data

Lo siguiente es entender algunos conceptos de Big Data. Además, resulta interesante adquirir conocimientos básicos sobre inteligencia artificial, inteligencia de negocios y análisis de datos sin la necesidad de profundizar demasiado.

### 🚀 Fundamentos

- [Video: Big Data para dummies de Datahack](https://www.youtube.com/watch?v=DabkDUsNfIg&ab_channel=Datahack)
- [Lectura: Big Data: ¿Qué es y cómo ayuda a mi negocio? de Salesforce](https://www.salesforce.com/mx/blog/big-data/)
- [Certificación: Diseña y programa soluciones IoT con el uso de Big Data de Universidad del Rosario](https://www.edx.org/es/certificates/professional-certificate/urosariox-internet-de-las-cosas-iot-big-data-y-sus-aplicaciones?index=spanish_product&queryID=b0edf1baa13ad200be83166c876cea13&position=1&results_level=second-level-results&term=big+data&objectID=program-04e59d90-d7a3-48bb-a1f6-603982989153&campaign=Internet+de+las+cosas+%28IoT%29%2C+Big+Data+y+sus+aplicaciones&source=edX&product_category=professional-certificate&placement_url=https%3A%2F%2Fwww.edx.org%2Fes%2Fsearch)
- [Certificación: Big Data de University of California San Diego](https://www.coursera.org/specializations/big-data)
- [Video: Big data y privacidad de Databits](https://www.youtube.com/watch?v=-5kmGbc0RFM&list=PLkNVRh-NXvLaEbeScgn1raK48gxiEj_1q&index=18&ab_channel=Databits)
- [Videos: Gobierno de Datos de Smart Data](https://www.youtube.com/playlist?list=PL25mmW8d60iny2m7X73turXMfTK9ztTkk)
- [Video: Cómo Iniciar con Gobierno de Datos sin Romper el Presupuesto de Software Gurú](https://www.youtube.com/watch?v=INggcxBN_lc&list=WL&index=84&ab_channel=SoftwareGuru)

### 📊 Analítica y exploración de datos

- [Certificación: Fundamentos profesionales del análisis de datos, de Microsoft y LinkedIn](https://www.linkedin.com/learning/paths/fundamentos-profesionales-del-analisis-de-datos-por-microsoft-y-linkedin?src=direct%2Fnone&veh=direct%2Fnone)
- [Certificación: Certificado profesional de Google Data Analytics](https://www.coursera.org/professional-certificates/google-data-analytics#courses)
- [Certificación: Certificado profesional de Analista de datos de IBM](https://www.coursera.org/professional-certificates/ibm-data-analyst)
- [Curso: Análisis de datos con Python de FreeCodeCamp](https://www.freecodecamp.org/learn/data-analysis-with-python/)
- [Video: Storytelling: ¿Cómo convertir tu contenido en una historia? de Coderhouse](https://www.youtube.com/watch?v=pPHRb1dVRDE&ab_channel=Coderhouse)

### 🛠️ Estadística

- []()

### 🤖 Inteligencia artificial 

- [Curso: Machine Learning con Python de FreeCodeCamp](https://www.freecodecamp.org/learn/machine-learning-with-python/)
- [Canal: AprendeIA con Ligdi Gonzalez](https://www.youtube.com/@aprendeIA/playlists)
- [Videos: Aprende Inteligencia Artificial de Dot CSV](https://www.youtube.com/playlist?list=PL-Ogd76BhmcC_E2RjgIIJZd1DQdYHcVf0)
- [Video: Cómo usar ChatGPT en ingeniería de datos de Datalytics](https://www.youtube.com/watch?v=cg9VGCqLe9U&ab_channel=Datalytics)
- [Curso: Inteligencia Artificial subtitulado de Universidad de Columbia](https://www.edx.org/es/learn/artificial-intelligence/columbia-university-artificial-intelligence-ai)
- [Recursos: DataSAM](https://datasam.notion.site/Recursos-b3acbebecd664f5f8571cc32eaabe542)

### 📈 Inteligencia de negocios

- [Videos: Google Business Intelligence Certificate subtitulado de Google Career](https://www.youtube.com/playlist?list=PLTZYG7bZ1u6r3YwRBuSE7xIMYAiN2Bl85)
- [Videos: ¡Business Intelligence para Todos! de PEALCALA](https://www.youtube.com/playlist?list=PLCpKRQB2yv13cmUkcVlxVWA3md3uE9u3N)

### 📊 DataViz

- []()

## Procesamiento de Datos

En esta sección está el corazón de la ingeniería de datos, veremos que son los data pipelines, qué es un ETL, orquestadores, y más. Además, dejo una lista de conceptos clave qué voy a ir actualizando con sus recursos respectivos a futuro, si te interesa aprenderlos en detalle, podés buscar en los libros subidos en el repositorio.
- [Canal: CodinEric](https://www.youtube.com/@CodinEric)
- [Canal: Data Engineering LATAM](https://www.youtube.com/@DataEngineeringLatam)
- [Canal: Datademia](https://www.youtube.com/@datademia)
- [Canal: Datalytics](https://www.youtube.com/@datalytics.mejorcondatos)
- [Blog: Start (inglés)](https://www.startdataengineering.com/)
- [Plataforma de aprendizaje DataWars](https://www.datawars.io/)

### 🔍 ETL y Data Pipelines

- [Video: Ingeniería de datos: viaje al corazón de los proyectos de datos de RockingData](https://www.youtube.com/watch?v=UPPOEFUrvFU&list=PLwKA-LJRe79hxt69x6GVqTofHUetiB9vQ&index=8&ab_channel=RockingData)
- [Video: ¿Cómo convertirte en un verdadero Ingeniero de Datos? de Databits](https://www.youtube.com/watch?v=OiWR3pRyvgE&list=PLkNVRh-NXvLaEbeScgn1raK48gxiEj_1q&index=4&ab_channel=Databits)
- [Videos: Preprocesamiento de Datos en Python de Rocio Chavez](https://www.youtube.com/playlist?list=PLUofJx5RUeFqAIVdzfnJayenwZFEGtCmg)
- [Videos: Preprocesamiento de Datos en R de Rocio Chavez](https://www.youtube.com/playlist?list=PLUofJx5RUeFqGJJxKflkhDRTot29M7CYj)
- [Video: Pruebas A/B: Datos, no opiniones de SantanDev](https://www.youtube.com/watch?v=Yxdsu-RWsPc&list=WL&index=70&ab_channel=SantanderTec)
- Cargas incrementales
- Colas de mensajería
- Expresiones Cron

### ❄️ Bases de datos avanzado

- Modelo relacional
- Modelo dimensional
- Facts y dimensiones
- Datalake, Datamart, Datawarehouse y Dataqube
- Diseño por columnas y basada por filas
- Esquemas star y snowflake
- Esquemas on read y on write

### 🎭 Orquestadores

- [Videos: Airflow de Data Engineering LATAM](https://www.youtube.com/playlist?list=PLdxuOh58KNA6tjwp2xMiucO53XsvaG48L)
- [Video: Automatizando ideas con Apache Airflow - Yesi Díaz de Software Gurú](https://www.youtube.com/watch?v=ewK4KszmeTI&list=WL&index=125&t=410s&ab_channel=SoftwareGuru)
- [Videos: Pentaho Spoon de LEARNING-BI](https://www.youtube.com/playlist?list=PLPgjON4ZM0JBdxxDUAfCS84X79e_2CLNQ)
- [Videos: Luigi subtitulado de Seattle Data Guy](https://www.youtube.com/playlist?list=PLXRKPZRrlvE4c5fkoYYC34MLTwZ7ZQoje)
- [Lectura: Azure Data Factory de Microsoft](https://learn.microsoft.com/es-es/azure/data-factory/introduction)

### 🏰 Arquitecturas

- Procesamiento de datos por lotes o batch
- Procesamiento en tiempo real o streaming
- Arquitecturas lambda y kappa
- [Lectura: Diferencias clave entre el OLAP y el OLTP de AWS](https://aws.amazon.com/es/compare/the-difference-between-olap-and-oltp/)
- [Video: Construye ETL en batch y streaming con Spark de Databits](https://www.youtube.com/watch?v=hvwuMCPSB3M&list=PLkNVRh-NXvLaEbeScgn1raK48gxiEj_1q&index=13&ab_channel=Databits)
- [Lectura: Comparación de contenedores y máquinas virtuales de Atlassian](https://www.atlassian.com/es/microservices/cloud-computing/containers-vs-vms)
- [Videos: Docker de Pelado Nerd](https://www.youtube.com/playlist?list=PLqRCtm0kbeHAep1hc7yW-EZQoAJqSTgD-)
- [Videos: Kubernetes de Pelado Nerd](https://www.youtube.com/playlist?list=PLqRCtm0kbeHA5M_E_Anwu-vh4NWlgrOY_)
- [Lectura: ¿Qué es un sistema distribuido? de Atlassian](https://www.atlassian.com/es/microservices/microservices-architecture/distributed-architecture)
- [Videos: Spark de Data Engineering LATAM](https://www.youtube.com/playlist?list=PLdxuOh58KNA6CH3sQS6zhuIVKoPllmXiB)
- [Video: Infraestructura como código para ingeniería de datos de Spark México](https://www.youtube.com/watch?v=FjB3-RS_s38&ab_channel=SparkMexico)
- [Videos: Apache Spark de NullSafe Architect](https://www.youtube.com/playlist?list=PLwH0tlWs8nkQ-56HPCFeKsCJIStOOn_3j)
- [Videos: Apache Kafka de NullSafe Architect](https://www.youtube.com/playlist?list=PLwH0tlWs8nkSQRxizVF5Uuu-sLVYqdjaW)

### 🧪 Testing

- [Video: Great Expectations: Validar Data Pipelines como un Profesional por CodingEric en la PyConAr 2020](https://www.youtube.com/watch?v=VjTYGVlKTLM)
- [Video: ETL Testing y su Automatización con Python por Patricio Miner en la #QSConf 2023](https://www.youtube.com/watch?v=1G-_uFkEv3U)

## Cloud

Es útil tener conocimientos de cloud computing. Llegado a este punto, te recomendaría considerar la preparación de certificaciones oficiales. Aunque estos exámenes suelen tener un costo, puedes encontrar recursos de preparación gratuitos y oficiales de los proveedores más conocidos en la industria.

### ☁️ Fundamentos de la nube

- [Video: Fundamentos de Cloud Computing de Datahack](https://www.youtube.com/watch?v=ck9qignm_uY&ab_channel=Datahack)
- [Lectura: Descubre las ventajas y desventajas de la nube de Platzi](https://platzi.com/blog/ventajas-y-desventajas-de-la-nube-guia-completa/)
- [Lectura: Arquitectura para Big Data en Cloud de Platzi](https://platzi.com/blog/arquitectura-para-big-data-cloud/)

### 📜 Certificaciones oficiales

- [Ingeniería de datos de Google Cloud](https://www.cloudskillsboost.google/paths/16?hl=es-419)
    - [Videos: Google Cloud (GCP) de Aprender Big Data](https://www.youtube.com/playlist?list=PLGnDOd349NCNtUCRdRbtMRKl0V2gPPr5G)
- [Ingeniería de datos de Microsoft Azure](https://learn.microsoft.com/es-es/credentials/certifications/exams/dp-203/)
    - [Videos: Azure de Data Engineering LATAM](https://www.youtube.com/playlist?list=PLdxuOh58KNA5KdJXw7Z3TZMgKD90guixl)
    - [Videos: Certificaciones de Azure de Aprender Big Data](https://www.youtube.com/playlist?list=PLGnDOd349NCNxtsQpeWtIyD7lY1IrYw-n)
- [Ingeniería de datos con Fabric de Microsoft Azure](https://learn.microsoft.com/es-es/credentials/certifications/exams/dp-700/)
- [Ingeniería de datos de AWS](https://aws.amazon.com/es/certification/certified-data-engineer-associate/)
    - [Videos: AWS de Data Engineering LATAM](https://www.youtube.com/playlist?list=PLdxuOh58KNA5wi_1xtajCa9WjpobZUZl-)
    - [Preguntas oficiales](https://www.linkedin.com/feed/update/urn:li:activity:7269780247145762817/)

## Búsqueda Laboral

Finalmente te dejo algunas lecturas y videos que ofrecen consejos y experiencias relacionadas con la búsqueda laboral en el ámbito de sistemas. Más adelante, se agregarán desafíos técnicos y otros recursos vinculados al tema.

### 🔍 Consejos

- [Video: ¿Cómo obtener tu primer empleo en ingeniería de datos? de Spark México](https://www.youtube.com/watch?v=E3AviR1_Y_c&ab_channel=SparkMexico)
- [Videos: Consejos Laborales para el mundo IT de TodoCode](https://www.youtube.com/playlist?list=PLQxX2eiEaqbwcH3zocNIeDNL6ExUorxa9)
- [Videos: Esenciales para comenzar en el mundo de los sistemas de Maxi Programa](https://www.youtube.com/playlist?list=PLQRFzsIQFmxq6DUftRLXkOE5hiEej2EoJ)
- [Hilo: Consejos para completar el perfil de LinkedIn de @natayadev](https://twitter.com/natayadev/status/1667477410172882944)
- [Hilo: Consejos para conseguir un trabajo remoto en IT de @natayadev](https://twitter.com/natayadev/status/1714336919876747318)
- [Hilo: Cómo crear un CV ordenado y legible de @iamdoomling](https://twitter.com/iamdoomling/status/1410207350418509825)
- [Hilo: Te dejo estos tips para sobrevivir entrevistas con recursos humanos de @iamdoomling](https://twitter.com/iamdoomling/status/1468294464636653569)
- [Video: Programar en empresas, startups o freelance ¿Qué es mejor? de @iamdoomling](https://www.youtube.com/watch?v=JyTOzSuh4Ho&ab_channel=ProgramandoconBel)
- [Video: Terminé el bootcamp de programación ¿Y ahora qué? de @iamdoomling](https://www.youtube.com/watch?v=XDrgvD5Vp9Q&ab_channel=ProgramandoconBel)
- [Video: Trabajar como contractor desde Argentina de @iamdoomling](https://www.youtube.com/watch?v=2rLfcDI9Oh0&ab_channel=ProgramandoconBel)
- [Podcast: DevRock de Jonatan Ariste](https://open.spotify.com/show/5uRPZ5r7bRkW29c5AkppXq)

### 🛠️ Desafíos técnicos

- [(2023) Repositorio: Desafíos de código de la comunidad de MoureDev](https://github.com/mouredev/Code-Challenges)
- [(2024) Repositorio: Roadmap retos de programación de la comunidad de MoureDev](https://github.com/mouredev/roadmap-retos-programacion)
- [Blog: Preguntas de entrevista de DataCamp](https://www.datacamp.com/es/blog/top-21-data-engineering-interview-questions-and-answers)
- [Hilo: Preguntas de entrevista de @natayadev](https://x.com/natayadev/status/1806308271516430693)

En proceso 😊

---
**Si te resultó útil este repositorio, regalame una estrella ⭐**

<a href=""https://cafecito.app/natayafs""> <img alt=""Static Badge"" src=""https://img.shields.io/badge/cafecito-purple?style=social&logo=buy%20me%20a%20coffee&link=https%3A%2F%2Fcafecito.app%2Fnatayafs""></a> <a href=""https://www.buymeacoffee.com/natayafs""> <img alt=""Static Badge"" src=""https://img.shields.io/badge/buy_me_a_coffee-purple?style=social&logo=buy%20me%20a%20coffee&link=https%3A%2F%2Fwww.buymeacoffee.com%2Fnatayafs""></a>
","['natayadev', 'bjchavez', 'EloyChavezDev', 'RRcoder']",0,0.59,0,,"# Contribución al Proyecto

¡Gracias por tu interés en contribuir al proyecto! Tu colaboración es muy valorada. A continuación, te proporcionaré una guía para hacerlo:

## 🤔 Cómo Contribuir

1. Asegúrate de tener una cuenta de GitHub. Si no la tenés, podés crear una por [acá](https://github.com/).
2. Hacé un fork de este repositorio a tu cuenta de GitHub.
3. Cloná tu repositorio fork en tu máquina local:
    
    ```bash
    git clone https://github.com/TU_USUARIO/dataengineering-roadmap.git
    cd dataengineering-roadmap
    ```
    
4. Creá una rama nueva para tu contribución:
    
    ```bash
    git checkout -b tu-rama
    ```
    
5. Agregá el recurso siguiendo el formato:
    
    ```markdown
    Para recursos en español:
    - [Libro: ""Aprender Markdown"" de Juan Pérez](<https://ejemplo.com/libro-markdown>)
    
    Para recursos subtitulados al español:
    - [Libro: ""Understanding Markdown"" subtitulado de Joan Smith](<https://example.com/book-markdown>)
    ```
    
6. Realizá un commit con un mensaje descriptivo:
    
    ```bash
    git add README.md
    git commit -m ""Agrega [Libro: Aprender Markdown de Juan Pérez]""
    ```
    
7. Hacé un push de tus cambios a tu repositorio en GitHub:
    
    ```bash
    git push origin tu-rama
    ```
    
8. Abrí un Pull Request desde tu rama a la rama principal de este repositorio.

9. Tu Pull Request será revisado y fusionado si es aceptado.

Si tienes alguna pregunta, no dudes en abrir un issue o contactarme ¡Esperamos tu contribución! 👏
",,,,20,,,,
65558310,MDEwOlJlcG9zaXRvcnk2NTU1ODMxMA==,machine-learning-refined,neonwatty/machine-learning-refined,0,neonwatty,https://github.com/neonwatty/machine-learning-refined,"Master the fundamentals of machine learning, deep learning, and mathematical optimization by building key concepts and models from scratch using Python.",0,2016-08-12 14:13:30+00:00,2025-03-07 05:12:38+00:00,2025-01-30 16:39:04+00:00,https://www.mlrefined.com/,73342,1756,1756,Python,1,1,1,1,0,0,620,0,0,0,other,1,0,0,public,620,0,1756,main,1,,,['neonwatty'],0,0.66,0,,,,,,76,,,,
805061695,R_kgDOL_xEPw,puppeteer,nicklashansen/puppeteer,0,nicklashansen,https://github.com/nicklashansen/puppeteer,"Code for ""Hierarchical World Models as Visual Whole-Body Humanoid Controllers""",0,2024-05-23 20:06:58+00:00,2025-02-25 04:29:43+00:00,2024-07-02 16:48:37+00:00,https://nicklashansen.com/rlpuppeteer,698,167,167,Python,1,1,1,1,0,0,9,0,0,2,mit,1,0,0,public,9,2,167,main,1,,"<h1>Hierarchical World Models as Visual Whole-Body Humanoid Controllers</span></h1>

Official implementation of

[Hierarchical World Models as Visual Whole-Body Humanoid Controllers](https://www.nicklashansen.com/rlpuppeteer) by

[Nicklas Hansen](https://nicklashansen.github.io), [Jyothir S V](https://jyothirsv.github.io), [Vlad Sobal](https://vladisai.github.io), [Yann LeCun](http://yann.lecun.com), [Hao Su](https://cseweb.ucsd.edu/~haosu)\*, [Xiaolong Wang](https://xiaolonw.github.io)\*<br/>
UC San Diego, NYU, Meta AI<br/>
\*Equal advising

<img src=""assets/0.png"" width=""100%""></br>

[[Webpage]](https://www.nicklashansen.com/rlpuppeteer) [[Paper]](https://arxiv.org/abs/2405.18418) [[Models]](https://drive.google.com/drive/folders/1cgt9HzquO5mcB71Krv0C0mD10scfMquO?usp=sharing)

----

## Overview

We present Puppeteer, a hierarchical world model for whole-body humanoid control with visual observations. Our method produces natural and human-like motions without any reward design or skill primitives, and traverses challenging terrain. 

<img src=""assets/1.png"" width=""100%"" style=""max-width: 640px""><br/>

This repository contains code for training and evaluating both low-level (tracking) and high-level (puppeteering) world models. We open-source model checkpoints for both levels of the hierarchy, so that you can get started without training any models yourself. Model checkpoints are available for download [here](https://drive.google.com/drive/folders/1cgt9HzquO5mcB71Krv0C0mD10scfMquO?usp=sharing).

----

## Getting started

You will need a machine with a GPU (>= 24 GB memory) for training; CPU and RAM usage is insignificant. We provide a `Dockerfile` for easy installation. You can build the docker image by running

```
cd docker && docker build . -t <user>/puppeteer:1.0.0
```

This docker image contains all dependencies needed for running training and inference.

If you prefer to install dependencies manually, start by installing dependencies via `conda` by running the following command:

```
conda env create -f docker/environment.yaml
```

Depending on your existing system packages, you may need to install other dependencies. See `docker/Dockerfile` for a list of recommended system packages.

----

## Supported tasks

This codebase currently supports **8** whole-body control tasks for the CMU Humanoid model, implemented in MuJoCo using DMControl. The tasks are defined as follows:

| task | vision
| --- | --- |
| stand | N
| walk | N
| run | N
| corridor | Y
| hurdles-corridor | Y
| gaps-corridor | Y
| walls-corridor  | Y
| stairs-corridor  | Y

which can be run by specifying the `task` argument for `train.py` and `evaluation.py`.

## Example usage

We provide examples on how to evaluate our provided Puppeteer model checkpoints, as well as how to train your own Puppeteer agents, below.

### Evaluation

See below examples on how to evaluate downloaded low-level and high-level checkpoints.

```
$ python evaluate.py task=corridor low_level_fp=/path/to/tracking.pt checkpoint=/path/to/corridor-1.pt
$ python evaluate.py task=gaps-corridor low_level_fp=/path/to/tracking.pt checkpoint=/path/to/gaps-corridor-1.pt
```

All high-level checkpoints were trained with the same low-level checkpoint. See `config.yaml` for a full list of arguments.

### Training

See below examples on how to train low-level and high-level world models for Puppeteer. We recommend configuring [Weights and Biases](https://wandb.ai) (`wandb`) in `config.yaml` to track training progress.

```
$ python train.py task=tracking
$ python train.py task=walk low_level_fp=/path/to/tracking.pt
$ python train.py task=corridor low_level_fp=/path/to/tracking.pt
```

We recommend using default hyperparameters for all tasks. See `config.yaml` for a full list of arguments.

----

## Citation

If you find our work useful, please consider citing our paper as follows:

```
@misc{hansen2024hierarchical,
  title={Hierarchical World Models as Visual Whole-Body Humanoid Controllers}, 
  author={Nicklas Hansen, Jyothir S V, Vlad Sobal, Yann LeCun, Xiaolong Wang, Hao Su},
  eprint={2405.18418},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  year={2024}
}
```

----

## Contributing

You are very welcome to contribute to this project. Feel free to open an issue or pull request if you have any suggestions or bug reports, but please review our [guidelines](CONTRIBUTING.md) first.

----

## License

This project is licensed under the MIT License - see the `LICENSE` file for details. Note that the repository relies on third-party code, which is subject to their respective licenses.
",['nicklashansen'],1,0.76,0,,"# Contributing to Puppeteer
We want to make contributing to this repository as easy and transparent as
possible.

## Pull requests
We actively welcome your pull requests.

1. Fork the repo and create your branch from `main`.
2. If you have added code that should be tested, add tests.
3. If you have changed APIs, update the documentation.
4. Make sure your code lints.
5. Issue that pull request!

## Issues
We use GitHub issues to track public bugs. Please ensure your description is
clear and has sufficient instructions to be able to reproduce the issue.

## License
By contributing to Puppeteer, you agree that your contributions will be licensed
under the `LICENSE` file in the root directory of this source tree.
",,,,4,,,,
710079101,R_kgDOKlLyfQ,tdmpc2,nicklashansen/tdmpc2,0,nicklashansen,https://github.com/nicklashansen/tdmpc2,"Code for ""TD-MPC2: Scalable, Robust World Models for Continuous Control""",0,2023-10-26 01:23:45+00:00,2025-03-07 09:24:53+00:00,2025-02-28 00:25:24+00:00,https://www.tdmpc2.com,6057,463,463,Python,1,1,1,1,0,0,105,0,0,4,mit,1,0,0,public,105,4,463,main,1,,"<h1>TD-MPC2</span></h1>

Official implementation of

[TD-MPC2: Scalable, Robust World Models for Continuous Control](https://www.tdmpc2.com) by

[Nicklas Hansen](https://nicklashansen.github.io), [Hao Su](https://cseweb.ucsd.edu/~haosu)\*, [Xiaolong Wang](https://xiaolonw.github.io)\* (UC San Diego)</br>

<img src=""assets/0.gif"" width=""12.5%""><img src=""assets/1.gif"" width=""12.5%""><img src=""assets/2.gif"" width=""12.5%""><img src=""assets/3.gif"" width=""12.5%""><img src=""assets/4.gif"" width=""12.5%""><img src=""assets/5.gif"" width=""12.5%""><img src=""assets/6.gif"" width=""12.5%""><img src=""assets/7.gif"" width=""12.5%""></br>

[[Website]](https://www.tdmpc2.com) [[Paper]](https://arxiv.org/abs/2310.16828) [[Models]](https://www.tdmpc2.com/models)  [[Dataset]](https://www.tdmpc2.com/dataset)

----

**Announcement: training just got ~4.5x faster!**

Expect **~4.5x** faster wall-time (depending on hardware and task) with the most recent release (Nov 10, 2024). A majority of the speedups in this branch are enabled with the additional flag `compile=true`. To run the code with `compile=true`, **you will need to install recent `nightly` versions of PyTorch, TensorDict, and TorchRL**. See `docker/environment.yaml` for a tested configuration. Thank you to [Vincent Moens](https://github.com/vmoens) who has been a key contributor to our torch.compile compatibility!

----


## Overview

TD-MPC**2** is a scalable, robust model-based reinforcement learning algorithm. It compares favorably to existing model-free and model-based methods across **104** continuous control tasks spanning multiple domains, with a *single* set of hyperparameters (*right*). We further demonstrate the scalability of TD-MPC**2** by training a single 317M parameter agent to perform **80** tasks across multiple domains, embodiments, and action spaces (*left*). 

<img src=""assets/8.png"" width=""100%"" style=""max-width: 640px""><br/>

This repository contains code for training and evaluating both single-task online RL and multi-task offline RL TD-MPC**2** agents. We additionally open-source **300+** [model checkpoints](https://www.tdmpc2.com/models) (including 12 multi-task models) across 4 task domains: [DMControl](https://arxiv.org/abs/1801.00690), [Meta-World](https://meta-world.github.io/), [ManiSkill2](https://maniskill2.github.io/), and [MyoSuite](https://sites.google.com/view/myosuite), as well as our [30-task and 80-task datasets](https://www.tdmpc2.com/dataset) used to train the multi-task models. Our codebase supports both state and pixel observations. We hope that this repository will serve as a useful community resource for future research on model-based RL.

----

## Getting started

You will need a machine with a GPU and at least 12 GB of RAM for single-task online RL with TD-MPC**2**, and 128 GB of RAM for multi-task offline RL on our provided 80-task dataset. A GPU with at least 8 GB of memory is recommended for single-task online RL and for evaluation of the provided multi-task models (up to 317M parameters). Training of the 317M parameter model requires a GPU with at least 24 GB of memory.

We provide a `Dockerfile` for easy installation. You can build the docker image by running

```
cd docker && docker build . -t <user>/tdmpc2:1.0.0
```

This docker image contains all dependencies needed for running DMControl, Meta-World, and ManiSkill2 experiments.

If you prefer to install dependencies manually, start by installing dependencies via `conda` by running the following command:

```
conda env create -f docker/environment.yaml
pip install gym==0.21.0
```

The `environment.yaml` file installs dependencies required for training on DMControl tasks. Other domains can be installed by following the instructions in `environment.yaml`.

If you want to run ManiSkill2, you will additionally need to download and link the necessary assets by running

```
python -m mani_skill2.utils.download_asset all
```

which downloads assets to `./data`. You may move these assets to any location. Then, add the following line to your `~/.bashrc`:

```
export MS2_ASSET_DIR=<path>/<to>/<data>
```

and restart your terminal. Meta-World additionally requires MuJoCo 2.1.0. We host the unrestricted MuJoCo 2.1.0 license (courtesy of Google DeepMind) at [https://www.tdmpc2.com/files/mjkey.txt](https://www.tdmpc2.com/files/mjkey.txt). You can download the license by running

```
wget https://www.tdmpc2.com/files/mjkey.txt -O ~/.mujoco/mjkey.txt
```

See `docker/Dockerfile` for installation instructions if you do not already have MuJoCo 2.1.0 installed. MyoSuite requires `gym==0.13.0` which is incompatible with Meta-World and ManiSkill2. Install separately with `pip install myosuite` if desired. Depending on your existing system packages, you may need to install other dependencies. See `docker/Dockerfile` for a list of recommended system packages.

----

## Supported tasks

This codebase currently supports **104** continuous control tasks from **DMControl**, **Meta-World**, **ManiSkill2**, and **MyoSuite**. Specifically, it supports 39 tasks from DMControl (including 11 custom tasks), 50 tasks from Meta-World, 5 tasks from ManiSkill2, and 10 tasks from MyoSuite, and covers all tasks used in the paper. See below table for expected name formatting for each task domain:

| domain | task
| --- | --- |
| dmcontrol | dog-run
| dmcontrol | cheetah-run-backwards
| metaworld | mw-assembly
| metaworld | mw-pick-place-wall
| maniskill | pick-cube
| maniskill | pick-ycb
| myosuite  | myo-key-turn
| myosuite  | myo-key-turn-hard

which can be run by specifying the `task` argument for `evaluation.py`. Multi-task training and evaluation is specified by setting `task=mt80` or `task=mt30` for the 80-task and 30-task sets, respectively.

**As of Dec 27, 2023 the TD-MPC2 codebase also supports pixel observations for DMControl tasks**; use argument `obs=rgb` if you wish to train visual policies.


## Example usage

We provide examples on how to evaluate our provided TD-MPC**2** checkpoints, as well as how to train your own TD-MPC**2** agents, below.

### Evaluation

See below examples on how to evaluate downloaded single-task and multi-task checkpoints.

```
$ python evaluate.py task=mt80 model_size=48 checkpoint=/path/to/mt80-48M.pt
$ python evaluate.py task=mt30 model_size=317 checkpoint=/path/to/mt30-317M.pt
$ python evaluate.py task=dog-run checkpoint=/path/to/dog-1.pt save_video=true
```

All single-task checkpoints expect `model_size=5`. Multi-task checkpoints are available in multiple model sizes. Available arguments are `model_size={1, 5, 19, 48, 317}`. Note that single-task evaluation of multi-task checkpoints is currently not supported. See `config.yaml` for a full list of arguments.

### Training

See below examples on how to train TD-MPC**2** on a single task (online RL) and on multi-task datasets (offline RL). We recommend configuring [Weights and Biases](https://wandb.ai) (`wandb`) in `config.yaml` to track training progress.

```
$ python train.py task=mt80 model_size=48 batch_size=1024
$ python train.py task=mt30 model_size=317 batch_size=1024
$ python train.py task=dog-run steps=7000000
$ python train.py task=walker-walk obs=rgb
```

We recommend using default hyperparameters for single-task online RL, including the default model size of 5M parameters (`model_size=5`). Multi-task offline RL benefits from a larger model size, but larger models are also increasingly costly to train and evaluate. Available arguments are `model_size={1, 5, 19, 48, 317}`. See `config.yaml` for a full list of arguments.

**As of Jan 7, 2024 the TD-MPC2 codebase also supports multi-GPU training for multi-task offline RL experiments**; use branch `distributed` and argument `world_size=N` to train on `N` GPUs. We cannot guarantee that distributed training will yield the same results, but they appear to be similar based on our limited testing.

----

## Citation

If you find our work useful, please consider citing our paper as follows:

```
@inproceedings{hansen2024tdmpc2,
  title={TD-MPC2: Scalable, Robust World Models for Continuous Control}, 
  author={Nicklas Hansen and Hao Su and Xiaolong Wang},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}
```
as well as the original TD-MPC paper:
```
@inproceedings{hansen2022tdmpc,
  title={Temporal Difference Learning for Model Predictive Control},
  author={Nicklas Hansen and Xiaolong Wang and Hao Su},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2022}
}
```

----

## Contributing

You are very welcome to contribute to this project. Feel free to open an issue or pull request if you have any suggestions or bug reports, but please review our [guidelines](CONTRIBUTING.md) first. Our goal is to build a codebase that can easily be extended to new environments and tasks, and we would love to hear about your experience!

----

## License

This project is licensed under the MIT License - see the `LICENSE` file for details. Note that the repository relies on third-party code, which is subject to their respective licenses.
","['nicklashansen', 'vmoens', 'asmith26']",1,0.79,0,,"# Contributing to TD-MPC2
We want to make contributing to this repository as easy and transparent as
possible.

## Pull requests
We actively welcome your pull requests.

1. Fork the repo and create your branch from `main`.
2. If you have added code that should be tested, add tests.
3. If you have changed APIs, update the documentation.
4. Make sure your code lints.
5. Issue that pull request!

## Issues
We use GitHub issues to track public bugs. Please ensure your description is
clear and has sufficient instructions to be able to reproduce the issue.

## License
By contributing to TD-MPC2, you agree that your contributions will be licensed
under the `LICENSE` file in the root directory of this source tree.
",,,,7,,,,
100784668,MDEwOlJlcG9zaXRvcnkxMDA3ODQ2Njg=,awesome-project-ideas,NirantK/awesome-project-ideas,0,NirantK,https://github.com/NirantK/awesome-project-ideas,"Curated list of Machine Learning, NLP, Vision, Recommender Systems Project Ideas",0,2017-08-19 09:37:58+00:00,2025-03-07 14:32:27+00:00,2023-03-13 16:28:22+00:00,http://www.nirantk.com/awesome-project-ideas/,113,8176,8176,,1,0,1,0,1,0,1249,0,0,3,mit,1,0,0,public,1249,3,8176,master,1,,,"['NirantK', 'ableabhinav', 'DataSenseiAryan', 'asher-lab', 'Ayush-Parhi', 'EdThePro101', 'sam17']",0,0.77,0,,,,,"DELETE EVERYTHING FROM THIS LINE AND BELOW

Your idea should be in this template: 

**Main Idea Title**
- Ask a question here
- Dataset: 
- (Optional) Getting Started

Here is an example: 

**Music Genre recognition using neural networks**

- Can you identify the musical genre using their spectrograms or other sound information?
- Datasets: [FMA](https://github.com/mdeff/fma) or GTZAN on Keras
- Get started with [Librosa](https://librosa.github.io/librosa/index.html) for feature extraction
",284,,,/mlpc-ucsd,ucsdlib
9999135,MDEwOlJlcG9zaXRvcnk5OTk5MTM1,nodogsplash,nodogsplash/nodogsplash,0,nodogsplash,https://github.com/nodogsplash/nodogsplash,Nodogsplash offers a simple way to provide restricted access to an Internet connection using a captive portal. Pull requests are welcome!,0,2013-05-11 12:29:13+00:00,2025-03-04 12:02:07+00:00,2025-02-02 18:54:12+00:00,,2400,864,864,C,1,1,1,1,0,0,232,0,0,36,gpl-2.0,1,0,0,public,232,36,864,master,1,1,,"['mwarning', 'bluewavenet', 'lynxis', 'sayuan', 'wanxewoj', 'smoe', 'drwyrm', 'champtar', 'vavrecan', 'martignoni', 'florida63', 'KeaneWang', 'jow-', 'efernandesng', 'ycsunjane', 'hylics', 'vonkinder', 'bedefaced', 'zhounanzhao', 'sassanh', 'quiint', 'horodchukanton', 'gazambuja', 'altergui', 'megatux', 'chrisotherwise', 'azsde', 'maxguru']",1,0.78,2155,,,,,,47,,,,
91028340,MDEwOlJlcG9zaXRvcnk5MTAyODM0MA==,linux-nova,NVSL/linux-nova,0,NVSL,https://github.com/NVSL/linux-nova,"NOVA is a log-structured file system designed for byte-addressable non-volatile memories, developed at the University of California, San Diego.",0,2017-05-11 22:22:39+00:00,2025-03-06 05:48:28+00:00,2022-06-23 00:34:55+00:00,http://nvsl.ucsd.edu/index.php?path=projects/nova,1271878,434,434,C,1,1,1,1,0,0,118,0,0,54,other,1,0,0,public,118,54,434,master,1,1,"# NOVA: NOn-Volatile memory Accelerated log-structured file system

### Linux versions supported
5.1 (current master), 5.0, 4.19, 4.18, 4.14, 4.13. Checkout each branch if you are interested.

### Description
NOVA's goal is to provide a high-performance, full-featured, production-ready
file system tailored for byte-addressable non-volatile memories (e.g., NVDIMMs
and Intel's soon-to-be-released 3DXpoint DIMMs).  It combines design elements
from many other file systems to provide a combination of high-performance,
strong consistency guarantees, and comprehensive data protection.  NOVA support
DAX-style mmap and making DAX performs well is a first-order priority in NOVA's
design.  NOVA was developed by the [Non-Volatile Systems Laboratory][NVSL] in
the [Computer Science and Engineering Department][CSE] at the [University of
California, San Diego][UCSD].


NOVA is primarily a log-structured file system, but rather than maintain a
single global log for the entire file system, it maintains separate logs for
each file (inode).  NOVA breaks the logs into 4KB pages, they need not be
contiguous in memory.  The logs only contain metadata.

File data pages reside outside the log, and log entries for write operations
point to data pages they modify.  File modification uses copy-on-write (COW) to
provide atomic file updates.

For file operations that involve multiple inodes, NOVA use small, fixed-sized
redo logs to atomically append log entries to the logs of the inodes involned.

This structure keeps logs small and make garbage collection very fast.  It also
enables enormous parallelism during recovery from an unclean unmount, since
threads can scan logs in parallel.

NOVA replicates and checksums all metadata structures and protects file data
with RAID-4-style parity.  It supports checkpoints to facilitate backups.

This repository contains a version of the mainline kernel with NOVA
added.  You can check the current version by looking at the first
lines of the Makefile.

A more thorough discussion of NOVA's design is avaialable in these two papers:

**NOVA: A Log-structured File system for Hybrid Volatile/Non-volatile Main Memories** 
[PDF](http://cseweb.ucsd.edu/~swanson/papers/FAST2016NOVA.pdf)<br>
*Jian Xu and Steven Swanson*<br>
Published in [FAST 2016][FAST2016]

**Hardening the NOVA File System**
[PDF](http://cseweb.ucsd.edu/~swanson/papers/TechReport2017HardenedNOVA.pdf) <br>
UCSD-CSE Techreport CS2017-1018
*Jian Xu, Lu Zhang, Amirsaman Memaripour, Akshatha Gangadharaiah, Amit Borase, Tamires Brito Da Silva, Andy Rudoff, Steven Swanson*<br>

Read on for further details about NOVA's overall design and its current status 

### Compatibilty with Other File Systems

NOVA aims to be compatible with other Linux file systems.  To help verify that it achieves this we run several test suites against NOVA each night.

* The latest version of XFSTests. ([Current failures](https://github.com/NVSL/linux-nova/issues?q=is%3Aopen+is%3Aissue+label%3AXFSTests))
* The (Linux testing project)(https://linux-test-project.github.io/) file system tests.
* The (fstest POSIX test suite)[POSIXtest].

Currently, nearly all of these tests pass for the `master` branch, and we have
run complex programs on NOVA.  There are, of course, many bugs left to fix.

NOVA uses the standard PMEM kernel interfaces for accessing and managing
persistent memory.

### Atomicity

By default, NOVA makes all metadata and file data operations atomic.

Strong atomicity guarantees make it easier to build reliable applications on
NOVA, and NOVA can provide these guarantees with sacrificing much performance
because NVDIMMs support very fast random access.

NOVA also supports ""unsafe data"" and ""unsafe metadata"" modes that
improve performance in some cases and allows for non-atomic updates of file
data and metadata, respectively.

### Data Protection

NOVA aims to protect data against both misdirected writes in the kernel (which
can easily ""scribble"" over the contents of an NVDIMM) as well as media errors.

NOVA protects all of its metadata data structures with a combination of
replication and checksums.  It protects file data using RAID-5 style parity.

NOVA can detects data corruption by verifying checksums on each access and by
catching and handling machine check exceptions (MCEs) that arise when the
system's memory controller detects at uncorrectable media error.

We use a fault injection tool that allows testing of these recovery mechanisms.

To facilitate backups, NOVA can take snapshots of the current filesystem state
that can be mounted read-only while the current file system is mounted
read-write.

The tech report list above describes the design of NOVA's data protection system in detail.

### DAX Support

Supporting DAX efficiently is a core feature of NOVA and one of the challenges
in designing NOVA is reconciling DAX support which aims to avoid file system
intervention when file data changes, and other features that require such
intervention.

NOVA's philosophy with respect to DAX is that when a program uses DAX mmap to
to modify a file, the program must take full responsibility for that data and
NOVA must ensure that the memory will behave as expected.  At other times, the
file system provides protection.  This approach has several implications:

1. Implementing `msync()` in user space works fine.

2. While a file is mmap'd, it is not protected by NOVA's RAID-style parity
mechanism, because protecting it would be too expensive.  When the file is
unmapped and/or during file system recovery, protection is restored.

3. The snapshot mechanism must be careful about the order in which in adds
pages to the file's snapshot image.

### Performance

The research paper and technical report referenced above compare NOVA's
performance to other file systems.  In almost all cases, NOVA outperforms other
DAX-enabled file systems.  A notable exception is sub-page updates which incur
COW overheads for the entire page.

The technical report also illustrates the trade-offs between our protection
mechanisms and performance.

## Gaps, Missing Features, and Development Status

Although NOVA is a fully-functional file system, there is still much work left
to be done.  In particular, (at least) the following items are currently missing:

1.  There is no mkfs or fsk utility (`mount` takes `-o init` to create a NOVA file system)
2.  NOVA doesn't scrub data to prevent corruption from accumulating in infrequently accessed data.
3.  NOVA doesn't read bad block information on mount and attempt recovery of the effected data.
4.  NOVA only works on x86-64 kernels.
5.  NOVA does not currently support extended attributes or ACL.
6.  NOVA does not currently prevent writes to mounted snapshots.
7.  Using `write()` to modify pages that are mmap'd is not supported.
8.  NOVA deoesn't provide quota support.
9.  Moving NOVA file systems between machines with different numbers of CPUs does not work.
10. Remounting a NOVA file system with different mount options may fail.

None of these are fundamental limitations of NOVA's design.  Additional bugs
and issues are [here](https://github.com/NVSL/linux-nova/issues).

NOVA is complete and robust enough to run a range of complex applications, but
it is not yet ready for production use.  Our current focus is on adding a few
missing features list above and finding/fixing bugs.

## Building and Using NOVA

This repo contains a version of the Linux with NOVA included.  You should be
able to build and install it just as you would the mainline Linux source.

### Building NOVA

To build NOVA, build the kernel with LIBNVDIMM (`CONFIG_LIBNVDIMM`), PMEM (`CONFIG_BLK_DEV_PMEM`), DAX (`CONFIG_FS_DAX`) and NOVA (`CONFIG_NOVA_FS`) support.  Install as usual.  (When running `make menuconfig`, you can find those options under the *Device Drivers* and *File Systems* sections, respectively.)

Documentation/filesystems/nova.txt provides more detailed instructions on building and using NOVA.

## Hacking and Contributing

The NOVA source code is almost completely contains in the `fs/nova` directory.
The execptions are some small changes in the kernel's memory management system
to support checkpointing.

`Documentation/filesystems/nova.txt` describes the internals of Nova in more detail.

If you find bugs, please [report them](https://github.com/NVSL/linux-nova/issues).

If you have other questions or suggestions you can contact the NOVA developers at [cse-nova-hackers@eng.ucsd.edu](mailto:cse-nova-hackers@eng.ucsd.edu).


[NVSL]: http://nvsl.ucsd.edu/ ""http://nvsl.ucsd.edu""
[POSIXtest]: http://www.tuxera.com/community/posix-test-suite/ 
[FAST2016]: https://www.usenix.org/conference/fast16/technical-sessions
[CSE]: http://cs.ucsd.edu
[UCSD]: http://www.ucsd.edu
","['torvalds', 'davem330', 'broonie', 'tiwai', 'arndb', 'mchehab', 'gregkh', 'ickle', 'KAGA-KOKO', 'bigguiness', 'htejun', 'geertu', 'olofj', 'danvet', 'JoePerches', 'jmberg-intel', 'acmel', 'airlied', 'morimoto', 'linusw', 'AxelLin', 'vsyrjala', 'pmundt', 'alexdeucher', 'bzolnier', 'rafaeljw', 'larsclausen', 'dhowells', 'ralfbaechle', 'jwrdegoede', 'rddunlap', 'jhovold', 'AdrianBunk', 'ebiederm', 'paulmck', 'tmlind', 'heicarst', 'andy-shev', 'vireshk', 'ozbenh', 'herbertx', 'ffainelli', 'hverkuil', 'bjorn-helgaas', 'masahir0y', 'mpe', 'jankara', 'groeck', 'kees', 'kaber', 'neilbrown', 'ian-abbott', 'ChristianKoenigAMD', 'chucklever', 'thierryreding', 'pinchartl', 'djbw', 'tomba', 'jtlayton', 'JuliaLawall', 'lwfinger', 'rostedt', 'linvjw', 'jmberg', 'lenb', 'bvanassche', 'AlanStern', 'wildea01', 'dtor', 'palosaari', 'krzk', 'egrumbach', 'bbrezillon', 'bebarino', 'rjwysocki', 'holtmann', 'elfring', 'GustavoARSilva', 'rustyrussell', 'avikivity', 'olsajiri', 'mripard', 'fweisbec', 'alexandrebelloni', 'rmilecki', 'akpm00', 'lyakh', 'kattisrinivasan', 'oleg-nesterov', 'paulusmack', 'sfrothwell', 'jnikula', 'ahunter6', 'acpibob', 'chrismason-xx', 'trondmypd', 'tytso', 'vinodkoul', 'antonblanchard', 'tpetazzoni', 'saschahauer', 'wens', 'axboe', 'ArvindYadavCs', 'nvswarren', 'shawnguo2', 'ummakynes', 'pH5', 'mszyprow', 'amalon', 'wfp5p', 'cladisch', 'aakoskin', 'chaseyu', 'sravnborg', 'bonzini', 'mita', 'lunn', 'swhiteho', 'Alan-Cox', 'paulburton', 'dwmw2', 'kvaneesh', 'borkmann', 'suryasaimadhu', 'moinejf', 'mkrufky', 'error27', 'djwong', 'agd5f', 'robherring', 'ctmarinas', 'mstsirkin', 'mmind', 'pzanoni-intel', 'buytenh', 'storulf', 'vapier', 'horms', 'fujita', 'peterhurley', 'hharrison', 'ideak', 'hkallweit', 'jbarnes993', 'idosch', 'hreinecke', 'glikely', 'eparis', 'jirislaby', 'kiryl', 'amluto', 'kumargala', 'ebiggers', 'vineetgarc', 'RoelKluin', 'namhyung', 'xemul', 'liewegas', 'jpirko', 'jsgf', 'takaswie', 'vivien', 'shcgit', 'standby24x7', 'mlankhorst', 'jacob-keller', 'konradwilk', 'chleroy', 'charleskeepax', 'cfd-36', 'richardweinberger', 'ldewangan', 'mhennerich', 'computersforpeace', 'paraka', 'pebolle', 'sudipm-mukherjee', 'elp', 'westeri', 'kdave', 'gregungerer', 'joergroedel', 'yoshfuji', 'alexaring', 'michalsimek', 'dsahern', 'lxin', 'npiggin', 'npitre', 'ecsv', 'goyalbhumika', 'anholt', 'bwallan', 'hdeller', 'snitm', 'idryomov', 'lge', 'atsushi-nemoto', 'gclement', 'hauke', 'aegl', 'Philipp-Reisner', 'shimoday', 'agraf', 'navin-patidar', 'verygreen', 'Leo-Kim', 'jbrandeb', 'rientjes', 'lynxeye-dev', 'lumag', 'bwhacks', 'Srinivas-Kandagatla', 'IvDoorn', 'hallor', 'jgarzik', 'jsmart-gh', 'avasquez01', 'kgene', 'jcmvbkbc', 'robclark', 'Villemoes', 'borntraeger', 'ajaykathat', 'dlezcano', 'thomashvmw', 'tgraf', 'agners', 'tklauser', 'hnaz', 'juhosg', 'jjuhl', 'jbrun3t', 'BenRomer', 'ariknem', 'chanwoochoi', 'kishon', 'LorenzoBianconi', 'jgunthorpe', 'yhlu', 'rodrigovivi', 'Gnurou', 'ogerlitz', 'nmenon', 'vittyvk', 'Krzysztof-H', 'BroadcomOpenSource', 'superna9999', 'xdarklight', 'konis', 'tursulin', 'bentiss', 'cmetcalf-tilera', 'rleon', 'jeffmahoney', 'sara-s', 'mikey', 'jic23', 'johnstultz-work', 'jpoimboe', 'pmachata', 'rjarzmik', 'ij1', 'bmork', 'zhang-rui', 'andersson', 'lorddoskias', 'jamesasimmons', 'jan-kiszka', 'mawilli1', 'sonicz', 'kvalo', 'awilliam', 'hmh', 'jdelvare', 'ezequielgarcia', 'mhiramathitachi', 'lategoodbye', 'hkamezawa', 'fdmanana', 'shawn1221', 'WangNan0', 'johnfwhitmore', 'rmurphy-arm', 'dedekind', 'jasowang', 'seanyoung', 'NicolasDichtel', 'cricard13', 'mhiramat', 'brgl', 'jkivilin', 'mikuint', 'zonque', 'klassert', 'daviddaney', 'kaysievers', 'kkaneshige', 'ukernel', 'dlunev']",1,0.63,0,,,,,,36,,,,
23065862,MDEwOlJlcG9zaXRvcnkyMzA2NTg2Mg==,pure-data,pure-data/pure-data,0,pure-data,https://github.com/pure-data/pure-data,Pure Data -  a free real-time computer music system,0,2014-08-18 09:36:46+00:00,2025-03-07 19:19:38+00:00,2025-03-07 02:59:28+00:00,,63176,1678,1678,C,1,1,1,1,1,1,254,0,0,574,other,1,0,0,public,254,574,1678,master,1,1,,"['millerpuckette', 'umlaeute', 'porres', 'danomatika', 'Spacechild1', 'Lucarda', 'Ant1r', 'eighthave', 'chr15m', 'abreubacelar', 'ben-wes', 'eldruin', 'pierreguillot', 'jpcima', 'njazz', 'claudeha', 'mxa', 'sebshader', 'valeriorlandini', 'yarons', 'jyg', 'bapch', 'smokhov', 'lar0sa', 'AtrashDingDong', 'avilleret', 'gusano', 'idonotfollowyou', 'luzpaz', 'myQwil', 'fdch', 'jcelerier', 'ritsch', 'reduzent', 'rectang', 'orbsmiv', 'charlesneimog', 'x37v', 'ch-nry', 'timothyschoen', 'mathrax-s', 'HenriAugusto', 'H-MLim', 'rezaalmanda', 'maftkd', 'arno-arno', 'SantosSi', 'aaaaalbert', 'thodg', 'mrshpot', 'rnkn', 'NiallMoody', 'matteo-mazzanti', 'mnvr', 'lukexi', 'jamshark70', 'weblate', 'HeckHeckHeckHeck', 'AlexHarker', 'aib', 'etk70182', 'giuliomoro', 'mcclure', 'megrimm', 'mitchmindtree', 'no382001', 'antlauzon', 'pmallard', 'simmon-nplob', 'user706', 'caitp', 'wimmuskee', 'titola', 'Samson026', 'PhilipStolz', 'zrnsm', 'lhondareyte', 'lorenzosu', 'gravedadcerodev', 'chughes87', 'Asmatzaile', 'AlbertoZ33n', 'agraef']",1,0.75,0,,,,,,73,,,,
61595715,MDEwOlJlcG9zaXRvcnk2MTU5NTcxNQ==,radiant,radiant-rstats/radiant,0,radiant-rstats,https://github.com/radiant-rstats/radiant,"Business analytics using R and Shiny. The radiant app combines the menus from radiant.data, radiant.design, radiant.basics, radiant.model, and radiant.multivariate.",0,2016-06-21 02:26:27+00:00,2025-02-25 17:00:58+00:00,2024-05-21 04:50:40+00:00,https://radiant-rstats.github.io/docs/,13911,459,459,HTML,1,1,1,1,1,0,136,0,0,39,other,1,0,0,public,136,39,459,master,1,1,,"['vnijs', 'yarikoptic']",1,0.71,0,,,,,,20,,,,
392708700,MDEwOlJlcG9zaXRvcnkzOTI3MDg3MDA=,LibFewShot,RL-VIG/LibFewShot,0,RL-VIG,https://github.com/RL-VIG/LibFewShot,LibFewShot: A Comprehensive Library for Few-shot Learning. TPAMI 2023.,0,2021-08-04 13:58:20+00:00,2025-03-05 07:10:25+00:00,2025-01-11 07:16:22+00:00,,5262,951,951,Python,1,1,1,1,0,1,180,0,0,5,mit,1,0,0,public,180,5,951,main,1,1,,"['wZuck', 'yangcedrus', 'HRonaldo', 'VincenDen', 'Solitude-lyh', 'WenbinLee', 'Liaoliao-Lee', 'HeyDemons', 'Cbphcr', 'came666', 'zjz0804', 'stu-yue', 'yizhibaiwuya', 'Stella-67', 'AlephNullVEVO', 'Neo-0-Gu', 'onlyyao']",0,0.64,0,,,,,,26,,,,
262453387,MDEwOlJlcG9zaXRvcnkyNjI0NTMzODc=,pocket-tech-writing-list,rubymorillo/pocket-tech-writing-list,0,rubymorillo,https://github.com/rubymorillo/pocket-tech-writing-list,A small but formidable list of technical writing resources for developers,0,2020-05-09 00:06:10+00:00,2025-02-08 17:01:52+00:00,2024-01-05 10:54:24+00:00,,51,703,703,,1,1,1,1,0,0,56,0,0,2,,1,0,0,public,56,2,703,main,1,,"# The Pocket Technical Writing List 🗂
A small but formidable list of technical writing resources for software developers.

---
## Courses and Other Resources 👨🏾‍💻
- [Write the Docs](https://www.writethedocs.org)
  - [WtD Learning Resources](https://www.writethedocs.org/about/learning-resources/)
  - [WtD Slack Community](https://www.writethedocs.org/slack/)
- [Google's Technical Writing Courses](https://developers.google.com/tech-writing)
- [UC San Diego's Technical Communication I Course](https://extension.ucsd.edu/courses-and-programs/technical-communication-i)
- [Society for Technical Communication Online Technical Writing Courses](https://www.stc.org/education/online-courses/)
- [Ritza's free Writing for Coders Intensive](https://ritza.co/writing-for-coders-coding-for-writers-course.html)
- [Sample Technical Writing Template by Bolaji Ayodeji](https://github.com/BolajiAyodeji/technical-writing-template)
- [RunDOC, a library for generate docs by Richard Schneeman](https://github.com/schneems/rundoc)
- [How to Document APIs Guide](https://idratherbewriting.com/learnapidoc/)


## Articles 📰
- [A Brief Introduction to Technical Writing](https://www.stephaniemorillo.co/post/a-brief-introduction-to-technical-writing)
- [7 Pieces of Information to Include in Technical Blog Posts](https://www.stephaniemorillo.co/post/7-pieces-of-information-to-include-in-technical-blog-posts)
- [4 ways to keep distractions out of screenshots](https://ddbeck.com/hide-distractions-in-screenshots/)

## Books 📕
- [_How to Make Sense of Any Mess_ by Abby Covert](https://www.amazon.com/How-Make-Sense-Any-Mess/dp/1500615994)
- [_The Product is Docs_ by Christopher Gales and the Splunk Docs team](https://www.amazon.com/Product-Docs-technical-documentation-development/dp/1973589400)
- [_Information Architecture for the Web and Beyond_ by Louis Rosenfeld et. al.](https://www.amazon.com/dp/B015D78JV6/ref=cm_sw_r_tw_dp_U_x_VfYSEbV3F80K4)
- [_Technical Communication Today_ by Richard Johnson-Sheehan](https://www.amazon.com/gp/product/0205171192/ref=dbs_a_def_rwt_bibl_vppi_i6)
- [_Technical Communication, 13th Edition_ by Mike Markel et. al](https://www.macmillanlearning.com/college/us/product/Technical-Communication/p/1319245005)
- [_Technical Editing, 5th Edition_ by Carolyn D. Rude and Angela Eaton](https://www.amazon.com/Technical-Editing-Allyn-Seriesin-Communication/dp/0205786715/ref=pd_lpo_14_t_0/134-4344360-0879020?_encoding=UTF8&pd_rd_i=0205786715&pd_rd_r=41904240-3355-4fdb-b6da-60fd33075a91&pd_rd_w=4dINj&pd_rd_wg=pBam8&pf_rd_p=7b36d496-f366-4631-94d3-61b87b52511b&pf_rd_r=AT3FGPAN9H34TFK18QQT&psc=1&refRID=AT3FGPAN9H34TFK18QQT)

## Documentation Style Guides 📝
- [README Checklist](https://github.com/ddbeck/readme-checklist/blob/main/checklist.md) by Daniel Beck
- [MDN Web Docs Style Guide](https://developer.mozilla.org/en-US/docs/MDN/Guidelines/Writing_style_guide)
- [Splunk Style Guide](https://docs.splunk.com/Documentation/StyleGuide/current/StyleGuide/Howtouse)
- [Divio's documentation system](https://documentation.divio.com/)
- [Microsoft's Writing Style Guide](https://docs.microsoft.com/en-us/style-guide/welcome/)
- [Google's Developer Documentation Style Guide](https://developers.google.com/style)
- [DigitalOcean's Technical Writing Guidelines](https://www.digitalocean.com/community/tutorials/digitalocean-s-technical-writing-guidelines)
- [GitLab's Documentation Style Guide](https://docs.gitlab.com/ee/development/documentation/styleguide.html)
- [Checklist for Technical Writing (PDF)](http://emlab.utep.edu/pdfs/Checklist%20for%20Technical%20Writing.pdf)

## Paid Writing Opportunities😎
- [Tech Community Writing Programs compiled by Daniel Madalitso Phiri](https://github.com/malgamves/CommunityWriterPrograms)

---
List compiled by [Stephanie Morillo](https://www.stephaniemorillo.co/links). Found it useful? [Consider becoming a GitHub Sponsor!](https://www.github.com/sponsors/rubymorillo)
","['rubymorillo', 'Markel']",0,0.63,0,,,,,,21,,,,
43477752,MDEwOlJlcG9zaXRvcnk0MzQ3Nzc1Mg==,hed,s9xie/hed,0,s9xie,https://github.com/s9xie/hed,code for Holistically-Nested Edge Detection,0,2015-10-01 04:07:31+00:00,2025-03-06 15:25:08+00:00,2024-04-07 16:13:16+00:00,,2431,1829,1829,C++,1,1,1,1,0,0,532,0,0,75,other,1,0,0,public,532,75,1829,master,1,,,['s9xie'],1,0.65,0,,,,,,65,,,,
106319167,MDEwOlJlcG9zaXRvcnkxMDYzMTkxNjc=,Courses-,salimt/Courses-,0,salimt,https://github.com/salimt/Courses-,Answers for Quizzes & Assignments that I have taken,0,2017-10-09 18:17:15+00:00,2025-02-28 16:40:14+00:00,2023-05-30 17:30:51+00:00,,935515,683,683,Jupyter Notebook,1,1,1,1,1,0,703,0,0,10,apache-2.0,1,0,0,public,703,10,683,master,1,,"# Coursera and edX Assignments
This repository is aimed to help Coursera and edX learners who have difficulties in their learning process.  
The quiz and programming homework is belong to coursera and edx and solutions to me.


- #### [AWS Certified Solutions Architect - Professional](./AWS%20Cloud%20Solutions%20Architect%20Professional)

- #### [DevOps on AWS Specialization](./DevOps%20on%20AWS%20Specialization)

- #### [Open Source Software Development, Linux and Git Specialization](./Open%20Source%20Software%20Development%2C%20Linux%20and%20Git%20Specialization)

- #### [EDHEC - Investment Management with Python and Machine Learning Specialization](./EDHEC%20-%20Investment%20Management%20with%20Python%20and%20Machine%20Learning%20Specialization)

1. [EDHEC - Portfolio Construction and Analysis with Python](./EDHEC%20-%20Investment%20Management%20with%20Python%20and%20Machine%20Learning%20Specialization/EDHEC%20-%20Portfolio%20Construction%20and%20Analysis%20with%20Python)

2. [EDHEC - Advanced Portfolio Construction and Analysis with Python](./EDHEC%20-%20Investment%20Management%20with%20Python%20and%20Machine%20Learning%20Specialization/EDHEC%20-%20Advanced%20Portfolio%20Construction%20and%20Analysis%20with%20Python)


- #### [Google Data Analytics Professional Certificate](./Google%20Data%20Analytics%20Professional%20Certificate)

1. [Google - Foundations: Data, Data, Everywhere](./Google%20Data%20Analytics%20Professional%20Certificate/course1-%20Foundations%20Data%2C%20Data%2C%20Everywhere)

2. [Google - Ask Questions to Make Data-Driven Decisions](./Google%20Data%20Analytics%20Professional%20Certificate/course2-%20Ask%20Questions%20to%20Make%20Data-Driven%20Decisions)

3. [Google - Prepare Data for Exploration](./Google%20Data%20Analytics%20Professional%20Certificate/course3-%20Prepare%20Data%20for%20Exploration)

4. [Google - Process Data from Dirty to Clean](./Google%20Data%20Analytics%20Professional%20Certificate/course4-%20Process%20Data%20from%20Dirty%20to%20Clean)

5. [Google - Analyze Data to Answer Questions](./Google%20Data%20Analytics%20Professional%20Certificate/course5-%20Analyze%20Data%20to%20Answer%20Questions)

6. [Google - Share Data Through the Art of Visualization](./Google%20Data%20Analytics%20Professional%20Certificate/course6-%20Share%20Data%20Through%20the%20Art%20of%20Visualization)

7. [Google - Data Analysis with R Programming](./Google%20Data%20Analytics%20Professional%20Certificate/course7-%20Data%20Analysis%20with%20R%20Programming)

8. [Google - Google Data Analytics Capstone: Complete a Case Study](./Google%20Data%20Analytics%20Professional%20Certificate/course8-%20Google%20Data%20Analytics%20Capstone%20Complete%20a%20Case%20Study)

- #### [University of Michigan - PostgreSQL for Everybody Specialization](.//University%20of%20Michigan%20-%20PostgreSQL%20for%20Everybody%20Specialization)

- #### [The University of Melbourne & The Chinese University of Hong Kong - Basic Modeling for Discrete Optimization](./The%20University%20of%20Melbourne%20-%20Basic%20Modeling%20for%20Discrete%20Optimization)

- #### [Stanford University - Machine Learning](./Stanford%20University%20-%20Machine%20Learning)

- #### [Imperial College London - Mathematics for Machine Learning Specialization](./Imperial%20College%20London%20-%20Mathematics%20for%20Machine%20Learning%20Specialization)

1. [Imperial College London - Linear Algebra](./Imperial%20College%20London%20-%20Mathematics%20for%20Machine%20Learning%20Specialization/Imperial%20College%20London%20-%20Mathematics%20for%20Machine%20Learning%20Linear%20Algebra)

2. [Imperial College London - Multivariate Calculus](./Imperial%20College%20London%20-%20Mathematics%20for%20Machine%20Learning%20Specialization/Imperial%20College%20London%20-%20Mathematics%20for%20Machine%20Learning%20Multivariate%20Calculus)

- #### [University of Colorado Boulder - Excel/VBA for Creative Problem Solving Specialization](./CU-Boulder%20-%20Excel%20VBA%20for%20Creative%20Problem%20Solving%20Specialization)

1. [University of Colorado Boulder - Excel/VBA for Creative Problem Solving, Part 1](./CU-Boulder%20-%20Excel%20VBA%20for%20Creative%20Problem%20Solving%20Specialization/CU-Boulder%20-%20Excel%20VBA%20for%20Creative%20Problem%20Solving%2C%20Part%201)

- #### [University of Washington - Machine Learning Specialization](./University%20of%20Washington%20-%20Machine%20Learning%20Specialization)

1. [University of Washington - A Case Study Approach](./University%20of%20Washington%20-%20Machine%20Learning%20Specialization/University%20of%20Washington%20-%20Machine%20Learning%20Foundations%20A%20Case%20Study%20Approach)

2. [University of Washington - Regression](./University%20of%20Washington%20-%20Machine%20Learning%20Specialization/University%20of%20Washington%20-%20Machine%20Learning%20Regression)

- #### [Rice University - Python Data Representations](./Rice-Python-Data-Representations)

- #### [Rice University - Python Data Analysis](./Rice-Python-Data-Analysis)

- #### [Rice University - Python Data Visualization](./Rice-Python-Data-Visualization)

- #### [Johns Hopkins University - Data Science Specialization](./Johns%20Hopkins%20University%20-%20Data%20Science%20Specialization)

1. [Johns Hopkins University - R Programming](./Johns%20Hopkins%20University%20-%20Data%20Science%20Specialization/Johns%20Hopkins%20University%20-%20R%20Programming)

2. [Johns Hopkins University - Getting and Cleaning Data](./Johns%20Hopkins%20University%20-%20Data%20Science%20Specialization/Johns%20Hopkins%20University%20-%20Getting%20and%20Cleaning%20Data)

3. [Johns Hopkins University - Exploratory Data Analysis](./Johns%20Hopkins%20University%20-%20Data%20Science%20Specialization/Johns%20Hopkins%20University%20-%20Exploratory%20Data%20Analysis)

4. [Johns Hopkins University - Reproducible Research](./Johns%20Hopkins%20University%20-%20Data%20Science%20Specialization/Johns%20Hopkins%20University%20-%20Reproducible%20Research)

- #### [Saint Petersburg State University - Competitive Programmer's Core Skills](./Saint%20Petersburg%20State%20University%20-%20Competitive%20Programmer's%20Core%20Skills)

- #### [Rice University - Business Statistics and Analysis Capstone](./Rice%20University%20-%20Business%20Statistics%20and%20Analysis%20Capstone)

- #### [University of California, San Diego - Object Oriented Programming in Java](./UCSD-Object-Oriented-Programming-in-Java)

- #### [University of California, San Diego - Data Structures and Performance](./UCSD-Data-Structures-and-Performance)

- #### [University of California, San Diego - Advanced Data Structures in Java](./UCSD-Advanced-Data-Structures-in-Java)

- #### [IBM: Applied Data Science Specialization](./Applied-Data-Science-Specialization-IBM)

1. [IBM: Open Source tools for Data Science](./Applied-Data-Science-Specialization-IBM/IBM%20-%20Open%20Source%20tools%20for%20Data%20Science)

2. [IBM: Data Science Methodology](./Applied-Data-Science-Specialization-IBM/IBM%20-%20Data%20Science%20Methodology)

3. [IBM: Python for Data Science](./Applied-Data-Science-Specialization-IBM/IBM%20-%20Python%20for%20Data%20Science)

4. [IBM: Databases and SQL for Data Science](./Applied-Data-Science-Specialization-IBM/IBM%20-%20Databases%20and%20SQL%20for%20Data%20Science)

5. [IBM: Data Analysis with Python](./Applied-Data-Science-Specialization-IBM/IBM%20-%20Data%20Analysis%20with%20Python)

6. [IBM: Data Visualization with Python](./Applied-Data-Science-Specialization-IBM/IBM%20-%20Data%20Visualization%20with%20Python)

7. [IBM: Machine Learning with Python](./Applied-Data-Science-Specialization-IBM/IBM%20-%20Machine%20Learning%20with%20Python)

8. [IBM: Applied Data Science Capstone Project](./Applied-Data-Science-Specialization-IBM/IBM%20-%20Applied%20Data%20Science%20Capstone%20Project)

- #### [deeplearning.ai - TensorFlow in Practice Specialization](./deeplearning.ai%20-%20TensorFlow%20in%20Practice%20Specialization)

1. [deeplearning.ai - Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning](./deeplearning.ai%20-%20TensorFlow%20in%20Practice%20Specialization/deeplearning.ai%20-%20TensorFlow%20for%20AI%2C%20ML%2C%20and%20Deep%20Learning)

2. [deeplearning.ai - Convolutional Neural Networks in TensorFlow](./deeplearning.ai%20-%20TensorFlow%20in%20Practice%20Specialization/deeplearning.ai%20-%20Convolutional%20Neural%20Networks%20in%20TensorFlow)

3. [deeplearning.ai - Natural Language Processing in TensorFlow](./deeplearning.ai%20-%20TensorFlow%20in%20Practice%20Specialization/deeplearning.ai%20-%20Natural%20Language%20Processing%20in%20TensorFlow)

4. [deeplearning.ai - Sequences, Time Series and Prediction](./deeplearning.ai%20-%20TensorFlow%20in%20Practice%20Specialization/deeplearning.ai%20-%20Sequences%2C%20Time%20Series%20and%20Prediction)

- #### [Alberta Machine Intelligence Institute - Machine Learning Algorithms: Supervised Learning Tip to Tail](./Amii%20-%20Machine%20Learning%20Algorithms)

- #### [University of Helsinki: Object-Oriented Programming with Java, part I](./Object-Oriented-Programming-with-Java-pt1-University-of%20Helsinki-moocfi)

- #### [The Hong Kong University of Science and Technology - Python and Statistics for Financial Analysis](./HKUST%20%20-%20Python%20and%20Statistics%20for%20Financial%20Analysis)

- #### [Google IT Automation with Python Professional Certificate](./Google%20IT%20Automation%20with%20Python)

1. [Google - Crash Course on Python](./Google%20IT%20Automation%20with%20Python/Google%20-%20Crash%20Course%20on%20Python)

2. [Google - Using Python to Interact with the Operating System](./Google%20IT%20Automation%20with%20Python/Google%20-%20Using%20Python%20to%20Interact%20with%20the%20Operating%20System)


- #### [Delft University of Technology - Automated Software Testing](./Delft%20University%20of%20Technology%20-%20Automated%20Software%20Testing)

- #### [University of Maryland, College Park: Cybersecurity Specialization](./University%20of%20Maryland%20-%20Cybersecurity%20Specialization) 

1. [University of Maryland, College Park: Software Security](./University%20of%20Maryland%20-%20Cybersecurity%20Specialization/University%20of%20Maryland%20-%20Software%20Security)

2. [University of Maryland, College Park: Usable Security](./University%20of%20Maryland%20-%20Cybersecurity%20Specialization/University%20of%20Maryland%20-%20Usable%20Security)

- #### [University of Maryland, College Park: Programming Mobile Applications for Android Handheld Systems: Part 1](./University%20of%20Maryland%20-%20Programming%20Mobile%20Applications%20for%20Android%20Handheld%20Systems%2C%20Part%20I)


- #### [Harvard University - Introduction to Computer Science CS50x](./Harvard-CS50x)

- #### [Duke University - Java Programming: Principles of Software Design](./Duke-Java-Programming-Principles-of-Software-Design)

- #### [Duke University - Java Programming: Solving Problems with Software](./Duke-Java-Programming-Solving-Problems-with-Software)

- #### [Duke University - Java Programming: Arrays, Lists, and Structured Data](./Duke-Java-Programming-Arrays-Lists-Structured-Data)

- #### [Duke University - Data Science Math Skills](./Duke-University-Data-Science-Math-Skills)

- #### [Massachusetts Institute of Technology - Introduction to Computer Science and Programming Using Python 6.00.1x](./MITx-6.00.1x)

- #### [Massachusetts Institute of Technology - Introduction to Computational Thinking and Data Science 6.00.2x](./MITx-6.00.2x)

- #### [Johns Hopkins University: Ruby on Rails Web Development Specialization](./Johns%20Hopkins%20University%20-%20Ruby%20on%20Rails%20Web%20Development%20Specialization)

1. [Johns Hopkins University - Ruby on Rails](./Johns%20Hopkins%20University%20-%20Ruby%20on%20Rails%20Web%20Development%20Specialization/Johns%20Hopkins%20University%20-%20Ruby%20on%20Rails)

2. [Johns Hopkins University - Rails with Active Record and Action Pack](./Johns%20Hopkins%20University%20-%20Ruby%20on%20Rails%20Web%20Development%20Specialization/Johns%20Hopkins%20University%20-%20Rails%20with%20Active%20Record%20and%20Action%20Pack)

3. [Johns Hopkins University - Ruby on Rails Web Services and Integration with MongoDB](./Johns%20Hopkins%20University%20-%20Ruby%20on%20Rails%20Web%20Development%20Specialization/JHU%20-%20Ruby%20on%20Rails%20Web%20Services%20and%20Integration%20with%20MongoDB)

4. [Johns Hopkins University - HTML, CSS, and Javascript for Web Developers](./Johns%20Hopkins%20University%20-%20Ruby%20on%20Rails%20Web%20Development%20Specialization/Johns%20Hopkins%20University%20-%20HTML%2C%20CSS%2C%20and%20Javascript%20for%20Web%20Developers)

5. [Johns Hopkins University - Single Page Web Applications with AngularJS](./Johns%20Hopkins%20University%20-%20Ruby%20on%20Rails%20Web%20Development%20Specialization/Johns%20Hopkins%20University%20-%20Single%20Page%20Web%20Applications%20with%20AngularJS)

- #### [University of Michigan - Web Design for Everybody: Web Development & Coding Specialization](./University%20of%20Michigan%20-%20Web%20Design%20for%20Everybody)

1. [University of Michigan - HTML5](./University%20of%20Michigan%20-%20Web%20Design%20for%20Everybody/University%20of%20Michigan%20-%20%20HTML5)

2. [University of Michigan - CSS3](./University%20of%20Michigan%20-%20Web%20Design%20for%20Everybody/University%20of%20Michigan%20-%20%20CSS3)

3. [University of Michigan -  Interactivity with JavaScript](./University%20of%20Michigan%20-%20Web%20Design%20for%20Everybody/University%20of%20Michigan%20-%20%20Interactivity%20with%20JavaScript)

- #### [Stanford University - Introduction to Mathematical Thinking](./Stanford-University-Introduction-to-Mathematical-Thinking)

- #### [University of London - Responsive Website Development and Design Specialization](./University%20of%20London%20-%20Responsive%20Website%20Development%20and%20Design%20Specialization)

1. [University of London - Responsive Web Design](./University%20of%20London%20-%20Responsive%20Website%20Development%20and%20Design%20Specialization/University%20of%20London%20-%20Responsive%20Web%20Design)

2. [University of London - Web Application Development with JavaScript and MongoDB](./University%20of%20London%20-%20Responsive%20Website%20Development%20and%20Design%20Specialization/University%20of%20London%20-%20Web%20Application%20Development%20with%20JavaScript%20and%20MongoDB)

3. [University of London - Responsive Website Tutorial and Examples](./University%20of%20London%20-%20Responsive%20Website%20Development%20and%20Design%20Specialization/University%20of%20London%20-%20Responsive%20Website%20Tutorial%20and%20Examples)

- #### [University of California, San Diego - Biology Meets Programming: Bioinformatics](./UCSD%20-%20Biology%20Meets%20Programming%20Bioinformatics)

- #### [University of Toronto - Learn to Program: The Fundamentals](./University-of-Toronto-The%20Fundamentals)

- #### [University of Toronto - Learn to Program: Crafting Quality Code](./University-of-Toronto-Crafting-Quality-Code)

- #### [University of British Columbia - How to Code: Simple Data HtC1x](./UBCx-HtC1x)

- #### [University of British Columbia - Software Construction: Data Abstraction](./UBCx-Software-Construction-Data-Abstraction-SoftConst1x)

- #### [University of British Columbia - Software Construction: Object-Oriented Design](./UBCx-Software-Construction-OOP-SoftConst2x)


","['salimt', 'dependabot[bot]']",0,0.64,0,,,,,,50,,,,
131909766,MDEwOlJlcG9zaXRvcnkxMzE5MDk3NjY=,eeglab,sccn/eeglab,0,sccn,https://github.com/sccn/eeglab,EEGLAB is an open source signal processing environment for electrophysiological signals running on Matlab and developed at the SCCN/UCSD,0,2018-05-02 21:53:50+00:00,2025-03-08 01:06:47+00:00,2025-02-18 00:37:45+00:00,https://eeglab.ucsd.edu/,93540,638,638,MATLAB,1,1,1,0,1,0,247,0,0,47,other,1,0,0,public,247,47,638,develop,1,1,,"['smakeig', 'arnodelorme', 'nucleuscub', 'tfernsle', 'bigdelys', 'CPernet', 'dpsarma', 'cll008', 'dungscout96', 'jiversen', 'jimatbrock', 'tstenner', 'rgougelet', 'allenbgruber', 'AminAlam', 'neuromechanist', 'amisepa', 'dellabellagabriel', 'DimitriPapadopoulos', 'widmann', 'arnevdk', 'behinger', 'lucapton', 'w222chen', 'sjeung', 'phoebelilius', 'lixiaomin328', 'bknakker', 'SCCN-EEGLAB', 'robertoostenveld', 'NirOfir', 'KlausGramannTUB', 'schoffelen', 'fardinafdideh', 'drpedapati', 'ammsimmons']",1,0.77,0,"# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
email.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.
","# Contributing to EEGLAB

There are several ways in which you can contribute to the ongoing development of EEGLAB:

## Bug reporting

1. Please search on both the [EEGLAB discussion list archive](https://sccn.ucsd.edu/pipermail/eeglablist/)
   (to search the list Google ""eeglablist your question"")
   and the [GitHub issue list](https://github.com/sccn/eeglab/issues)
   to see if anybody else has lodged a similar observation.

3. How confident are you that the behavior you have observed is in fact a
   genuine bug, and not a misunderstanding?

   -  *Confident*: Please [open a new GitHub issue](https://github.com/sccn/eeglab/issues/new);
      select the ""bug report"" issue template to get started.

   -  *Not so confident*: That's fine! Consider instead creating a new topic
      on the [EEGLAB discussion list](https://eeglab.org/others/EEGLAB_mailing_lists.html);
      others can then comment on your observation and determine the
      appropriate level of escalation.

## Requesting a new feature

Please search the [GitHub issue list](https://github.com/sccn/eeglab/issues)
to see if anybody else has made a comparable request:

   -  If a corresponding issue already exists, please add a comment to that
      issue to escalate the request. Additionally, describe any
      aspect of that feature not yet described in the existing issue.

   -  If no such listing exists, then you are welcome to create a [new
      issue](https://github.com/sccn/eeglab/issues/new) outlining the
      request. Be sure to select the ""feature request"" option to get started
      with writing the issue.

## Asking questions

General questions regarding EEGLAB download, usage, or any other
aspect that is not specific to the EEGLAB *code*, should be directed to
the [EEGLAB discussion list](https://eeglab.org/others/EEGLAB_mailing_lists.html). Also check
the [online documentation](https://eeglab.org/).

## Making direct contributions

Thanks for your interest in making direct contributions to EEGLAB!
We are excited to expand the breadth of researchers involved in improving
and expanding this software, and to ensure that all who make such
contributions receive appropriate acknowledgment through Git.

The instructions below give a short overview of how to go about generating a
proposed change to EEGLAB. A more detailed tutorial on using Git and contributing
to the code (or website) is presented as [online tutorial](https://eeglab.org/tutorials/contribute/)
on the EEGLAB website.

1. You will need to create a *fork* of the [EEGLAB repository](https://github.com/sccn/eeglab)
   into your GitHub account, where unlike the main EEGLAB repository,
   you will have full write access to make the requisite changes.

2. Create a Git branch that is named appropriately according to the
   modifications that are being made. The existing code branch on which
   this new derived branch should be based depends on the nature of the
   proposed change (described later below).

3. Generate one or more Git commits that apply your proposed changes to
   the repository:

   -  Individual commits should ideally have a clear singular purpose,
      and not incorporate multiple unrelated changes. If your proposed
      changes involve multiple disparate components, consider breaking
      those changes up into individual commits.

      Conversely, if multiple code changes are logically grouped with /
      linked to one another, these should ideally be integrated into a
      single commit.

   -  Commits should contain an appropriate message that adequately
      describes the change encapsulated within.

      If the change demands a longer description, then the commit message
      should be broken into a synopsis (less than 80 characters) and message
      body, separated by two newline characters (as this enables GitHub to
      parse them appropriately).

      This can be achieved at the command-line as follows:

      `$ git commit -m $'Commit synopsis\n\nHere is a much longer and wordier description of my proposed changes that doesn\'t fit into 80 characters.\nI can even spread the message body across multiple lines.'`

      (Note also the escape character ""`\`"" necessary for including an
      apostrophe in the message text)

   -  Where relevant, commit messages can also contain references to
      GitHub issues or pull requests (type the ""`#`"" character followed
      by the issue / PR number), and/or other individual commits (copy
      and paste the first 8-10 characters of the commit hash).

   -  If multiple persons have contributed to the proposed changes, it is
      possible to modify individual Git commits to have [multiple
      authors](https://help.github.com/en/articles/creating-a-commit-with-multiple-authors),
      to ensure that all contributors receive appropriate acknowledgement.

   As a general rule: Git commits and commit messages should be constructed
   in such a way that, at some time in the future, when one is navigating
   through the contribution history, the evolution of the code is as clear
   as possible.

4. Identify the appropriate classification of the change that you propose
   to make, and read the relevant instructions there:

   -  ""[**Fix**](#fix)"": If the current code behaviour is
      *clearly incorrect*.

   -  ""[**Enhancement**](#enhancement)"": If the proposed change improves the *performance* or extends the *functionality* of a particular command or process.

   -  ""[**Documentation**](#documentation)"": If you made some changes to the function description in the help section of the function.

5. Check that your modified code does not prevent EEGLAB from
   passing existing tests, wherever possible (all test files are in the EEGLAB test directory).

6. For code contributions, if possible, a unit test or reproducibility
   test should be added. This not only demonstrates the behavior of the
   proposed code, but will also preclude future regression of the behavior
   of that code.

1. Once completed, a Pull Request should be generated that merges the
   corresponding branch in your forked version of the EEGLAB repository
   into the appropriate target branch of the main EEGLAB repository
   itself:

   -  If your intended changes are complete, and you consider them ready
      to be reviewed by an EEGLAB team member and merged imminently,
      then create a standard Pull Request.

   -  If your changes are ongoing, and you are seeking feedback from the
      EEGLAB developers before completing them, then also create a standard pull
      request. When you modify your code, the pull request will automatically
      be updated.

#### Fix

1. If there does not already exist a [GitHub issue](https://github.com/sccn/eeglab/issues)
   describing the bug, consider reporting the bug as a standalone issue
   prior to progressing further; that way developers can confirm the issue,
   and possibly provide guidance if you intend to resolve the issue yourself.
   Later, the Pull Request incorporating the necessary changes should then
   reference the listed issue (simply add somewhere in the description
   section the ""`#`"" character followed by the issue number).

2. Bug fixes are merged directly to `master`; as such, modifications to the
   code should be made in a branch that is derived from `master`, and the
   corresponding Pull Request should select `master` as the target branch
   for code merging.

3. A unit test or reproducibility test should ideally be added: such a
   test should fail when executed using the current `master` code, but pass
   when executed with the proposed changes.

#### Enhancement

1. New features, as well as any code changes that extend the functionality of
   EEGLAB, are merged to the `develop` branch, which contains
   all resolved changes since the most recent tag update. As such, any
   proposed changes that fall under this classification should be made
   in a branch that is based off of the `develop` branch, and the corresponding
   Pull Request should select `develop` as the target branch for code merging.

#### Documentation

If you want to contribute to the documentation on the EEGLAB website, please refer to the website's [Github repository](https://github.com/sccn/sccn.github.io).

#### Coding conventions

Please follow the [MATLAB style guidelines](https://www.mathworks.com/matlabcentral/fileexchange/46056-matlab-style-guidelines-2-0) for guidelines on contributing to the EEGLAB code base.

Non-exhaustive summary of coding guidelines:

* 2 space indents; indent using spaces and not tabs
* No spaces between function name and opening parenthesis of argument list
* One space after the comma that separates function arguments
* Vertically align code with spaces in case it improves readability

#### References

This document is based on the excellent CONTRIBUTING.md document from the [MRTRIX repository](https://github.com/MRtrix3/mrtrix3/), and adjusted accordingly. 
",,Directory exists,,26,,,,
15168264,MDEwOlJlcG9zaXRvcnkxNTE2ODI2NA==,scikit-bio,scikit-bio/scikit-bio,0,scikit-bio,https://github.com/scikit-bio/scikit-bio,"scikit-bio: a community-driven Python library for bioinformatics, providing versatile data structures, algorithms and educational resources.",0,2013-12-13 16:24:41+00:00,2025-03-07 02:44:14+00:00,2025-01-24 18:37:20+00:00,https://scikit.bio,27851,914,914,Python,1,1,1,1,0,1,268,0,0,242,bsd-3-clause,1,0,0,public,268,242,914,main,1,1,"|license| |build| |coverage| |bench| |release| |pypi| |conda| |gitter|

.. image:: logos/logo.svg
   :width: 600 px
   :target: https://scikit.bio
   :alt: scikit-bio logo

*scikit-bio is an open-source, BSD-licensed Python 3 package providing data structures, algorithms and educational resources for bioinformatics.*

Visit the scikit-bio website: https://scikit.bio to learn more about this project.


Releases
--------

Latest release: `0.6.3 <https://github.com/scikit-bio/scikit-bio/releases/tag/0.6.3>`_ (`documentation <https://scikit.bio/docs/0.6.3/index.html>`_, `changelog <https://github.com/scikit-bio/scikit-bio/blob/main/CHANGELOG.md#version-063>`_). Compatible with Python 3.9 and above.


Installation
------------

Install the latest release of scikit-bio using ``conda``::

    conda install -c conda-forge scikit-bio

Or using ``pip``::

    pip install scikit-bio

See further `instructions on installing <https://scikit.bio/install.html>`_ scikit-bio on various platforms.


Adoption
--------

Some of the projects that we know of that are using scikit-bio are:

- `QIIME 2 <https://qiime2.org/>`_, `Qiita <https://qiita.ucsd.edu/>`_, `Emperor <https://biocore.github.io/emperor/>`_, `tax2tree <https://github.com/biocore/tax2tree>`_, `ghost-tree <https://github.com/JTFouquier/ghost-tree>`_, `Platypus-Conquistador <https://github.com/biocore/Platypus-Conquistador>`_, `An Introduction to Applied Bioinformatics <https://readiab.org>`_.


License
-------

scikit-bio is available under the new BSD license. See `LICENSE.txt <LICENSE.txt>`_ for scikit-bio's license, and the `licenses directory <licenses>`_ for the licenses of third-party software that is (either partially or entirely) distributed with scikit-bio.


Team
----

Our core development team consists of three lead developers: **Dr. Qiyun Zhu** at Arizona State University (ASU) (@qiyunzhu), **Dr. James Morton** at Gutz Analytics (@mortonjt), and **Dr. Daniel McDonald** at the University of California San Diego (UCSD) (@wasade), one software engineer: **Matthew Aton** (@mataton) and one bioinformatician: **Dr. Lars Hunger** (@LarsHunger). **Dr. Rob Knight** at UCSD (@rob-knight) provides guidance on the development and research. **Dr. Greg Caporaso** (@gregcaporaso) at Northern Arizona University (NAU), the former leader of the scikit-bio project, serves as an advisor on the current project.


Credits
-------

We thank the many contributors to scikit-bio. A complete `list of contributors <graphs/contributors>`_ to the scikit-bio codebase is available at GitHub. This however may miss the larger community who contributed by testing the software and providing valuable comments, who we hold equal appreciation to.

Wanna contribute? We enthusiastically welcome community contributors! Whether it's adding new features, improving code, or enhancing documentation, your contributions drive scikit-bio and open-source bioinformatics forward. Start your journey by reading the `Contributor's guidelines <https://scikit.bio/contribute.html>`_.


Funding
-------

The development of scikit-bio is currently supported by the U.S. Department of Energy, Office of Science under award number `DE-SC0024320 <https://genomicscience.energy.gov/compbioawards2023/#Expanding>`_, awarded to Dr. Qiyun Zhu at ASU (lead PI), Dr. James Morton at Gutz Analytics, and Dr. Rob Knight at UCSD.


Citation
--------

If you use scikit-bio for any published research, please see our `Zenodo page <https://zenodo.org/doi/10.5281/zenodo.593387>`_ for how to cite.


Collaboration
-------------

For collaboration inquiries and other formal communications, please reach out to **Dr. Qiyun Zhu** at `qiyun.zhu@asu.edu`. We welcome academic and industrial partnerships to advance our mission.


Branding
--------

The logo of scikit-bio was created by `Alina Prassas <https://cargocollective.com/alinaprassas>`_. Vector and bitmap image files are available at the `logos <logos>`_ directory.


Pre-history
-----------

scikit-bio began from code derived from `PyCogent <https://github.com/pycogent/pycogent>`_ and `QIIME <https://github.com/biocore/qiime>`_, and the contributors and/or copyright holders have agreed to make the code they wrote for PyCogent and/or QIIME available under the BSD license. The contributors to PyCogent and/or QIIME modules that have been ported to scikit-bio are listed below:

- Rob Knight (@rob-knight), Gavin Huttley (@gavinhuttley), Daniel McDonald (@wasade), Micah Hamady, Antonio Gonzalez (@antgonza), Sandra Smit, Greg Caporaso (@gregcaporaso), Jai Ram Rideout (@jairideout), Cathy Lozupone (@clozupone), Mike Robeson (@mikerobeson), Marcin Cieslik, Peter Maxwell, Jeremy Widmann, Zongzhi Liu, Michael Dwan, Logan Knecht (@loganknecht), Andrew Cochran, Jose Carlos Clemente (@cleme), Damien Coy, Levi McCracken, Andrew Butterfield, Will Van Treuren (@wdwvt1), Justin Kuczynski (@justin212k), Jose Antonio Navas Molina (@josenavas), Matthew Wakefield (@genomematt) and Jens Reeder (@jensreeder).


.. |license| image:: https://img.shields.io/badge/License-BSD%203--Clause-blue.svg
   :alt: License
   :target: https://opensource.org/licenses/BSD-3-Clause
.. |build| image:: https://github.com/scikit-bio/scikit-bio/actions/workflows/ci.yml/badge.svg
   :alt: Build Status
   :target: https://github.com/scikit-bio/scikit-bio/actions/workflows/ci.yml
.. |coverage| image:: https://codecov.io/gh/scikit-bio/scikit-bio/graph/badge.svg?token=1qbzC6d2F5 
   :alt: Coverage Status
   :target: https://codecov.io/gh/scikit-bio/scikit-bio
.. |bench| image:: https://img.shields.io/badge/benchmarked%20by-asv-green.svg
   :alt: ASV Benchmarks
   :target: https://s3-us-west-2.amazonaws.com/scikit-bio.org/benchmarks/main/index.html
.. |release| image:: https://img.shields.io/github/v/release/scikit-bio/scikit-bio.svg
   :alt: Release
   :target: https://github.com/scikit-bio/scikit-bio/releases
.. |pypi| image:: https://img.shields.io/pypi/dm/scikit-bio.svg?label=PyPI%20downloads
   :alt: PyPI Downloads
   :target: https://pypi.org/project/scikit-bio/
.. |conda| image:: https://img.shields.io/conda/dn/conda-forge/scikit-bio.svg?label=Conda%20downloads
   :alt: Conda Downloads
   :target: https://anaconda.org/conda-forge/scikit-bio
.. |gitter| image:: https://badges.gitter.im/Join%20Chat.svg
   :alt: Gitter
   :target: https://gitter.im/biocore/scikit-bio
","['jairideout', 'gregcaporaso', 'ebolyen', 'wasade', 'ElDeveloper', 'Jorge-C', 'anderspitman', 'mortonjt', 'qiyunzhu', 'josenavas', 'kestrelgorlick', 'jwdebelius', 'RNAer', 'mataton', 'llcooljohn', 'squirrelo', 'laurentluce', 'wdwvt1', 'johnchase', 'charudatta-navare', 'antgonza', 'colinbrislawn', 'patena', 'kschwarzberg', 'teravest', 'jensreeder', 'sfiligoi', 'shiffer1', 'nbresnick', 'kdm9', 'christapo', 'aryankeluskar', 'mr-karan', 'alexbrc', 'jradinger', 'raivivek', 'bunop', 'mr-c', 'thermokarst', 'HannesHolste', 'terrycojones', 'nirmithah', 'constellation99', 'stevendbrown', 'sjanssen2', 'paarth-b', 'hotdogee', 'BANSHEE-', 'theAeon', 'LarsHunger', 'luispedro', 'mandel01', 'genomematt', 'maxibor', 'michaelsilverstein', 'mwilliamson', 'MikeDeberg', 'niemasd', 'RaeedA', 'rohithpudari', 'seberg', 'adamrp', 'code-wolf-byte', 'hsiaoyi0504', 'KOLANICH', 'jakereps', 'tanaes', 'jmeppley', 'JTFouquier', 'grst', 'Deeeeen', 'dansondergaard', 'danodonovan', 'clintval', 'bsmith89', 'Bharath-Sathappan', 'AnnaTruzzi', 'ajtritt', 'actapia']",1,0.65,0,,,,,"Please complete the following checklist:

* [ ] I have read the [contribution guidelines](https://scikit.bio/contribute.html).

* [ ] I have documented all public-facing changes in the [changelog](https://github.com/scikit-bio/scikit-bio/blob/main/CHANGELOG.md).

* [ ] **This pull request includes code, documentation, or other content derived from external source(s).** If this is the case, ensure the external source's license is compatible with scikit-bio's [license](https://github.com/scikit-bio/scikit-bio/blob/main/LICENSE.txt). Include the license in the `licenses` directory and add a comment in the code giving proper attribution. Ensure any other requirements set forth by the license and/or author are satisfied.

  - **It is your responsibility to disclose** code, documentation, or other content derived from external source(s). If you have questions about whether something can be included in the project or how to give proper attribution, include those questions in your pull request and a reviewer will assist you.

* [ ] This pull request does not include code, documentation, or other content derived from external source(s).

Note: [This document](https://scikit.bio/devdoc/review.html) may also be helpful to see some of the things code reviewers will be verifying when reviewing your pull request.
",51,,,,
93882897,MDEwOlJlcG9zaXRvcnk5Mzg4Mjg5Nw==,slang,shader-slang/slang,0,shader-slang,https://github.com/shader-slang/slang,Making it easier to work with shaders,0,2017-06-09 17:42:49+00:00,2025-03-08 07:53:15+00:00,2025-03-07 20:21:11+00:00,http://shader-slang.com,56697,3666,3666,C++,1,1,1,1,1,1,257,0,0,337,other,1,0,0,public,257,337,3666,master,1,1,,"['csyonghe', 'jsmall-zzz', 'tangent-vector', 'expipiplus1', 'saipraveenb25', 'jkwak-work', 'kaizhangNV', 'ArielG-NV', 'aleino-nv', 'skallweitNV', 'arquelion', 'cheneym2', 'fairywreath', 'venkataram-nv', 'juliusikkala', 'natevm', 'Checkmate50', 'chuckgenome', 'apanteleev', 'sriramm-nv', 'mkeshavaNV', 'pmistryNV', 'kyaoNV', 'winmad', 'eliemichel', 'waywardmonkeys', 'Dynamitos', 'burak-efe', 'dsiher', 'westlicht', 'ccummingsNV', 'tgrimesnv', 'cek', 'NBickford-NV', 'kopaka1822', 'swoods-nv', 'chaoticbob', 'Devon7925', 'mighdoll', 'jarcherNV', 'zopsicle', 'bprb', 'yknishidate', 'aroidzap', 'SirKero', 'pema99', 'fknfilewalker', 'KoT3isGood', 'KhronosWebservices', '1ace', 'davidar', 'arcady-lunarg', 'ZanderMajercik', 'vincentisambart', 'zlatinski', 'tobyc11', 'TheJackiMonster', 'theHamsta', 'stanoddly', 'skogler', 'Qubaef', 'PeturDarri', 'philcn', 'AdamYuan', 'arya3d', 'bspeice', 'c00t', 'dubiousconst282', 'dzysk', 'exdal', 'gan74', 'ksavoie-nv', 'mTvare6', 'sdawson-nvidia', 'spking11', 'tareksander', 'wijiler', 'AlexAUT', 'ennis', 'Lephar', 'abadams', '0xafbf', 'englercj', 'chrisd-work', 'colinmarc', 'ramenguy99', 'behindthepixels', 'entropylost', 'HappyLi3', 'JKot-Coder', 'James2022-rgb', 'jesta88', 'kanashimia', 'kmshanah', 'gusandrianos', 'laurooyen', 'chaosink', 'mmp', 'mmoult', 'Nimanf', 'McNopper', 'pablode']",1,0.66,33148,"A reminder that this issue tracker is managed by the Khronos Group. Interactions here should follow the Khronos Code of Conduct ([https://www.khronos.org/developers/code-of-conduct](https://www.khronos.org/developers/code-of-conduct)), which prohibits aggressive or derogatory language. Please keep the discussion friendly and civil.
","# Shader-Slang Open Source Project

## Contribution Guide

Thank you for considering contributing to the Shader-Slang project! We welcome your help to improve and enhance our project. Please take a moment to read through this guide to understand how you can contribute.

This document is designed to guide you in contributing to the project. It is intended to be easy to follow without sending readers to other pages and links. You can simply copy and paste the command lines described in this document.

* Contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant the rights to use your contribution.
* When you submit a pull request, a CLA bot will determine whether you need to sign a CLA. Simply follow the instructions provided.
* Please read and follow the contributor [Code of Conduct](CODE_OF_CONDUCT.md).
* Bug reports and feature requests should be submitted via the GitHub issue tracker.
* Changes should ideally come in as small pull requests on top of master, coming from your own personal fork of the project.
* Large features that will involve multiple contributors or a long development time should be discussed in issues and broken down into smaller pieces that can be implemented and checked in stages.

## Table of Contents
1. [Contribution Process](#contribution-process)
   - [Forking the Repository](#forking-the-repository)
   - [Cloning Your Fork](#cloning-your-fork)
   - [Creating a Branch](#creating-a-branch)
   - [Build Slang from Source](#build-slang-from-source)
   - [Making Changes](#making-changes)
   - [Testing](#testing)
   - [Commit to the Branch](#commit-to-the-branch)
   - [Push to Forked Repository](#push-to-forked-repository)
2. [Pull Request](#pull-request)
   - [Addressing Code Reviews](#addressing-code-reviews)
   - [Labeling Breaking Changes](#labeling-breaking-changes)
   - [Source Code Formatting](#source-code-formatting)
   - [Document Changes](#document-changes)
3. [Code Style](#code-style)
4. [Issue Tracking](#issue-tracking)
5. [Communication](#communication)
6. [License](#license)

## Contribution Process

### Forking the Repository
Navigate to the [Shader-Slang repository](https://github.com/shader-slang/slang).
Click on the ""Fork"" button in the top right corner to create a copy of the repository in your GitHub account.
This document will assume that the name of your forked repository is ""slang"".
Make sure your ""Actions"" are enabled. Visit your forked repository, click on the ""Actions"" tab, and enable the actions.

### Cloning Your Fork
1. Clone your fork locally, replacing ""USER-NAME"" in the command below with your actual username.
   ```
   $ git clone --recursive --tags https://github.com/USER-NAME/slang.git
   $ cd slang
   ```

2. Fetch tags by adding the original repository as an upstream.
   It is important to have tags in your forked repository because our workflow/action uses the information for the build process. But the tags are not fetched by default when you fork a repository in GitHub. You need to add the original repository as an upstream and fetch tags manually.
   ```
   $ git remote add upstream https://github.com/shader-slang/slang.git
   $ git fetch --tags upstream
   ```

   You can check whether the tags are fetched properly with the following command.
   ```
   $ git tag -l
   ```

3. Push tags to your forked repository.
   The tags are fetched to your local machine but haven't been pushed to the forked repository yet. You need to push tags to your forked repository with the following command.
   ```
   $ git push --tags origin
   ```

### Creating a Branch
Create a new branch for your contribution:
```
$ git checkout -b feature/your-feature-name
```

### Build Slang from Source
Please follow the instructions on how to [Build Slang from Source](docs/building.md).

For a quick reference, follow the instructions below.

#### Windows
Download and install CMake from [CMake.org/download](https://cmake.org/download).

Run CMake with the following command to generate a Visual Studio 2022 Solution:
```
C:\git\slang> cmake.exe --preset vs2022 # For Visual Studio 2022
C:\git\slang> cmake.exe --preset vs2019 # For Visual Studio 2019
```

Open `build/slang.sln` with Visual Studio IDE and build it for ""x64"".

Or you can build with the following command:
```
C:\git\slang> cmake.exe --build --preset release
```

On Windows ARM64, prebuilt binaries for LLVM isn't available.
Please build Slang without LLVM dependency by running:

```
cmake.exe --preset vs2022 -DSLANG_SLANG_LLVM_FLAVOR=DISABLE
```

during configuration step.

#### Linux
Install CMake and Ninja.
```
$ sudo apt-get install cmake ninja-build
```
> Warning: Currently the required CMake version is 3.25 or above.

Run CMake with the following command to generate Makefile:
```
$ cmake --preset default
```

Build with the following command:
```
$ cmake --build --preset release
```

#### MacOS
Install Xcode from the App Store.

Install CMake and Ninja; we recommend using [Homebrew](https://brew.sh/) for installing them.
```
$ brew install ninja
$ brew install cmake
```

Run CMake with the following command to generate Makefile:
```
$ cmake --preset default
```

Build with the following command:
```
$ cmake --build --preset release
```

#### Building with a Local Build of slang-llvm
`slang-llvm` is required to run `slang-test` properly.
Follow the instructions below if you wish to build `slang-llvm` locally.
```
$ external/build-llvm.sh --source-dir build/slang-llvm_src --install-prefix build/slang-llvm_install
```

> Note: On Windows you can use `external/build-llvm.ps1` in Powershell.

You need to use the following command to regenerate the Makefile:
```
$ cmake --preset default --fresh -DSLANG_SLANG_LLVM_FLAVOR=USE_SYSTEM_LLVM -DLLVM_DIR=build/slang-llvm_install/lib/cmake/llvm -DClang_DIR=build/slang-llvm_install/lib/cmake/clang
```

Build with the following command:
```
$ cmake --build --preset release
```

#### GitHub REST API Limit
When you execute `cmake --preset`, CMake uses the GitHub REST API, and there is a daily/hourly API limit for each IP address. If you are using an IP address shared by many people, you may hit this limit occasionally. Refer to [Rate limits for the REST API](https://docs.github.com/en/rest/using-the-rest-api/rate-limits-for-the-rest-api) for more information.

When this happens, you will see a warning message from CMake as follows:
```
CMake Warning at cmake/GitHubRelease.cmake:53 (message):
  If API rate limit is exceeded, Github allows a higher limit when you use
  token.  Try a cmake option -DSLANG_GITHUB_TOKEN=your_token_here
Call Stack (most recent call first):
  cmake/GitHubRelease.cmake:114 (check_release_and_get_latest)
  CMakeLists.txt:141 (get_best_slang_binary_release_url)
```

The limit is higher when you use your personal account with a ""personal access token"".

To generate a ""personal access token"" on GitHub, follow steps in [Creating a personal access token (classic)](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-personal-access-token-classic)

Use the generated ""token"" with a cmake option ""-DSLANG_GITHUB_TOKEN=your-token-here"".

### Making Changes
Make your changes and ensure to follow our [Design Decisions](docs/design/README.md).

### Testing
Test your changes thoroughly to ensure they do not introduce new issues. This is done by building and running `slang-test` from the repository root directory. For more details about `slang-test`, please refer to the [Documentation on testing](tools/slang-test/README.md).

> Note: `slang-test` is meant to be launched from the root of the repository. It uses a hard-coded directory name ""tests/"" that is expected to exist in the current working directory.

> Note: One of the options for `slang-test.exe` is `-api`, and it takes an additional keyword to specify which API to test. When the option is `-api all-cpu`, as an example, it means it tests all APIs except CPU. The minus sign (-) after `all` means ""exclude,"" and you can ""include"" with a plus sign (+) like `-api gl+dx11`.

If you are familiar with Workflows/Actions in GitHub, you can check [Our Workflows](.github/workflows). The ""Test Slang"" section in [ci.yml](.github/workflows/ci.yml) is where `slang-test` runs.

For a quick reference, follow the instructions below.

#### Windows
1. Download and install VulkanSDK from the [LunarG SDK page](https://www.lunarg.com/vulkan-sdk).
2. Set an environment variable to enable SPIR-V validation in the Slang compiler:
   ```
   C:\git\slang> set SLANG_RUN_SPIRV_VALIDATION=1
   ```
3. Run `slang-test` with multiple threads. This may take 10 minutes or less depending on the performance of your computer.
   ```
   C:\git\slang> build\Release\bin\slang-test.exe -use-test-server -server-count 8
   ```
   > Note: If you increase `-server-count` to more than 16, you may find some of the tests randomly fail. This is a known issue on the graphics driver side.
4. Check whether the tests finished as expected.

#### Linux
1. Install VulkanSDK by following the [Instructions from LunarG](https://vulkan.lunarg.com/doc/view/latest/linux/getting_started_ubuntu.html).
   ```
   $ sudo apt update
   $ sudo apt install vulkan-sdk
   ```
2. Run `slang-test` with multiple threads. This may take 10 minutes or less depending on the performance of your computer.
   ```
   $ ./build/Release/bin/slang-test -use-test-server -server-count 8
   ```
3. Check whether the tests finished as expected.

### Commit to the Branch
Commit your changes to the branch with a descriptive commit message:
```
$ git commit
```

It is important to have a descriptive commit message. Unlike comments inside the source code, the commit messages don't spoil over time because they are tied to specific changes and can be reviewed by many people many years later.

Here is a good example of a commit message:

> Add user authentication feature
> 
> Fixes #1234
> 
> This commit introduces a new user authentication feature. It includes changes to the login page, user database, and session management to provide secure user authentication.

### Push to Forked Repository
Push your branch to your forked repository with the following command:
```
$ git push origin feature/your-feature-name
```

After the changes are pushed to your forked repository, the change needs to be merged to the final destination `shader-slang/slang`.
In order to proceed, you will need to create a ""Pull Request,"" or ""PR"" for short.

When you push to your forked repository, `git-push` usually prints a URL that allows you to create a PR.

If you missed a chance to use the URL, you can still create a PR from the GitHub webpage.
Go to your forked repository and change the branch name to the one you used for `git-push`.
It will show a message like ""This branch is 1 commit ahead of `shader-slang/slang:master`.""
You can create a PR by clicking on the message.

## Pull Request
Once a PR is created against `shader-slang/slang:master`, the PR will be merged when the following conditions are met:
1. The PR is reviewed and got approval.
1. All of the workflows pass.

When the conditions above are all met, you will have a chance to rewrite the commit message. Since the Slang repo uses the ""squash"" strategy for merging, multiple commits in your PR will become one commit. By default, GitHub will concatenate all of the commit messages sequentially, but often it is not readable. Please rewrite the final commit message in a way that people can easily understand what the purpose of the commit is.

There are two cases where the workflow may fail for reasons that are not directly related to the change:
1. ""Breaking change"" labeling is missing.
1. Source code ""Format"" needs to be changed.

### Addressing Code Reviews
After your pull request is created, you will receive code reviews from the community within 24 hours.

The PR requires approval from people who have permissions. They will review the changes before approving the pull. During this step, you will get feedback from other people, and they may request you to make some changes.

Follow-up changes that address review comments should be pushed to your pull request branch as additional commits. Any additional commits made to the same branch in your forked repository will show up on the PR as incremental changes.

When your branch is out of sync with top-of-tree, submit a merge commit to keep them in sync. Do not rebase and force push after the PR is created to keep the change history during the review process.

Use these commands to sync your branch:
```
$ git fetch upstream master
$ git merge upstream/master # resolve any conflicts here
$ git submodule update --recursive
```

The Slang repository uses the squash strategy for merging pull requests, which means all your commits will be squashed into one commit by GitHub upon merge.

### Labeling Breaking Changes
All pull requests must be labeled as either `pr: non-breaking` or `pr: breaking change` before it can be merged to the main branch. If you are already a committer, you are expected to label your PR when you create it. If you are not yet a committer, a reviewer will do this for you.

A PR is considered to introduce a breaking change if an existing application that uses Slang may no longer compile or behave the same way with the change. Typical examples of breaking changes include:

- Changes to `slang.h` that modify the Slang API in a way that breaks binary compatibility.
- Changes to the language syntax or semantics that may cause existing Slang code to not compile or produce different run-time results. For example, changing the overload resolution rules.
- Removing or renaming an existing intrinsic from the core module.

### Source Code Formatting
When the PR contains source code changes, one of the workflows will check the formatting of the code.

Code formatting can be automatically fixed on your branch by commenting `/format`; a bot will proceed to open a PR targeting *your* branch. You can merge the generated PR into your branch, and the problem will be resolved.

### Document Changes
When the PR contains document changes for the [Slang User's Guide](https://shader-slang.com/slang/user-guide/), you need to update the table of contents by running a PowerShell script as follows:
```
# Open PowerShell on Windows
cd docs
.\build_toc.ps1

# Add to git commit
git add gfx-user-guide/toc.html
git add user-guide/toc.html
```

Similar to the `/format` bot-command described in the previous section, you can also use `/regenerate-toc` instead.

When the PR is limited to document changes, the build workflows may not start properly. This is because the building process is unnecessary when the PR is limited to document changes. This may lead to a case where some of the required build workflows are stuck waiting to start. When this happens, the committers will manually merge the PR as a workaround, and it will not give you a chance to rewrite the commit message.

## Code Style
Follow our [Coding Conventions](docs/design/coding-conventions.md) to maintain consistency throughout the project.

Here are a few highlights:
1. Indent by four spaces. Don't use tabs except in files that require them (e.g., Makefiles).
1. Don't use the STL containers, iostreams, or the built-in C++ RTTI system.
1. Don't use the C++ variants of C headers (e.g., use `<stdio.h>` instead of `<cstdio>`).
1. Don't use exceptions for non-fatal errors (and even then support a build flag to opt out of exceptions).
1. Types should use UpperCamelCase, values should use lowerCamelCase, and macros should use `SCREAMING_SNAKE_CASE` with a prefix `SLANG_`.
1. Global variables should have a `g` prefix, non-const static class members can have an `s` prefix, constant data (in the sense of static const) should have a `k` prefix, and an `m_` prefix on member variables and a `_` prefix on member functions are allowed.
1. Prefixes based on types (e.g., `p` for pointers) should never be used.
1. In function parameter lists, an `in`, `out`, or `io` prefix can be added to a parameter name to indicate whether a pointer/reference/buffer is intended to be used for input, output, or both input and output.
1. Trailing commas should always be used for array initializer lists.
1. Try to write comments that explain the ""why"" of your code more than the ""what.""

## Issue Tracking
We track all our work with GitHub issues. Check the [Issues](https://github.com/shader-slang/slang/issues) for open issues. If you find a bug or want to suggest an enhancement, please open a new issue.

If you're new to the project or looking for a good starting point, consider exploring issues labeled as [Good first bug](https://github.com/shader-slang/slang/issues?q=is%3Aissue+is%3Aopen+label%3AGoodFirstBug). These are beginner-friendly bugs that provide a great entry point for new contributors.

## Communication
Join our [Discussions](https://github.com/shader-slang/slang/discussions).

## License
By contributing to Shader-Slang, you agree that your contributions will be licensed under the Apache License 2.0 with LLVM Exception. The full text of the License can be found in the [LICENSE](LICENSE) file in the root of the repository.
",,,,54,,,gmtsar,
62279451,MDEwOlJlcG9zaXRvcnk2MjI3OTQ1MQ==,Coursera,shenweichen/Coursera,0,shenweichen,https://github.com/shenweichen/Coursera,Quiz & Assignment of Coursera,0,2016-06-30 04:31:41+00:00,2025-03-08 09:21:26+00:00,2020-12-20 09:32:52+00:00,,75368,885,885,Jupyter Notebook,1,1,1,1,0,0,663,0,0,7,,1,0,0,public,663,7,885,master,1,,,['shenweichen'],0,0.73,0,,,,,,54,,,,
191743342,MDEwOlJlcG9zaXRvcnkxOTE3NDMzNDI=,Awesome-Meta-Learning,sudharsan13296/Awesome-Meta-Learning,0,sudharsan13296,https://github.com/sudharsan13296/Awesome-Meta-Learning," A curated list of Meta Learning papers, code, books, blogs, videos, datasets and other resources.",0,2019-06-13 10:38:04+00:00,2025-03-02 05:52:48+00:00,2020-11-24 09:32:33+00:00,,108,1520,1520,,1,1,1,1,0,0,298,0,0,1,,1,0,0,public,298,1,1520,master,1,,,"['sudharsan13296', 'a-parida12']",0,0.64,0,,,,,,68,,,,
324073351,MDEwOlJlcG9zaXRvcnkzMjQwNzMzNTE=,FLAVR,tarun005/FLAVR,0,tarun005,https://github.com/tarun005/FLAVR,Code for FLAVR: A fast and efficient frame interpolation technique.,0,2020-12-24 05:25:40+00:00,2025-03-05 14:02:54+00:00,2024-05-07 22:02:36+00:00,,34615,478,478,Python,1,1,1,1,0,0,72,0,0,6,apache-2.0,1,0,0,public,72,6,478,main,1,,,"['tarun005', 'Tarun-Kalluri', 'n00mkrad', 'around-star', 'virtualramblas']",1,0.65,0,,,,,,16,,,,
233469488,MDEwOlJlcG9zaXRvcnkyMzM0Njk0ODg=,domains,tb0hdan/domains,0,tb0hdan,https://github.com/tb0hdan/domains,World’s single largest Internet domains dataset,0,2020-01-12 22:39:04+00:00,2025-03-05 23:18:42+00:00,2025-03-05 23:18:39+00:00,https://domainsproject.org,1803475,751,751,HTML,1,1,1,1,1,0,122,0,0,7,bsd-3-clause,1,0,0,public,122,7,751,master,1,,,"['tb0hdan', 'cesnokov']",0,0.61,0,,,,,,31,,,,
186476932,MDEwOlJlcG9zaXRvcnkxODY0NzY5MzI=,tensorflow2-generative-models,timsainb/tensorflow2-generative-models,0,timsainb,https://github.com/timsainb/tensorflow2-generative-models,"Implementations of a number of generative models in Tensorflow 2. GAN, VAE, Seq2Seq, VAEGAN, GAIA, Spectrogram Inversion. Everything is self contained in a jupyter notebook for easy export to colab.",0,2019-05-13 18:47:06+00:00,2025-02-03 08:40:58+00:00,2021-07-18 10:26:20+00:00,,11957,998,998,Jupyter Notebook,1,1,1,1,0,0,186,0,0,10,,1,0,0,public,186,10,998,master,1,,,['timsainb'],0,0.56,0,,,,,,30,,,,
160432571,MDEwOlJlcG9zaXRvcnkxNjA0MzI1NzE=,pytorch-meta,tristandeleu/pytorch-meta,0,tristandeleu,https://github.com/tristandeleu/pytorch-meta,A collection of extensions and data-loaders for few-shot learning & meta-learning in PyTorch,0,2018-12-04 23:36:45+00:00,2025-03-05 06:09:34+00:00,2023-07-17 16:05:00+00:00,https://tristandeleu.github.io/pytorch-meta/,1133,2012,2012,Python,1,1,1,1,1,0,257,0,0,59,mit,1,0,0,public,257,59,2012,master,1,,,"['tristandeleu', 'mmahsereci', 'MarcCoru', 'egrefen', 'MSiam', 'sleepy-owl', 'eugenelet', 'akreal', 'marcociccone']",0,0.6,0,,,,,,41,,,,
84633944,MDEwOlJlcG9zaXRvcnk4NDYzMzk0NA==,elsa,ucsd-progsys/elsa,0,ucsd-progsys,https://github.com/ucsd-progsys/elsa,Elsa is a lambda calculus evaluator,0,2017-03-11 08:02:59+00:00,2025-03-06 22:34:58+00:00,2024-06-10 14:52:03+00:00,,89,185,185,Haskell,1,1,1,1,0,0,22,0,0,1,mit,1,0,0,public,22,1,185,master,1,1,,"['ranjitjhala', 'jevancc', 'justinyaodu', 'mb64', 'themattchan', 'cole-k', 'lkuper', 'glapa-grossklag', 'rosekunkel', 'rejectedcode']",1,0.84,0,,,,,,6,,,,
8011618,MDEwOlJlcG9zaXRvcnk4MDExNjE4,liquid-fixpoint,ucsd-progsys/liquid-fixpoint,0,ucsd-progsys,https://github.com/ucsd-progsys/liquid-fixpoint,Horn Clause Constraint Solving for Liquid Types,0,2013-02-04 16:53:22+00:00,2025-03-06 19:32:49+00:00,2025-03-08 05:52:09+00:00,,6226,144,144,Haskell,1,1,1,1,1,0,62,0,0,40,bsd-3-clause,1,0,0,public,62,40,144,develop,1,1,,"['ranjitjhala', 'nikivazou', 'facundominguez', 'gridaphobe', 'philderbeast', 'atondwal', 'zgrannan', 'clayrat', 'AlecsFerra', 'michaelborkowski', 'curiousleo', 'tbidne', 'alanz', 'panagosg7', 'Fizzixnerd', 'adinapoli', 'mkolosick', 'BenjaminCosman', 'qaristote', 'spinda', 'vrindisbacher', 'RobinWebbers', 'christetreault', 'shingarov', 'plredmond', 'serras', 'andrewthad', 'googleson78', 'nilehmann', 'rolph-recto', 'yiyunliu', 'peti', 'waivio', 'kosmikus', 'Alf0nso', 'abakst', 'gergoerdi', 'jarctan', 'woehr', 'phadej', 'gabrielhdt', 'michaelbeaumont']",1,0.81,0,,,,,,11,,,,
5913967,MDEwOlJlcG9zaXRvcnk1OTEzOTY3,liquidhaskell,ucsd-progsys/liquidhaskell,0,ucsd-progsys,https://github.com/ucsd-progsys/liquidhaskell,Liquid Types For Haskell,0,2012-09-22 15:06:55+00:00,2025-03-08 06:24:09+00:00,2025-03-06 20:52:55+00:00,,60341,1226,1226,Haskell,1,1,1,1,1,0,142,0,0,446,bsd-3-clause,1,0,0,public,142,446,1226,develop,1,1,,"['ranjitjhala', 'nikivazou', 'facundominguez', 'gridaphobe', 'adinapoli', 'pbougou', 'spinda', 'clayrat', 'varosi', 'yiyunliu', 'zgrannan', 'alanz', 'oquechy', 'philderbeast', 'yanhasu', 'AlecsFerra', 'atondwal', 'tbidne', 'Fizzixnerd', 'jprider63', 'christetreault', 'gergoerdi', 'renanroberto', 'mboes', 'curiousleo', 'kosmikus', 'phadej', 'peti', 'abakst', 'jarctan', 'nomeata', 'varosi-chaosgroup', 'michaelborkowski', 'plredmond', 'Alf0nso', 'thoughtpolice', 'Gabriella439', 'RCoeurjoly', 'BenjaminCosman', 'waddlaw', 'andrewthad', 'michaelbeaumont', 'googleson78', 'archbung', 'rosekunkel', 'RyanGlScott', 'Ptival', 'sorawee', 'stefkin', 'y-taka-23', 'qaristote', 'notcome', 'themattchan', 'lcycon', 'Kyly', 'k4rtik', 'bwignall', 'mystaroll', 'gruhn', 'OlaoluwaM', 'racherb', 'romac', 'ssaavedra', 'shlevy', 'bartavelle', 'bbyler-sideqik', 'dependabot[bot]', 'Lysxia', 'superfunc', 'michaelficarra', 'manuel-uberti', 'hvr', 'gabrielhdt', 'EJahren', 'dredozubov', 'Wollw', 'chrisdone', 'shingarov', 'rpglover64', 'Abhiroop']",1,0.79,0,,,,,,29,,,,
656167444,R_kgDOJxxSFA,Transformers_And_LLM_Are_What_You_Dont_Need,valeman/Transformers_And_LLM_Are_What_You_Dont_Need,0,valeman,https://github.com/valeman/Transformers_And_LLM_Are_What_You_Dont_Need,The best repository showing why transformers might not be the answer for time series forecasting and showcasing the best SOTA non transformer models.,0,2023-06-20 11:41:50+00:00,2025-03-07 07:57:54+00:00,2025-03-03 16:39:47+00:00,,249,625,625,,1,1,1,1,0,0,38,0,0,2,,1,0,0,public,38,2,625,main,1,,"# Transformers_And_LLM_Are_What_You_Dont_Need
The best repository showing why transformers don’t work in time series forecasting 

## [Table of Contents]() 

* [PhD and MSc Theses](#theses)

* [Videos](#videos) 
 
* [Papers](#papers)

* [Articles](#articles)

## Videos
1. [Problems in the current research on forecasting with transformers, foundational models, etc.](https://www.youtube.com/watch?v=vNul_AjRPFw&t=1084s) by Christof Bergmeir

## Theses
1. [Cotton Price Long-Term Time Series Forecasting: A look at Transformers Suitability](https://repository.eafit.edu.co/server/api/core/bitstreams/27739bb8-7237-498f-8fba-ddb4ed6ca4fe/content)

## Papers
1. [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504) by Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu (The Chinese University of Hong Kong, International Digital Economy Academy (IDEA), 2022) [code](https://github.com/cure-lab/LTSF-Linear) 🔥🔥🔥🔥🔥
2.  [LLMs and foundational models for time series forecasting: They are not (yet) as good as you may hope](https://www.linkedin.com/pulse/llms-foundational-models-time-series-forecasting-yet-good-bergmeir-bprwf) by Christoph Bergmeir (2023) 🔥🔥🔥🔥🔥
3.   [Transformers Are What You Do Not Need](https://medium.com/@valeman/transformers-are-what-you-do-not-need-cf16a4c13ab7) by Valeriy Manokhin (2023) 🔥🔥🔥🔥🔥
4.   [Time Series Foundational Models: Their Role in Anomaly Detection and Prediction](https://arxiv.org/abs/2412.19286v1) (2024) [code](https://github.com/smtmnfg/TSFM)
5.   [Deep Learning is What You Do Not Need](https://medium.com/@valeman/-86655805a676) by Valeriy Manokhin (2022) 🔥🔥🔥🔥🔥
6. [Why do Transformers suck at Time Series Forecasting](https://machine-learning-made-simple.medium.com/why-do-transformers-suck-at-time-series-forecasting-46ae3a4d6b11) by Devansh (2023)
7. [Frequency-domain MLPs are More Effective Learners in Time Series Forecasting](https://arxiv.org/abs/2311.06184) by Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Defu Lian, Ning An, Longbing Cao, Zhendong Niu (Bejing Institute of Technology, Tongji University, University of Oxford, Universuty of Technology Sydney, University of Macau, HeFei University of Technology, Macquarie University) (2023) 🔥🔥🔥🔥🔥
8. [Forecasting CPI inflation under economic policy and geo-political uncertainties](https://arxiv.org/abs/2401.00249) by Shovon Sengupta, Tanujit Chakraborty, Sunny Kumar Singh (Fidelity Investments, Sorbonne University, BITS Pilani Hyderabad). (2024) 🔥🔥🔥🔥🔥
9. [Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping](https://arxiv.org/abs/2305.10721) by Zhe Li, Shiyi Qi, Yiduo Li, Zenglin Xu (Harbin Institute of Technology, Shenzhen, 2023) [code](https://github.com/plumprc/RTSF)
10. [SCINet: Time Series Modeling and Forecasting with Sample Convolution and Interaction](https://arxiv.org/abs/2106.09305) by Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, Qiang Xu (The Chinese University of Hong Kong,2022) [code](https://github.com/cure-lab/SCINet)
11. [WINNET:TIME SERIES FORECASTING WITH A WINDOW-ENHANCED PERIOD EXTRACTING AND INTERACTING](https://arxiv.org/pdf/2311.00214.pdf) by Wenjie Ou, Dongyue Guo, Zheng Zhang, Zhishuo Zhao, Yi Lin (Sichuan University, China, 2023)
12. [A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis](https://arxiv.org/abs/2310.11959) by Shuhan Zhong, Sizhe Song, Guanyao Li, Weipeng Zhuo, Yang Liu, S.-H. Gary Chan, The Hong Kong University of Science and Technology
Hong Kong, 2023) [code](https://github.com/zshhans/MSD-Mixer) 🔥🔥🔥🔥🔥
15. [TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis](https://arxiv.org/abs/2210.02186) by (Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, Mingsheng Longj, , Tsinghua University, 2023) [code](https://github.com/thuml/TimesNet) 🔥🔥🔥🔥🔥
16. [MTS-Mixers: Multivariate Time Series Forecasting via Factorized Temporal and Channel Mixing](https://arxiv.org/abs/2302.04501) [code](https://github.com/plumprc/MTS-Mixers) 🔥🔥🔥🔥🔥
17. [Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift](https://openreview.net/forum?id=cGDAkQo1C0p) by Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, Jaegul Choo (Kaist AI, Vuno, Naver Corp, ETRI, ICLR 2022) [code](https://github.com/ts-kim/RevIN) [project page](https://seharanul17.github.io/RevIN/) 🔥🔥🔥🔥🔥
18. [WINNet: Wavelet-inspired Invertible Network for Image Denoising](https://arxiv.org/abs/2311.00214) by 
Wenjie Ou, Dongyue Guo, Zheng Zhang, Zhishuo Zhao, Yi Lin (College of Computer Science, Sichuan University, China) [code](https://github.com/jjhuangcs/WINNet) 🔥🔥🔥🔥🔥
19. [Mlinear: Rethink the Linear Model for Time-series Forecasting](https://arxiv.org/abs/2305.04800) Wei Li, Xiangxu Meng, Chuhao Chen and Jianing Chen (Harbin Engineering University, 2023) 🔥🔥🔥🔥🔥
20. [Minimalist Traffic Prediction: Linear Layer Is All You Need](https://arxiv.org/abs/2308.10276) by Wenying Duan, Hong Rao, Wei Huang, Xiaoxi He (Nanchang, University, Universify of Macau, 2023)
21. [Frequency-domain MLPs are More Effective Learners in Time Series Forecasting](https://arxiv.org/abs/2311.06184) by Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Defu Lian, Ning An, Longbing Cao, Zhendong Niu (Beijing Institute of Technology, Tongji University, University of Oxford University of Technology Sydney, University of Macau, USTC, HeFei University of Technology, Macquarie University, 2023) [code](https://github.com/aikunyi/FreTS) 🔥🔥🔥🔥🔥
22. [AN END-TO-END TIME SERIES MODEL FOR SIMULTANEOUS IMPUTATION AND FORECAST](https://arxiv.org/abs/2306.00778) by Trang H. Tran, Lam M. Nguyen, Kyongmin Yeo, Nam Nguyen, Dzung Phan, Roman Vaculin Jayant Kalagnanam (School of Operations Research and Information Engineering, Cornell University; IBM Research, Thomas J. Watson Research Center, Yorktown Heights, NY, USA, 2023) 🔥🔥🔥🔥🔥
23. [Long-term Forecasting with TiDE: Time-series Dense Encoder](https://arxiv.org/abs/2304.08424) by Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, Rose Yu (Google Cloud, University of California, San Diego, 2023)
24. [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/abs/2306.09364) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam (IBM Research, 2023) [code](https://huggingface.co/docs/transformers/main/en/model_doc/patchtsmixer) [code](https://github.com/IBM/tsfm/blob/main/wiki.md)
25. [Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors](https://arxiv.org/abs/2305.18803) by Yong Liu, Chenyu Li, Jianmin Wang, Mingsheng Long (Tsinghua University, 2023) [code](https://github.com/thuml/Time-Series-Library/blob/main/models/Koopa.py) 🔥🔥🔥🔥🔥
26. [Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective](https://arxiv.org/abs/2402.11463) Jiaxi Hu, Yuehong Hu, Wei Chen, Ming Jin, Shirui Pan, Qingsong Wen, Yuxuan Liang (2024) 🔥🔥🔥🔥🔥
27. [When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting](https://arxiv.org/abs/2402.12767) (2024) 🔥🔥🔥🔥🔥
28. [Deep Coupling Network For Multivariate Time Series Forecasting](https://arxiv.org/abs/2402.15134) (2024)
29. [Linear Dynamics-embedded Neural Network for Long-Sequence Modeling](https://arxiv.org/abs/2402.15290) by Tongyi Liang and Han-Xiong Li (City University of Hong Kong, 2024).
30. [PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from the perspective of partial differential equations](https://arxiv.org/abs/2402.16913) (2024)
31. [CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables](https://arxiv.org/abs/2403.01673) (2024) 🔥🔥🔥🔥🔥
32. [Is Mamba Effective for Time Series Forecasting?](https://arxiv.org/abs/2403.11144) [code](https://github.com/wzhwzhwzh0921/S-D-Mamba) (2024) 🔥🔥🔥🔥🔥
33. [STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model](https://arxiv.org/abs/2403.12418) (2024)
34. [TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting](https://arxiv.org/abs/2403.09898) [code](https://github.com/Atik-Ahamed/TimeMachine) (2024)🔥🔥🔥🔥🔥
35. [FITS: Modeling Time Series with 10k Parameters](https://arxiv.org/abs/2307.03756) [code](https://github.com/VEWOXIC/FITS) (2023)
36. [TSLANet: Rethinking Transformers for Time Series Representation Learning](https://arxiv.org/abs/2404.08472) [code](https://github.com/emadeldeen24/TSLANet) (2024) 🔥🔥🔥🔥🔥
37. [WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting](https://arxiv.org/abs/2309.11319) [code](https://github.com/Hank0626/WFTNet) (2024) 🔥🔥🔥🔥🔥
38. [SiMBA: Simplified Mamba-based Architecture for Vision and Multivariate Time series](https://arxiv.org/abs/2403.15360) [code](https://github.com/badripatro/Simba) (2024) 🔥🔥🔥🔥🔥
39. [SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core Fusion](https://arxiv.org/abs/2404.14197) [code](https://github.com/Secilia-Cxy/SOFTS) (2024) 🔥🔥🔥🔥🔥
40. [Integrating Mamba and Transformer for Long-Short Range Time Series Forecasting](https://arxiv.org/abs/2404.14757) [code](https://github.com/XiongxiaoXu/Mambaformer-in-Time-Series) (2024) 🔥🔥🔥🔥🔥
41. [SparseTSF: Modeling Long-term Time Series Forecasting with 1k Parameters](https://arxiv.org/abs/2405.00946) (2024) 🔥🔥🔥🔥🔥
42. [Boosting MLPs with a Coarsening Strategy for Long-Term Time Series Forecasting](https://arxiv.org/abs/2405.03199) (2024) 🔥🔥🔥🔥🔥
43. [Multi-Scale Dilated Convolution Network for Long-Term Time Series Forecasting](https://arxiv.org/abs/2405.05499) (2024)
44. [ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis](https://openreview.net/forum?id=vpJMJerXHU#) [code](https://github.com/luodhhh/ModernTCN) (ICLR 2024 Spotlight)
45. [Adaptive Extraction Network for Multivariate Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2405.12038) (2024) 🔥🔥🔥🔥🔥
46. [Interpretable Multivariate Time Series Forecasting Using Neural Fourier Transform](https://arxiv.org/abs/2405.13812) (2024) 🔥🔥🔥🔥🔥
47. [PERIODICITY DECOUPLING FRAMEWORK FOR LONG- TERM SERIES FORECASTING](https://openreview.net/pdf?id=dp27P5HBBt) [code](https://github.com/Hank0626/PDF)  (2024) 🔥🔥🔥🔥🔥
48. [Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models](https://arxiv.org/abs/2406.04320) 🔥🔥🔥🔥🔥 (2024)
49. [Time Evidence Fusion Network: Multi-source View in Long-Term Time Series Forecasting](https://arxiv.org/abs/2405.06419) [code](https://github.com/ztxtech/Time-Evidence-Fusion-Network) (2024)
50. [ATFNet: Adaptive Time-Frequency Ensembled Network for Long-term Time Series Forecasting](https://arxiv.org/abs/2404.05192) [code](https://github.com/yhyhyhyhyhy/atfnet) (2024) 🔥🔥🔥🔥
51. [C-Mamba: Channel Correlation Enhanced State Space Models for Multivariate Time Series Forecasting](https://arxiv.org/abs/2406.05316) (2024) 🔥🔥🔥🔥
52. [The Power of Minimalism in Long Sequence Time-series Forecasting](https://openreview.net/pdf?id=hF8jnnexSB)
53. [WindowMixer: Intra-Window and Inter-Window Modeling for Time Series Forecasting](https://arxiv.org/abs/2406.12921)
54. [xLSTMTime : Long-term Time Series Forecasting With xLSTM](https://arxiv.org/abs/2407.10240) [code](https://github.com/muslehal/xLSTMTime) (2024)
55. [Not All Frequencies Are Created Equal:Towards a Dynamic Fusion of Frequencies in Time-Series Forecasting](https://arxiv.org/abs/2407.12415) (2024) 🔥🔥🔥🔥
56. [FMamba: Mamba based on Fast-attention for Multivariate Time-series Forecasting](https://arxiv.org/abs/2407.14814) (2024)
57. [Long Input Sequence Network for Long Time Series Forecasting](https://arxiv.org/abs/2407.15869) (2024)
58. [Time-series Forecasting with Tri-Decomposition Linear-based Modelling and Series-wise Metrics](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4913290) (2024) 🔥🔥🔥🔥
59. [An Evaluation of Standard Statistical Models and LLMs on Time Series Forecasting](https://arxiv.org/abs/2408.04867) (2024) LLM 🔥🔥🔥🔥
60. [Macroeconomic Forecasting with Large Language Models](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4881094) (2024) LLM 🔥🔥🔥🔥
61. [Language Models Still Struggle to Zero-shot Reason about Time Series](https://arxiv.org/abs/2404.11757) (2024) LLM 🔥🔥🔥🔥
62. [KAN4TSF: Are KAN and KAN-based models Effective for Time Series Forecasting?](https://arxiv.org/abs/2408.11306) (2024)
63. [Simplified Mamba with Disentangled Dependency Encoding for Long-Term Time Series Forecasting](https://arxiv.org/abs/2408.12068) (2024)
64. [Transformers are Expressive, But Are They Expressive Enough for Regression?](https://arxiv.org/abs/2402.15478) (2024) paper showing transformers cant approximate smooth functions
65. [MixLinear: Extreme Low Resource Multivariate Time Series Forecasting with 0.1K Parameters](https://arxiv.org/abs/2410.02081)
66. [MMFNet: Multi-Scale Frequency Masking Neural Network for Multivariate Time Series Forecasting](https://arxiv.org/abs/2410.02070)
67. [Neural Fourier Modelling: A Highly Compact Approach to Time-Series Analysis](https://arxiv.org/abs/2410.04703) [code](https://github.com/minkiml/NFM)
68. [CMMamba: channel mixing Mamba for time series forecasting](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-01001-9)
69. [EffiCANet: Efficient Time Series Forecasting with Convolutional Attention](https://arxiv.org/abs/2411.04669)
70. [Curse of Attention: A Kernel-Based Perspective for Why Transformers Fail to Generalize on Time Series Forecasting and Beyond](https://arxiv.org/abs/2412.06061)
71. [CycleNet: Enhancing Time Series Forecasting through Modeling Periodic Patterns](https://github.com/ACAT-SCUT/CycleNet) [code](https://github.com/ACAT-SCUT/CycleNet)
72. [Are Language Models Actually Useful for Time Series Forecasting?](https://arxiv.org/abs/2406.16964)
73. [SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core Fusion](https://arxiv.org/abs/2404.14197) [code](https://github.com/Secilia-Cxy/SOFTS)
74. [FTLinear: MLP based on Fourier Transform for Multivariate Time-series Forecasting](https://www.researchsquare.com/article/rs-5654336/v1)
75. [WPMixer: Efficient Multi-Resolution Mixing for Long-Term Time Series Forecasting](https://arxiv.org/abs/2412.17176) [code](https://github.com/Secure-and-Intelligent-Systems-Lab/WPMixer)
76. [Zero Shot Time Series Forecasting Using Kolmogorov Arnold Networks](https://arxiv.org/abs/2412.17853)
77. [BEAT: Balanced Frequency Adaptive Tuning for Long-Term Time-Series Forecasting](https://arxiv.org/abs/2501.19065) (2025) 🔥🔥🔥🔥🔥
78. [A Multi-Task Learning Approach to Linear Multivariate Forecasting](https://arxiv.org/abs/2502.03571) (2025)
79. [Benchmarking Time Series Forecasting Models: From Statistical Techniques to Foundation Models in Real-World Applications](https://arxiv.org/abs/2502.03395) (2025)
80. [Day-ahead demand response potential prediction in residential buildings with HITSKAN: A fusion of Kolmogorov-Arnold networks and N-HiTS](https://www.sciencedirect.com/science/article/pii/S0378778825001859) (2025)
81. [Do We Really Need Deep Learning Models for Time Series Forecasting?](https://arxiv.org/abs/2101.02118) (2021)
82. [Two Steps Forward and One Behind: Rethinking Time Series Forecasting with Deep Learning](https://arxiv.org/abs/2304.04553) (2023)
83. [Are Self-Attentions Effective for Time Series Forecasting?](https://arxiv.org/abs/2405.16877?utm_source=chatgpt.com) (2024)
84. [What Matters in Transformers? Not All Attention is Needed](https://arxiv.org/abs/2406.15786) (2024)
85. [Time Series Foundational Models: Their Role in Anomaly Detection and Prediction](https://arxiv.org/abs/2412.19286v1) (2024)
86. [Performance of Zero-Shot Time Series Foundation Models on Cloud Data](https://arxiv.org/abs/2502.12944) (2025) 🔥🔥🔥🔥🔥
87. [Position: There are no Champions in Long-Term Time Series Forecasting](https://arxiv.org/abs/2502.12161) (2025)
88. [FinTSB: A Comprehensive and Practical Benchmark for Financial Time Series Forecasting](https://arxiv.org/abs/2502.18834)
89. [Cherry-Picking in Time Series Forecasting: How to Select Datasets to Make Your Model Shine](https://arxiv.org/abs/2412.14435) 

















    
## Articles
1. [TimeGPT vs TiDE: Is Zero-Shot Inference the Future of Forecasting or Just Hype?]([https://arxiv.org/abs/2205.13504](https://towardsdatascience.com/timegpt-vs-tide-is-zero-shot-inference-the-future-of-forecasting-or-just-hype-9063bdbe0b76) by Luís Roque
and Rafael Guedes. (2024)🔥🔥🔥🔥🔥
2. [TimeGPT-1, discussion on Hacker News](https://news.ycombinator.com/item?id=37874891) (2023) 
3. [TimeGPT : The first Generative Pretrained Transformer for Time-Series Forecasting](https://www.reddit.com/r/MachineLearning/comments/176wsne/r_timegpt_the_first_generative_pretrained/)


",['valeman'],0,0.55,0,,,,,,18,,,,
3298647,MDEwOlJlcG9zaXRvcnkzMjk4NjQ3,CommPy,veeresht/CommPy,0,veeresht,https://github.com/veeresht/CommPy,Digital Communication with Python,0,2012-01-29 17:57:27+00:00,2025-03-06 09:33:42+00:00,2023-05-04 03:23:21+00:00,http://veeresht.github.com/CommPy,1498,581,581,Python,1,1,1,1,1,0,185,0,0,24,bsd-3-clause,1,0,0,public,185,24,581,master,1,,,"['veeresht', 'BastienTr', 'kirlf', 'eSoares', 'helion-du-mas-des-bourboux-thales', 'akou97', 'edsonportosilva', 'rtucker', 'aholtzma-am', 'datlife', 'oliveiraleo', 'mborgerding', 'matchius', 'msmttchr', 'prsudheer']",1,0.78,0,,,,,,34,,,,
368062906,MDEwOlJlcG9zaXRvcnkzNjgwNjI5MDY=,OpenRooms,ViLab-UCSD/OpenRooms,0,ViLab-UCSD,https://github.com/ViLab-UCSD/OpenRooms,"This is the dataset and code release of the OpenRooms Dataset. For more information, please refer to our webpage below. Thanks a lot for your interest in our research!",0,2021-05-17 05:12:47+00:00,2025-01-21 16:45:29+00:00,2024-03-26 18:50:48+00:00,https://vilab-ucsd.github.io/ucsd-openrooms/,168,146,146,,1,1,1,1,0,0,9,0,0,10,mit,1,0,0,public,9,10,146,main,1,1,,"['ucsd-openrooms', 'lzqsd', 'manukc', 'zhlucsd']",1,0.8,0,,,,,,7,,,,
599547518,R_kgDOI7xefg,vllm,vllm-project/vllm,0,vllm-project,https://github.com/vllm-project/vllm,A high-throughput and memory-efficient inference and serving engine for LLMs,0,2023-02-09 11:23:20+00:00,2025-03-08 09:29:23+00:00,2025-03-08 08:11:57+00:00,https://docs.vllm.ai,39640,40749,40749,Python,1,1,1,0,0,1,6133,0,0,1900,apache-2.0,1,0,0,public,6133,1900,40749,main,1,1,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png"">
    <img alt=""vLLM"" src=""https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png"" width=55%>
  </picture>
</p>

<h3 align=""center"">
Easy, fast, and cheap LLM serving for everyone
</h3>

<p align=""center"">
| <a href=""https://docs.vllm.ai""><b>Documentation</b></a> | <a href=""https://vllm.ai""><b>Blog</b></a> | <a href=""https://arxiv.org/abs/2309.06180""><b>Paper</b></a> | <a href=""https://x.com/vllm_project""><b>Twitter/X</b></a> | <a href=""https://slack.vllm.ai""><b>Developer Slack</b></a> |
</p>

---

We’re excited to invite you to the first **vLLM China Meetup** on **March 16** in **Beijing**!  

Join us to connect with the **vLLM team** and explore how vLLM is leveraged in **post-training, fine-tuning, and deployment**, including [verl](https://github.com/volcengine/verl), [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory), and [vllm-ascend](https://github.com/vllm-project/vllm-ascend).

👉 **[Register Now](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)** to be part of the discussion!  

---

*Latest News* 🔥

- [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.
- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).
- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).
- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!
- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).
- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!
- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!
- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).
- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).
- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).
- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).
- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).

---

## About

vLLM is a fast and easy-to-use library for LLM inference and serving.

Originally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.

vLLM is fast with:

- State-of-the-art serving throughput
- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)
- Continuous batching of incoming requests
- Fast model execution with CUDA/HIP graph
- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), INT4, INT8, and FP8.
- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.
- Speculative decoding
- Chunked prefill

**Performance benchmark**: We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](.buildkite/nightly-benchmarks/) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.

vLLM is flexible and easy to use with:

- Seamless integration with popular Hugging Face models
- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more
- Tensor parallelism and pipeline parallelism support for distributed inference
- Streaming outputs
- OpenAI-compatible API server
- Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.
- Prefix caching support
- Multi-lora support

vLLM seamlessly supports most popular open-source models on HuggingFace, including:
- Transformer-like LLMs (e.g., Llama)
- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)
- Embedding Models (e.g. E5-Mistral)
- Multi-modal LLMs (e.g., LLaVA)

Find the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).

## Getting Started

Install vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):

```bash
pip install vllm
```

Visit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.
- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation/index.html)
- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)

## Contributing

We welcome and value any contributions and collaborations.
Please check out [CONTRIBUTING.md](./CONTRIBUTING.md) for how to get involved.

## Sponsors

vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!

<!-- Note: Please sort them in alphabetical order. -->
<!-- Note: Please keep these consistent with docs/source/community/sponsors.md -->
Cash Donations:
- a16z
- Dropbox
- Sequoia Capital
- Skywork AI
- ZhenFund

Compute Resources:
- AMD
- Anyscale
- AWS
- Crusoe Cloud
- Databricks
- DeepInfra
- Google Cloud
- Lambda Lab
- Nebius
- Novita AI
- NVIDIA
- Replicate
- Roblox
- RunPod
- Trainy
- UC Berkeley
- UC San Diego

Slack Sponsor: Anyscale

We also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.

## Citation

If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):

```bibtex
@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
```

## Contact Us

- For technical questions and feature requests, please use Github issues or discussions.
- For discussing with fellow users and coordinating contributions and development, please use Slack.
- For security disclosures, please use Github's security advisory feature.
- For collaborations and partnerships, please contact us at vllm-questions AT lists.berkeley.edu.

## Media Kit

- If you wish to use vLLM's logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).
","['WoosukKwon', 'youkaichao', 'DarkLight1337', 'mgoin', 'simon-mo', 'Isotr0py', 'ywang96', 'zhuohan123', 'njhill', 'jeejeelee', 'tlrmchlsmth', 'robertgshaw2-redhat', 'khluu', 'russellb', 'Yard1', 'comaniac', 'hmellor', 'rkooo567', 'LucasWilkinson', 'dsikka', 'alexm-redhat', 'joerunde', 'esmeetu', 'tdoublep', 'jikunshang', 'LiuXiaoxuanPKU', 'tjohnson31415', 'cadedaniel', 'heheda12345', 'pcmoritz', 'varun-sundar-rabindranath', 'bigPYJ1151', 'KuntaiDu', 'ruisearch42', 'terrytangyuan', 'hongxiayang', 'markmc', 'alex-jw-brooks', 'bnellnm', 'sroy745', 'zifeitong', 'dtrifiro', 'gshtras', 'ProExpertProg', 'MengqingCao', 'SolitaryThinker', 'sasha0552', 'divakar-amd', 'houseroad', 'andoorve', 'wangxiyuan', 'ronensc', 'maxdebayser', 'dependabot[bot]', 'yma11', 'rafvasq', 'patrickvonplaten', 'mzusman', 'Alexei-V-Ivanov-AMD', 'kylesayrs', 'NickLucche', 'AllenDou', 'DamonFool', 'wallashss', 'afeldman-nm', 'tomeras91', 'ShangmingCai', 'SageMoore', 'noamgat', 'liangfu', 'Jeffwan', 'ElizaWszola', 'jinzhen-lin', 'CRZbulabula', 'beginlner', 'rasmith', 'pavanimajety', 'alexeykondrat', 'WangErXiao', 'rickyyx', 'ilya-lavrenov', 'petersalas', 'HermitSun', 'zhouyuan', 'leiwen83', 'CatherineSue', 'kzawora-intel', 'tjtanaa', 'pooyadavoodi', 'chenqianfzh', 'xwjiang2010', 'chenxu2048', 'whyiug', 'gcalmettes', 'Imss27', 'hanzhi713', 'aarnphm', 'zhaoyang-star', 'chaunceyjiang', 'DefTruth', 'janimo', 'waltforme', 'aurickq', 'sangstar', 'sighingnow', 'liuyanyi', 'omrishiv', 'imkero', 'Abatom', 'zspo', 'mawong-amd', 'lk-chen', 'WrRan', 'llsj14', 'lsy323', 'schoennenbeck', 'maleksan85', 'HwwwwwwwH', 'avshalomman', 'B-201', 'b8zhong', 'xuechendi', 'jsato8094', 'youngkent', 'mspronesti', 'peng1999', 'Sanster', 'gesanqiu', 'cennn', 'kliuae', 'litianjian', 'liuyhwangyh', 'twaka', 'wchen61', 'yansh97', 'xffxff', 'HollowMan6', 'wuisawesome', 'gaocegege', 'charlifu', 'jamestwhedbee', 'dylanwhawk', 'elfiegg', 'br3no', 'K-Mistele', 'megha95', 'michaelfeil', 'npanpaliya', 'prashantgupta24', 'richardsliu', 'SzymonOzog', 'gty111', 'xiangxu-google', 'xyang16', 'Michaelvll', 'wangshuai09', 'noooop', 'sixsixcoder', 'kushanam', 'vanbasten23', 'fzyzcjy', 'kingljl', 'yunfeng-scale', 'zhengy001', 'tterrysun', 'SwapnilDreams100', 'xendo', 'zhyncs', 'zixuanzhang226', 'SmartManoj', 'abhigoyal1997', 'garg-amit', 'chu-tianxiang', 'Chen-0210', 'DearPlanet', 'fgreinacher', 'GeneDer', 'FurtherAI', 'noemotiovon', 'chiragjn', 'casper-hansen', 'Dinghow', 'caiom', 'LunrEclipse', 'flaviabeo', 'hbikki', 'eltociear', 'Etelis', 'JArnoldAMD', 'stas00', 'skylee-01', 'shen-shanshan', 'SanjuCSudhakaran', 'bufferoverflow', 'rahul-tuli', 'Qubitium', 'NikolaBorisov', 'mmoskal', 'lucas-tucker', 'kevin314', 'kerthcet', 'kaixih', 'xu-song', 'XiaobingSuper', 'zxdvd', 'fyabc', 'yecohn', 'kevinbu233', 'fabianlim', 'zhouyu5', 'cermeng', 'zachzzc', 'FerdinandZhong', 'linotfan', 'blueyo0', 'wooyeonlee0', 'wenxcs', 'sywangyi', 'wangruohui', 'sh1ng', 'varad-ahirwadkar', 'UranusSeven', 'haitwang-cloud', 'SuhongMoon', 'stevegrubb', 'skrider', 'Oliver-ss', 'suquark', 'shawntan', 'benchislett', 'akeshet', 'andylolu2', 'wulipc', 'zhaotyer', 'SUDA-HLT-ywfang', 'xcnick', 'zwd003', 'tastelikefeet', 'sumitd2', 'okakarpa', 'omer-dayan', 'nunjunj', 'milo157', 'maang-h', 'ftgreat', 'kylehh', 'jon-chuang', 'hissu-hyvarinen', 'hhzhang16', 'HarryWu99', 'explainerauthors', 'danieljannai21', 'dancingpipi', 'codethazine', 'cjackal', 'sh0416', 'ichernev', 'ita9naiwa', 'huydhn', 'hliuca', 'helena-intel', 'haichuan1221', 'gongdao123', 'gmarinho2', 'fpaupier', 'FlorianJoncour', 'sfc-gh-zhwang', 'ericperfect', 'eldarkurtic', 'dllehr-amd', 'blahblahasdf', 'declark1', 'cli99', 'wangchen615', 'bong-furiosa', 'AoyuQC', 'tmm1', 'c3-ali', 'Tostino', 'AlexHe99', 'agt', 'Akashcodes732', 'sethkimmel3', '2015aroras', 'ShawnD200', 'samos123', 'mrsalehi', 'zcnrex', '0-hero', 'panpan0000', 'Pernekhan', 'Juelianqvq', 'NadavShmayo', 'MoeedDar', 'joennlae', 'JasonZhu1313', 'jaywonchung', 'sjuxax', 'ycool', 'jimburtoft', 'Jingru', 'g-eoj', 'GindaChen', 'ChristopherCho', 'JustinLin610', 'lingfanyu', 'sfc-gh-mkeralapura', 'mikegre-google', 'AlpinDale', 'darthhexx', 'dskhudia', 'naed90', 'dhiaEddineRhaiem', 'dmoliveira', 'dilipgb', 'domenVres', 'dominik-schwabe', 'pfldy2850', 'DriverSong', 'etwk', 'nFunctor', 'Edwinhr716', 'eli-b', 'elinx', 'chongmni-aws', 'chrisociepa', 'Bam4d', 'christian-pinto', 'frittentheke', 'chujiezheng', 'chunyang-wen', 'cedonley', 'Concurrensee', 'congcongchen123', 'conroy-cheers', 'DAIZHENWEI', 'DaividFrank', 'Danielkinz', 'dmarasco', 'danielhanchen', 'dyli-google', 'Delviet', 'danilopeixoto', 'flozi00', 'Fluder-Paradyne', 'frreiss', 'FuryMartin', 'ElefHead', 'huiwy', 'GeauxEric', 'horheynm', 'shing100', 'GhaziSyed', 'GoHomeToMacDonal', 'gracehonv', 'gnpinkert', 'gurpreet-dhami', 'guspan-tanadi', 'hzhwcmhf', 'HandH1998', 'sfc-gh-hazhang', 'kezouke', 'zeyugao', 'qeternity', 'ErezSC42', 'taisazero', 'linxihui', 'MErkinSag', 'EthanqX', 'ezliu', 'EwoutH', 'Falko1', 'mklf', 'farzadab', 'lpfhs', 'cassanof', 'galatolofederico', 'FeiDeng', 'lxline', 'AmadeusChan', 'AndreSlavescu', 'andrew', 'a-ys', 'aw632', 'atalman', 'andrejonasson', 'aandyw', 'angusYuhao', 'eaplatanios', 'AnyISalIn', 'apatke', 'ArthurZucker', 'kimdwkimdw', 'Atllkks10', 'spliii', 'bfontain', '22quinn', '44670', 'A-Mahla', 'akx', 'Abraham-Xu', 'adamrb', 'alugowski', 'VastoLorde95', 'AdrianAbeyta', 'codefromthecrypt', 'AguirreNicolas', 'fahadh4ilyas', 'Akshat-Tripathi', 'tianyuzhou95', 'bet0x', 'payoto', 'allenhaozi', 'alpayariyak', 'bringlein', 'xiyuan-lee', 'Calvinnncy97', 'deep-diver', 'SiriusNEO', 'charlesfrye', 'CharlesRiggins', 'CNTRYROA', 'Jason-CKY', 'c21', 'cw75', 'yangalan123', 'yaochengji', 'zhangch9', 'Chenyaaang', 'NiuBlibing', 'ChipKerchner', 'Avinash-Raj', 'ayusher', 'qibaoyuan', 'BasicCoder', 'bm777', 'Beim', 'Bellk17', 'chuanzhubin', 'BKitor', 'mrbesher', 'aliencaocao', 'BlackBird-Coding', 'DreamTeamWangbowen', 'abmfy', 'bradhilton', 'brian-dellabetta', 'brian14708', 'bvrockwell']",1,50,66709,"
# vLLM Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socioeconomic status,
nationality, personal appearance, race, caste, color, religion, or sexual
identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
  community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or advances of
  any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email address,
  without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official email address,
posting via an official social media account, or acting as an appointed
representative at an online or offline/IRL event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement in the #code-of-conduct
channel in the [vLLM Slack](https://slack.vllm.ai).
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org/),
version 2.1, available at
[v2.1](https://www.contributor-covenant.org/version/2/1/code_of_conduct.html).

Community Impact Guidelines were inspired by
[Mozilla's code of conduct enforcement ladder](https://github.com/mozilla/inclusion).

For answers to common questions about this code of conduct, see the
[Contributor Covenant FAQ](https://www.contributor-covenant.org/faq). Translations are available at
[Contributor Covenant translations](https://www.contributor-covenant.org/translations).
","# Contributing to vLLM

You may find information about contributing to vLLM on [docs.vllm.ai](https://docs.vllm.ai/en/latest/contributing/overview.html).
","# Security Policy

## Reporting a Vulnerability

If you believe you have found a security vulnerability in vLLM, we encourage you to let us know right away. We will investigate all legitimate reports and do our best to quickly fix the problem.

Please report security issues privately using [the vulnerability submission form](https://github.com/vllm-project/vllm/security/advisories/new). Reports will then be triaged by the [vulnerability management team](https://docs.vllm.ai/en/latest/contributing/vulnerability_management.html).

---

Please see [PyTorch's Security Policy](https://github.com/pytorch/pytorch/blob/main/SECURITY.md) for more information and recommendations on how to securely interact with models.
","blank_issues_enabled: false
contact_links:
  - name: Questions
    url: https://discuss.vllm.ai
    about: Ask questions and discuss with other vLLM community members
","FILL IN THE PR DESCRIPTION HERE

FIX #xxxx (*link existing issues this PR will resolve*)

<!--- pyml disable-next-line no-emphasis-as-heading -->
**BEFORE SUBMITTING, PLEASE READ <https://docs.vllm.ai/en/latest/contributing/overview.html>** (anything written below this line will be removed by GitHub Actions)
",367,,,ucsd-progsys,acmucsd
153326558,MDEwOlJlcG9zaXRvcnkxNTMzMjY1NTg=,3d-bat,walzimmer/3d-bat,0,walzimmer,https://github.com/walzimmer/3d-bat,3D Bounding Box Annotation Tool (3D-BAT) Point cloud and Image Labeling,0,2018-10-16 17:29:37+00:00,2025-03-06 12:18:57+00:00,2024-03-07 21:28:53+00:00,,1384781,685,685,TypeScript,1,1,1,1,0,1,151,0,0,40,other,1,0,0,public,151,40,685,master,1,,,['walzimmer'],1,0.73,441,,,,,,19,,,,
81423389,MDEwOlJlcG9zaXRvcnk4MTQyMzM4OQ==,image-text-localization-recognition,whitelok/image-text-localization-recognition,0,whitelok,https://github.com/whitelok/image-text-localization-recognition,A general list of resources to image text localization and recognition  场景文本位置感知与识别的论文资源与实现合集 シーンテキストの位置認識と識別のための論文リソースの要約,0,2017-02-09 07:41:41+00:00,2025-02-27 02:26:53+00:00,2023-09-17 13:41:49+00:00,,341,948,948,,1,1,1,1,0,1,233,0,0,0,,1,0,0,public,233,0,948,master,1,,,"['whitelok', 'skrish13', 'karlluo-tencent', 'YangJiao1996', 'khurram18']",0,0.31,0,,,,,,75,,,,
219897924,MDEwOlJlcG9zaXRvcnkyMTk4OTc5MjQ=,Awesome_Underwater_Datasets,xahidbuffon/Awesome_Underwater_Datasets,0,xahidbuffon,https://github.com/xahidbuffon/Awesome_Underwater_Datasets,Pointers to large-scale underwater datasets and relevant resources.,0,2019-11-06 02:47:31+00:00,2025-03-05 22:47:33+00:00,2024-06-08 09:37:30+00:00,,598,583,583,,1,1,1,0,0,0,127,0,0,3,,1,0,0,public,127,3,583,master,1,,,"['xahidbuffon', 'uf-robopi']",0,0.73,0,,,,,,13,,,,
134295152,MDEwOlJlcG9zaXRvcnkxMzQyOTUxNTI=,Autofocus-Layer,yaq007/Autofocus-Layer,0,yaq007,https://github.com/yaq007/Autofocus-Layer,Autofocus Layer for Semantic Segmentation,0,2018-05-21 16:24:54+00:00,2024-09-26 07:38:17+00:00,2019-05-13 15:28:37+00:00,,438,180,180,Python,1,1,1,1,0,0,33,0,0,8,mit,1,0,0,public,33,8,180,master,1,,"# Autofocus Layer for Semantic Segmentation

## Introduction
This is a PyTorch implementation of the autofocus convolutional layer proposed for semantic segmentation with the objective of enhancing the capabilities of neural networks for multi-scale processing.
Autofocus layers adaptively change the size of the effective receptive field 
based on the processed context to generate more powerful features.
The proposed autofocus layer can be easily integrated into existing networks to improve a model's representational power. 

Here we apply the autofocus convolutional layer to deep neural networks for 3D semantic segmentation. We run experiments on the [Brain Tumor Image Segmentation dataset (BRATS2015)](https://www.smir.ch/BRATS/Start2015) as an example to show how the models work. In addition, we also implement a series of deep learning based models used for 3D Semantic Segmentation. The details of all the models implemented here can be found in our paper: [Autofocus Layer for Semantic Segmentation](https://arxiv.org/pdf/1805.08403.pdf).

<img src=""./src/autofocus.png"" width=""900""/>

Figure 1. An autofocus convolutional layer with four candidate dilation rates. (a) The attention model. (b) A weighted summation of activations from parallel dilated convolutions. (c) An example of attention maps for a small (r^1) and large (r^2) dilation rate. The first row is the input and the segmentation result of AFN6. 

## Citation
If you find the code or the models implemented here are useful, please cite our paper:

[Autofocus Layer for Semantic Segmentation](https://arxiv.org/pdf/1805.08403.pdf). 
[Y. Qin](http://cseweb.ucsd.edu/~yaq007/), K. Kamnitsas, S. Ancha, J. Nanavati, G. Cottrell, A. Criminisi, A. Nori, MICCAI 2018.

## Data

You can download the full dataset with training and testing images from https://www.smir.ch/BRATS/Start2015. To run all the models here,
you need to do a series of data pre-processing to the input images. 
- Provide a mask including the Region of Interest (RoI) as one of the input image. For example, in the BRATS dataset, the region outside the brain should be masked out with the provided mask.
- The intensity of the data within the RoI must be normalized to be zero-mean, unit-variance. For the BRATS dataset, each image must be normalized independently other than doing the normalization with the mean and variance of the whole training dataset.
- Make sure the ground-truth labels for training and testing represent the background with zero. For example, we have four different 
classes in the BRATS dataset, then the number of classes in this dataset will be 5, including the background (```[--num_classes 5]```) and number zero will be used to represent the background.
- When you use the training code for your own data, please change the data path correspondingly.

We provide the example codes for data preprocessing, including converting the data format, generating the masks and normalizing the input image. The corresponding text file is also provided to show the directory where the image are saved. You can create your own text file to save the image data path and change the corresponding code in the python scripts. The data normalization code is mainly derived from [DeepMedic](https://github.com/Kamnitsask/deepmedic).

A small subset of the BRATS dataset (after all the above data pre-processing) is provided [here](http://cseweb.ucsd.edu/~yaq007/dataset.zip) to run the preset examples. 

## Supported Models
Please refer ""[Autofocus Layer for Semantic Segmentation](https://arxiv.org/pdf/1805.08403.pdf)"" for the details of all the supported models.
- Basic Model: half pathway of [DeepMedic](https://arxiv.org/abs/1603.05959) with the last 6 layers with dilation rates equal 2.
- ASPP-c: adding an [ASPP](https://arxiv.org/abs/1606.00915) module on top of Basic model (parallel features are merged via concatenation).
- ASPP-s: adding an [ASPP](https://arxiv.org/abs/1606.00915) module on top of Basic model (parallel features are merged via summation).
- AFN1-6: with the last 1-6 dilated convolutional layers replaced by our proposed aufofocus convolutional layer.

## Performance

The performance reported here is an average over three runs on the 54 images from BRATS 2015 dataset. All the trained models can be downloaded [here](http://cseweb.ucsd.edu/~yaq007/models.tar.gz).
<img src=""./src/performance.png"" width=""900""/>
Table 1: Dice scores shown in format mean (standard deviation).

## Environment

The code is developed under the follwing configurations.
- 1-3 GPUs with at least 12G GPU memories. You can choose the number of GPUs used via ```[--num_gpus NUM_GPUS]```.
- ```PyTorch 0.3.0``` or higher is required to run the codes. 
- ```Nibabel``` is used here for loading the NIFTI images. 
- ```SimpleITK``` is used for saving the output into images. 

### Installation
``` bash
git clone https://github.com/yaq007/Autofocus-Layer.git
conda install pytorch torchvision -c pytorch
pip install nibabel
pip install SimpleITK
```

## Quick Start
First, you need to download the provided subset of BRATS dataset and all the trained models. Please run
``` bash
chmod +x download_sub_dataset.sh
./download_sub_daset.sh

chmod +x download_models.sh
./download_models.sh
```
Then you can run the following script to choose a model and do the testing. Here we use AFN1 as an example.
```bash
python test.py --num_gpus 1 --id AFN1 --test_epoch 390 
```
You can change the number of used GPUs via ```[--num_gpus NUM_GPUS]``` and choose the tested model that you want via ```[--id MODEL_ID]```. Make sure the test epoch is included in the downloaded directory ""saved_models"".

You can check all the input arguments via ```python test.py -h```.

## Training
In the provided subset of dataset, we also provided 20 example images for training. You can start training via:
```bash
python train.py --num_gpus NUM_GPUS --id MODEL_ID 
```
For the models like ""Basic"", you may only need one gpu to run the experiments. For the models like ""AFN6"", you may need to increase the number of GPUs to be 2 or 3. This depends on the GPU memory that you are using. Please check all the input arguments via ```python train.py -h```.

## Evaluation
You can evaluate a series of models saved after different epochs for one network via.
```bash
python val.py --num_gpus NUM_GPUS --id MODEL_ID 
```
Please make sure that you have already provided a validation list in order to load the validation images. You can specify the steps of epochs that you want to evaluate. Please check all the input arguments via ```python val.py -h```.

## Testing
### Case 1
If you have labels for test data and want to see the accuracy (e.g., Dice score for BRATS dataset), you can use the following two testing codes:
- ```test.py``` The input of the network are small image segments as in the training stage.
- ```test_full.py``` The input of the network is a full image rather than a smaller image segment.

There are small differences of these two different testing methods due to the padding in the convolutions. For the performance that we report above, we use the ```test.py``` to get all the results. 

To test, you can simply run 
```bash
python test.py/test_full.py --num_gpus NUM_GPUS --id MODEL_ID --test_epoch NUM_OF_TEST_EPOCH
```

You can increase the number of GPUs to speed up the evaluation process. You can also use ```--visualize``` action to save the prediction as an output image.

### Case 2
If you do not have ground truth for test data, you should use ```test_online.py``` to do the testing and save the output. For the BRATS dataset, you can simply run the following script to generate the predicted images and submit them to the online evaluation server.

```bash
python test_online.py --num_gpus NUM_GPUS --id MODEL_ID --test_epoch NUM_OF_TEST_EPOCH --visualize
```
Note! If you want to run ```test_online.py``` on the provided sample testing images, you need to change the directory of data when loading images.

## Contact
If you have any problems when using our codes or models, please feel free to contact me via e-mail: yaq007@eng.ucsd.edu.
",['yaq007'],1,0.73,0,,,,,,9,,,,
668159220,R_kgDOJ9NM9A,Awesome-Embodied-Agent-with-LLMs,zchoi/Awesome-Embodied-Agent-with-LLMs,0,zchoi,https://github.com/zchoi/Awesome-Embodied-Agent-with-LLMs,"This is a curated list of ""Embodied AI or robot with Large Language Models"" research. Watch this repository for the latest updates! 🔥",0,2023-07-19 06:55:49+00:00,2025-03-07 10:20:42+00:00,2025-03-06 15:52:29+00:00,,1933,1189,1189,,1,1,1,1,0,0,67,0,0,1,apache-2.0,1,0,0,public,67,1,1189,main,1,,,"['zchoi', 'jeasinema', 'Zhoues', 'JamesHujy', 'RealSuSeven', 'TinnkE', 'lafmdp']",0,0.76,0,,,,,,51,,,,
112993117,MDEwOlJlcG9zaXRvcnkxMTI5OTMxMTc=,cascade-rcnn,zhaoweicai/cascade-rcnn,0,zhaoweicai,https://github.com/zhaoweicai/cascade-rcnn,Caffe implementation of multiple popular object detection frameworks,0,2017-12-04 04:06:12+00:00,2025-03-05 14:16:22+00:00,2019-09-10 01:56:40+00:00,,5304,1047,1047,C++,1,1,1,1,0,0,291,0,0,50,,1,0,0,public,291,50,1047,master,1,,"# Cascade R-CNN: Delving into High Quality Object Detection

by Zhaowei Cai and Nuno Vasconcelos

This repository is written by Zhaowei Cai at UC San Diego.

## Introduction

This repository implements mulitple popular object detection algorithms, including Faster R-CNN, R-FCN, FPN, and our recently proposed Cascade R-CNN, on the MS-COCO and PASCAL VOC datasets. Multiple choices are available for backbone network, including AlexNet, VGG-Net and ResNet. It is written in C++ and powered by [Caffe](https://github.com/BVLC/caffe) deep learning toolbox. 

[Cascade R-CNN](http://www.svcl.ucsd.edu/publications/conference/2018/cvpr/cascade-rcnn.pdf) is a multi-stage extension of the popular two-stage R-CNN object detection framework. The goal is to obtain high quality object detection, which can effectively reject close false positives. It consists of a sequence of detectors trained end-to-end with increasing IoU thresholds, to be sequentially more selective against close false positives. The output of a previous stage detector is forwarded to a later stage detector, and the detection results will be improved stage by stage. This idea can be applied to any detector based on the two-stage R-CNN framework, including Faster R-CNN, R-FCN, FPN, Mask R-CNN, etc, and reliable gains are available independently of baseline strength. A vanilla Cascade R-CNN on FPN detector of ResNet-101 backbone network, without any training or inference bells and whistles, achieved state-of-the-art results on the challenging MS-COCO dataset.

## Update

The re-implementation of Cascade R-CNN in Detectron has been released. See [Detectron-Cascade-RCNN](https://github.com/zhaoweicai/Detectron-Cascade-RCNN). Very consistent improvements are available for all tested models, independent of baseline strength.

It is also recommended to use the third-party implementation, [mmdetection](https://github.com/open-mmlab/mmdetection) based on PyTorch and [tensorpack](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN) based on TensorFlow.

## Citation

If you use our code/model/data, please cite our paper:

    @inproceedings{cai18cascadercnn,
      author = {Zhaowei Cai and Nuno Vasconcelos},
      Title = {Cascade R-CNN: Delving into High Quality Object Detection},
      booktitle = {CVPR},
      Year  = {2018}
    }
or its extension:

    @article{cai2019cascadercnn,
      author = {Zhaowei Cai and Nuno Vasconcelos},
      title = {Cascade R-CNN: High Quality Object Detection and Instance Segmentation},
      journal = {arXiv preprint arXiv:1906.09756},
      year = {2019}
    }

## Benchmarking

We benchmark mulitple detector models on the MS-COCO and PASCAL VOC datasets in the below tables.

1. MS-COCO (Train/Test: train2017/val2017, shorter size: 800 for FPN and 600 for the others)

model     | #GPUs | bs | lr | iter | train time | test time | AP | AP50 | AP75 
---------|--------|-----|--------|-----|-----|-------|-------|--------|----- 
VGG-RPN-baseline     | 2 | 4    |3e-3| 100k   |  12.5 hr |  0.075s | 23.6 | 43.9 | 23.0 
VGG-RPN-Cascade     | 2 | 4    |3e-3| 100k   |  15.5 hr |  0.115s | 27.0 | 44.2 | 27.7
Res50-RFCN-baseline     | 4 | 1    |3e-3| 280k   |  19 hr |  0.07s | 27.0 | 48.7 | 26.9 
Res50-RFCN-Cascade     | 4 | 1    |3e-3| 280k   |  22.5 hr |  0.075s | 31.1 | 49.8 | 32.8
Res101-RFCN-baseline     | 4 | 1    |3e-3| 280k   |  29 hr |  0.075s | 30.3 | 52.2 | 30.8 
Res101-RFCN-Cascade     | 4 | 1    |3e-3| 280k   |  30.5 hr |  0.085s | 33.3 | 52.0 | 35.2
Res50-FPN-baseline     | 8 | 1    |5e-3| 280k   |  32 hr |  0.095s | 36.5 | 58.6 | 39.2 
Res50-FPN-Cascade     | 8 | 1    |5e-3| 280k   |  36 hr |  0.115s | 40.3 | 59.4 | 43.7
Res101-FPN-baseline     | 8 | 1    |5e-3| 280k   |  37 hr |  0.115s | 38.5 | 60.6 | 41.7 
Res101-FPN-Cascade     | 8 | 1    |5e-3| 280k   |  46 hr |  0.14s | 42.7 | 61.6 | 46.6


2. PASCAL VOC 2007 (Train/Test: 2007+2012trainval/2007test, shorter size: 600)

model     | #GPUs | bs | lr | iter | train time | AP | AP50 | AP75 
---------|--------|-----|--------|-----|-----|-------|--------|----- 
Alex-RPN-baseline     | 2 | 4    |1e-3| 45k   |  2.5 hr | 29.4 | 63.2 | 23.7 
Alex-RPN-Cascade     | 2 | 4    |1e-3| 45k   |  3 hr | 38.9 | 66.5 | 40.5
VGG-RPN-baseline     | 2 | 4    |1e-3| 45k   |  6 hr | 42.9 | 76.4 | 44.1 
VGG-RPN-Cascade     | 2 | 4    |1e-3| 45k   |  7.5 hr | 51.2 | 79.1 | 56.3
Res50-RFCN-baseline     | 2 | 2    |2e-3| 90k   |  8 hr | 44.8 | 77.5 | 46.8 
Res50-RFCN-Cascade     | 2 | 2    |2e-3| 90k   |  9 hr | 51.8 | 78.5 | 57.1
Res101-RFCN-baseline     | 2 | 2    |2e-3| 90k   |  10.5 hr | 49.4 | 79.8 | 53.2 
Res101-RFCN-Cascade     | 2 | 2    |2e-3| 90k   |  12 hr | 54.2 | 79.6 | 59.2

**NOTE**. In the above tables, all models have been run at least two times with close results. The training is relatively stable. RPN means Faster R-CNN. The annotations of PASCAL VOC are transformed to COCO format, and COCO API was used for evaluation. The results are different from the official VOC evaluation. If you want to compare the VOC results in publication, please use the official VOC code for evaluation.

## Requirements

1. NVIDIA GPU and cuDNN are required to have fast speeds. For now, CUDA 8.0 with cuDNN 6.0.20 has been tested. The other versions should be working.

2. Caffe MATLAB wrapper is required to run the detection/evaluation demo. 

## Installation

1. Clone the Cascade-RCNN repository, and we'll call the directory that you cloned Cascade-RCNN into `CASCADE_ROOT`
    ```Shell
    git clone https://github.com/zhaoweicai/cascade-rcnn.git
    ```
  
2. Build Cascade-RCNN
    ```Shell
    cd $CASCADE_ROOT/
    # Follow the Caffe installation instructions here:
    #   http://caffe.berkeleyvision.org/installation.html

    # If you're experienced with Caffe and have all of the requirements installed
    # and your Makefile.config in place, then simply do:
    make all -j 16

    # If you want to run Cascade-RCNN detection/evaluation demo, build MATLAB wrapper as well
    make matcaffe
    ```

## Datasets

If you already have a COCO/VOC copy but not as organized as below, you can simply create Symlinks to have the same directory structure. 

### MS-COCO

In all MS-COCO experiments, we use `train2017` for training, and `val2017` (a.k.a. `minival`) for validation. Follow [MS-COCO website](http://cocodataset.org/#download) to download images/annotations, and set-up the COCO API.

Assumed that your local COCO dataset copy is at `/your/path/to/coco`, make sure it has the following directory structure:

```
coco
|_ images
  |_ train2017
  |  |_ <im-1-name>.jpg
  |  |_ ...
  |  |_ <im-N-name>.jpg
  |_ val2017
  |_ ...
|_ annotations
   |_ instances_train2017.json
   |_ instances_val2017.json
   |_ ...
|_ MatlabAPI
```

### PASCAL VOC

In all PASCAL VOC experiments, we use VOC2007+VOC2012 `trainval` for training, and VOC2007 `test` for validation. Follow [PASCAL VOC website](http://host.robots.ox.ac.uk/pascal/VOC/) to download images/annotations, and set-up the VOCdevkit.

Assumed that your local VOCdevkit copy is at `/your/path/to/VOCdevkit`, make sure it has the following directory structure:

```
VOCdevkit
|_ VOC2007
  |_ JPEGImages
  |  |_ <000001>.jpg
  |  |_ ...
  |  |_ <009963>.jpg
  |_ Annotations
  |  |_ <000001>.xml
  |  |_ ...
  |  |_ <009963>.xml
  |_ ...
|_ VOC2012
  |_ JPEGImages
  |  |_ <2007_000027>.jpg
  |  |_ ...
  |  |_ <2012_004331>.jpg
  |_ Annotations
  |  |_ <2007_000027>.xml
  |  |_ ...
  |  |_ <2012_004331>.xml
  |_ ...
|_ VOCcode
```

## Training Cascade-RCNN

1. Get the training data
    ```Shell
    cd $CASCADE_ROOT/data/
    sh get_coco_data.sh
    ```
    
    This will download the window files required for the experiments. You can also use the provided MATLAB scripts `coco_window_file.m` under `$CASCADE_ROOT/data/coco/` to generate your own window files.

2. Download the pretrained models on ImageNet. For AlexNet and VGG-Net, the FC layers are pruned and 2048 units per FC layer are remained. In addition, the two FC layers are copied three times for Cascade R-CNN training. For ResNet, the `BatchNorm` layers are merged into `Scale` layers and frozen during training as common practice.
    ```Shell
    cd $CASCADE_ROOT/models/
    sh fetch_vggnet.sh
    ```

3. Multiple shell scripts are provided to train Cascade-RCNN on different baseline detectors as described in our paper. Under each model folder, you need to change the `root_folder` of the data layer in `train.prototxt` and `test.prototxt` to your COCO path. After that, you can start to train your own Cascade-RCNN models. Take `vgg-12s-600-rpn-cascade` for example. 
    ```Shell
    cd $CASCADE_ROOT/examples/coco/vgg-12s-600-rpn-cascade/
    sh train_detection.sh
    ```
   Log file will be generated along the training procedure. The total training time depends on the complexity of models and datasets. If you want to quickly check if the training works well, try the light AlexNet model on VOC dataset. 
 
**NOTE**. Occasionally, the training of the Res101-FPN-Cascade will be out of memory. Just resume the training from the latest solverstate.

## Pretrained Models

We only provide the Res50-FPN-baseline, Res50-FPN-Cascade and Res101-FPN-Cascade models for COCO dataset, and Res101-RFCN-Cascade for VOC dataset.

Download pre-trained models
```Shell
cd $CASCADE_ROOT/examples/coco/
sh fetch_cascadercnn_models.sh
``` 
The pretrained models produce exactly the same results as described in our paper.

## Testing/Evaluation Demo

Once the models pretrained or trained by yourself are available, you can use the MATLAB script `run_cascadercnn_coco.m` to obtain the detection and evaluation results. Set the right dataset path and choose the model of your interest to test in the demo script. The default setting is for the pretrained model. The final detection results will be saved under `$CASCADE_ROOT/examples/coco/detections/` and the evaluation results will be saved under the model folder.

You also can run the shell script `test_coco_detection.sh` under each model folder for evalution, but it is not identical to the official evaluation. For publication, use the MATLAB script.

## Disclaimer

1. When we were re-implementing the FPN framework and `roi_align` layer, we only referred to their published papers. Thus, our implementation details could be different from the official [Detectron](https://github.com/facebookresearch/Detectron).

If you encounter any issue when using our code or model, please let me know.
",[],1,0.73,0,,"# Contributing

## Issues

Specific Caffe design and development issues, bugs, and feature requests are maintained by GitHub Issues.

_Please do not post usage, installation, or modeling questions, or other requests for help to Issues._
Use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) instead. This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.

When reporting a bug, it's most helpful to provide the following information, where applicable:

* What steps reproduce the bug?
* Can you reproduce the bug using the latest [master](https://github.com/BVLC/caffe/tree/master), compiled with the `DEBUG` make option?
* What hardware and operating system/distribution are you running?
* If the bug is a crash, provide the backtrace (usually printed by Caffe; always obtainable with `gdb`).

Try to give your issue a title that is succinct and specific. The devs will rename issues as needed to keep track of them.

## Pull Requests

Caffe welcomes all contributions.

See the [contributing guide](http://caffe.berkeleyvision.org/development.html) for details.

Briefly: read commit by commit, a PR should tell a clean, compelling story of _one_ improvement to Caffe. In particular:

* A PR should do one clear thing that obviously improves Caffe, and nothing more. Making many smaller PRs is better than making one large PR; review effort is superlinear in the amount of code involved.
* Similarly, each commit should be a small, atomic change representing one step in development. PRs should be made of many commits where appropriate.
* Please do rewrite PR history to be clean rather than chronological. Within-PR bugfixes, style cleanups, reversions, etc. should be squashed and should not appear in merged PR history.
* Anything nonobvious from the code should be explained in comments, commit messages, or the PR description, as appropriate.
",,,,40,,,,
150503944,MDEwOlJlcG9zaXRvcnkxNTA1MDM5NDQ=,Detectron-Cascade-RCNN,zhaoweicai/Detectron-Cascade-RCNN,0,zhaoweicai,https://github.com/zhaoweicai/Detectron-Cascade-RCNN,Cascade R-CNN in Detectron,0,2018-09-27 00:01:38+00:00,2024-11-09 04:08:58+00:00,2019-09-10 01:55:54+00:00,,3951,471,471,Python,1,1,1,1,0,0,104,0,0,7,apache-2.0,1,0,0,public,104,7,471,master,1,,"# Cascade R-CNN: Delving into High Quality Object Detection

by Zhaowei Cai and Nuno Vasconcelos

This repository is written by Zhaowei Cai at UC San Diego, on the base of [Detectron](https://github.com/facebookresearch/Detectron) @ [e8942c8](https://github.com/facebookresearch/Detectron/tree/e8942c882abf6e28fe68a626ec55028c9bdfe1cf).

## Introduction

This repository re-implements Cascade R-CNN on the base of [Detectron](https://github.com/facebookresearch/Detectron). Very consistent gains are available for all tested models, regardless of baseline strength.

Please follow [Detectron](https://github.com/facebookresearch/Detectron) on how to install and use Detectron-Cascade-RCNN.

It is also recommended to use our original implementation, [cascade-rcnn](https://github.com/zhaoweicai/cascade-rcnn) based on Caffe, and the third-party implementation, [mmdetection](https://github.com/open-mmlab/mmdetection) based on PyTorch and [tensorpack](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN) based on TensorFlow.

## Citation

If you use our code/model/data, please cite our paper:

```
@inproceedings{cai18cascadercnn,
  author = {Zhaowei Cai and Nuno Vasconcelos},
  Title = {Cascade R-CNN: Delving into High Quality Object Detection},
  booktitle = {CVPR},
  Year  = {2018}
}
```

or its extension:

```
@article{cai2019cascadercnn,
  author = {Zhaowei Cai and Nuno Vasconcelos},
  title = {Cascade R-CNN: High Quality Object Detection and Instance Segmentation},
  journal = {arXiv preprint arXiv:1906.09756},
  year = {2019}
}
```

and Detectron:

```
@misc{Detectron2018,
  author =       {Ross Girshick and Ilija Radosavovic and Georgia Gkioxari and
                  Piotr Doll\'{a}r and Kaiming He},
  title =        {Detectron},
  howpublished = {\url{https://github.com/facebookresearch/detectron}},
  year =         {2018}
}
```

## Benchmarking

### End-to-End Faster & Mask R-CNN Baselines

<table><tbody>
<!-- START E2E FASTER AND MASK TABLE -->
<!-- TABLE HEADER -->
<!-- Info: we use wrap text in <sup><sub></sub><sup> to make is small -->
<th valign=""bottom""><sup><sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;backbone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</sub></sup></th>
<th valign=""bottom""><sup><sub>type</sub></sup></th>
<th valign=""bottom""><sup><sub>lr<br/>schd</sub></sup></th>
<th valign=""bottom""><sup><sub>im/<br/>gpu</sub></sup></th>
<th valign=""bottom""><sup><sub>box<br/>AP</sub></sup></th>
<th valign=""bottom""><sup><sub>box<br/>AP50</sub></sup></th>
<th valign=""bottom""><sup><sub>box<br/>AP75</sub></sup></th>
<th valign=""bottom""><sup><sub>mask<br/>AP</sub></sup></th>
<th valign=""bottom""><sup><sub>mask<br/>AP50</sub></sup></th>
<th valign=""bottom""><sup><sub>mask<br/>AP75</sub></sup></th>
<th valign=""bottom""><sup><sub>download<br/>links</sub></sup></th>
<!-- TABLE BODY -->
<tr>
<td align=""left""><sup><sub>R-50-FPN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>36.7</sub></sup></td>
<td align=""right""><sup><sub>58.4</sub></sup></td>
<td align=""right""><sup><sub>39.6</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub><a href=""https://s3-us-west-2.amazonaws.com/detectron/35857345/12_2017_baselines/e2e_faster_rcnn_R-50-FPN_1x.yaml.01_36_30.cUF7QR7I/output/train/coco_2014_train%3Acoco_2014_valminusminival/generalized_rcnn/model_final.pkl"">model</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/35857345/12_2017_baselines/e2e_faster_rcnn_R-50-FPN_1x.yaml.01_36_30.cUF7QR7I/output/test/coco_2014_minival/generalized_rcnn/bbox_coco_2014_minival_results.json"">boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-50-FPN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>40.9</sub></sup></td>
<td align=""right""><sup><sub>59.0</sub></sup></td>
<td align=""right""><sup><sub>44.6</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub><a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/R50-FPN-cascade-rcnn.pkl"">model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_R50-FPN-cascade-rcnn.json"">boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-101-FPN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>39.4</sub></sup></td>
<td align=""right""><sup><sub>61.2</sub></sup></td>
<td align=""right""><sup><sub>43.4</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub><a href=""https://s3-us-west-2.amazonaws.com/detectron/35857890/12_2017_baselines/e2e_faster_rcnn_R-101-FPN_1x.yaml.01_38_50.sNxI7sX7/output/train/coco_2014_train%3Acoco_2014_valminusminival/generalized_rcnn/model_final.pkl"">model</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/35857890/12_2017_baselines/e2e_faster_rcnn_R-101-FPN_1x.yaml.01_38_50.sNxI7sX7/output/test/coco_2014_minival/generalized_rcnn/bbox_coco_2014_minival_results.json"">boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-101-FPN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>42.8</sub></sup></td>
<td align=""right""><sup><sub>61.4</sub></sup></td>
<td align=""right""><sup><sub>46.1</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub><a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/R101-FPN-cascade-rcnn.pkl"">model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_R101-FPN-cascade-rcnn.json"">boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>X-101-64x4d-FPN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>41.5</sub></sup></td>
<td align=""right""><sup><sub>63.8</sub></sup></td>
<td align=""right""><sup><sub>44.9</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub><a href=""https://s3-us-west-2.amazonaws.com/detectron/35858015/12_2017_baselines/e2e_faster_rcnn_X-101-64x4d-FPN_1x.yaml.01_40_54.1xc565DE/output/train/coco_2014_train%3Acoco_2014_valminusminival/generalized_rcnn/model_final.pkl"">model</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/35858015/12_2017_baselines/e2e_faster_rcnn_X-101-64x4d-FPN_1x.yaml.01_40_54.1xc565DE/output/test/coco_2014_minival/generalized_rcnn/bbox_coco_2014_minival_results.json"">boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>X-101-64x4d-FPN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>45.4</sub></sup></td>
<td align=""right""><sup><sub>64.0</sub></sup></td>
<td align=""right""><sup><sub>49.8</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_X101-64x4d-FPN-cascade-rcnn.json"">boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>X-101-32x8d-FPN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>41.3</sub></sup></td>
<td align=""right""><sup><sub>63.7</sub></sup></td>
<td align=""right""><sup><sub>44.7</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub><a href=""https://s3-us-west-2.amazonaws.com/detectron/36761737/12_2017_baselines/e2e_faster_rcnn_X-101-32x8d-FPN_1x.yaml.06_31_39.5MIHi1fZ/output/train/coco_2014_train%3Acoco_2014_valminusminival/generalized_rcnn/model_final.pkl"">model</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/36761737/12_2017_baselines/e2e_faster_rcnn_X-101-32x8d-FPN_1x.yaml.06_31_39.5MIHi1fZ/output/test/coco_2014_minival/generalized_rcnn/bbox_coco_2014_minival_results.json"">boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>X-101-32x8d-FPN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>44.7</sub></sup></td>
<td align=""right""><sup><sub>63.7</sub></sup></td>
<td align=""right""><sup><sub>48.8</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_X101-32x8d-FPN-cascade-rcnn.json"">boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-50-FPN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>37.7</sub></sup></td>
<td align=""right""><sup><sub>59.2</sub></sup></td>
<td align=""right""><sup><sub>40.9</sub></sup></td>
<td align=""right""><sup><sub>33.9</sub></sup></td>
<td align=""right""><sup><sub>55.8</sub></sup></td>
<td align=""right""><sup><sub>35.8</sub></sup></td>
<td align=""left""><sup><sub><a href=""https://s3-us-west-2.amazonaws.com/detectron/35858933/12_2017_baselines/e2e_mask_rcnn_R-50-FPN_1x.yaml.01_48_14.DzEQe4wC/output/train/coco_2014_train%3Acoco_2014_valminusminival/generalized_rcnn/model_final.pkl"">model</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/35858933/12_2017_baselines/e2e_mask_rcnn_R-50-FPN_1x.yaml.01_48_14.DzEQe4wC/output/test/coco_2014_minival/generalized_rcnn/bbox_coco_2014_minival_results.json"">boxes</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/35858933/12_2017_baselines/e2e_mask_rcnn_R-50-FPN_1x.yaml.01_48_14.DzEQe4wC/output/test/coco_2014_minival/generalized_rcnn/segmentations_coco_2014_minival_results.json"">masks</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-50-FPN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>41.3</sub></sup></td>
<td align=""right""><sup><sub>59.6</sub></sup></td>
<td align=""right""><sup><sub>44.9</sub></sup></td>
<td align=""right""><sup><sub>35.4</sub></sup></td>
<td align=""right""><sup><sub>56.2</sub></sup></td>
<td align=""right""><sup><sub>37.8</sub></sup></td>
<td align=""left""><sup><sub><a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/R50-FPN-cascade-rcnn-mask.pkl"">model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_R50-FPN-cascade-rcnn-mask.json"">boxes</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/segmentations_coco_2014_minival_results_R50-FPN-cascade-rcnn-mask.json"">masks</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-101-FPN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>40.0</sub></sup></td>
<td align=""right""><sup><sub>61.8</sub></sup></td>
<td align=""right""><sup><sub>43.7</sub></sup></td>
<td align=""right""><sup><sub>35.9</sub></sup></td>
<td align=""right""><sup><sub>58.3</sub></sup></td>
<td align=""right""><sup><sub>38.0</sub></sup></td>
<td align=""left""><sup><sub><a href=""https://s3-us-west-2.amazonaws.com/detectron/35861795/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_1x.yaml.02_31_37.KqyEK4tT/output/train/coco_2014_train%3Acoco_2014_valminusminival/generalized_rcnn/model_final.pkl"">model</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/35861795/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_1x.yaml.02_31_37.KqyEK4tT/output/test/coco_2014_minival/generalized_rcnn/bbox_coco_2014_minival_results.json"">boxes</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/35861795/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_1x.yaml.02_31_37.KqyEK4tT/output/test/coco_2014_minival/generalized_rcnn/segmentations_coco_2014_minival_results.json"">masks</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-101-FPN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>43.3</sub></sup></td>
<td align=""right""><sup><sub>61.7</sub></sup></td>
<td align=""right""><sup><sub>47.2</sub></sup></td>
<td align=""right""><sup><sub>37.1</sub></sup></td>
<td align=""right""><sup><sub>58.6</sub></sup></td>
<td align=""right""><sup><sub>39.8</sub></sup></td>
<td align=""left""><sup><sub><a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/R101-FPN-cascade-rcnn-mask.pkl"">model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_R101-FPN-cascade-rcnn-mask.json"">boxes</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/segmentations_coco_2014_minival_results_R101-FPN-cascade-rcnn-mask.json"">masks</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>X-101-64x4d-FPN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>42.4</sub></sup></td>
<td align=""right""><sup><sub>64.3</sub></sup></td>
<td align=""right""><sup><sub>46.4</sub></sup></td>
<td align=""right""><sup><sub>37.5</sub></sup></td>
<td align=""right""><sup><sub>60.6</sub></sup></td>
<td align=""right""><sup><sub>39.9</sub></sup></td>
<td align=""left""><sup><sub><a href=""https://s3-us-west-2.amazonaws.com/detectron/36494496/12_2017_baselines/e2e_mask_rcnn_X-101-64x4d-FPN_1x.yaml.07_50_11.fkwVtEvg/output/train/coco_2014_train%3Acoco_2014_valminusminival/generalized_rcnn/model_final.pkl"">model</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/36494496/12_2017_baselines/e2e_mask_rcnn_X-101-64x4d-FPN_1x.yaml.07_50_11.fkwVtEvg/output/test/coco_2014_minival/generalized_rcnn/bbox_coco_2014_minival_results.json"">boxes</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/36494496/12_2017_baselines/e2e_mask_rcnn_X-101-64x4d-FPN_1x.yaml.07_50_11.fkwVtEvg/output/test/coco_2014_minival/generalized_rcnn/segmentations_coco_2014_minival_results.json"">masks</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>X-101-64x4d-FPN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>45.9</sub></sup></td>
<td align=""right""><sup><sub>64.4</sub></sup></td>
<td align=""right""><sup><sub>50.2</sub></sup></td>
<td align=""right""><sup><sub>38.8</sub></sup></td>
<td align=""right""><sup><sub>61.3</sub></sup></td>
<td align=""right""><sup><sub>41.6</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_X101-64x4d-FPN-cascade-rcnn-mask.json"">boxes</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/segmentations_coco_2014_minival_results_X101-64x4d-FPN-cascade-rcnn-mask.json"">masks</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>X-101-32x8d-FPN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>42.1</sub></sup></td>
<td align=""right""><sup><sub>64.1</sub></sup></td>
<td align=""right""><sup><sub>45.9</sub></sup></td>
<td align=""right""><sup><sub>37.3</sub></sup></td>
<td align=""right""><sup><sub>60.3</sub></sup></td>
<td align=""right""><sup><sub>39.5</sub></sup></td>
<td align=""left""><sup><sub><a href=""https://s3-us-west-2.amazonaws.com/detectron/36761843/12_2017_baselines/e2e_mask_rcnn_X-101-32x8d-FPN_1x.yaml.06_35_59.RZotkLKI/output/train/coco_2014_train%3Acoco_2014_valminusminival/generalized_rcnn/model_final.pkl"">model</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/36761843/12_2017_baselines/e2e_mask_rcnn_X-101-32x8d-FPN_1x.yaml.06_35_59.RZotkLKI/output/test/coco_2014_minival/generalized_rcnn/bbox_coco_2014_minival_results.json"">boxes</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/36761843/12_2017_baselines/e2e_mask_rcnn_X-101-32x8d-FPN_1x.yaml.06_35_59.RZotkLKI/output/test/coco_2014_minival/generalized_rcnn/segmentations_coco_2014_minival_results.json"">masks</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>X-101-32x8d-FPN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>45.8</sub></sup></td>
<td align=""right""><sup><sub>64.1</sub></sup></td>
<td align=""right""><sup><sub>50.3</sub></sup></td>
<td align=""right""><sup><sub>38.6</sub></sup></td>
<td align=""right""><sup><sub>60.6</sub></sup></td>
<td align=""right""><sup><sub>41.5</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_X101-32x8d-FPN-cascade-rcnn-mask.json"">boxes</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/segmentations_coco_2014_minival_results_X101-32x8d-FPN-cascade-rcnn-mask.json"">masks</a></sub></sup></td>
</tr>
<!-- END E2E FASTER AND MASK TABLE -->
</tbody></table>

### Mask R-CNN with Bells & Whistles

<table><tbody>
<!-- START BELLS TABLE -->
<!-- TABLE HEADER -->
<!-- Info: we use wrap text in <sup><sub></sub><sup> to make is small -->
<th valign=""bottom""><sup><sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;backbone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</sub></sup></th>
<th valign=""bottom""><sup><sub>type</sub></sup></th>
<th valign=""bottom""><sup><sub>lr<br/>schd</sub></sup></th>
<th valign=""bottom""><sup><sub>im/<br/>gpu</sub></sup></th>
<th valign=""bottom""><sup><sub>box<br/>AP</sub></sup></th>
<th valign=""bottom""><sup><sub>box<br/>AP50</sub></sup></th>
<th valign=""bottom""><sup><sub>box<br/>AP75</sub></sup></th>
<th valign=""bottom""><sup><sub>mask<br/>AP</sub></sup></th>
<th valign=""bottom""><sup><sub>mask<br/>AP50</sub></sup></th>
<th valign=""bottom""><sup><sub>mask<br/>AP75</sub></sup></th>
<th valign=""bottom""><sup><sub>download<br/>links</sub></sup></th>
<!-- TABLE BODY -->
<tr>
<td align=""left""><sup><sub>X-152-32x8d-FPN-IN5k-baseline</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>s1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>48.1</sub></sup></td>
<td align=""right""><sup><sub>68.3</sub></sup></td>
<td align=""right""><sup><sub>52.9</sub></sup></td>
<td align=""right""><sup><sub>41.5</sub></sup></td>
<td align=""right""><sup><sub>65.1</sub></sup></td>
<td align=""right""><sup><sub>44.7</sub></sup></td>
<td align=""left""><sup><sub><a href=""https://s3-us-west-2.amazonaws.com/detectron/37129812/12_2017_baselines/e2e_mask_rcnn_X-152-32x8d-FPN-IN5k_1.44x.yaml.09_35_36.8pzTQKYK/output/train/coco_2014_train%3Acoco_2014_valminusminival/generalized_rcnn/model_final.pkl"">model</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/37129812/12_2017_baselines/e2e_mask_rcnn_X-152-32x8d-FPN-IN5k_1.44x.yaml.09_35_36.8pzTQKYK/output/test/coco_2014_minival/generalized_rcnn/bbox_coco_2014_minival_results.json"">boxes</a>&nbsp;|&nbsp;<a href=""https://s3-us-west-2.amazonaws.com/detectron/37129812/12_2017_baselines/e2e_mask_rcnn_X-152-32x8d-FPN-IN5k_1.44x.yaml.09_35_36.8pzTQKYK/output/test/coco_2014_minival/generalized_rcnn/segmentations_coco_2014_minival_results.json"">masks</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>[above without test-time aug.]</sub></sup></td>
<td align=""right""><sup><sub></sub></sup></td>
<td align=""right""><sup><sub></sub></sup></td>
<td align=""right""><sup><sub></sub></sup></td>
<td align=""right""><sup><sub>45.2</sub></sup></td>
<td align=""right""><sup><sub>66.9</sub></sup></td>
<td align=""right""><sup><sub>49.7</sub></sup></td>
<td align=""right""><sup><sub>39.7</sub></sup></td>
<td align=""right""><sup><sub>63.5</sub></sup></td>
<td align=""right""><sup><sub>42.4</sub></sup></td>
<td align=""right""><sup><sub></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>X-152-32x8d-FPN-IN5k-cascade</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>s1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>50.2</sub></sup></td>
<td align=""right""><sup><sub>68.2</sub></sup></td>
<td align=""right""><sup><sub>55.0</sub></sup></td>
<td align=""right""><sup><sub>42.3</sub></sup></td>
<td align=""right""><sup><sub>65.4</sub></sup></td>
<td align=""right""><sup><sub>45.8</sub></sup></td>
<td align=""left""><sup><sub><a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/X152-32x8d-FPN-cascade-rcnn-mask-aug.pkl"">model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_X152-32x8d-FPN-cascade-rcnn-mask-aug.json"">boxes</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/segmentations_coco_2014_minival_results_X152-32x8d-FPN-cascade-rcnn-mask-aug.json"">masks</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>[above without test-time aug.]</sub></sup></td>
<td align=""right""><sup><sub></sub></sup></td>
<td align=""right""><sup><sub></sub></sup></td>
<td align=""right""><sup><sub></sub></sup></td>
<td align=""right""><sup><sub>48.1</sub></sup></td>
<td align=""right""><sup><sub>66.7</sub></sup></td>
<td align=""right""><sup><sub>52.6</sub></sup></td>
<td align=""right""><sup><sub>40.7</sub></sup></td>
<td align=""right""><sup><sub>63.7</sub></sup></td>
<td align=""right""><sup><sub>43.8</sub></sup></td>
<td align=""right""><sup><sub></sub></sup></td>
</tr>
<!-- END BELLS TABLE -->
</tbody></table>

### Faster & Mask R-CNN with GN

<table><tbody>
<!-- START E2E FASTER AND MASK TABLE -->
<!-- TABLE HEADER -->
<!-- Info: we use wrap text in <sup><sub></sub><sup> to make is small -->
<th valign=""bottom""><sup><sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;backbone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</sub></sup></th>
<th valign=""bottom""><sup><sub>type</sub></sup></th>
<th valign=""bottom""><sup><sub>lr<br/>schd</sub></sup></th>
<th valign=""bottom""><sup><sub>im/<br/>gpu</sub></sup></th>
<th valign=""bottom""><sup><sub>box<br/>AP</sub></sup></th>
<th valign=""bottom""><sup><sub>box<br/>AP50</sub></sup></th>
<th valign=""bottom""><sup><sub>box<br/>AP75</sub></sup></th>
<th valign=""bottom""><sup><sub>mask<br/>AP</sub></sup></th>
<th valign=""bottom""><sup><sub>mask<br/>AP50</sub></sup></th>
<th valign=""bottom""><sup><sub>mask<br/>AP75</sub></sup></th>
<th valign=""bottom""><sup><sub>download<br/>links</sub></sup></th>
<!-- TABLE BODY -->
<tr>
<td align=""left""><sup><sub>R-50-FPN-GN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>38.4</sub></sup></td>
<td align=""right""><sup><sub>59.9</sub></sup></td>
<td align=""right""><sup><sub>41.7</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-50-FPN-GN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>42.2</sub></sup></td>
<td align=""right""><sup><sub>60.6</sub></sup></td>
<td align=""right""><sup><sub>45.8</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_R50-FPN-GN-cascade-rcnn.json"">boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-101-FPN-GN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>39.9</sub></sup></td>
<td align=""right""><sup><sub>61.3</sub></sup></td>
<td align=""right""><sup><sub>43.3</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-101-FPN-GN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Faster</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>43.8</sub></sup></td>
<td align=""right""><sup><sub>62.2</sub></sup></td>
<td align=""right""><sup><sub>47.6</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""right""><sup><sub>-</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_R101-FPN-GN-cascade-rcnn.json"">boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-50-FPN-GN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>39.2</sub></sup></td>
<td align=""right""><sup><sub>60.5</sub></sup></td>
<td align=""right""><sup><sub>42.9</sub></sup></td>
<td align=""right""><sup><sub>34.9</sub></sup></td>
<td align=""right""><sup><sub>57.1</sub></sup></td>
<td align=""right""><sup><sub>36.9</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;boxes</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-50-FPN-GN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>42.9</sub></sup></td>
<td align=""right""><sup><sub>60.7</sub></sup></td>
<td align=""right""><sup><sub>46.6</sub></sup></td>
<td align=""right""><sup><sub>36.6</sub></sup></td>
<td align=""right""><sup><sub>57.7</sub></sup></td>
<td align=""right""><sup><sub>39.2</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_R50-FPN-GN-cascade-rcnn-mask.json"">boxes</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/segmentations_coco_2014_minival_results_R50-FPN-GN-cascade-rcnn-mask.json"">masks</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-101-FPN-GN-baseline</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>2</sub></sup></td>
<td align=""right""><sup><sub>41.1</sub></sup></td>
<td align=""right""><sup><sub>62.1</sub></sup></td>
<td align=""right""><sup><sub>45.1</sub></sup></td>
<td align=""right""><sup><sub>36.3</sub></sup></td>
<td align=""right""><sup><sub>58.9</sub></sup></td>
<td align=""right""><sup><sub>38.5</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;boxes</a>&nbsp;|&nbsp;masks</a></sub></sup></td>
</tr>
<tr>
<td align=""left""><sup><sub>R-101-FPN-GN-cascade</sub></sup></td>
<td align=""left""><sup><sub>Mask</sub></sup></td>
<td align=""left""><sup><sub>1x</sub></sup></td>
<td align=""right""><sup><sub>1</sub></sup></td>
<td align=""right""><sup><sub>44.8</sub></sup></td>
<td align=""right""><sup><sub>62.8</sub></sup></td>
<td align=""right""><sup><sub>48.8</sub></sup></td>
<td align=""right""><sup><sub>38.0</sub></sup></td>
<td align=""right""><sup><sub>59.8</sub></sup></td>
<td align=""right""><sup><sub>40.8</sub></sup></td>
<td align=""left""><sup><sub>model</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/bbox_coco_2014_minival_results_R101-FPN-GN-cascade-rcnn-mask.json"">boxes</a>&nbsp;|&nbsp;<a href=""http://www.svcl.ucsd.edu/projects/cascade-rcnn/results/segmentations_coco_2014_minival_results_R101-FPN-GN-cascade-rcnn-mask.json"">masks</a></sub></sup></td>
</tr>
<!-- END E2E FASTER AND MASK TABLE -->
</tbody></table>
",[],1,0.75,0,,"# Contributing to Detectron
We want to make contributing to this project as easy and transparent as
possible.

## Our Development Process
Minor changes and improvements will be released on an ongoing basis. Larger
changes (e.g., changesets implementing a new paper) will be released on a more
periodic basis.

## Pull Requests
We actively welcome your pull requests.

1. Fork the repo and create your branch from `master`.
2. If you've added code that should be tested, add tests.
3. If you've changed APIs, update the documentation.
4. Ensure the test suite passes.
5. Make sure your code lints.
6. Ensure no regressions in baseline model speed and accuracy.
7. If you haven't already, complete the Contributor License Agreement (""CLA"").

## Contributor License Agreement (""CLA"")
In order to accept your pull request, we need you to submit a CLA. You only need
to do this once to work on any of Facebook's open source projects.

Complete your CLA here: <https://code.facebook.com/cla>

## Issues
GitHub issues will be largely unattended and are mainly intended as a community
forum for collectively debugging issues, hopefully leading to pull requests with
fixes when appropriate.

## Coding Style  
* 4 spaces for indentation rather than tabs
* 80 character line length
* PEP8 formatting

## License
By contributing to Detectron, you agree that your contributions will be licensed
under the LICENSE file in the root directory of this source tree.
",,,,15,,,,
63558093,MDEwOlJlcG9zaXRvcnk2MzU1ODA5Mw==,mscnn,zhaoweicai/mscnn,0,zhaoweicai,https://github.com/zhaoweicai/mscnn,Caffe implementation of our multi-scale object detection framework,0,2016-07-18 00:08:29+00:00,2024-12-22 01:46:13+00:00,2019-07-03 00:53:10+00:00,,7795,404,404,C++,1,1,1,1,0,0,211,0,0,56,,1,0,0,public,211,56,404,master,1,,"## A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection

by Zhaowei Cai, Quanfu Fan, Rogerio Feris and Nuno Vasconcelos

This implementation is written by Zhaowei Cai at UC San Diego.

### Introduction

MS-CNN is a unified multi-scale object detection framework based on deep convolutional networks, which includes an object proposal sub-network and an object detection sub-network. The unified network can be trained altogether end-to-end. 

### Citations

If you use our code/model/data, please cite our paper:

    @inproceedings{cai16mscnn,
      author = {Zhaowei Cai and Quanfu Fan and Rogerio Feris and Nuno Vasconcelos},
      Title = {A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection},
      booktitle = {ECCV},
      Year  = {2016}
    }

### Updates

This repository is merged to the latest Caffe. There is very minor numerical difference from the old version. By using the latest vresions of Caffe, CUDA and cuDNN, the speeds could be doubled. If you want to use the old version of code, you can download it from [MSCNN-V1.0](http://www.svcl.ucsd.edu/projects/mscnn/mscnn_v1.0.zip). 

### Requirements

1. cuDNN is required to avoid the issue of out-of-memory and have the same running speed described in our paper. For now, CUDA 8.0 with cuDNN v5 is tested. The other versions should be working.

2. If you want to use our MATLAB scripts to run the detection demo, caffe MATLAB wrapper is required. Please build matcaffe before running the detection demo. 

3. This code has been tested on Ubuntu 14.04 with an NVIDIA Titan GPU.

### Installation

1. Clone the MS-CNN repository, and we'll call the directory that you cloned MS-CNN into `MSCNN_ROOT`
    ```Shell
    git clone https://github.com/zhaoweicai/mscnn.git
    ```
  
2. Build MS-CNN
    ```Shell
    cd $MSCNN_ROOT/
    # Follow the Caffe installation instructions here:
    #   http://caffe.berkeleyvision.org/installation.html

    # If you're experienced with Caffe and have all of the requirements installed
    # and your Makefile.config in place, then simply do:
    make all -j 16

    # If you want to use MSCNN detection demo, build MATLAB wrapper as well
    make matcaffe
    ```

### Training MS-CNN (KITTI car)

1. Set up KITTI dataset by yourself.

2. Get the training data for KITTI
    ```Shell
    cd $MSCNN_ROOT/data/
    sh get_kitti_data.sh
    ```
    
    This will download train/val split image lists for the experiments, and window files for training/finetuning MS-CNN models. You can also use the provided MATLAB scripts `mscnn_kitti_car_window_file.m` under `$MSCNN_ROOT/data/kitti/` to generate your own window files. If you use the provided window files, replace `/your/KITTI/path/` in the files to your KITTI path.

3. Download VGG16 from [Caffe Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo), and put it into `$MSCNN_ROOT/models/VGG/`.

4. Now you can start to train MS-CNN models. Multiple shell scripts are provided to train different models described in our paper. We take `mscnn-7s-576-2x` for example. 
    ```Shell
    cd $MSCNN_ROOT/examples/kitti_car/mscnn-7s-576-2x/
    sh train_mscnn.sh
    ```
   As described in the paper, the training process is split into two steps. Usually the first step can be shared by different models if you only have modifications on detection sub-network. For example, the first training step can be shared by `mscnn-7s-576-2x` and `mscnn-7s-576`. Meanwhile, log files will be generated along the training procedures. 
 

### Pretrained model (KITTI car)

Download pre-trained MS-CNN models
```Shell
cd $MSCNN_ROOT/examples/kitti_car/
sh fetch_mscnn_car_model.sh
``` 
This will download the pretrained model for KITTI car into `$MSCNN_ROOT/examples/kitti_car/mscnn-8s-768-trainval-pretrained/`. You can produce exactly the same results as described in our paper with these pretrained models.

### Testing Demo (KITTI car)

Once the pretrained models or models trained by yourself are available, you can use the MATLAB script `run_mscnn_detection.m` under `$MSCNN_ROOT/examples/kitti_car/` to obtain the detection and proposal results. Set the right dataset path and choose the model that you want to test in the demo script. The default setting is to test the pretrained model. The final results will be saved as .txt files.

### KITTI Evaluation

Compile `evaluate_object.cpp` under `$MSCNN_ROOT/examples/kitti_result/eval/` by yourself. Use `writeDetForEval.m` under `$MSCNN_ROOT/examples/kitti_result/` to transform the detection results into KITTI data format and evaluate the detection performance. Remember to change the corresponding directories in the evaluation script. 

### Disclaimer

1. The CPU version is not fully tested. The GPU version is strongly recommended.
 
2. Since some changes have been made after ECCV submission, you may not have exactly the same results in the paper by training your own models. But you should have equivelant performance. 

3. Since the numbers of training samples vary vastly for different classes, the model robustness varies too (car>ped>cyc).

4. Although the final results we submitted were from model `mscnn-8s-768-trainval`, our later experiments have shown that `mscnn-7s-576-2x-trainval` can achieve even better performance for car, and 2x faster speed. For ped/cyc however, the performance decreases due to the much less training instances.  

5. If the training does not converge or the performance is very bad, try some other random seeds. You should obtain fair performance after a few tries. Due to the randomness, you cann't fully reproduce the same models, but the performance should be close.

If you encounter any issue when using our code or model, please let me know.
",[],1,0.79,0,,,,,,39,,,,
196211092,MDEwOlJlcG9zaXRvcnkxOTYyMTEwOTI=,fun-with-computer-graphics,zheng95z/fun-with-computer-graphics,0,zheng95z,https://github.com/zheng95z/fun-with-computer-graphics,"This is a collection of computer graphics related courses, books, tutorials, articles, blogs, resources, researcher homepages, lab homepages, video channels, open source projects, websites, etc.",0,2019-07-10 13:32:46+00:00,2025-03-07 19:03:11+00:00,2022-03-31 17:36:14+00:00,,78,2297,2297,,0,0,1,0,0,1,308,0,0,0,cc0-1.0,1,0,0,public,308,0,2297,master,1,,"<h1 align=""center"">fun-with-computer-graphics</h1>
<h5 align=""center"">Hi, you have just found fun-with-computer-graphics!</h5>

This is a collection of computer graphics related courses, books, tutorials, articles, blogs, resources, researcher homepages, lab homepages, video channels, open source projects, websites, etc.

Here are some useful tips for you to find what you want:

- You can now find the autogenerate table of contents in the top left corner (provided by GitHub).

## Computer Graphics

### Open-source Renderers

| Open-source Renderers                                        |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [mmp](https://github.com/mmp)/**[pbrt-v3](https://github.com/mmp/pbrt-v3)** | The renderer described in the third edition of ""Physically Based Rendering: From Theory To Implementation"", by Matt Pharr, Wenzel Jakob, and Greg Humphreys. |
| [mmp](https://github.com/mmp)/**[pbrt-v4](https://github.com/mmp/pbrt-v4)** | This is an early release of pbrt-v4, the rendering system that will be described in the (eventually) forthcoming fourth edition of *Physically Based Rendering: From Theory to Implementation*. |
| [mitsuba-renderer](https://github.com/mitsuba-renderer)/**[mitsuba](https://github.com/mitsuba-renderer/mitsuba)** | Mitsuba is a research-oriented rendering system in the style of PBRT, from which it derives much inspiration. |
| [mitsuba-renderer](https://github.com/mitsuba-renderer)/**[mitsuba2](https://github.com/mitsuba-renderer/mitsuba2)** | Mitsuba 2 is a research-oriented rendering system written in portable C++17. |
| [shiinamiyuki](https://github.com/shiinamiyuki)/**[AkariRender](https://github.com/shiinamiyuki/AkariRender)** | AkariRender is a highly modular CPU/GPU physically based renderer written in C++17. |
| [AirGuanZ](https://github.com/AirGuanZ)/**[Atrc](https://github.com/AirGuanZ/Atrc)** | Offline rendering lab based on ray tracing.                  |
| [Mike-Leo-Smith](https://github.com/Mike-Leo-Smith)/**[LuisaRender](https://github.com/Mike-Leo-Smith/LuisaRender)** | High-Performance Renderer on GPU.                            |
| [neverfelly](https://github.com/neverfelly)/**[misaki-render](https://github.com/neverfelly/misaki-render)** | A modular physically-based photorealistic global illumination renderer. |
| [FaithZL](https://github.com/FaithZL)/**[Paladin](https://github.com/FaithZL/Paladin)** |                                                              |
| [shiinamiyuki](https://github.com/shiinamiyuki)/**[minpt](https://github.com/shiinamiyuki/minpt)** | Small yet (almost) complete modern path tracer.              |
| [JiayinCao](https://github.com/JiayinCao)/**[SORT](https://github.com/JiayinCao/SORT)** | Simple Open-source Ray Tracer.                               |
| [g1n0st](https://github.com/g1n0st)/**[AyaRay](https://github.com/g1n0st/AyaRay)** | A Modern C++ Windows-platform physically based renderer developing by Chang Yu. |
| [tunabrain](https://github.com/tunabrain)/**[tungsten](https://github.com/tunabrain/tungsten)** | High performance physically based renderer in C++11.         |
| [google](https://github.com/google)/**[filament](https://github.com/google/filament)** | Filament is a real-time physically-based renderer written in C++. It is mobile-first, but also multi-platform. |
| [appleseedhq](https://github.com/appleseedhq)/**[appleseed](https://github.com/appleseedhq/appleseed)** | A modern open source rendering engine for animation and visual effects. |
| [jbikker](https://github.com/jbikker)/**[lighthouse2](https://github.com/jbikker/lighthouse2)** | Lighthouse 2 framework for real-time ray tracing.            |
| [harskish](https://github.com/harskish)/**[fluctus](https://github.com/harskish/fluctus)** | An interactive OpenCL wavefront path tracer.                 |

### Courses 

| Courses                                                      |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [GAMES101: 现代计算机图形学入门](https://sites.cs.ucsb.edu/~lingqi/teaching/games101.html)<br>([bilibili](https://www.bilibili.com/video/av90798049)) | **GAMES101**<br>本课程将全面而系统地介绍现代计算机图形学的四大组成部分：（1）光栅化成像，（2）几何表示，（3）光的传播理论，以及（4）动画与模拟。每个方面都会从基础原理出发讲解到实际应用，并介绍前沿的理论研究。通过本课程，你可以学习到计算机图形学背后的数学和物理知识，并锻炼实际的编程能力。<br>Taught by [@Lingqi Yan](https://sites.cs.ucsb.edu/~lingqi/). |
| [GAMES202: 高质量实时渲染](https://sites.cs.ucsb.edu/~lingqi/teaching/games202.html)<br/>([bilibili](https://www.bilibili.com/video/BV1YK4y1T7yY)) | **GAMES202**<br/>本课程将全面地介绍现代实时渲染中的关键问题与解决方法。由于实时渲染 (>30 FPS) 对速度要求极高，因此本课程的关注点将是在苛刻的时间限制下，人们如何打破速度与质量之间的权衡，同时保证实时的高速度与照片级的真实感。<br/>Taught by [@Lingqi Yan](https://sites.cs.ucsb.edu/~lingqi/). |
| [Rendering Algorithms (Fall21)](https://cs87-dartmouth.github.io/Fall2021/) | **Dartmouth**<br>This class focuses on advanced 3D graphics techniques for realistic image synthesis. You will learn how light interacts with objects in the real world, and how to translate the underlying math and physics into practical algorithms for rendering photorealistic images.<br>Taught by [@Wojciech Jarosz](https://cs.dartmouth.edu/~wjarosz/?tdsourcetag=s_pctim_aiomsg). |
| [Introduction to Computer Graphics](https://sites.cs.ucsb.edu/~lingqi/teaching/cs180.html) | **UCSB CS180**<br>This course is an introduction to the foundations of three-dimensional computer graphics. Topics covered include 2D and 3D transformations, Rasterization based interactive 3D graphics, shading and reflectance models, texture mapping, geometric modeling using Bézier and B-Spline curves, ray tracing, and animation. There will be an emphasis on both the mathematical and geometric aspects of graphics, as well as the ability to write fully functional 3D graphics programs.<br>Taught by [@Lingqi Yan](https://sites.cs.ucsb.edu/~lingqi/). |
| [Real-Time High Quality Rendering](http://www.cs.ucsb.edu/~lingqi/teaching/cs291a.html) | **UCSB CS291A**<br>In this course, we will review the history and some of the recent ideas that seek to bridge the gap between realism and interactivity. We will focus on the use of complex lighting and shading within limited computation time. Specifically, topics will cover programmable shaders, real-time shadows, interactive global illumination, image-based rendering, precomputed rendering, adaptive sampling and reconstruction, and real-time ray tracing.<br/>Taught by [@Lingqi Yan](https://sites.cs.ucsb.edu/~lingqi/). |
| [Introduction to Offline Rendering](https://sites.cs.ucsb.edu/~lingqi/teaching/cs190I.html) | **UCSB CS190I**<br>This course will teach you everything about offline rendering, so you will be able to write a fully functional industry-level renderer (such as Disney's Hyperion and Pixar's RenderMan) that produces stunning graphics. Topics in this course will cover the physics of light, the rendering equation, Monte Carlo integration, path tracing, physically-based reflectance models, participating media, other advanced light transport methods, production rendering approaches, and so on.<br>Taught by [@Lingqi Yan](https://sites.cs.ucsb.edu/~lingqi/). |
| [TU Wien Rendering/Ray Tracing Course](https://www.cg.tuwien.ac.at/courses/Rendering/VU.SS2018.html)<br>([YouTube](https://www.youtube.com/watch?v=pjc1QAI6zS0&list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi)) | **TU WIEN Rendering**<br>This course aims to give an overview of basic and state-of-the-art methods of rendering. Offline methods such as ray and path tracing, photon mapping and many other algorithms are introduced and various refinement are explained.<br>Taught by [@Károly Zsolnai-Fehér](https://users.cg.tuwien.ac.at/zsolnai/about/). |
| [Computer Graphics (Fall18)](https://canvas.dartmouth.edu/courses/30008) | **Dartmouth**<br/>This course provides a broad introduction to the mathematical and programmatic foundations of computer graphics, including modeling, rendering (drawing), and animating three-dimensional scenes.<br/>Taught by [@Wojciech Jarosz](https://cs.dartmouth.edu/~wjarosz/?tdsourcetag=s_pctim_aiomsg). |
| [Introduction to Computer Graphics and Imaging](https://web.stanford.edu/class/cs148/index.html) | **Stanford CS148**<br>This is the introductory prerequisite course in the computer graphics sequence which introduces students to the technical concepts behind creating synthetic computer generated images. |
| [Interactive Computer Graphics](http://cs248.stanford.edu/)  | **Stanford CS248**<br/>This course provides a comprehensive introduction to computer graphics, focusing on fundamental concepts and techniques, as well as their cross-cutting relationship to multiple problem domains in interactive graphics (such as rendering, animation, geometry, image processing). |
| [Image Synthesis Techniques](http://graphics.stanford.edu/courses/cs348b/) | **Stanford CS348b**<br>This course provides a broad overview of the theory and practice of making photo-realistic imagery. Rendering is treated as a problem in modeling and simulating the physics of light and appearance. |
| [Computer Graphics](http://15462.courses.cs.cmu.edu)         | **CMU 15-462/662**<br>This course provides a comprehensive introduction to computer graphics. Focuses on fundamental concepts and techniques, and their cross-cutting relationship to multiple problem domains in graphics (rendering, animation, geometry, imaging). |
| [Computer Graphics - AS 19](https://cgl.ethz.ch/teaching/cg19/home.php) | **ETH**<br>This course covers some of the fundamental concepts of modern computer graphics. The main topics of the course are modeling and rendering. During the course, we will discuss how digital 3D scenes are represented and modeled, and how a realistic image can be generated from a digital representation of a 3D scene.<br>Taught by [Computer Graphics Laboratory (CGL)](https://cgl.ethz.ch/teaching/cg19/home.php). |
| [Physically-based Simulation - AS 19](https://cgl.ethz.ch/teaching/simulation19/home.php) | **ETH**<br>Physically-based simulations are fundamental to many applications of computer graphics, including 3D video games, animated movies and films, or virtual surgery. This course introduces the physical concepts as well as the numerical methods required for simulating deformable objects, fluids, rigid bodies, and other physical systems. The material covered in this lecture ranges from simple mass-spring systems to advanced topics such as finite elements.<br>Taught by [Computer Graphics Laboratory (CGL)](https://cgl.ethz.ch/teaching/cg19/home.php). |
| [COS 426 Computer Graphics (Spring19)](https://www.cs.princeton.edu/courses/archive/spring19/cos426/index.php) | **Princeton COS 426**<br>This course will study topics in computer graphics, covering methods in image processing, modeling, rendering, and animation. |
| [COS 526 Advanced Computer Graphics (Fall16)](https://www.cs.princeton.edu/courses/archive/fall16/cos526/index.php) | **Princeton COS 526**<br>This course will study advanced topics in computer graphics, covering methods in computational photography, geometric modeling, photorealistic rendering, and other topics in computer graphics. |
| [Physically Based Rendering and Material Appearance Modelling](http://courses.compute.dtu.dk/02941/) | **DTU 02941**<br>This course takes its outset in the appearance of real world materials. The goal is to get as close as possible to replicating the appearance of real materials by computer graphical rendering based on mathematical/physical models. |
| [Interactive 3D Graphics by Autodesk](https://www.udacity.com/course/interactive-3d-graphics--cs291) | **Udacity CS291**<br>This class will teach you about the basic principles of 3D computer graphics: meshes, transforms, cameras, materials, lighting, and animation.<br>Taught by [@Eric Haines](https://erich.realtimerendering.com/) |
| [Computer Graphics and Imaging](https://cs184.eecs.berkeley.edu/sp19) | **Berkeley cs184/284a**<br>This course provides a broad introduction to the fundamentals of computer graphics. The main areas covered are modeling, rendering, animation and imaging. Topics include 2D and 3D transformations, drawing to raster displays, sampling, texturing, antialiasing, geometric modeling, ray tracing and global illumination, animation, cameras, image processing and computational imaging. There will be an emphasis on mathematical and geometric aspects of graphics, and the ability to write complete 3D graphics programs. |
| [Introduction To Computer Graphics](https://cs.brown.edu/courses/cs123/index.shtml) | **Brown CS123**<br>This course offers an in-depth exploration of fundamental concepts in 2D and 3D computer graphics. It introduces 2D raster graphics techniques, including scan conversion, simple image processing, interaction techniques and user interface design. The bulk of the course is devoted to 3D modeling, geometric transformations, and 3D viewing and rendering. A sequence of assignments culminates in a simple geometric modeler and ray tracer. C++ and the graphics library OGL are used throughout the course, as is shader programming on the GPU, taught from the first lab onwards. The final project is typically a small group project spec'd and implemented by the group using shaders to create special effects. |
| [Introduction To Computer Graphics](http://www.cs.cornell.edu/courses/cs4620/2019fa/) | **Cornell CS4620**<br>The study of creating manipulating, and using visual images in the computer. |

### SIGGRAPH Courses

| SIGGRAPH Courses                                             |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Path tracing in production](https://jo.dreggn.org/path-tracing-in-production/) | This is the web page for the SIGGRAPH courses on path tracing in production. |

### Books

| Books                                                        |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Fundamentals of Computer Graphics, 4th Edition               | Drawing on an impressive roster of experts in the field, **Fundamentals of Computer Graphics, Fifth Edition** offers an ideal resource for computer course curricula as well as a user-friendly personal or professional reference. |
| Computer Graphics: Principles and Practice, 3rd Edition<br>([book website](cgpp.net)) | In this book, we explain the principles, as well as the mathematics, underlying computer graphics--knowledge that is essential for successful work both now and in the future. Early chapters show how to create 2D and 3D pictures right away, supporting experimentation. Later chapters, covering a broad range of topics, demonstrate more sophisticated approaches. Sections on current computer graphics practice show how to apply given principles in common situations, such as how to approximate an ideal solution on available hardware, or how to represent a data structure more efficiently. Topics are reinforced by exercises, programming problems, and hands-on projects. |
| Real-Time Rendering, 4th Edition<br>([book website](http://www.realtimerendering.com/)) | This edition discusses current, practical rendering methods used in games and other applications. It also presents a solid theoretical framework and relevant mathematics for the field of interactive computer graphics, all in an approachable style. New to this edition: new chapter on VR and AR as well as expanded coverage of Visual Appearance, Advanced Shading, Global Illumination, and Curves and Curved Surfaces. |
| Physically Based Rendering: From Theory To Implementation, Third Edition<br>([read for free](http://www.pbr-book.org/)) | *Physically Based Rendering* describes both the mathematical theory behind a modern photorealistic rendering system as well as its practical implementation. A method known as “literate programming” combines human-readable documentation and source code into a single reference that is specifically designed to aid comprehension. The ideas and software in this book show the reader how to design and employ a full-featured rendering system capable of creating stunning imagery. |
| Robust Monte Carlo Methods for Light Transport Simulation<br>([thesis website](https://graphics.stanford.edu/papers/veach_thesis/)) | Eric Veach. PhD Thesis, Stanford University, 1997<br>Nearly 20 years later, this monster thesis is *still* relevant when it comes to developing rendering algorithms. Introduces Monte Carlo rendering methods, multiple importance sampling, bidirectional path tracing, Metropolis Light Transport |
| Advanced Global Illumination<br>([authors' site](http://graphics.cs.kuleuven.be/publications/AGI2E/index.html), [Google Books sample](https://books.google.com/books?id=TB1jDAAAQBAJ&printsec=frontcover)) | This book provides the reader with a fundamental understanding of global illumination algorithms. It discusses a broad class of algorithms for realistic image synthesis and introduces a theoretical basis for the algorithms presented. |
| 《Ray Tracing in One Weekend》series <br>([read for free](https://raytracing.github.io/)) | [Peter Shirley](https://twitter.com/Peter_shirley)'s *The Ray Tracing in One Weekend* series of books |
| 《Ray Tracing Gems》<br>([book website](http://raytracinggems.com/)) | This book is a collection of articles focused on ray tracing techniques for serious practitioners. Like other ""gems"" books, it focuses on subjects commonly considered too advanced for introductory texts, yet rarely addressed by research papers. |

### Tutorials

| Tutorials                                                    |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Nori 2](https://wjakob.github.io/nori/)                     | Nori is a minimalistic ray tracer written in C++. It runs on Windows, Linux, and Mac OS and provides a foundation for the homework assignments in the course [Advanced Computer Graphics](http://rgl.epfl.ch/courses/ACG17) taught at EPFL. |
| [darts](https://cs87-dartmouth.github.io/Fall2021/darts-overview.html) | darts is a minimalistic skeleton for a Monte Carlo ray tracer, written in C++17. The name is an acronym for *The Dartmouth Academic Ray Tracer Skeleton* while also being a nod to the random dart-throwing-like process involved in Monte Carlo ray tracing. It runs on recent versions of Windows, Linux, and macOS and provides the foundation for the programming assignments we'll be doing in this class. |
| [The Graphics Codex](http://graphicscodex.com/)              | by [Morgan McGuire](https://twitter.com/CasualEffects)       |
| [Scratchapixel 2.0](http://www.scratchapixel.com/index.php)  | 32 lessons, 166 chapters, 450,000 words, C++ source code     |
| [Windows渲染引擎入门](https://www.zhihu.com/column/c_1465096004047822849) | by [MaxwellGeng](https://www.zhihu.com/people/maxwellgeng)   |
| [Vulkan 渲染器开发实战小师班](https://zhuanlan.zhihu.com/p/478021889) | by [SaeruHikari](https://www.zhihu.com/people/SaeruHikari)   |
| [Learn Vulkan](https://learnvulkan.com/book/)                |                                                              |
| [Vulkan Tutorial](https://vulkan-tutorial.com/)              |                                                              |
| Learn OpenGL<br>([English](https://learnopengl.com/), [Chinese](https://learnopengl-cn.github.io/)) | The aim of LearnOpenGL is to show you all there is to modern OpenGL in an easy-to-understand fashion with clear examples, while also providing a useful reference for later studies. |
| [Joey de Vries OpenGL Tutorials](http://learnopengl.com/)    | OpenGL                                                       |
| [Pixar in a Box: Rendering](https://www.khanacademy.org/partner-content/pixar/rendering) | by Pixar                                                     |
| [The Book of Shaders](https://thebookofshaders.com/)         |                                                              |
| [3d-game-shaders-for-beginners](https://github.com/lettier/3d-game-shaders-for-beginners) |                                                              |
| [Daily Pathtracer](http://aras-p.info/blog/2018/03/28/Daily-Pathtracer-Part-0-Intro/) | by [Aras Pranckevičius](http://aras-p.info/)                 |
| [Tiny renderer or how OpenGL works](https://github.com/ssloy/tinyrenderer/wiki) | software rendering in 500 lines of code                      |
| [Rasterization in One Weekend](https://tayfunkayhan.wordpress.com/2018/11/24/rasterization-in-one-weekend/) | by [Tayfun Kayhan](https://tayfunkayhan.wordpress.com/)      |

### Articles

| Articles                                                     |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [什么是计算机图形学](http://staff.ustc.edu.cn/~lgliu/Resources/CG/What_is_CG.htm)<br>[什么是深度学习？](http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.html)<br/>[数学在计算机图形学中的应用](http://staff.ustc.edu.cn/~lgliu/Resources/SummerSchool/USTC-summer-school.html) | by [刘利刚](http://staff.ustc.edu.cn/~lgliu)                 |
| [系统的学习计算机图形学，有哪些不同阶段的书籍的推荐？](https://www.zhihu.com/question/26720808) | Q&A                                                          |
| [现阶段应该怎么学习计算机图形学呢？](https://www.zhihu.com/question/26341836) | Q&A                                                          |
| [光线追踪与实时渲染的未来](https://zhuanlan.zhihu.com/p/34851503) | by [Edward Liu](http://behindthepixels.io/about/)            |
| 基于物理着色<br>([1](https://zhuanlan.zhihu.com/p/20091064), [2](https://zhuanlan.zhihu.com/p/20119162), [3](https://zhuanlan.zhihu.com/p/20122884), [4](https://zhuanlan.zhihu.com/p/21247702)) | by [Edward Liu](http://behindthepixels.io/about/)            |
| [How to Start Learning Computer Graphics Programming](https://erkaman.github.io/posts/beginner_computer_graphics.html)<br>([Chinese](https://zhuanlan.zhihu.com/p/55518151)) | by [Eric Arnebäck](https://erkaman.github.io/index.html)     |
| [Finding Your Home in Game Graphics Programming](https://alextardif.com/LearningGraphics.html) | by Alex Tardif                                               |
| [Bidirectional Path Tracing](https://rendering-memo.blogspot.com/2016/03/bidirectional-path-tracing-1-kickoff.html) series | by [Wei-Feng Wayne Huang](https://www.blogger.com/profile/01056048230170636241) |
| [BSSRDF Importance Sampling](https://rendering-memo.blogspot.com/2015/01/bssrdf-importance-sampling-1-kickoff.html) series | by [Wei-Feng Wayne Huang](https://www.blogger.com/profile/01056048230170636241) |

### Blogs

| Blogs                                                        | [(Back to TOC)](#fwg)                                        |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [A Graphics Guy's Note](https://agraphicsguynotes.com/)      | Jiayin Cao's some random notes about computer graphics.      |
| [Rendering Memo](https://rendering-memo.blogspot.com/)       | [Wei-Feng Wayne Huang](https://www.blogger.com/profile/01056048230170636241), Walt Disney Animation Studio rendering software developer. |
| [Self Shadow](https://blog.selfshadow.com/)                  | Walt Disney Animation Studio rendering software developer. I worked on few film production renderers[@self_shadow](https://twitter.com/self_shadow) has been collecting [Siggraph courses/papers links](https://blog.selfshadow.com/categories/conference/) for many years:  Especially interesting are the Physically Based Shading in Theory and Practice course presentations. |
| [Code & Visuals](https://blog.yiningkarlli.com/)             | Yining Karl Li, computer graphics senior software engineer at [Walt Disney Animation Studios](http://www.disneyanimation.com/) working on Disney's in-house production physically based renderer, [Hyperion.](https://www.disneyanimation.com/technology/hyperion) |
| [INTERPLAY OF LIGHT](https://interplayoflight.wordpress.com/) | ""This blog is my scratchpad for graphics techniques I try and experiment with.""<br>by [@Kostas Anagnostou](https://twitter.com/KostasAAA) |
| [Alan Zucconi](https://www.alanzucconi.com/)                 | author of the book *Unity 2018 Shaders and Effects Cookbook* |
| [Linden Reid](https://lindenreid.wordpress.com/)             | Procedural geometry & graphics tutorials<br>A game developer at Blizzard |
| [Harold Serrano](https://www.haroldserrano.com/home/)        | Creator of the [Untold Engine](https://www.untoldengine.com/) |
| [Behind the Pixels](http://behindthepixels.io/)              | Edward(Shiqiu) Liu, a Senior Real Time Rendering Engineer at NVIDIA<br> |
| [iquilezles](http://www.iquilezles.org/index.html)           | Inigo Quilez                                                 |
| [JOEY DE VRIES](https://joeydevries.com/#home)               | Joey, author of [learnopengl.com](https://learnopengl.com/)  |
| [Coding Labs](http://www.codinglabs.net/default.aspx)        |                                                              |
| [TYLER HOBBS](https://tylerxhobbs.com/)                      |                                                              |
| [HUMUS](http://humus.name/)                                  |                                                              |
| [Icare3D Blog](https://blog.icare3d.org/)                    |                                                              |

### Resources pages

| Resources pages                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Real-Time Rendering Resources](http://www.realtimerendering.com/) | Resources for Real-Time Rendering                            |
| [Ke-Sen Huang's Home Page](http://kesen.realtimerendering.com/) | A collection of CG papers (from SIGGRAPH, Asia, EG, PG, etc.) |
| [Rendering Resources](https://benedikt-bitterli.me/resources/) | This page offers 32 different 3D scenes that you can use for free in your rendering research, publications and classes. |
| [Graphics Programming weekly](https://jendrikillner.bitbucket.io/#posts) | Update per week, by [@Jendrik Illner](https://jendrikillner.bitbucket.io/) |
| [McGuire Computer Graphics Archive](https://casual-effects.com/data/) | A collection of models.                                      |
| [Technically Art](https://halisavakis.com/category/technically-art/) | by [@Harry Alisavakis](https://halisavakis.com/)             |
| [Open-Source Real-Time Rendering Engines and Libs](https://konstantinkhomyakov3d.github.io/real-time-engines/) |                                                              |
| [GDCVault](https://www.gdcvault.com/)                        | GDC Vault is a trove of in-depth design, technical and inspirational talks and slides from the influencers of the game development industry, taken from over 20 years of the worldwide Game Developers Conferences. |
| [Graphics Research Tools](https://developer.nvidia.com/graphics-research-tools) | by Nvidia                                                    |
| [mattdesl/graphics-resources](mattdesl/graphics-resources)   | A list of graphic programming resources                      |
| [Readings on The State of The Art in Rendering](https://interplayoflight.wordpress.com/2018/09/30/readings-on-the-state-of-the-art-in-rendering/) | by [Kostas Anagnostou](https://twitter.com/KostasAAA)        |
| [Readings on Physically Based Rendering](https://interplayoflight.wordpress.com/2013/12/30/readings-on-physically-based-rendering/) | by [Kostas Anagnostou](https://twitter.com/KostasAAA)        |
| [Advances in Real-Time Rendering in 3D Graphics and Games](http://advances.realtimerendering.com/) | the well-established series of SIGGRAPH courses<br>covering late-breaking work and advances in real-time computer graphics |
| [terkelg/awesome-creative-coding](https://github.com/terkelg) | Creative Coding: Generative Art, Data visualization, Interaction Design, Resources |
| [eug/awesome-opengl](https://github.com/eug/awesome-opengl)  | A curated list of awesome OpenGL libraries, debuggers and resources |
| [vinjn/awesome-vulkan](https://github.com/vinjn/awesome-vulkan) | A curated list of awesome Vulkan libraries, debuggers and resources. Inspired by [awesome-opengl](https://github.com/eug/awesome-opengl) and other awesome-... stuff. |
| [ericjang/awesome-graphics](https://github.com/ericjang/awesome-graphics) | This is a curated list of computer graphics tutorials and resources. |
| [Real-time Rendering Blogs](http://svenandersson.se/2014/realtime-rendering-blogs.html) |                                                              |

### Researchers

| Researchers                                                  |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Benedikt Bitterli](https://benedikt-bitterli.me/index.html) | A PhD at Dartmouth College.                                  |
| [Kun Xu (徐昆)](https://cg.cs.tsinghua.edu.cn/people/~kun/)  | Associate professor at Graphics and Geometric Computing Group, Tsinghua University. |
| [Lingqi Yan (闫令琪)](http://www.cs.ucsb.edu/~lingqi/?tdsourcetag=s_pctim_aiomsg#bio) | Assistant Professor at UC Santa Barbara.                     |
| [Ligang Liu (刘利刚)](http://staff.ustc.edu.cn/~lgliu/)      | A Professor at Graphics & Geometric Computing Laboratory (GCL), school of mathematical sciences USTC. |
| [Matt Pharr](https://pharr.org/matt/)                        | A research scientist at NVIDIA Research.                     |
| [Morgan McGuire](https://www.cs.williams.edu/~morgan/#courses) | A Professor of Computer Science at Williams College.         |
| [Pradeep Sen](https://web.ece.ucsb.edu/~psen/)               | A Professor at UC Santa Barbara.                             |
| [Ravi Ramamoorthi](http://cseweb.ucsd.edu/~ravir/)           | Professor at UC San Diego.                                   |
| [Thomas Müller](https://tom94.net/index.php)                 | A senior research scientist at NVIDIA Zürich.                |
| [Toshiya Hachisuka](https://www.ci.i.u-tokyo.ac.jp/~hachisuka/) | An Associate Professor at The University of Tokyo.           |
| [Wenzel Jakob](https://people.epfl.ch/265226)                | An assistant professor leading the Realistic Graphics Lab at EPFL's School of Computer and Communication Sciences. |
| [Wojciech Jarosz](https://cs.dartmouth.edu/~wjarosz/)        | An Assistant Professor at Dartmouth College.                 |

###  Labs

| Labs                                                         |                   |
| ------------------------------------------------------------ | ----------------- |
| [Graphics & Geometric Computing Group](https://cg.cs.tsinghua.edu.cn/) | at Tsinghua Univ. |
| [UCSB MIRAGE Lab](http://graphics.cvc.ucsb.edu/)             | at UCSB           |
| [Visual Computing Lab](http://vcl.cs.dartmouth.edu/#about)   | at Dartmouth      |
| [Utah Graphics Lab](https://graphics.cs.utah.edu/#u)         | at Utah           |
| [Realistic Graphics Lab](http://rgl.epfl.ch/)                | at EPFL           |
| [Computer Graphics Lab](https://cgl.ethz.ch/)                | at ETH Zürich     |
| [Graphics Lab](http://graphics.stanford.edu/)                | at Stanford       |
| [Cornell Graphics and Vision Group](https://rgb.cs.cornell.edu/) | at Cornell        |
| [Princeton ImageX Labs](https://pixl.cs.princeton.edu/)      | at Princeton      |
| [Carnegie Mellon Graphics Lab](http://graphics.cs.cmu.edu/)  | at CMU            |
| [MIT Computer Graphics Group](http://graphics.csail.mit.edu/) | at MIT            |
| [Center for Visual Computing](http://visualcomputing.ucsd.edu/) | at UCSD           |

### Video Channels

| Video Channels                                               |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [ACMSIGGRAPH](https://www.youtube.com/channel/UCbaxUExGKrH2zxY4AkY9wCg) |                                                              |
| [The Cherno](https://www.youtube.com/user/TheChernoProject)  | C++,<br/>Game Engine,<br/>...                                |
| [ChiliTomatoNoodle](https://www.youtube.com/user/ChiliTomatoNoodle) | DirectX & C++ Game Programming,<br>3D Programming Fundamentals,<br>... |
| [MIT OpenCourseWare](https://www.youtube.com/user/MIT)       |                                                              |

###  Open-source Projects

| Projects                                                     |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Intel® Embree](https://www.embree.org/)                     | Intel® Embree is a collection of high-performance ray tracing kernels, developed at Intel. |
| [The Mesa 3D Graphics Library](https://www.mesa3d.org/intro.html) | The Mesa project began as an open-source implementation of the [OpenGL](https://www.opengl.org/) specification - a system for rendering interactive 3D graphics. |
| [The Advanced Rendering Toolkit](https://cgg.mff.cuni.cz/ART/) | ART is a command-line system for physically based image synthesis. |
| [TAICHI](http://taichi.graphics/)                            | TAICHI: Open-source computer graphics library                |
| [Intel® Embree](https://embree.github.io/)                   | Intel® Embree is a collection of high-performance ray tracing kernels, developed at Intel |
| [MERL BRDF Database](http://www.merl.com/brdf/)              | The MERL BRDF database contains reflectance functions of 100 different materials |
| [yocto-gl](https://github.com/xelatihy/yocto-gl)             | Tiny C++ Libraries for Data-Driven Physically-based Graphics |
| [id-Software](https://github.com/id-Software)                | id-Software公司的所有游戏的开源代码                          |
| [WebGL Fluid Simulation](https://paveldogreat.github.io/WebGL-Fluid-Simulation/) |                                                              |
| [Scotty3D](https://github.com/cmu462/Scotty3D)               | 3D graphics software for mesh editing, path tracing, and animation |

###  Websites

| Websites                                                     |      |
| ------------------------------------------------------------ | ---- |
| [Shadertoy](https://www.shadertoy.com/)                      |      |
| [CSRankings: Computer Science Rankings](http://csrankings.org/#/index?graph&world) |      |

### Essential Mathematics

| Essential Mathematics                                        |                                              |
| ------------------------------------------------------------ | -------------------------------------------- |
| [Probability Theory for Physically Based Rendering Part 1](https://jacco.ompf2.com/2019/12/11/probability-theory-for-physically-based-rendering/), [Part 2](https://jacco.ompf2.com/2019/12/13/probability-theory-for-physically-based-rendering-part-2/) | by [Jacco Bikker.](https://jacco.ompf2.com/) |
| [Immersive linear Algebra](http://immersivemath.com/ila/index.html) |                                              |

","['infancy', 'nikoong', 'zheng95z', 'setoye', 'limberc']",0,0.65,0,,,,,,90,,,AlexandrovLab,
375644095,MDEwOlJlcG9zaXRvcnkzNzU2NDQwOTU=,nlp-phd-global-equality,zhijing-jin/nlp-phd-global-equality,0,zhijing-jin,https://github.com/zhijing-jin/nlp-phd-global-equality,A repo for open resources & information for people to succeed in PhD in CS & career in AI / NLP,0,2021-06-10 09:33:18+00:00,2025-03-08 06:26:35+00:00,2024-09-22 15:07:28+00:00,,657,901,901,,1,1,1,1,0,0,75,0,0,1,,1,0,0,public,75,1,901,main,1,,,"['zhijing-jin', 'pitehu', 'its-fatemeh-mh', 'awebson']",0,0.61,0,,,,,,19,,,,
14942891,MDEwOlJlcG9zaXRvcnkxNDk0Mjg5MQ==,Genomic-Interactive-Visualization-Engine,Zhong-Lab-UCSD/Genomic-Interactive-Visualization-Engine,0,Zhong-Lab-UCSD,https://github.com/Zhong-Lab-UCSD/Genomic-Interactive-Visualization-Engine,Genomic Interactive Visualization Engine,0,2013-12-05 03:27:33+00:00,2024-11-11 06:22:01+00:00,2023-01-05 00:56:45+00:00,https://www.givengine.org/,75473,145,145,JavaScript,1,1,1,1,1,0,32,0,0,29,apache-2.0,1,0,0,public,32,29,145,master,1,1,"# GIVE (Genomic Interactive Visualization Engine)

[![DOI](https://zenodo.org/badge/14942891.svg)](https://zenodo.org/badge/latestdoi/14942891)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](code_of_conduct.md)

GIVE (Genomic Interactive Visualization Engine) is a HTML5 library that lets you embed genomic visualization panels as you would standard HTML elements to build a customized genome browser to visualize data from public deposits (such as ENCODE) and/or in-house data.

GIVE uses [Web Components](https://www.webcomponents.org/), specifically [Polymer Library](https://www.polymer-project.org/) for user interface and [Scaled Vector Graphics (SVG) 1.1](https://www.w3.org/TR/SVG/) for graphics. These components are supported by all major browsers.

```html
<!-- Polyfill Web Components for browsers without native support -->
<script src=""https://www.givengine.org/bower_components/webcomponentsjs/webcomponents-lite.js""></script>

<!-- Import GIVE component -->
<link rel=""import"" href=""https://www.givengine.org/components/chart-controller/chart-controller.html"">

<!-- Embed the browser in your web page -->
<chart-controller ref=""mm10"" title-text=""My GIVE Browser""
  coordinate='[""chr17:35504032-35512777""]'
  group-id-list='[""genes"", ""singleCell"", ""customTracks""]'></chart-controller>
```

## Table of Contents
*   [Installation](#installation)
*   [Usage](#usage)
    *   [Importing GIVE Components](#importing-give-components)
    *   [Implementing A Customized Genome Browser by Embedding GIVE Components](#implementing-a-customized-genome-browser-by-embedding-give-components)
*   [Supported Tracks](#supported-tracks)
*   [Tutorial](#tutorial)
*   [Manual](#manual)
*   [Credits](#credits)
*   [License](#license)

## Installation

*Installation of GIVE is optional and not required for the use of any of the Web Components of GIVE. By installing GIVE components, you can serve codes and/or data sources directly from your own server.*

GIVE consists of three major parts: GIVE Web Components, the client-side codes running in browsers, implemented by HTML5, GIVE server, including bare server codes, implemented by PHP, and GIVE Data Source.

To install any part of GIVE, a web-hosting environment is needed on your server. Please refer to [GIVE Tutorial 2.2: Custom Installation of GIVE](tutorials/2.2-custom-installation.md) for detailed instructions. Alternatively, you can use GIVE-Docker (__*recommended*__), a Docker image delivering an already configured GIVE server and GIVE Web Components. Please refer to [GIVE Tutorial 2.1: Easy local deployment of GIVE with GIVE-Docker](tutorials/2.1-GIVE-Docker.md)

## Usage

You may use __GIVE-HUG (HTML Universal Generator)__ on the GIVE Data Hub page to generate the HTML code needed for embedding. The GIVE Data Hub is accessible on the public GIVE instance at <https://www.givengine.org/give-data-hub.html> and will be at the same relative location after you install your own GIVE instance. If you would like to code the HTML files by yourself, you can use GIVE Web Components by importing the component and using the tags directly.

### Importing GIVE Web Components

To use GIVE Web Components, just use HTML `import` to import Web Components polyfill and the required Web Components.

All components, including Web Component polyfill, are available on our website, the public GIVE instance, for direct HTML import without any installation.
```html
<script src=""https://www.givengine.org/bower_components/webcomponentsjs/webcomponents-lite.js""></script>
<link rel=""import"" href=""https://www.givengine.org/components/chart-controller/chart-controller.html"">
```

If you already installed GIVE Web Components on your server, please use your own path.
```html
<script src=""/bower_components/webcomponentsjs/webcomponents-lite.js""></script>
<link rel=""import"" href=""/components/chart-controller/chart-controller.html"">
```

### Implementing A Customized Genome Browser by Embedding GIVE Components

After you have imported the components in your HTML page, you can use them in several ways. The most straightforward way is to use them as if you are using common HTML tags (like `<div>` or `<video>`):
```html
<chart-controller ref=""mm10"" title-text=""My First GIVE Browser""
  group-id-list='[""genes"", ""singleCell"", ""customTracks""]'></chart-controller>
```

You can also use `Document.createElement()` to create the element in your JavaScript code:
```JavaScript
var myChart = document.createElement('chart-controller')
myChart.titleText = ""My First GIVE Browser""
myChart.groupIdList = [""genes"", ""singleCell"", ""customTracks""]
```
Or to use the built-in JavaScript constructors:
```JavaScript
var myChart = new GIVE.ChartController({
  titleText: ""My First GIVE Browser"",
  groupIdList: [""genes"", ""singleCell"", ""customTracks""]
})
```

## Supported Tracks

Currently GIVE supports three types of tracks:
*   [BED Track](html/components/track-object/bed-track/)
*   [bigWig Track](html/components/track-object/bigwig-track/)
*   [Interaction Track](html/components/track-object/interaction-track/)

## Tutorial

We have compiled a series of tutorials for different GIVE functionalities and components. Please check out the [Tutorial Homepage](tutorials/Readme.md).

## Manual

The GIVE Manual is available under the [Manual Homepage](manuals/Readme.md).

## How to contribute to GIVE

Any kind of contributions to GIVE development are welcome. Please read the [Contribution Guides](./CONTRIBUTING.md) and the [Covenant Code of Conduct](./CODE_OF_CONDUCT.md) first.


## Credits

GIVE is developed by Xiaoyi Cao, Zhangming Yan, Qiuyang Wu, Alvin Zheng from Dr. Sheng Zhong's lab at University of California, San Diego.

## License

Copyright 2017 GIVe Authors

This project is licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this project except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
","['frankyan', 'alvintzheng', 'qiuyangwu', 'caoxiaoyi03', 'zhonglab', 'Irenexzwen', 'dependabot[bot]', 'adelq']",1,0.65,0,"# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
 advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
 address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
 professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at szhong@ucsd.edu. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq
","# How to contribute to GIVE 
Welcome and we're so glad that you would like to contribute to GIVE! We're dedicated to make GIVE an easy-to-master tool for genome visualization and we value all your input!
### Do you find a new bug?
Before creating bug reports, please check the issues panel to make sure there hasn't been a similar issue. When you are creating a bug report,  we recommend to follow a template that been used in many open source packages, and it will help us fix the bugs faster! 
```
- Brief description 
- Steps to reproduce:
  - Expected behaviour:
  - Actual behaviour:
- Versions 
- Additional information
```
### Do you have problem using GIVE?
Before you would like to submit a problem relate to how to use a specific function in GIVE, we would strongly recommend you first go through our manuals and tutorials on GIVE hub page. If still not able to solve your question, you could either raise a issue or to developer team email. 

### Do you want to add a new feature or a patch?
To submit a suggestion of new feature or a enhancement patch, we suggest:
- First raise a issue decribe the current limition. Every enhancement suggestion would be tracked as Github issues. 
- Provide a detailed description of the feature you want to add in. Could compare the current behavior and your expectation.
- Indicate the specific repository location. 
- Specify the version of GIVE you're using. 

Thanks for all your input :+1::tada: <br>
Sincerely <br>
GIVE team :heart:
",,,,9,,,,
250979951,MDEwOlJlcG9zaXRvcnkyNTA5Nzk5NTE=,RecLearn,ZiyaoGeng/RecLearn,0,ZiyaoGeng,https://github.com/ZiyaoGeng/RecLearn,Recommender Learning with Tensorflow2.x,0,2020-03-29 07:47:59+00:00,2025-03-07 14:04:39+00:00,2022-04-29 06:10:37+00:00,,113850,1914,1914,Python,1,0,1,1,0,0,497,0,0,17,mit,1,0,0,public,497,17,1914,reclearn,1,,,['ZiyaoGeng'],0,0.62,0,,,,,,35,,,,