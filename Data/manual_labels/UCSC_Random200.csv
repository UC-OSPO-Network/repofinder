id,node_id,name,full_name,private,owner,html_url,description,fork,created_at,updated_at,pushed_at,homepage,size,stargazers_count,watchers_count,language,has_issues,has_projects,has_downloads,has_wiki,has_pages,has_discussions,forks_count,archived,disabled,open_issues_count,license,allow_forking,is_template,web_commit_signoff_required,visibility,forks,open_issues,watchers,default_branch,score,contributors,organization,readme,manual_label,prediction,prediction_nn,code_of_conduct,contributing,security_policy,issue_templates,pull_request_template,release_downloads,subscribers_count,,Groups/Clubs,Organizations
243135874,MDEwOlJlcG9zaXRvcnkyNDMxMzU4NzQ=,site-ucsc-notable-women,ucsc/site-ucsc-notable-women,0,ucsc,https://github.com/ucsc/site-ucsc-notable-women,UC Santa Cruz phenomenal  women..,0,2020-02-26 00:55:25+00:00,2020-10-20 06:59:39+00:00,2023-04-11 23:20:26+00:00,,1571,0,0,HTML,1,1,1,1,0,0,0,0,0,4,mit,1,0,0,public,0,4,0,master,1,"['LuckyLuke001', 'luckyluke007', 'dependabot[bot]']",1,"# jekyll-docker-base
 Create a static site using Jekyll running in a Docker container.


### Site Deploy Status
[![Netlify Status](https://api.netlify.com/api/v1/badges/09db9ddb-dc7e-4394-b8b3-025ad87c111e/deploy-status)](https://app.netlify.com/sites/site-shenomenal/deploys)

### [Codepen Layout](https://codepen.io/luckyluke007/pen/ExjgNYE)

All CSS and Javascript editing should be make on Codepen.

- [W3bits Mansonry grid](https://w3bits.com/css-grid-masonry/)
- [Font Awesome](https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css)
- [Google webfonts](https://fonts.googleapis.com/css?family=Lora|Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i&amp;display=swap)
- [Jquery 3.4.1](https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js)



",1,0.96,0.96,,,,,,0,5,,linqs,uccross
426666384,R_kgDOGW5pkA,bigbed,yroochun/bigbed,0,yroochun,https://github.com/yroochun/bigbed,contains bigbed files for ucsc genome browser,0,2021-11-10 15:05:24+00:00,2021-11-16 13:48:09+00:00,2021-11-16 13:48:06+00:00,,239,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['yroochun'],,"# bigbed
",0,0.6,0.6,,,,,,0,1,,BD2KGenomics,UCSCLibrary
704366793,R_kgDOKfvIyQ,R2H,eric-ai-lab/R2H,0,eric-ai-lab,https://github.com/eric-ai-lab/R2H,"Official implementation of the EMNLP 2023 paper ""R2H: Building Multimodal Navigation Helpers that Respond to Help Requests""",0,2023-10-13 05:39:37+00:00,2024-07-11 10:53:59+00:00,2024-06-19 22:38:35+00:00,https://sites.google.com/view/response2helprequests/home,1157,4,4,Python,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,4,main,1,['UeFan'],1,"# R2H: Building Multimodal Navigation Helpers that Respond to Help Requests
**Authors:** Yue Fan, Jing Gu, Kaizhi Zheng, Xin Eric Wang (UC Santa Cruz)

**Abstract:**
Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help Requests (R2H), to promote the development of multi-modal navigation helpers capable of responding to requests for help, utilizing existing dialog-based embodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH), which assesses the helper agent's ability to generate informative responses based on a given dialog history, and (2) Respond during Interaction (RdI), which evaluates the effectiveness and efficiency of the response during consistent cooperation with a task performer. Furthermore, we explore two approaches to construct the navigation-helper agent, including fine-tuning a novel task-oriented multi-modal response generation model that can see and respond, named SeeRee, and employing a multi-modal large language model in a zero-shot manner. Analysis of the task and method was conducted based on both automatic benchmarking and human evaluations.

[Paper](https://arxiv.org/abs/2305.14260)

[Webpage](https://sites.google.com/view/response2helprequests/home)


## Data

In order to automatically
evaluate conversational multi-modal navigation helpers in a cooperative dynamic, we propose two tasks, Response to Dialog History (RDH) and Response during Interaction (RdI). **We first convert the three exsisting Vision-and-Dialog datasets to fit the input and output of RDH task,** where the three datasets are:
- CVDN with photo-realistic indoor visual environment.
- AVDN with photo-realistic ourdoor visual environment.
- DialFRED with sythetic indoor visual environment.

We format and convert these datasets to suit our goal of training and evaluating multimodal navigation-helper agents. Each data sample contains a natural language inquirey about the navigation from the task performer, visual observation from the task performer, a sequence of images showing oracle information for the navigation and a natural language response corresponding to the inquirey. 

The converted datasets (AVDN and DialFRED) are available at [https://drive.google.com/drive/folders/11Le4tX3A_tPePgpc31c7Acgv33OX9JDl?usp=share_link](https://drive.google.com/drive/folders/16AuTcEwtAmUijw_izNghdHzWVftmVeUO?usp=sharing). For converted CVDN dataset, due to the license issue, please email me to get it.

Each dataset is split to train, seen and unseen validation set according to the original splits. **Especially, we create a sample set for each dataset to help better understanding of our data**.

In RDH task, the helper agent outputs responses based on individual task performer's help requests among three different environments. We format and convert all three datasets to suit the training and evaluation based on RDH task. Each data sample contains the input as a natural language inquirey about the navigation from the task performer, visual observation from the task performer and a sequence of images showing oracle information for the navigation; the output as a natural language response corresponding to the inquirey. 

For RdI task, the helper agent needs to interact with the task performer consistently. Therefore the input data are sampled in real-time from the simulator without the need of any offline dataset, except some task definitions i.e. trajectory starting points and target positions.



## Code 

We demonstrate how to use R2H benchmark based on our helper agent, SeeRee.

SeeRee takes vision and language inputs and outputs the natural language response correspond to the inquirey in the input. The language input is history dialog (between the helper and task performer) and latest language inquirey from the task performer. The vision input is the latest task performer's observation and the observation along the future ground truth trajectory to the navigation target. Please follow the following to train and evaluate SeeRee. The training and evaluation are based on the data with RDH format we shared above.

**Prerequisite**

 * We recommand using the docker envirionment provided by [SwinBERT](https://github.com/microsoft/SwinBERT#before-running-code-launch-docker-container) to run our code. We will later provide a non-docker envirionment setup tutorial. 

 * [Weight download](https://drive.google.com/drive/folders/1hQqS9WJF9u0YmTVOb4VyBFho3TLT4pzl?usp=sharing)
   * Weights of Video Swin model. 
   * evalcap.

 * (optional) You may download the weight of SeeRee trained by us and skip training it by yourself: [TODO]

### Script for train (for either task):

```./SeeRee/scripts/train_seeree.sh```

### Script for eval (on RDH task):

By running the evaluation script, we get a coco format json file containing the predicted responses for every trajectories in the validation set. 

```./SeeRee/scripts/eval_seeree.sh```

We provide the raw evaluation outputs (coco format) that we used for the experiment result in our paper [here](https://drive.google.com/drive/folders/1Adjwyj2l7sYxJ0W4Mf7WS0QKodTdNBfN?usp=sharing). 

Then, we replace the original human responses in the CVDN/AVDN/DialFRED validation set with the predicted responses from SeeRee and run evaluation of the task performer on this modified validation set. 

### Script for real-time inference (for RdI task):

In RdI task, since the helper agent needs to interact with the task performer consistently, we deploy both the helper agent and task performer agent in the Matterport3D simulator. With the script below, SeeRee will run as a real-time api for responding to any help request. 

```./SeeRee/scripts/online_inference_seeree.sh```

<br />
<br />


<br />
<br />

*Please cite our paper as below if you use our work.*
```
@misc{fan2023r2h,
      title={R2H: Building Multimodal Navigation Helpers that Respond to Help Requests}, 
      author={Yue Fan and Jing Gu and Kaizhi Zheng and Xin Eric Wang},
      year={2023},
      eprint={2305.14260},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

We build our code based on [SwinBERT](https://github.com/microsoft/SwinBERT/tree/main). Our code is release with the under MIT license.
",1,0.76,0.76,,,,,,0,3,,santacruzml,ucsc-ospo
391294637,MDEwOlJlcG9zaXRvcnkzOTEyOTQ2Mzc=,r-ucscxenashiny-feedstock,conda-forge/r-ucscxenashiny-feedstock,0,conda-forge,https://github.com/conda-forge/r-ucscxenashiny-feedstock,A conda-smithy repository for r-ucscxenashiny.,0,2021-07-31 08:15:11+00:00,2024-11-05 21:52:19+00:00,2024-11-05 21:52:15+00:00,,85,2,2,Shell,1,0,1,0,0,0,2,0,0,0,bsd-3-clause,1,0,0,public,2,0,2,main,1,"['regro-cf-autotick-bot', 'github-actions[bot]', 'conda-forge-admin', 'ShixiangWang', 'mfansler', 'conda-forge-curator[bot]', 'cf-blacksmithy', 'conda-forge-webservices[bot]']",1,"About r-ucscxenashiny-feedstock
===============================

Feedstock license: [BSD-3-Clause](https://github.com/conda-forge/r-ucscxenashiny-feedstock/blob/main/LICENSE.txt)


About r-ucscxenashiny
---------------------

Home: https://openbiox.github.io/UCSCXenaShiny/

Package license: GPL-3.0-or-later

Summary: Provides functions and a Shiny application for downloading, analyzing and visualizing datasets from UCSC Xena (<http://xena.ucsc.edu/>), which is a collection of UCSC-hosted public databases such as TCGA, ICGC, TARGET, GTEx, CCLE, and others.

Development: https://github.com/openbiox/UCSCXenaShiny

About r-ucscxenashiny
---------------------

Home: https://openbiox.github.io/UCSCXenaShiny/

Package license: GPL-3.0-or-later

Summary: Provides functions and a Shiny application for downloading, analyzing and visualizing datasets from UCSC Xena (<http://xena.ucsc.edu/>), which is a collection of UCSC-hosted public databases such as TCGA, ICGC, TARGET, GTEx, CCLE, and others.

Development: https://github.com/openbiox/UCSCXenaShiny

Current build status
====================


<table>
    
  <tr>
    <td>Azure</td>
    <td>
      <details>
        <summary>
          <a href=""https://dev.azure.com/conda-forge/feedstock-builds/_build/latest?definitionId=13545&branchName=main"">
            <img src=""https://dev.azure.com/conda-forge/feedstock-builds/_apis/build/status/r-ucscxenashiny-feedstock?branchName=main"">
          </a>
        </summary>
        <table>
          <thead><tr><th>Variant</th><th>Status</th></tr></thead>
          <tbody><tr>
              <td>linux_64_r_base4.3</td>
              <td>
                <a href=""https://dev.azure.com/conda-forge/feedstock-builds/_build/latest?definitionId=13545&branchName=main"">
                  <img src=""https://dev.azure.com/conda-forge/feedstock-builds/_apis/build/status/r-ucscxenashiny-feedstock?branchName=main&jobName=linux&configuration=linux%20linux_64_r_base4.3"" alt=""variant"">
                </a>
              </td>
            </tr><tr>
              <td>linux_64_r_base4.4</td>
              <td>
                <a href=""https://dev.azure.com/conda-forge/feedstock-builds/_build/latest?definitionId=13545&branchName=main"">
                  <img src=""https://dev.azure.com/conda-forge/feedstock-builds/_apis/build/status/r-ucscxenashiny-feedstock?branchName=main&jobName=linux&configuration=linux%20linux_64_r_base4.4"" alt=""variant"">
                </a>
              </td>
            </tr><tr>
              <td>osx_64_r_base4.3</td>
              <td>
                <a href=""https://dev.azure.com/conda-forge/feedstock-builds/_build/latest?definitionId=13545&branchName=main"">
                  <img src=""https://dev.azure.com/conda-forge/feedstock-builds/_apis/build/status/r-ucscxenashiny-feedstock?branchName=main&jobName=osx&configuration=osx%20osx_64_r_base4.3"" alt=""variant"">
                </a>
              </td>
            </tr><tr>
              <td>osx_64_r_base4.4</td>
              <td>
                <a href=""https://dev.azure.com/conda-forge/feedstock-builds/_build/latest?definitionId=13545&branchName=main"">
                  <img src=""https://dev.azure.com/conda-forge/feedstock-builds/_apis/build/status/r-ucscxenashiny-feedstock?branchName=main&jobName=osx&configuration=osx%20osx_64_r_base4.4"" alt=""variant"">
                </a>
              </td>
            </tr><tr>
              <td>win_64_r_base4.3</td>
              <td>
                <a href=""https://dev.azure.com/conda-forge/feedstock-builds/_build/latest?definitionId=13545&branchName=main"">
                  <img src=""https://dev.azure.com/conda-forge/feedstock-builds/_apis/build/status/r-ucscxenashiny-feedstock?branchName=main&jobName=win&configuration=win%20win_64_r_base4.3"" alt=""variant"">
                </a>
              </td>
            </tr><tr>
              <td>win_64_r_base4.4</td>
              <td>
                <a href=""https://dev.azure.com/conda-forge/feedstock-builds/_build/latest?definitionId=13545&branchName=main"">
                  <img src=""https://dev.azure.com/conda-forge/feedstock-builds/_apis/build/status/r-ucscxenashiny-feedstock?branchName=main&jobName=win&configuration=win%20win_64_r_base4.4"" alt=""variant"">
                </a>
              </td>
            </tr>
          </tbody>
        </table>
      </details>
    </td>
  </tr>
</table>

Current release info
====================

| Name | Downloads | Version | Platforms |
| --- | --- | --- | --- |
| [![Conda Recipe](https://img.shields.io/badge/recipe-r--ucscxenashiny-green.svg)](https://anaconda.org/conda-forge/r-ucscxenashiny) | [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/r-ucscxenashiny.svg)](https://anaconda.org/conda-forge/r-ucscxenashiny) | [![Conda Version](https://img.shields.io/conda/vn/conda-forge/r-ucscxenashiny.svg)](https://anaconda.org/conda-forge/r-ucscxenashiny) | [![Conda Platforms](https://img.shields.io/conda/pn/conda-forge/r-ucscxenashiny.svg)](https://anaconda.org/conda-forge/r-ucscxenashiny) |

Installing r-ucscxenashiny
==========================

Installing `r-ucscxenashiny` from the `conda-forge` channel can be achieved by adding `conda-forge` to your channels with:

```
conda config --add channels conda-forge
conda config --set channel_priority strict
```

Once the `conda-forge` channel has been enabled, `r-ucscxenashiny` can be installed with `conda`:

```
conda install r-ucscxenashiny
```

or with `mamba`:

```
mamba install r-ucscxenashiny
```

It is possible to list all of the versions of `r-ucscxenashiny` available on your platform with `conda`:

```
conda search r-ucscxenashiny --channel conda-forge
```

or with `mamba`:

```
mamba search r-ucscxenashiny --channel conda-forge
```

Alternatively, `mamba repoquery` may provide more information:

```
# Search all versions available on your platform:
mamba repoquery search r-ucscxenashiny --channel conda-forge

# List packages depending on `r-ucscxenashiny`:
mamba repoquery whoneeds r-ucscxenashiny --channel conda-forge

# List dependencies of `r-ucscxenashiny`:
mamba repoquery depends r-ucscxenashiny --channel conda-forge
```


About conda-forge
=================

[![Powered by
NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)

conda-forge is a community-led conda channel of installable packages.
In order to provide high-quality builds, the process has been automated into the
conda-forge GitHub organization. The conda-forge organization contains one repository
for each of the installable packages. Such a repository is known as a *feedstock*.

A feedstock is made up of a conda recipe (the instructions on what and how to build
the package) and the necessary configurations for automatic building using freely
available continuous integration services. Thanks to the awesome service provided by
[Azure](https://azure.microsoft.com/en-us/services/devops/), [GitHub](https://github.com/),
[CircleCI](https://circleci.com/), [AppVeyor](https://www.appveyor.com/),
[Drone](https://cloud.drone.io/welcome), and [TravisCI](https://travis-ci.com/)
it is possible to build and upload installable packages to the
[conda-forge](https://anaconda.org/conda-forge) [anaconda.org](https://anaconda.org/)
channel for Linux, Windows and OSX respectively.

To manage the continuous integration and simplify feedstock maintenance
[conda-smithy](https://github.com/conda-forge/conda-smithy) has been developed.
Using the ``conda-forge.yml`` within this repository, it is possible to re-render all of
this feedstock's supporting files (e.g. the CI configuration files) with ``conda smithy rerender``.

For more information please check the [conda-forge documentation](https://conda-forge.org/docs/).

Terminology
===========

**feedstock** - the conda recipe (raw material), supporting scripts and CI configuration.

**conda-smithy** - the tool which helps orchestrate the feedstock.
                   Its primary use is in the construction of the CI ``.yml`` files
                   and simplify the management of *many* feedstocks.

**conda-forge** - the place where the feedstock and smithy live and work to
                  produce the finished article (built conda distributions)


Updating r-ucscxenashiny-feedstock
==================================

If you would like to improve the r-ucscxenashiny recipe or build a new
package version, please fork this repository and submit a PR. Upon submission,
your changes will be run on the appropriate platforms to give the reviewer an
opportunity to confirm that the changes result in a successful build. Once
merged, the recipe will be re-built and uploaded automatically to the
`conda-forge` channel, whereupon the built conda packages will be available for
everybody to install and use from the `conda-forge` channel.
Note that all branches in the conda-forge/r-ucscxenashiny-feedstock are
immediately built and any created packages are uploaded, so PRs should be based
on branches in forks and branches in the main repository should only be used to
build distinct package versions.

In order to produce a uniquely identifiable distribution:
 * If the version of a package **is not** being increased, please add or increase
   the [``build/number``](https://docs.conda.io/projects/conda-build/en/latest/resources/define-metadata.html#build-number-and-string).
 * If the version of a package **is** being increased, please remember to return
   the [``build/number``](https://docs.conda.io/projects/conda-build/en/latest/resources/define-metadata.html#build-number-and-string)
   back to 0.

Feedstock Maintainers
=====================

* [@Byronxy](https://github.com/Byronxy/)
* [@ShixiangWang](https://github.com/ShixiangWang/)
* [@conda-forge/r](https://github.com/orgs/conda-forge/teams/r/)
* [@kaigu1990](https://github.com/kaigu1990/)
* [@lishensuo](https://github.com/lishensuo/)
* [@longfei8533](https://github.com/longfei8533/)

",0,0.84,0.84,,,,,,0,9,,ucscGenomeBrowser,ucsc/
213293493,MDEwOlJlcG9zaXRvcnkyMTMyOTM0OTM=,CompilerDesign,jaAcKrABbit/CompilerDesign,0,jaAcKrABbit,https://github.com/jaAcKrABbit/CompilerDesign,Compiler for the custom language oc. This assigment was done in pairs.,0,2019-10-07 04:17:48+00:00,2020-02-17 21:14:22+00:00,2019-10-07 04:17:53+00:00,,151,0,0,C++,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['jaAcKrABbit'],,"*Compiler for the custom language oc. This assigment was done in pairs.*

**Institution:** University of California, Santa Cruz<br/>
**Course:** Fundamentals of Compiler Design I<br/>
**Professor:** Wesley, Mackey<br/>
**Student(s):** Wai Chun Leung, Shineng Tang

### Assignment Breakdown (click to see more information)
1. [String Set and C Preprocessor](./asg1)
2. [Lexical Analyzer using flex](./asg2)
3. [LALR(1) Parser using bison](./asg3)
4. [Symbols and Type Checking](./asg4)
5. [Intermediate Code Generation](./asg5)
",1,0.71,0.71,,,,,,0,1,,ucscCancer,
558144756,R_kgDOIUSc9A,derrickdebosejr,derrickdebosejr/derrickdebosejr,0,derrickdebosejr,https://github.com/derrickdebosejr/derrickdebosejr,Config files for my GitHub profile.,0,2022-10-27 01:19:51+00:00,2024-04-24 23:47:42+00:00,2024-04-24 23:47:39+00:00,https://github.com/derrickdebosejr,2,0,0,,0,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['derrickdebosejr'],,"- 👋 Hi, I’m @derrickdebosejr
- 👀 I’m interested in Software Engineering and Data Analytics Positions
- 🌱 I’m a University of California, Santa Cruz graduate with a B.S in Computer Science
- 📫 Reach me via email at: derrickdebosejr@gmail.com

- During the height of the COVID pandemic, Spring 2020, I was in the process of graduating college. During that time, I was unable to find a job. Many companies were laying off employees rather than hiring entry-level and internship level positions. Although I was unable to find a position, I have not lost my dream. I will keep pushing until I can pursue my career of being a Software Engineer.
",1,0.73,0.73,,,,,,0,1,,eric-ai-lab,
725914651,R_kgDOK0SUGw,Earthquake-prediction-using-single-station,Benz-Poobua/Earthquake-prediction-using-single-station,0,Benz-Poobua,https://github.com/Benz-Poobua/Earthquake-prediction-using-single-station,,0,2023-12-01 06:13:25+00:00,2024-05-02 06:01:15+00:00,2023-12-11 01:32:04+00:00,,17099,0,0,Jupyter Notebook,1,1,1,1,0,0,2,0,0,0,mit,1,0,0,public,2,0,0,main,1,"['Benz-Poobua', 'anonymaew']",,"# Machine Learning (ESS 469) Project

## Contributors
- Benz Poobua (spoobu@uw.edu)
- Jake Ward (jakobtw@uw.edu)

## Introduction
Earthquakes, known for their unpredictable nature, challenge our current understanding of seismic behavior. The Tohoku Earthquake, for instance, demonstrated the unpredictability of seismic events by striking an area considered low risk at the time (Seth Stein et al., 2011). Tragically, nearly a million lives have been lost since 1990 due to such events (Lara et al., 2023).

In response to the unpredictable nature of earthquakes, seismologists have developed Early Warning Systems (EWS). These systems involve complex models incorporating various factors, and the advancements in Machine Learning, as exemplified by the work of Mousavi and his team, have led to the continuous development of new models. Notably, a recent paper by Pablo Lara and his team introduces E3WS, a novel single-station Early Warning System characterized by cost-effectiveness and remarkably accurate predictions.

Motivated by Lara's innovative model, our objective is to construct a layered XGBoost Regression model. This model will be trained on offshore low-magnitude earthquakes in the Pacific Northwest, offering insights into its performance and how it compares to existing models.

## Methodology
### Workflow Overview
In this project, we used the stacked model (ensemble), the same as Lara et al. (2023). A stacked model combines all results from all base models and provides the prediction using the meta-model. From the mentioned literature, the authors used stacking for the source characterization. For this project, we will follow the steps provided by Lara et al. (2023) to see the difference in performance when using different datasets. We will use K-fold (K = 10) cross-validation and XGBoost (XGBRegressor) as the base model. XGBoost does a great job predicting magnitude based on the previously obtained residuals scaled by the learning rate. Lasso is used to regularize the model. 

We will obtain the out-of-fold (OOF) predictions during the iterations and use those values as inputs to the meta-model instead of performance evaluation by mean score. Then, we will construct a new meta-input using the feature vectors and the predicted magnitude during training. The stacked model consists of XGBoost as the base model, Lasso as the meta-model, and a test set. The final results are the predicted magnitude we can use to evaluate the model’s performance using Mean Absolute Error (MAE). This project will compare the model's performance with and without Lasso (i.e., only XGBoost) to the actual magnitudes. 

### Datasets
We use the IRIS database (PNSN cataloged). We applied the filter to get the raw data: location constraints, magnitude range, time frame from 2015 to 2023, and Mount St. Helens events excluded. After obtaining the raw data, we must drop NaN values before model training and fitting. As input to the models, the feature vectors serve as a numerical representation of each seismic wave, capturing information from both temporal and spectral dimensions. Notable characteristics included in this representation encompass energy levels, amplitude values, and maximum amplitude measurements (More details on feature vectors here: https://github.com/UW-ESS-DS/mlgeo-2023-PNSN-E3WS/blob/feature-stevens-examples/PNSN_src/util/display_featurevector.py). We acquired feature vectors (f000-f139) and magnitude data frames. We converted those data frames to numpy arrays for later use. Namely, the model prediction functions expect the arrays as input. We performed data splitting using an 80% train set and a 20% test set, according to Lara et al. (2023). The histogram shows the nearly normal distribution of the dataset.

## Results
The MAE of trained data during K-fold cross-validation iteration is 0.1923, displaying a good fit for the data. The MAE of XGBoost alone is 0.1960, slightly higher than the trained values. However,  the insignificant difference in the errors indicates that XGBoost did a great job and was sufficient to predict the earthquake's magnitude. Combining the meta-model or Lasso, the MAE is larger than XGBoost's (0.2003 for stacking). Overall, employing an ensemble (i.e., Lasso) is potentially unnecessary if we have only one base model, which is XGBoost. ![Alt text](https://github.com/Benz-Poobua/ESS-469-Project/blob/main/EQ_Result)

## Acknowledgement
I sincerely thank Napat Srichan (nsrichan@ucsc.edu) for his invaluable assistance in the knowledge domain of coding and insightful suggestions on coding methodologies. His expertise and guidance have significantly contributed to the success of this project. Also, I would like to express my gratitude to Nate Stevens (ntsteven@uw.edu) for his essential role in facilitating the construction of the dataset and feature vectors. His efforts have significantly contributed to the foundational aspects of this academic endeavor.

## References

Lara, Pablo, Quentin Bletery, Jean‐paul Ampuero, Adolfo Inza, and Hernando Tavera. ""Earthquake Early Warning starting from 3 s of records on a single station with machine learning."" Journal of Geophysical Research: Solid Earth 128, no. 11 (2023): e2023JB026575. https://doi.org/10.1029/2023JB026575
![image](https://github.com/Benz-Poobua/ESS-469-Project/assets/146503034/5f404cac-cd05-47e6-8c41-ed10a949c516)
Seth Stein, Robert Geller, Mian Liu; Bad Assumptions or Bad Luck: Why Earthquake Hazard Maps Need Objective Testing. Seismological Research Letters 2011; 82 (5): 623–626. doi: https://doi.org/10.1785/gssrl.82.5.623
",0,0.75,0.75,,,,,,0,1,,UCSC-LoweLab,
41523502,MDEwOlJlcG9zaXRvcnk0MTUyMzUwMg==,DNA-seq-analysis,crazyhottommy/DNA-seq-analysis,0,crazyhottommy,https://github.com/crazyhottommy/DNA-seq-analysis,DNA sequencing analysis notes from Ming Tang,0,2015-08-28 02:53:49+00:00,2024-11-10 18:22:18+00:00,2023-03-26 18:09:15+00:00,,253,142,142,Shell,1,1,1,1,0,0,61,0,0,0,,1,0,0,public,61,0,142,master,1,['crazyhottommy'],,"# DNA-seq

### Databases for variants
* [Disease Variant Store](https://rvs.u.hpc.mssm.edu/divas/)
* [The ExAC Browser: Displaying reference data information from over 60,000 exomes](https://github.com/konradjk/exac_browser)
* [Pathogenic Germline Variants in 10,389 Adult Cancers](https://www.cell.com/cell/fulltext/S0092-8674(18)30363-5)

**Important paper** [DNA damage is a major cause of sequencing errors, directly confounding variant identification](http://biorxiv.org/content/early/2016/08/19/070334)

>However, in this study we show that false positive variants can account for more than 70% of identified somatic variations, rendering conventional detection methods inadequate for accurate determination of low allelic variants. Interestingly, these false positive variants primarily originate from mutagenic DNA damage which directly confounds determination of genuine somatic mutations. Furthermore, we developed and validated a simple metric to measure mutagenic DNA damage and demonstrated that mutagenic DNA damage is the leading cause of sequencing errors in widely-used resources including the **1000 Genomes Project** and **The Cancer Genome Atlas**.

[Functional equivalence of genome sequencing analysis pipelines enables harmonized variant calling across human genetics projects](https://www.biorxiv.org/content/early/2018/02/22/269316) 


### How to represent sequence variants
[Sequence Variant Nomenclature from Human Genome Variation Society](http://varnomen.hgvs.org/)

dbSNP IDs are not unique?

<blockquote class=""twitter-tweet"" data-lang=""en""><p lang=""en"" dir=""ltr"">Oh God, why are people still using dbSNP IDs as though they&#39;re unique identifiers?</p>&mdash; Daniel MacArthur (@dgmacarthur) <a href=""https://twitter.com/dgmacarthur/status/758331620080422912"">July 27, 2016</a></blockquote>
<script async src=""//platform.twitter.com/widgets.js"" charset=""utf-8""></script>

#### The Evolving Utility of dbSNP 

see a [post:dbSNP (build 147) exceeds a ridiculous 150 million variants](http://massgenomics.org/2016/09/dbsnp-ridiculous-variants.html)

>In the early days of next-generation sequencing, dbSNP provided a vital discriminatory tool. In exome sequencing studies of Mendelian disorders, any variant already present in dbSNP was usually common, and therefore unlikely to cause rare genetic diseases. Some of the first high-profile disease gene studies therefore used dbSNP as a filter. Similarly, in cancer genomics, a candidate somatic mutation observed at the position of a known polymorphism typically indicated a germline variant that was under-called in the normal sample. Again, dbSNP provided an important filter.

>Now, **the presence or absence of a variant in dbSNP carries very little meaning.** The database includes over 100,000 variants from disease mutation databases such as OMIM or HGMD. It also contains some appreciable number of somatic mutations that were submitted there before databases like COSMIC became available. And, like any biological database, dbSNP undoubtedly includes false positives.

>Thus, while the mere presence of a variant in dbSNP is a blunt tool for variant filtering, dbSNP’s deep allele frequency data make it incredibly powerful for genetics studies: it can rule out variants that are too prevalent to be disease-causing, and prioritize ones that are rarely observed in human populations. This discriminatory power will only increase as ambitious large-scale sequencing projects like CCDG make their data publicly available.


### Tips and lessons learned during my DNA-seq data analysis journey.  

1. [Allel frequency(AF)](https://en.wikipedia.org/wiki/Allele_frequency)  
  Allele frequency, or gene frequency, is the proportion of a particular allele (variant of a gene) among all allele copies       being considered. It can be formally defined as the percentage of all alleles at a given locus on a chromosome in a population   gene pool represented by a particular allele. AF is affected by copy-number variation, which is common for cancers. tools such as pyclone take tumor purity and copy-number data into account to calculate Cancer Cell Fraction (CCFs).

2. ""for SNVs, we are interested in genotype 0/1, 1/1 for tumor and 0/0 for normal. 1/1 genotype is very rare.  
   It requires the same mutation occurs at the same place in two sister chromsomes which is very rare. one possible way to get 
   1/1 is deletion of one chromosome and duplication of the mutated chromosome"". Quote from Siyuan Zheng.

3. ""Mutect analysis on the TCGA samples finds around 5000 ~ 8000 SNVs per sample."" Quote from Siyuan Zheng. 
4. Cell lines might be contamintated or mislabled. [The Great Big Clean-Up](http://mobile.the-scientist.com/article/43821/the-great-big-clean-up)  
5. Tumor samples are not pure, you will always have stromal cells and infiltrating immnue cells in the tumor bulk. When you analyze the data, keep this in mind.
6. the devil 0 based and 1 based coordinate systems! Make sure you know which system your file is using:
![](https://camo.githubusercontent.com/3937606a47ad455b9bf2ba9bfca9e91f5afbb3a8/68747470733a2f2f692e696d6775722e636f6d2f337449445574442e706e67)

credit from Vince Buffalo.
Also, read this [post](https://standage.github.io/on-genomic-interval-notation.html) and this [post](https://www.biostars.org/p/84686/)  
![](https://cloud.githubusercontent.com/assets/4106146/21367352/77371b1e-c6c3-11e6-9843-8dc9812fce3b.png)
![](https://cloud.githubusercontent.com/assets/4106146/21367351/7736fc38-c6c3-11e6-9382-59a9a8e14b7a.png)

Also read [The UCSC Genome Browser Coordinate Counting Systems](http://genome.ucsc.edu/blog/the-ucsc-genome-browser-coordinate-counting-systems/)

* [Which human reference genome to use?](https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use) by Heng Li

TL;DR: If you map reads to GRCh37 or hg19, use hs37-1kg:

ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.gz
If you map to GRCh37 and believe decoy sequences help with better variant calling, use hs37d5:

ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz
If you map reads to GRCh38 or hg38, use the following:

ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

* [Reference Genome Components](https://software.broadinstitute.org/gatk/documentation/article?id=7857) by GATK team.

* [Human genome reference builds - GRCh38/hg38 - b37 - hg19](https://software.broadinstitute.org/gatk/documentation/article?id=11010) by GATK team.


### get the reference files and mapping index programmatically

* [Go Get Data](https://gogetdata.github.io/quick-start.html#searching-for-data-packages) from Aaron's lab.
* [Refgenie: a reference genome resource manager](http://refgenie.databio.org/en/latest/)
* [genomepy](https://github.com/vanheeringen-lab/genomepy)

### some useful tools for preprocessing

* [FastqPuri](https://github.com/jengelmann/FastqPuri) fastq quality assessment and filtering tool.
* [fastp](https://github.com/OpenGene/fastp) A tool designed to provide fast all-in-one preprocessing for FastQ files. This tool is developed in C++ with multithreading supported to afford high performance. really promising, take a look!
* A new tool [bazam](https://github.com/ssadedin/bazam) A read extraction and realignment tool for next generation sequencing data. Take a look!
* [bwa-mem2](https://github.com/bwa-mem2/bwa-mem2) exact the same results of bwa-mem, 80% faster!

### check sample swapping

* [somalier](https://github.com/brentp/somalier) sample-swap checking directly on BAMs/CRAMs for cancer data

### Mutation caller, structural variant caller

* [Sarek, a nextflow pipeline for variant calling](https://github.com/nf-core/sarek)
* [sample-swap checking directly on BAMs/CRAMs for cancer data](https://github.com/brentp/somalier)
* paper [Making the difference: integrating structural variation detection tools](http://bib.oxfordjournals.org/content/16/5/852.short?rss=1&utm_source=twitterfeed&utm_medium=twitter)
* [Mapping and characterization of structural variation in 17,795 deeply sequenced human genomes](https://www.biorxiv.org/content/early/2018/12/31/508515)
* [GATK HaplotypeCaller Analysis of BWA (mem) mapped Illumina reads](http://wiki.bits.vib.be/index.php/GATK_HaplotypeCaller_Analysis_of_BWA_(mem)_mapped_Illumina_reads)
* [NGS-DNASeq_GATK-session.pdf](https://github.com/crazyhottommy/DNA-seq-analysis/files/94758/NGS-DNASeq_GATK-session.pdf)  
* [GATK pipeline](https://github.com/crazyhottommy/GATK-pipeline)  
* [An ensemble approach to accurately detect somatic mutations using SomaticSeq](http://www.genomebiology.com/2015/16/1/197#B14) [tool github page](https://github.com/bioinform/somaticseq/)
* [A synthetic-diploid benchmark for accurate variant-calling evaluation](https://www.nature.com/articles/s41588-018-0165-1) A benchmark dataset from Heng Li. [github repo](https://github.com/lh3/CHM-eval)
* [Strelka2: fast and accurate calling of germline and somatic variants](https://github.com/Illumina/strelka) paper: https://www.nature.com/articles/s41592-018-0051-x
* [lancet](https://github.com/nygenome/lancet) is a somatic variant caller (SNVs and indels) for short read data. Lancet uses a localized micro-assembly strategy to detect somatic mutation with high sensitivity and accuracy on a tumor/normal pair. paper: https://www.nature.com/articles/s42003-018-0023-9
*  [needlestack](https://github.com/IARCbioinfo/needlestack) an ultra-sensitive variant caller for multi-sample next
generation sequencing data. This tool seems to be very useful for multi-region tumor sample analysis. [paper](https://www.biorxiv.org/content/biorxiv/early/2019/05/21/639377.full.pdf)

* [PerSVade: personalized structural variant detection in any species of interest](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02737-4)
* [lumpy](https://github.com/arq5x/lumpy-sv)
* [wham](https://github.com/zeeev/wham)
* [SV-Bay](https://github.com/InstitutCurie/SV-Bay )  
* [Delly](https://github.com/tobiasrausch/delly)
* [Delly2](https://github.com/dellytools/delly)
>Delly is the best sv caller in the DREAM challenge
https://www.synapse.org/#!Synapse:syn312572/wiki/70726

* [SV caller benchmark](http://shiny.wehi.edu.au/cameron.d/sv_benchmark)*
* [Comprehensive evaluation of structural variation detection algorithms for whole genome sequencing]
(https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1720-5)
* [Comprehensive evaluation and characterisation of short read general-purpose structural variant calling software](https://www.nature.com/articles/s41467-019-11146-4)
* [Genotyping structural variants in pangenome graphs using the vg toolkit (https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1949-z)
* [SVAFotate](https://github.com/fakedrtom/SVAFotate) Annotate a (lumpy) structual variant (SV) VCF with allele frequencies (AFs) from large population SV cohorts (currently CCDG and/or gnomAD) with a simple command line tool.
* [Comprehensively benchmarking applications for detecting copy number variation](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007069) Our results show that the sequencing depth can strongly affect CNV detection. Among the ten applications benchmarked, LUMPY performs best for both high sensitivity and specificity for each sequencing depth. 
* [minigraph](https://github.com/lh3/minigraph#callsv) from Heng Li to call complex SVs.
* [Parliament2](https://github.com/slzarate/parliament2): Accurate structural variant calling at scale. by Fritz group in BCM. https://academic.oup.com/gigascience/article/9/12/giaa145/6042728


* Bent Perderson works on [smoove](https://github.com/brentp/smoove) which improves upon lumpy.

* [COSMOS](http://seselab.org/cosmos/): Somatic Large Structural Variation Detector
* Fusion And Chromosomal Translocation Enumeration and Recovery Algorithm [(FACTERA)](https://factera.stanford.edu/)  
* [VarDict](https://github.com/AstraZeneca-NGS/VarDict): a novel and versatile variant caller for next-generation sequencing in cancer research. we demonstrated that VarDict has improved sensitivity over `Manta` and equivalent sensitivity to `Lumpy`. SNP call rates are on par with `MuTect`, and VarDict is more sensitive and precise than `Scalpel` and other callers for insertions and deletions. see a [post](http://bcb.io/2016/04/04/vardict-filtering/) by Brad Chapman. Looks very promising.
* [Weaver: Allele-Specific Quantification of Structural Variations in Cancer Genomes](https://github.com/leofountain/Weaver). [Paper](http://biorxiv.org/content/early/2016/04/12/048207)
* [SVScore: An Impact Prediction Tool For Structural Variation](http://biorxiv.org/content/early/2016/09/06/073833)
* [Prioritisation of Structural Variant Calls in Cancer Genomes](http://biorxiv.org/content/early/2016/11/04/084640) [simple_sv_annotation.py](https://github.com/AstraZeneca-NGS/simple_sv_annotation) to annotate Lumpy and Mannta SV calls.
* [Genome-wide profiling of heritable and de novo STR variations](http://biorxiv.org/content/early/2016/09/27/077727) short tandem repeats.

### SNV filtering

* [paper: Using high-resolution variant frequencies to empower clinical genome interpretation](http://biorxiv.org/content/early/2016/09/02/073114) [shiny App](https://jamesware.shinyapps.io/alleleFrequencyApp/)  
>Whole exome and genome sequencing have transformed the discovery of genetic variants that cause human Mendelian disease, but discriminating pathogenic from benign variants remains a daunting challenge. Rarity is recognised as a necessary, although not sufficient, criterion for pathogenicity, but frequency cutoffs used in Mendelian analysis are often arbitrary and overly lenient. Recent very large reference datasets, such as the Exome Aggregation Consortium (ExAC), provide an unprecedented opportunity to obtain robust frequency estimates even for very rare variants. Here we present a statistical framework for the frequency-based filtering of candidate disease-causing variants, accounting for disease prevalence, genetic and allelic heterogeneity, inheritance mode, penetrance, and sampling variance in reference datasets.

* a new database called [dbDSM](http://bioinfo.ahu.edu.cn:8080/dbDSM/index.jsp)
A database of Deleterious Synonymous Mutation, a continually updated database that collects, curates and manages available human disease-related SM data obtained from published literature.

* [LncVar](http://bioinfo.ibp.ac.cn/LncVar/): a database of genetic variation associated with long non-coding genes

### Annotation of the variants

* VEP
* ANNOVAR
* VCFanno
* [Personal Cancer Genome Reporter (PCGR)](https://github.com/sigven/pcgr)
* [awesome-cancer-variant-databases](https://github.com/seandavi/awesome-cancer-variant-databases)

### Mannual review of the variants called by IGV

* [Standard operating procedure for somatic variant refinement of tumor sequencing data](https://www.biorxiv.org/content/early/2018/02/21/266262) 

* [Variant Review with the Integrative Genomics Viewer](https://www.ncbi.nlm.nih.gov/pubmed/29092934)

* [Skyhawk: An Artificial Neural Network-based discriminator for reviewing clinically significant genomic variant](https://www.biorxiv.org/content/early/2018/05/01/311985)

### Third generation sequencing for Structural variants (works on short reads as well!)
* [beautiful “Ribbon” viewer to visualize complicated SVs revealed by PacBio reads](http://genomeribbon.com/) [github page](https://github.com/MariaNattestad/ribbon)
* [Sniffles: Structural variation caller using third generation sequencing](https://github.com/fritzsedlazeck/Sniffles) is a structural variation caller using third generation sequencing (PacBio or Oxford Nanopore). It detects all types of SVs using evidence from split-read alignments, high-mismatch regions, and coverage analysis.
* [splitThreader](http://splitthreader.com/) for visualizing structural variants. Finally a good visualizer!
* [New Genome Browser (NGB)](https://github.com/epam/NGB) - a Web - based NGS data viewer with unique Structural Variations (SVs) visualization capabilities, high performance, scalability, and cloud data support. Looks very promising.

### tools useful for everyday bioinformatics

* [bedtools](http://bedtools.readthedocs.io/en/latest/index.html) one must know how to use it!
* [bedops](https://bedops.readthedocs.io/en/latest/) useful as bedtools.
* [valr](http://rnabioco.github.io/valr/) provides tools to read and manipulate genome intervals and signals. (dplyr friendly!)
* [tidygenomics](https://github.com/Artjom-Metro/tidygenomics) similar to GRanges but operate on dataframes!
* [InteractionSet](https://bioconductor.org/packages/release/bioc/html/InteractionSet.html) useful for Hi-C, ChIA-PET. I used it for [Breakpoints clustering for structural variants](http://crazyhottommy.blogspot.com/2016/03/breakpoints-clustering-for-structural.html)
* [Paired Genomic Loci Tool Suite](https://github.com/billgreenwald/pgltools) `gpltools intersect` can do breakpoint merging.
* [svtools](https://github.com/hall-lab/svtools) Tools for processing and analyzing structural variants.
* [sveval](https://github.com/jmonlong/sveval) Functions to compare a SV call sets against a truth set.
* [Teaser](https://github.com/Cibiv/Teaser) A tool to benchmark mappers and different parameters within minutes.

**A series of posts from Brad Chapman**  

1. [Validating multiple cancer variant callers and prioritization in tumor-only samples](http://bcb.io/2015/03/05/cancerval/)  
2. [Benchmarking variation and RNA-seq analyses on Amazon Web Services with Docker](http://bcb.io/2014/12/19/awsbench/)  
3. [Validating generalized incremental joint variant calling with GATK HaplotypeCaller, FreeBayes, Platypus and samtools](http://bcb.io/2014/10/07/joint-calling/)  
4. [Validated whole genome structural variation detection using multiple callers](http://bcb.io/2014/08/12/validated-whole-genome-structural-variation-detection-using-multiple-callers/)  
5. [Validated variant calling with human genome build 38](http://bcb.io/2015/09/17/hg38-validation/)


### Copy number variants 
* [Interactive analysis and assessment of single-cell copy-number variations](http://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.3578.html): [Ginkgo](http://qb.cshl.edu/ginkgo)   
* [Copynumber Viewer](https://github.com/RCollins13/CNView)
* [paper: Computational tools for copy number variation (CNV) detection using next-generation sequencing data: features and perspectives](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-S11-S1)
* [bioconductor copy number work flow](https://www.bioconductor.org/help/course-materials/2014/SeattleOct2014/B02.2.3_CopyNumber.html)
* [paper: Assessing the reproducibility of exome copy number variations predictions](http://www.ncbi.nlm.nih.gov/pubmed/27503473?dopt=Abstract&utm_source=dlvr.it&utm_medium=twitter)
* [CNVkit](https://github.com/etal/cnvkit) A command-line toolkit and Python library for detecting copy number variants and alterations genome-wide from targeted DNA sequencing.
* [SavvyCNV: genome-wide CNV calling from off-target reads](https://www.biorxiv.org/content/10.1101/617605v2)
* [dryclean](https://github.com/mskilab/dryclean) Robust foreground detection in somatic copy number data https://www.biorxiv.org/content/10.1101/847681v2

### Tools for visulization 
1. [New app gene.iobio](http://iobio.io/)  
[App here](http://gene.iobio.io/?rel0=proband&rel1=mother&rel2=father) I will definetely have it a try.

2. [ASCIIGenome](https://github.com/dariober/ASCIIGenome) is a command-line genome browser running from terminal window and solely based on ASCII characters. Since ASCIIGenome does not require a graphical interface it is particularly useful for quickly visualizing genomic data on remote servers. The idea is to make ASCIIGenome the Vim of genome viewers.

### Tools for vcf files
1. [tools for pedigree files](https://github.com/brentp/peddy). It can determine sex from PED and VCF files. Developed by Brent Pedersen. I really like tools from Aaron Quinlan's lab.
2. [cyvcf2](https://github.com/brentp/cyvcf2) is a cython wrapper around htslib built for fast parsing of Variant Call Format (VCF) files
3. [PyVCF](http://pyvcf.readthedocs.org/en/latest/) - A Variant Call Format Parser for Python
4. [VcfR: an R package to manipulate and visualize VCF format data](https://cran.r-project.org/web/packages/vcfR/index.html)
5. [Varapp](https://varapp-demo.vital-it.ch/docs/src/about.html) is an application to filter genetic variants, with a reactive graphical user interface. Powered by [GEMINI](https://varapp-demo.vital-it.ch/docs/src/about.html).
6. [varmatch: robust matching of small variant datasets using flexible scoring schemes](https://github.com/medvedevgroup/varmatch)
7. [vcf-validator](https://github.com/EBIvariation/vcf-validator) validate your VCF files!
8. [BrowseVCF](https://github.com/BSGOxford/BrowseVCF): a web-based application and workflow to quickly prioritize disease-causative variants in VCF files

### mutation signature

* [signeR](http://bioconductor.org/packages/release/bioc/html/signeR.html)
* [deconstructSigs](https://github.com/raerose01/deconstructSigs)
* [MutationalPatterns](https://github.com/CuppenResearch/MutationalPatterns)
* [sigminer](https://github.com/ShixiangWang/sigminer/tree/devel): an easy-to-use and scalable toolkit for genomic alteration signature analysis and visualization in R

### Tools for MAF files
TCGA has all the variants calls in MAF format. Please read a [post](https://www.biostars.org/p/69222/) by Cyriac Kandoth. 

1. [convert vcf to MAF](https://github.com/mskcc/vcf2maf): perl script by Cyriac Kandoth.
2. once converted to MAF, one can use this [MAFtools](https://github.com/PoisonAlien/maftools) to do visualization: oncoprint wraps complexHeatmap, Lollipop and Mutational Signatures etc. Very cool, I just found it...
3. [MutationalPatterns](https://github.com/CuppenResearch/MutationalPatterns): an integrative R package for studying patterns in base substitution catalogues


### Tools for bam files

1. [VariantBam](https://github.com/jwalabroad/VariantBam): Filtering and profiling of next-generational sequencing data using region-specific rules


### Annotate and explore variants 
1. Variant Effect Predictor: [VEP](http://useast.ensembl.org/info/docs/tools/vep/index.html)
2. [SNPEFF](http://snpeff.sourceforge.net/)
3. [vcfanno](https://github.com/brentp/vcfanno)
4. [myvariant.info](http://myvariant.info/) [tutorial](https://github.com/SuLab/myvariant.info/blob/master/docs/ipynb/myvariant_R_miller.ipynb) 
5. [FunSeq2](http://funseq2.gersteinlab.org/)- A flexible framework to prioritize regulatory mutations from cancer genome sequencing
6. [ClinVar](https://github.com/macarthur-lab/clinvar)  
7. [ExAC](http://exac.broadinstitute.org/)
8. [vcf2db](https://github.com/quinlan-lab/vcf2db) and [GEMINI](https://gemini.readthedocs.org/en/latest/index.html): a flexible framework for exploring genome variation from Qunlan lab.

### Plotting
1.[oncoprint](https://bioconductor.org/packages/release/bioc/vignettes/ComplexHeatmap/inst/doc/s8.oncoprint.html)
2.[deconstructSigs](https://github.com/raerose01/deconstructSigs) aims to determine the contribution of known mutational processes to a tumor sample. By using deconstructSigs, one can: Determine the weights of each mutational signature contributing to an individual tumor sample; Plot the reconstructed mutational profile (using the calculated weights) and compare to the original input sample
3. [Fast Principal Component Analysis of Large-Scale Genome-Wide Data](https://github.com/gabraham/flashpca)

### Identify driver genes

* [MUFFINN: cancer gene discovery via network analysis of somatic mutation data](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0989-x?platform=hootsuite)

### intra-Tumor heterogenity 
* ESTIMATE
* ABSOLUTE
* [THetA: Tumor Heterogeneity Analysis](http://compbio.cs.brown.edu/projects/theta/) is an algorithm that estimates the tumor purity and clonal/sublconal copy number aberrations directly from high-throughput DNA sequencing data. The latest release is called THetA2 and includes a number of improvements over previous versions.
* [CIBERSORT](https://cibersort.stanford.edu/index.php) is an analytical tool developed by Newman et al. to provide an estimation of the abundances of member cell types in a mixed cell population, using gene expression data
* [xcell](http://xcell.ucsf.edu/) is a webtool that performs cell type enrichment analysis from gene expression data for 64 immune and stroma cell types. xCell is a gene signatures-based method learned from thousands of pure cell types from various sources.  
* [paper: Digitally deconvolving the tumor microenvironment](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1036-7?utm_content=buffer45a4c&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)
* [Comprehensive analyses of tumor immunity: implications for cancer immunotherapy](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1028-7?platform=hootsuite) by Shierly Liu's lab. [TIMER](http://cistrome.org/TIMER/): Tumor IMmune Estimation Resource
A comprehensive resource for the clinical relevance of tumor-immune infiltrations
* [Reference-free deconvolution of DNA methylation data and mediation by cell composition effects](http://biorxiv.org/content/early/2016/01/22/037671). The R package's documentation is minimal... see tutorial [here](http://people.oregonstate.edu/~housemae/software/TutorialLondon2014/) from the author. Brent Perdson has a tool implementing the same method used by Houseman: [celltypes450](https://github.com/brentp/celltypes450).
* [paper: Toward understanding and exploiting tumor heterogeneity](http://www.ncbi.nlm.nih.gov/pubmed/26248267)  
* [paper: The prognostic landscape of genes and infiltrating immune cells across human cancers](http://www.ncbi.nlm.nih.gov/pubmed/26193342)  from Alizadeh lab.
* [Robust enumeration of cell subsets from tissue expression profiles](http://www.nature.com/nmeth/journal/v12/n5/abs/nmeth.3337.html)  from Alizadeh lab, and the [CIBERSORT tool](https://cibersort.stanford.edu/index.php) 
* [A series of posts on tumor evolution](https://scientificbsides.wordpress.com/2014/06/02/inferring-tumour-evolution-1-the-intra-tumour-phylogeny-problem/)
* [mapscape bioc package](http://bioconductor.org/packages/devel/bioc/html/mapscape.html) MapScape integrates clonal prevalence, clonal hierarchy, anatomic and mutational information to provide interactive visualization of spatial clonal evolution.
* [cellscape bioc package](http://bioconductor.org/packages/devel/bioc/html/cellscape.html) Explores single cell copy number profiles in the context of a single cell tree

### tumor colonality and evolution

* [A step-by-step guide to estimate tumor clonality/purity from variant allele frequency data](https://github.com/hammerlab/vaf-experiments)
* [densityCut: an efficient and versatile topological approach for automatic clustering of biological data](http://m.bioinformatics.oxfordjournals.org/content/early/2016/04/23/bioinformatics.btw227.short?rss=1) can be used to cluster allel frequence.
* [phyC: Clustering cancer evolutionary trees](http://biorxiv.org/content/early/2016/08/12/069302)
* [CloneCNA](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1174-7): detecting subclonal somatic copy number alterations in heterogeneous tumor samples from whole-exome sequencing data
* [paper: Distinct evolution and dynamics of epigenetic and genetic heterogeneity in acute myeloid leukemia](http://www.nature.com/nm/journal/v22/n7/full/nm.4125.html)
* [paper: Visualizing Clonal Evolution in Cancer](http://www.cell.com/molecular-cell/pdf/S1097-2765(16)30188-5.pdf)
* [An R package for inferring the subclonal architecture of tumors:sciclone](https://github.com/genome/sciclone)
* [Inferring and visualizing clonal evolution in multi-sample cancer sequencing: clonevol](https://github.com/hdng/clonevol)
* [fishplot: Create timecourse ""fish plots"" that show changes in the clonal architecture of tumors](https://github.com/chrisamiller/fishplot)
* [tools from OMICS tools website](https://omictools.com/tumor-purity-and-heterogeneity-category)
* [PhyloWGS](https://github.com/morrislab/phylowgs): Reconstructing subclonal composition and evolution from whole-genome sequencing of tumors.
* [SCHISM](http://karchinlab.org/apps/appSchism.html) SubClonal Hierarchy Inference from Somatic Mutation

### mutual exclusiveness of mutations
* [MEGSA](http://biorxiv.org/content/early/2015/04/09/017731): A powerful and flexible framework for analyzing mutual exclusivity of tumor mutations.
* [CoMet](https://github.com/raphael-group/comet) 
* [DISCOVER](https://github.com/NKI-CCB/DISCOVER) co-occurrence and mutual exclusivity analysis for cancer genomics data.

### mutation enrich in pathways
*[PathScore: a web tool for identifying altered pathways in cancer data](http://pathscore.publichealth.yale.edu/)

### Non-coding mutations
* [Large-scale Analysis of Variants in noncoding Annotations:LARVA](http://larva.gersteinlab.org/)


### CRISPR
* [The caRpools package - Analysis of pooled CRISPR Screens](https://cran.r-project.org/web/packages/caRpools/vignettes/CaRpools.html)
* [CRISPR Library Designer (CLD): a software for the multispecies design of sgRNA libraries](https://github.com/boutroslab/cld)

### long reads
[Quality Assessment Tools for Oxford Nanopore MinION data](https://bioconductor.org/packages/3.2/bioc/html/IONiseR.html)
[Signal-level algorithms for MinION data](https://github.com/jts/nanopolish)

### Single-cell DNA sequencing

* A review paper 2016: [Single-cell genome sequencing:current state of the science](http://www.nature.com/nrg/journal/v17/n3/abs/nrg.2015.16.html) 
* [Monovar](http://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.3835.html): single-nucleotide variant detection in single cells
* [R2C2: Improving nanopore read accuracy enables the sequencing of highly-multiplexed full-length single-cell cDNA](https://www.biorxiv.org/content/early/2018/06/04/338020)
* [sci-LIANTI](https://www.biorxiv.org/content/early/2018/06/04/338053), a high-throughput, high-coverage single-cell DNA sequencing method that combines single-cell combinatorial indexing (sci) and linear amplification via transposon insertion (LIANTI)
",0,0.46,0.46,,,,,,0,13,,LokeyLab,
318091968,MDEwOlJlcG9zaXRvcnkzMTgwOTE5Njg=,BEIDOU,YangLab/BEIDOU,0,YangLab,https://github.com/YangLab/BEIDOU,Base/Prime Editor Induced DNA Off-target site identification Unified toolkit,0,2020-12-03 06:01:11+00:00,2024-10-15 11:50:13+00:00,2023-10-08 02:27:46+00:00,,257,6,6,Shell,1,1,1,1,0,0,2,0,0,0,,1,0,0,public,2,0,6,master,1,"['fzc1997', 'xueweireally']",1,"# BEIDOU 
**B**ase/Prime **E**ditor **I**nduced **D**NA **O**ff-target site identification **U**nified toolkit

Version: 1.0.0

*About the name: ""BeiDou"" is also the name of China's navigation satellite system*

-----------------------------------

## Schema
![image](doc/BEIDOU_workflow.001.png)
Authors: Wei Xue(xuewei@picb.ac.cn), Zhi-Can Fu(fuzhican@picb.ac.cn) and Li Yang (liyang@picb.ac.cn)
Maintainer: Zhi-Can Fu(fuzhican@picb.ac.cn)

-----------------------------------

## Installation requirements [mandatory]
* Software
    - [bwa](https://github.com/lh3/bwa) [(version 0.7.17-r1188)](https://github.com/lh3/bwa/releases/tag/v0.7.17)
    - perl (version 5.26.2)
    - samtools (version 1.9)
    - bedtools (version 2.28.0)
    - picard (version 2.21.2)
    - bamtools (version 2.5.1)
    - bcftools (version 1.9)
    - GATK (version 4.1.3.0-0)
    - lofreq (version 2.1.3.1) 
    - Strelka2 (version 2.9.10)
    - Scalpel (version 0.5.4)
    - Manta (version 1.6.0)
    - GNU Parallel (version 20200722)
    - R (version 3.5.1)

## Data requirements [mandatory]
* reference sequences
    - **[hg38.fa](https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz)|[mm10.fa](https://hgdownload.soe.ucsc.edu/goldenPath/mm10/bigZips/mm10.fa.gz)**
    - **hg38_all.fai|mm10_all.fai** (Created by ""samtools faidx"")
    - **hg38_all.dict|mm10_all.dict** (Created by ""picard CreateSequenceDictionary"")
* vcfs for GATK BaseRecalibrator
    - **[NCBI_dbSNP_all_hg38.vcf.gz](https://ftp.ncbi.nih.gov/snp/organisms/human_9606/VCF/00-All.vcf.gz)|[EVA_SNP_all_mm10.vcf.gz](http://ftp.ebi.ac.uk/pub/databases/eva/rs_releases/release_1/by_species/Mouse_10090/GRCm38.p4/GCA_000001635.6_current_ids.vcf.gz)** (Very importantly, chromosome names in the annotations GTF file have to match chromosome names in the FASTA genome sequence files)
* vcfs for GATK VariantRecalibrator (Human)
    - **[hapmap_3.3.hg38.vcf.gz](https://console.cloud.google.com/storage/browser/_details/genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz)** 
    - **[1000G_omni2.5.hg38.vcf.gz](https://console.cloud.google.com/storage/browser/_details/genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz)** 
    - **[1000G_phase1.snps.high_confidence.hg38.vcf.gz](https://console.cloud.google.com/storage/browser/_details/genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz)** 
    - **[NCBI_dbSNP_all_hg38.vcf.gz](https://ftp.ncbi.nih.gov/snp/organisms/human_9606/VCF/00-All.vcf.gz)** 
    - **[Mills_and_1000G_gold_standard.indels.hg38.vcf.gz](https://console.cloud.google.com/storage/browser/_details/genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz)** 
* vcfs for GATK VariantRecalibrator (Mouse)
    - **[EVA_SNP_all_mm10.vcf.gz](http://ftp.ebi.ac.uk/pub/databases/eva/rs_releases/release_1/by_species/Mouse_10090/GRCm38.p4/GCA_000001635.6_current_ids.vcf.gz)** 
    - **MGP_SNP_indel_v5.vcf.gz** (merged from [MGP_SNP_v5.vcf.gz](http://ftp-mouse.sanger.ac.uk/current_snps/mgp.v5.merged.snps_all.dbSNP142.vcf.gz) and [MGP_indel_v5.vcf.gz](http://ftp-mouse.sanger.ac.uk/current_snps/mgp.v5.merged.indels.dbSNP142.normed.vcf.gz))
    - **[MGP_indel_v5.vcf.gz](http://ftp-mouse.sanger.ac.uk/current_snps/mgp.v5.merged.indels.dbSNP142.normed.vcf.gz)** 
* files used to filter out the background variants (Human)
    - **[NCBI_dbSNP_all_hg38.vcf.gz](https://ftp.ncbi.nih.gov/snp/organisms/human_9606/VCF/00-All.vcf.gz)** 
    - **[UCSC_RepeatMask_hg38.bed](http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/rmsk.txt.gz)** 
* files used to filter out the background variants (Mouse)
    - **[EVA_SNP_all_mm10.vcf.gz](http://ftp.ebi.ac.uk/pub/databases/eva/rs_releases/release_1/by_species/Mouse_10090/GRCm38.p4/GCA_000001635.6_current_ids.vcf.gz)** 
    - **[UCSC_RepeatMask_mm10.bed](http://hgdownload.soe.ucsc.edu/goldenPath/mm10/database/rmsk.txt.gz)** 

## Installation
```bash
git clone https://github.com/YangLab/BEIDOU.git
```

## Usage
```bash
BEIDOU -f Function -1 Path_of_fastq1 -2 Path_of_fastq2 -o Output_path -n Output_name -c Path_of_config_file -t number_of_maximum_threads -g genome_build_version -d tmp_folder
       [-f Function, ""SNV"", ""Indel"" or ""all_steps""(default all_steps)]
       [-1 Path of fastq1]
       [-2 Path of fastq2]
       [-o Output directory(default current directory)]
       [-n Output name]
       [-c Path of config file(default ./BEIDOU_config_GENOME_BUILD_VERSION)]
       [-t Maximum_threads]
       [-g Genome build version, ""hg38"" or ""mm10""]
       [-d wirtable temporary folder(default [Output directory]/BEIDOU_tmp)]
```


## Output
* **[Output directory]/[Output name]_Novel_SNVs** and/or **[Output directory]/[Output name]_Novel_Indels** is the result of BEIDOU pipeline.

-----------------------------------

## Example of BEIDOU_config file
```bash
#[Software]
dir_of_bwa=~/bin
dir_of_samtools=~/bin
dir_of_gatk=~/bin
dir_of_picard=~/picard/build/libs
dir_of_bamtools=~/bin
dir_of_bcftools=~/bin
dir_of_lofreq=~/lofreq_star-2.1.3.1/bin
dir_of_Strelka2=~/strelka-2.9.10.centos6_x86_64/bin
dir_of_Scalpel=~/scalpel-0.5.4
dir_of_Manta=~/bin
dir_of_perl=~/bin
dir_of_parallel=~/bin
dir_of_intersectBed=~/bin
#[ref_genome]
ref_genome_path=path/to/hg38_all.fa
dict_of_ref_genome_path=path/to/hg38_all.dict
#[vcf_files_for_BaseRecalibrator]
dbsnp_vcf_for_BaseRecalibrator=path/to/NCBI_dbSNP_all_hg38.vcf.gz
#[vcf_files_for_VariantRecalibrator]
hapmap_vcf=path/to/hapmap_3.3.hg38.vcf.gz
file_1000G_omni_vcf=path/to/1000G_omni2.5.hg38.vcf.gz
file_1000G_phase1_vcf=path/to/1000G_phase1.snps.high_confidence.hg38.vcf.gz
dbsnp_vcf=path/to/dbsnp_146.hg38.vcf.gz
Mills_and_1000G_vcf=path/to/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz
#[filtering file]
filtering_dbSNP_vcf=path/to/NCBI_dbSNP_all_hg38.vcf.gz
UCSC_RepeatMask_bed=path/to/UCSC_RepeatMask_hg38.bed
#[optional files(these files can be created automatic)]
dir_of_individual_chr_genome_range_bed=path/to/dir
dir_of_individual_chr_ref_genome_path=path/to/dir
```
-----------------------------------

## Citation
Runze Gao#, Zhi-Can Fu#, Xiangyang Li#, Ying Wang#, Jia Wei, Guangye Li, Lijie Wang, Jing Wu, Wei Xue, Xingxu Huang\*, Li Yang\*, Jia Chen\*. Background levels of genome-wide off-target mutations induced by prime editor. 2020, xxxxxx

-----------------------------------

## License
Copyright (C) 2020 YangLab. Licensed GPLv3 for open source use or contact YangLab (yanglab@picb.ac.cn) for commercial use.
",0,0.64,0.64,,,,,,0,8,,masc-ucsc,
416501658,R_kgDOGNNPmg,cifar-10-100n,UCSC-REAL/cifar-10-100n,0,UCSC-REAL,https://github.com/UCSC-REAL/cifar-10-100n,Human annotated noisy labels for CIFAR-10 and CIFAR-100. The website of CIFAR-N is available at http://www.noisylabels.com/.,0,2021-10-12 21:20:03+00:00,2025-01-22 10:53:36+00:00,2023-05-17 22:57:03+00:00,,3598,172,172,Python,1,1,1,1,0,0,20,0,0,3,other,1,0,0,public,20,3,172,main,1,"['weijiaheng', 'zwzhu-d']",1,"**[Update 5/17/2023]** A [demo](https://github.com/Docta-ai/docta/blob/master/demo/docta_cifar10.ipynb) for automatically detecting label errors on CIFAR-N is availabel at [Docta](https://github.com/Docta-ai/docta) now!
- **Docta**: A **Doc**tor for your da**ta**
- An advanced data-centric AI platform that offers a comprehensive range of services aimed at detecting and rectifying issues in your data.


This repository is the official dataset release and Pytorch implementation of ""[Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations](https://openreview.net/forum?id=TBWA6PLJZQm&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2022%2FConference%2FAuthors%23your-submissions))"" accepted by ICLR2022. We collected and published re-annotated versions of the CIFAR-10 and CIFAR-100 data which contains real-world human annotation errors. We show how these noise patterns deviate from the classically assumed ones and what the new challenges are. The website of CIFAR-N is available at [http://www.noisylabels.com/](http://www.noisylabels.com/).

----------------

**Competition:** Please refer to the branch `ijcai-lmnl-2022` for details of 1st Learning with Noisy Labels Challenge in IJCAI 2022. Also available at [http://competition.noisylabels.com/](http://competition.noisylabels.com/).

# Dataloader for CIFAR-N (PyTorch)

### CIFAR-10N 
```python
import torch
noise_file = torch.load('./data/CIFAR-10_human.pt')
clean_label = noise_file['clean_label']
worst_label = noise_file['worse_label']
aggre_label = noise_file['aggre_label']
random_label1 = noise_file['random_label1']
random_label2 = noise_file['random_label2']
random_label3 = noise_file['random_label3']
```

### CIFAR-100N 
```python
import torch
noise_file = torch.load('./data/CIFAR-100_human.pt')
clean_label = noise_file['clean_label']
noisy_label = noise_file['noisy_label']
```

# Dataloader for CIFAR-N (Tensorflow)

Note: Image order of tensorflow dataset (tfds.load, binary version of CIFAR) does not match with PyTorch dataloader (python version of CIFAR).

### CIFAR-10N 
```python
import numpy as np
noise_file = np.load('./data/CIFAR-10_human_ordered.npy', allow_pickle=True)
clean_label = noise_file.item().get('clean_label')
worst_label = noise_file.item().get('worse_label')
aggre_label = noise_file.item().get('aggre_label')
random_label1 = noise_file.item().get('random_label1')
random_label2 = noise_file.item().get('random_label2')
random_label3 = noise_file.item().get('random_label3')
# The noisy label matches with following tensorflow dataloader
train_ds, test_ds = tfds.load('cifar10', split=['train','test'], as_supervised=True, batch_size = -1)
train_images, train_labels = tfds.as_numpy(train_ds) 
# You may want to replace train_labels by CIFAR-N noisy label sets
```

**Reminder:** CIFAR-10N is now available at tensorflow datasets. Please check [here](https://www.tensorflow.org/datasets/catalog/cifar10_n) for more details!

### CIFAR-100N 
```python
import numpy as np
noise_file = np.load('./data/CIFAR-100_human_ordered.npy', allow_pickle=True)
clean_label = noise_file.item().get('clean_label')
noise_label = noise_file.item().get('noise_label')
# The noisy label matches with following tensorflow dataloader
train_ds, test_ds = tfds.load('cifar100', split=['train','test'], as_supervised=True, batch_size = -1)
train_images, train_labels = tfds.as_numpy(train_ds) 
# You may want to replace train_labels by CIFAR-N noisy label sets
```

The image order from tfds to pytorch dataloader is given below:
- **image_order_c10.npy:** a numpy array with length 50K, the i-th element denotes the index of i-th unshuffled tfds (binary-version) CIFAR-10 training image in the Pytorch (python-version) ones.
- **image_order_c100.npy:** a numpy array with length 50K, the i-th element denotes the index of i-th unshuffled tfds (binary-version) CIFAR-100 training image in the Pytorch (python-version) ones.


# Training on CIFAR-N with Cross-Entropy (PyTorch)
### CIFAR-10N 
```shell
# NOISE_TYPE: [clean, aggre, worst, rand1, rand2, rand3]
# Use human annotations
CUDA_VISIBLE_DEVICES=0 python3 main.py --dataset cifar10 --noise_type NOISE_TYPE --is_human
# Use the synthetic noise that has the same noise transition matrix as human annotations
CUDA_VISIBLE_DEVICES=0 python3 main.py --dataset cifar10 --noise_type NOISE_TYPE
```

### CIFAR-100N 
```shell
# NOISE_TYPE: [clean100, noisy100]
# Use human annotations
CUDA_VISIBLE_DEVICES=0 python3 main.py --dataset cifar100 --noise_type NOISE_TYPE --is_human
# Use the synthetic noise that has the same noise transition matrix as human annotations
CUDA_VISIBLE_DEVICES=0 python3 main.py --dataset cifar100 --noise_type NOISE_TYPE
```

# Additional dataset information
We include additional side information during the noisy-label collection in <code>side_info_cifar10N.csv</code> and <code>side_info_cifar100N.csv</code>.
A brief introduction of these two files:
- **Image-batch:** a subset of indexes of the CIFAR training images.
- **Worker-id:** the encrypted worker id on Amazon Mechanical Turk.
- **Work-time-in-seconds:** the time (in seconds) a worker spent on annotating the corresponding image batch.
",1,0.95,0.95,,,,,,0,5,,VLSIDA,
76807756,MDEwOlJlcG9zaXRvcnk3NjgwNzc1Ng==,discovie,Mjle/discovie,0,Mjle,https://github.com/Mjle/discovie,Helps you find a movie to watch for the night,0,2016-12-18 21:28:31+00:00,2016-12-29 22:50:24+00:00,2016-12-18 21:41:52+00:00,,30117,1,1,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,master,1,['Mjle'],,"# Discovie

### Synopsis
This project was made for our web application class cmps 183. We developed in a fullstack web2py python framework along with our teacher's selected single page application JS: vueJS.

### Installation

Students followed this [LINK](https://sites.google.com/a/ucsc.edu/luca/classes/cmps-183-hypermedia-and-the-web/cmps-183-environment-installation-instructions) provided by the instructor to set up their development environment. 

A brief rundown of what you will need is:

Python ([anaconda](https://store.continuum.io/cshop/anaconda/) suggested)

[Git] (https://git-scm.com/)

Terminal (MAC OS X) OR for Windows [Cygwin](https://cygwin.com/index.html)

Distrbution of [web2py](http://www.web2py.com/examples/static/web2py_src.zip)

All you have to do now is clone this repository into your ~/web2py/applications/

And run in your terminal

```
python web2py/web2py.py -e
```

For an in-depth tutorial, I suggest to follow the [LINK](https://sites.google.com/a/ucsc.edu/luca/classes/cmps-183-hypermedia-and-the-web/cmps-183-environment-installation-instructions) above.

### Links
[GitHub](https://github.com/acchiao/discovie)

DEMO: [Pythonanywhere](http://discovie.pythonanywhere.com/)

### Contributors
Arthur Chiao - acchiao@ucsc.edu

Andrew Hsu - ahsu5@ucsc.edu

Chris Myau - cmyau@ucsc.edu

Michael Le - mjle@ucsc.edu

### Setup


### Notes
Only things that do not match up in our database are synopsis and movie posters.
Those are the things that we had to assign to each movie in order for there to be a placeholder for each movie.

One thing that we did not get time to cover in our presentation was our preferences page, where each user can rate 
movies. We were cut short and did not get to go over this part.

### Acknowledgements
[Kaggle](https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset) for the list of movies in our database.
",1,0.83,0.83,,,,,,0,1,,UCSC-VLAA,
78874084,MDEwOlJlcG9zaXRvcnk3ODg3NDA4NA==,ascar-lustre-2.9-client,mlogic/ascar-lustre-2.9-client,0,mlogic,https://github.com/mlogic/ascar-lustre-2.9-client,,0,2017-01-13 18:14:59+00:00,2022-05-25 12:32:11+00:00,2017-02-08 23:49:45+00:00,,62729,1,1,C,1,1,1,1,0,0,3,0,0,0,other,1,0,0,public,3,0,1,ascar_v2_9_0,1,"['jhammond-intel', 'Nasf-Fan', 'wangdi1', 'jasimmons1973', 'adilger', 'NiuYawei', 'morrone', 'fzago-cray', 'liangzhen', 'bfaccini', 'ShengYang1', 'miketappro', 'sebastienbuisson', 'LiXi-storage', 'laisiyao', 'gnailzenh', 'liw', 'kitwestneat', 'hdoreau', 'johannlombardi', 'lafoucriere', 'mdiep25', 'pichong', 'nedbass', 'behlendorf', 'paf-49', 'MorpheusTeam', 'Kmannth', 'jfilizetti', 'jeffmahoney', 'liuw1', 'huangheintel', 'dinatale2', 'sbuisson-ddn', 'tstibor', 'saurabhtandan', 'verygreen', 'ofaaland', 'sihara', 'mlogic', 'sgi-schamp', 'bibimbop', 'mjmac', '531308', 'akoehler', 'tl-cea', 'mattaezell', 'parinay', 'bergwolf', 'arno', 'alexxy', 'brianjmurrell', 'gmahajan', 'mtfs', 'patrick-valentin', 'agelastic', 'soerenb', 'inashivb', 'Sandhya-Bankar', 'SamTechnologeek', 'gormanm', 'matt-tyler', 'matt-wu', 'msalve', 'jhammond', 'jlan', 'herbertx', 'cristina9209', 'chrisgearing', 'bacaldwell', 'ashleypittman', 'awellington', 'musicakc']",,"Introducing the ASCAR QoS framework for Lustre.

This is the ASCAR client for Lustre. ASCAR, the Automated Contention
Management for High-Performance Storage Systems, is designed by the
Storage Systems Research Center (SSRC) in University of California,
Santa Cruz.

This client is based on Lustre 2.9.0 and follows the Lustre's license,
which is GPL version 2. For more information about ASCAR, please read
the project page at: TODO

Introduction
============

ASCAR is different than server-side schedulers like NRS because ASCAR
limits rate from the client.


The following installation instruction is based on the official guide:
[Walk-thru- Build Lustre MASTER on RHEL 6.4/CentOS 6.4 from Whamcloud
git](https://wiki.hpdd.intel.com/pages/viewpage.action?pageId=8126821)
(use this [archive](https://archive.fo/rfqVH) if the link stopped
working).  It has been tested with Lustre 2.4 on CentOS 6.7.

Requirements
============

ASCAR is based on Lustre, so make sure you have installed the kernel
and kernel-devel packages correctly and boot into the Lustre
kernel. The simplest way is to install the official 2.4 RPMs, which
can be found at
https://downloads.hpdd.intel.com/public/lustre/lustre-2.4.0/el6.

The following instructions are for reference only. You can skip them
if you already have Lustre up and running or if you prefer to use
other methods to install Lustre.

Download Lustre server RPMs:

    yum install -y w3m
    # download the server RPMs
    mkdir -p ~/lustre-rpm/server/RPMS; cd ~/lustre-rpm/server/RPMS
    w3m https://downloads.hpdd.intel.com/public/lustre/lustre-2.4.0/el6/server/RPMS/x86_64/ -dump | grep rpm | awk '{print $3}' | xargs -n 1 -I{} wget https://downloads.hpdd.intel.com/public/lustre/lustre-2.4.0/el6/server/RPMS/x86_64/{}
    # download the server SRPMs
    mkdir -p ~/lustre-rpm/server/SRPMS; cd ~/lustre-rpm/server/SRPMS
    w3m https://downloads.hpdd.intel.com/public/lustre/lustre-2.4.0/el6/server/SRPMS -dump | grep rpm | awk '{print $3}' | xargs -n 1 -I{} wget https://downloads.hpdd.intel.com/public/lustre/lustre-2.4.0/el6/server/SRPMS/{}


Install everything except for ZFS packages:

    cd ~/lustre-rpm/server/RPMS
    ls | grep -v zfs | xargs yum localinstall -y

Remove other non-Lustre kernels, reboot, then test it:

    /usr/lib64/lustre/tests/llmount.sh


Instructions for Building Lustre
================================

    yum -y groupinstall ""Development Tools""
    yum -y install xmlto asciidoc elfutils-libelf-devel zlib-devel binutils-devel newt-devel python-devel hmaccalc perl-ExtUtils-Embed bison elfutils-devel audit-libs-devel

ASCAR only patches the client kernel modules so we can disable
everything else to accelerate the building:

    sh autogen.sh
    ./configure --disable-server --disable-snmp --disable-doc --disable-utils --disable-tests --disable-maintainer-mode --disable-liblustre
    make -j9
",1,0.85,0.85,,,,,,0,1,,UCSC-REAL,
694402594,R_kgDOKWO-Ig,sbelambe,sbelambe/sbelambe,0,sbelambe,https://github.com/sbelambe/sbelambe,,0,2023-09-20 23:52:12+00:00,2024-11-04 01:48:54+00:00,2024-11-04 01:48:51+00:00,,11,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['sbelambe'],,"### Hi there 👋

<!--
**sbelambe/sbelambe** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...
-->

🚀 **About Me**  
- 🎓 Third-year CS major and Stats minor at UC Santa Cruz  
- 🔐 Recent Google STEP Intern focused on Android app security and reverse engineering with the Privacy Sandbox team  
- 🔍 Machine Learning Researcher with experience in network intrusion detection systems for botnet-based attacks  
- 👯 Currently leading a clustering team in UCSC's Tech4Good Lab, exploring advanced techniques in real-time survey response clustering  

---

🌱 **What I’m Working On**  
- **Machine Learning-Based Cybersecurity**: Building robust models to detect network threats in real-time  
- **Real-Time Clustering for Dynamic Surveys**: Enhancing survey platforms to capture and analyze nuanced feedback with OpenAI's API and concept induction  
- **Web Development Projects**: From sentiment analysis tools to team sponsorship websites, I'm passionate about designing and deploying impactful applications.  

---

💡 **Skills & Technologies**  
- **Languages**: Python, Java, C++, JavaScript, TypeScript, Swift  
- **Frameworks**: Angular, Django, React, Bootstrap, AWS  
- **Tools**: Protocol Buffers, Git/GitHub, TensorFlow, SciKit-Learn, AWS, Bazel, OpenAI API  

---

📫 **Let's Connect!**  
- **Email**: shivanibelambe@gmail.com  
- **LinkedIn**: [linkedin.com/in/shivani-belambe/](https://linkedin.com/in/shivani-belambe/)  
- **GitHub**: [github.com/sbelambe](https://github.com/sbelambe)  
",1,0.84,0.84,,,,,,0,1,,ucsc-cgp,
426507163,R_kgDOGWv7mw,UCSCXenaTools-Retrieve-Gene-Expression-and-Clinical-Information-from-UCSC-Xena-for-Survival-Analysis,binmishr/UCSCXenaTools-Retrieve-Gene-Expression-and-Clinical-Information-from-UCSC-Xena-for-Survival-Analysis,0,binmishr,https://github.com/binmishr/UCSCXenaTools-Retrieve-Gene-Expression-and-Clinical-Information-from-UCSC-Xena-for-Survival-Analysis,,0,2021-11-10 06:19:15+00:00,2021-11-10 06:35:13+00:00,2022-06-04 06:58:49+00:00,,135,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['binmishr'],,"# UCSCXenaTools-Retrieve-Gene-Expression-and-Clinical-Information-from-UCSC-Xena-for-Survival-Analysis

The details of the codeset and plots are included in the attached Microsoft Word Document (.docx) file in this repository. 
You need to view the file in ""Read Mode"" to see the contents properly after downloading the same.
",0,0.76,0.76,,,,,,0,1,,UCSC-Rocket-Club,
597237585,R_kgDOI5kfUQ,CSE130-1,Scorpio69t/CSE130-1,0,Scorpio69t,https://github.com/Scorpio69t/CSE130-1,UCSC/ Spring2022/ Principles_of_Computer_Systems_Design,0,2023-02-03 23:57:55+00:00,2023-01-29 02:31:50+00:00,2023-01-26 23:48:25+00:00,,420,0,0,,0,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['Mayuzh'],,"# CSE130 Repo

Repo to hold CSE130 work.
",1,0.7,0.7,,,,,,0,0,,ucsc-vama,
762425974,R_kgDOLXGydg,UTRome-trackhub,meekrob/UTRome-trackhub,0,meekrob,https://github.com/meekrob/UTRome-trackhub,UCSC-visible genome files,0,2024-02-23 18:54:04+00:00,2024-03-16 06:28:22+00:00,2024-03-15 21:15:50+00:00,,187965,1,1,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,main,1,['meekrob'],,,0,0.7,0.7,,,,,,0,1,,netdisco,
564499419,R_kgDOIaWT2w,UCSC_2,d0minicO/UCSC_2,0,d0minicO,https://github.com/d0minicO/UCSC_2,,0,2022-11-10 21:12:47+00:00,2022-11-10 21:12:47+00:00,2022-11-10 21:34:07+00:00,,93214,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['d0minicO'],,,0,0.77,0.77,,,,,,0,1,,ucscXena/,
628799402,R_kgDOJXq3qg,Computer-Science-Courses,ADJIBOULOU/Computer-Science-Courses,0,ADJIBOULOU,https://github.com/ADJIBOULOU/Computer-Science-Courses,,0,2023-04-17 02:18:19+00:00,2023-04-17 02:19:32+00:00,2023-04-17 02:18:30+00:00,,276,1,1,,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,1,master,1,"['Developer-Y', 'DateBro', 'PeskyPotato', 'Alaharon123', 'inf3cti0n95', 'mundher', 'bivashpandey', 'bhuthesh', 'anishathalye', 'MasoudKaviani', 'spekulatius', 'solomonbstoner', 'Suraj7879', 'tentena', 'unfode', 'ppisa', 'ProgrammingPete', 'P7h', 'R-Mahmoudi', 'richwill28', 'sashedher', 'JZZQuant', 'ShashankP19', 'trisolaris233', 'paulosalvatore', 'pvcraven', 'guybrush', 'Omar-Yasser', 'osyvokon', 'nishanths', 'emperor-jimmu', 'norswap', 'naeemshaikh90', 'rishabhb-git', 'phscloq', 'krnets', 'eternalfool', 'elarabyelaidy19', 'sootysec', 'yugborana', 'awxiaoxian2020', 'wcrasta', 'hxt365', 'taylorty', 'tejasurya', 'Sushants-Git', 'soum-c', '0xsomnus', 'shreyasdeotare', 'Ghost93', 'gusnaughton', 'GianAndreaSechi', 'floe', 'FabienTregan', 'danielrbradley', 'Chirag-Bansal', 'alebcay', 'bbhart', 'lamberta', 'BikramHalder', 'BemwaMalak', 'ayushpandey830', 'aleichtm', 'Atcold', 'cage433', 'wp-lai', 'ei-blue', 'htarsoo', 'm4salah', 'mperreux', 'mterwill', 'MaaniGhaffari', 'miangraham', 'laithshadeed', 'kairat-beep', 'joel-porquet', 'JVKdouk', 'jeffin07', 'jeffheaton', 'xtangle', 'tientaidev', 'IureRosa', 'hrithik254', 'hridaydutta123']",,,0,0.56,0.56,,"# Contribution Guidelines

- Recently quality of MOOCs has diminished, therefore only MOOCs with comprehensive lecture material which cover a subject/topic in ample detail will be added. For example, MOOC on Computer Networks or Machine Learning with 4-5 hours may not be able to cover all topics in sufficient detail and thus should be avoided.
- One philosophy used in this list while integrating MOOCs is that link should directly point to videos for viewing/downloading than registration and waiting for the next session. If videos are directly accessible through the platform/youtube or any other source, please use the direct source. This is list of video courses, not a list of MOOCs.
- Courses within a section are roughly sorted in terms of level i.e. undergraduate courses followed by upper level undergraduate, followed by graduate courses. As courses are from multiple Universities, sorting is not perfect and only an approximation. For example, while adding a new undergraduate course on Algorithms, please feel free to add it along with other Algorithms courses than after graduate courses.
",,,,0,1,,lsd-ucsc,
313137140,MDEwOlJlcG9zaXRvcnkzMTMxMzcxNDA=,Mja_UCSC,surykartka/Mja_UCSC,0,surykartka,https://github.com/surykartka/Mja_UCSC,,0,2020-11-15 22:38:33+00:00,2020-11-22 19:43:19+00:00,2020-11-22 19:43:17+00:00,,284720,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,[],,,0,0.63,0.63,,,,,,0,1,,corbett-lab,
330489206,MDEwOlJlcG9zaXRvcnkzMzA0ODkyMDY=,Cpp-Programming-Language,mosalaheg/Cpp-Programming-Language,0,mosalaheg,https://github.com/mosalaheg/Cpp-Programming-Language,Cpp programming language lectures for computer science students.,0,2021-01-17 21:21:13+00:00,2021-07-29 10:13:59+00:00,2021-01-16 09:07:24+00:00,,11390,0,0,,0,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,master,1,['cs-MohamedAyman'],,"<img align=""right"" width=""120"" height=""120"" src=""https://github.com/cs-MohamedAyman/Computer-Science-Textbooks/blob/master/logos/cpp.jpg"">

# Cpp-Programming-Language `50H`
Cpp programming language lectures for computer science students.

## `Part 1:` C++ Basics and Arrays `25H`

## Lecture 01 - [Overview](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-01-Overview)
## Lecture 02 - [Variable Types](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-02-Variable-Types)
## Lecture 03 - [Basic Operations](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-03-Basic-Operations)
## Lecture 04 - [Conditions](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-04-Conditions)
## Lecture 05 - [Loops](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-05-Loops)
## Lecture 06 - [Arrays](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-06-Arrays)

## `Part 2:` C++ Pointers and Functions `25H`

## Lecture 07 - [Pointers and References](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-07-Pointers-and-References)
## Lecture 08 - [Functions](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-08-Functions)
## Lecture 09 - [Strings](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-09-Strings)
## Lecture 10 - [Structures](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-10-Structures)
## Lecture 11 - [Enumerations and Unions](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-11-Enumerations-and-Unions)
## Lecture 12 - [Numbers](https://github.com/cs-MohamedAyman/Cpp-Programming-Language/tree/master/Lecture-12-Numbers)

<img align=""right"" width=""80"" height=""120"" src=""https://github.com/cs-MohamedAyman/Computer-Science-Textbooks/blob/master/logos/textbooks.jpg"">

## Textbooks

* ***C++ Primer Stanley***, B. Lippman and Josee lajoie and Barbara E. Moo
* ***Effective Modern C++***, Scott Meyers
* ***Accelerated C++***, Andrew Koenig and Barbara E. Moo
* ***C++ The Complete Reference***, Herbert Schildt
* ***C++ TutorialsPoint***
* ***A Complete Guide to Programming in C++***, Ulla Kirch-Prinz

<img align=""right"" width=""90"" height=""50"" src=""https://github.com/cs-MohamedAyman/Coursera-Specializations/blob/master/organizations-logos/coursera.jpg"">

## Coursera-Specializations

* ***Introduction to Programming in C Specialization*** by Duke University
* ***C for Everyone: Programming Fundamentals*** by University of California, Santa Cruz
* ***C for Everyone: Structured Programming*** by University of California, Santa Cruz
* ***C++ For C Programmers, Part A*** by University of California, Santa Cruz
* ***C++ For C Programmers, Part B*** by University of California, Santa Cruz
* ***Computational Thinking with Beginning C Programming Specialization*** by University of Colorado Boulder

<img align=""right"" width=""80"" height=""50"" src=""https://github.com/cs-MohamedAyman/YouTube-Playlists/blob/master/organizations-logos/youtube.jpg"">

## YouTube-Playlists

* thenewboston - C++ Programming Tutorials Playlist
* ProgrammingKnowledge - QT C++ GUI Tutorial For Beginners
* ProgrammingKnowledge - C++ Programming Tutorial for Beginners (For Absolute Beginners)
* Naresh i Technologies - C++ Programming Language
* CodeWithHarry - C++ Tutorials In Hindi
* easytuts4you - C++ Programming Tutorials (HINDI/URDU)
* by The Cherno - C++
* Geeky Shows - C++ Programming in Hindi
* Caleb Curry - C++ Tutorials
* GeeksforGeeks - C++ Programming Language Tutorials
* C++ by Saurabh Shukla Sir - C++ by Saurabh Shukla
* LearningLad - Learn C++ Programming
* Simple Snippets - C++ Programming Tutorials for Beginners & IT students
",0,0.33,0.33,,,,,,0,0,,seadsystem,
23424326,MDEwOlJlcG9zaXRvcnkyMzQyNDMyNg==,BSgenome.Hsapiens.UCSC.hg19,Przemol/BSgenome.Hsapiens.UCSC.hg19,0,Przemol,https://github.com/Przemol/BSgenome.Hsapiens.UCSC.hg19,Mirror of BSgenome.Hsapiens.UCSC.hg19,0,2014-08-28 10:38:21+00:00,2014-08-28 12:37:01+00:00,2014-08-28 12:37:01+00:00,,843276,0,0,R,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['Przemol'],,,0,0.54,0.54,,,,,,0,1,,UCSC-Treehouse,
433466200,R_kgDOGdYrWA,5pL1sc,wmckerrow/5pL1sc,0,wmckerrow,https://github.com/wmckerrow/5pL1sc,,0,2021-11-30 14:39:59+00:00,2025-02-12 22:16:11+00:00,2025-02-12 22:16:07+00:00,,4357,4,4,Jupyter Notebook,1,1,1,1,0,0,2,0,0,1,other,1,0,0,public,2,1,4,main,1,['wmckerrow'],,"## 5' scL1seq

This repository contains python scripts for counting UMIs from 10x genomics 5' targeted 
single cell RNA-seq with 100+ base pair paired end reads. Instructions are given in this
readme.

### Step 1: Build the custom cellranger index (human)
Building the cellranger custom index, will require bedtools and cellranger. Both are
available for install via anaconda:
-https://anaconda.org/bioconda/bedtools
-https://anaconda.org/hcc/cellranger

Before beginning you will need the ucsc genome browser version of the hg38 human genome,
which can be downloaded as follows:
```
curl -L -O http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz
zcat hg38.fa.gz > hg38.fa
```

You will also need hg38 gene annotations in gtf format, which can be downloaded as follows:

```
curl -L -O https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/genes/hg38.refGene.gtf.gz
zcat hg38.refGene.gtf.gz > hg38.refGene.gtf
awk '!($1 ~ /_/)' hg38.refGene.gtf > hg38.refGene.fixed.gtf
```

You can then use the provided shell script to build the index:
```
bash make_custom_cellranger_reference.sh /path/to/hg38.fa /path/to/5pL1sc/L1_annotation/L1HS_and_PA.bed /path/to/5pL1sc/L1_annotation/L1HS_and_dfam_L1PA.fa /path/to/hg38.refGene.fixed.gtf L1HS_L1PA_seperated_hg38
```

This will create the custom cellranger index in a new directory called L1HS\_L1PA\_seperated\_hg38

### Step 1 v2: Build the custom cellranger index (mouse)
Building the cellranger custom index, will require bedtools and cellranger. Both are
available for install via anaconda:
-https://anaconda.org/bioconda/bedtools
-https://anaconda.org/hcc/cellranger

Before beginning you will need the ucsc genome browser version of the mm39 mouse genome,
which can be downloaded as follows:
```
curl -L -O http://hgdownload.cse.ucsc.edu/goldenPath/mm39/bigZips/mm39.fa.gz
zcat mm39.fa.gz > mm39.fa
```

You will also need mm39 gene annotations in gtf format, which can be downloaded as follows:

```
curl -L -O https://hgdownload.soe.ucsc.edu/goldenPath/mm39/bigZips/genes/refGene.gtf.gz
zcat refGene.gtf.gz > mm39.refGene.gtf
awk '!($1 ~ /_/)' mm39.refGene.gtf > mm39.refGene.fixed.gtf
```

You can then use the provided shell script to build the index:
```
bash make_custom_cellranger_reference.sh /path/to/mm39.fa /path/to/5pL1sc/L1_annotation/L1Md.bed /path/to/5pL1sc/L1_annotation/L1MdI.Consensus.fa /path/to/mm39.refGene.fixed.gtf L1Md_seperated_mm39
```

This will create the custom cellranger index in a new directory called L1Md\_seperated\_mm39


### Step 2: Cellranger

Then you can run cellranger count to align reads and count gene UMIs. Instructions to
obtain and run cellranger count can be found here.
- [Cell Ranger from 10x genomics](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/count)

On a machine with 16 cores and 64gb of memory, cellranger count can be executed as follows:
```
cellranger count --sample=<sample name> --id=<output id> --transcriptome=/path/to/custom/index/L1HS_L1PA_seperated_hg38[L1Md_seperated_mm39] --fastqs=/path/to/folder/with/raw/fastqs --localcores=16 --localmem=64
```

If combining multiple samples, we recommend running [cellranger aggr](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate) to downsample reads
and avoid artifacts that may result from samples sequenced to different depths.

### Step 3: Count UMIs
LINE-1 UMIs are counted using:
- count\_properpairUMIs\_in\_range\_by\_nM.py
To count human LINE-1 in a single sample, you need only specify the bam file output by
Cell Ranger:
```
python count_properpairUMIs_in_range_by_nM.py --bam 5p_sc_sample/outs/possorted_genome_bam.bam > L1HS_properUMI_counts.txt
```

To count mouse LINE-1, you will need specify relevant ranges for the LINE-1 reads to fall.
We used:
  * --read1\_range L1MdTf\_I\_5end:1-600 --read2\_range L1MdTf\_I\_5end for L1MdTf
  * --read1\_range L1MdGf\_I\_5end:1-550 --read2\_range L1MdGf\_I\_5end for L1MdGf
  * --read1\_range L1MdA\_I\_5end:1-750 --read2\_range L1MdA\_I\_5end for L1MdA

If you intend to do differential expression, we recommend that you use cellranger aggr
to merge samples and down sample reads to a constant number per cell. The amount of
down sampling done by aggr can be found in the summary.json output file. You can then
pass that amount of down sampling forward using the --downsample_to option.

The output will be a two column tab delimited table, where column 1 is the cell barcode
and column two is the number of L1Hs UMIs with the cell barcode. Cells with 0 L1Hs UMIs
are not reported.

### Optional: add L1Hs UMIs to seurat object

With a Seurat object (seurat\_10x5p) and the output from above (L1HS\_properUMI\_counts.txt)
The UMIs can be added to the Seurat object in R as follows:
```
seurat_10x5p_L1_UMIs = read.table(""~/Documents/LINE1_projects/sc_LINE1/vdj_v1_hs_nsclc_5gex/L1HS_properUMI_counts.txt"",sep='\t')
seurat_10x5p_L1_UMIs = seurat_10x5p_L1_UMIs[seurat_10x5p_L1_UMIs[,1]%in%colnames(seurat_10x5p),]
L1_count = rep(0,ncol(seurat_10x5p))
names(L1_count) = colnames(seurat_10x5p)
L1_count[seurat_10x5p_L1_UMIs[,1]] = seurat_10x5p_L1_UMIs[,2]
seurat_10x5p$L1_count = L1_count
seurat_10x5p$L1_lognorm = log(seurat_10x5p$L1_count/seurat_10x5p$nCount_RNA*10^4+1)
```

### Important notes
- The Cell Ranger output will quantify an L1Hs transcript. This quantification does not
include the additional filtering in step 2. Only consider with extreme caution.
- The Cell Ranger index also includes Alu consensus that could be used to quantify Alu.
We chose not to include any Alu quantifications as we did not have a sample with a known
pattern of Alu expression that could serve as test case. Again extreme caution should be
excercised when using an Alu related output.
",0,0.49,0.49,,,,,,0,1,,Leeps-lab,
73535250,MDEwOlJlcG9zaXRvcnk3MzUzNTI1MA==,118Project,TeamPutterworth/118Project,0,TeamPutterworth,https://github.com/TeamPutterworth/118Project,Repository for the UCSC 118 Project.,0,2016-11-12 06:15:31+00:00,2016-11-12 08:53:41+00:00,2016-12-08 20:24:56+00:00,,2130,0,0,C,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,"['JeremyCrowley', 'grant1994']",1,"# 118Project_jdgrant_rlhill_jcrowley
Repository for the UCSC 118 Project. 

# Team Members:

	Joseph Grant, email: jdgrant@ucsc.edu
	Randall Hill, email: rlhill@ucsc.edu
	Jeremy Crowley, email: jcrowley@ucsc.edu

#Description

This is the code-base for Team 6th Order Putterworth's robot to participate in UCSC's CMPE-118 Slug-O-Lympics Competition.
In this competition an autonomous robot must navigate an arena to navigate to one of potentially four active loading 
towers. There it will load ammo (three ping pong balls) and then it must navigate to and unload into at least two seperate targets.

# Directory Structure
Organization of the directory should be maintained to follow these general rules: 
* Drivers for various peripherals (actuators and sensors) are located in the Drivers folder. 
* Services for synchronous sampling and motor control in the services folder.
* EventCheckers for beacon, track wire, and bump detection in the EventCheckers folder.

",1,0.88,0.88,,,,,,0,1,,jlab-sensing,
392551896,MDEwOlJlcG9zaXRvcnkzOTI1NTE4OTY=,CmiRClustFinder_v1.0,msls-bioinfo/CmiRClustFinder_v1.0,0,msls-bioinfo,https://github.com/msls-bioinfo/CmiRClustFinder_v1.0,"CmiRClustFinder v1.0: a tool for identification of CNV co-locolized miRNA, miRNA clusers and genes in TCGA cancer cohorts. ",0,2021-08-04 04:48:23+00:00,2023-02-08 11:13:12+00:00,2022-11-17 07:49:55+00:00,,130,3,3,R,1,1,1,1,0,0,13,0,0,1,,1,0,0,public,13,1,3,main,1,"['msls-bioinfo', 'akshayware94', 'deepsourcebot']",,"# *CmiRClustFinder v1.0*
## Overview
Interestingly ~ 25% of annotated human microRNAs (miRNAs) occur as a cluster in intragenic and intergenic regions of the human genome. Each miRNA cluster consists of two or more miRNAs that are transcribed from physically adjacent miRNA genes from a single promoter. These miRNA clusters are proposed to work more efficiently than a single miRNA as it contains multiple miRNA encoding genes. These multiple members of a cluster show high sequence similarity in the seed region and they often have the same targets or target different genes belonging to specific pathways. The expression of miRNA clusters is regulated by several genetic and epigenetic reprogramming. The miRNA genes are most commonly affected by the structural variations as compared to single nucleotide variation. The critical reason for the structural variation is the clustering of miRNAs at chromosome fragile sites. Here, we developed a user-friendly pipeline `CmiRClustFinder`, which integrates user-inputted miRNA cluster information with copy number variation (CNV) datasets from TCGA and identifies CNV co-localized miRNA clusters. The primary version of `CmiRClustFinder` is restricted to TCGA CNV datasets, it will be expanded with subsequent updates. <br /><br />
*CmiRClustFinder* utility is developed to identify Recurrent Copy Number Variations (RCNV) - colocalized miRNA clusters from TCGA Cancer datasets. This pipeline is not only limited to miRNA clusters, users can also supply any genomic element (eg. any specific region or gene) to check for RCNV associations in specific TCGA cancer types. (Note: please use hg38 genome build coordinates)

## Citation
If you are using this pipeline, please cite:
```
Ware, A.P., Kabekkodu, S.P., Chawla, A., Paul, B., Satyamoorthy K. Diagnostic and prognostic potential clustered miRNAs in bladder cancer. 3 Biotech 12, 173 (2022). https://doi.org/10.1007/s13205-022-03225-z

```

The automated version of this pipeline is now published and available for use:

```
Ware, A.P., Satyamoorthy K., Paul, B. CmirC: an integrated database of clustered miRNAs co-localized with copy number variations in cancer. Funct Integr Genomics (2022). https://doi.org/10.1007/s10142-022-00909-w 

```
## Installation
  *CmiRClustFinder* is designed for Linux operating system. If you wish to use this pipeline, follow the instructions below. <br />
  
### Prerequisite
  The following Linux utilities are required to run this pipeline. Please make sure the following are installed and available on your system prior to run `install.sh` from the source directory.<br />
  ```
  1. R = 4.0 (or higher)
  2. git
  3. unzip
  ```
  
If the above prerequisites are satisfied, you are ready to install dependencies and build the program. Note during the building procedure, `install.sh` will attempt to download and install several packages, so an active internet connection is required.
  
To obtain *CmiRClustFinder*, Use: <br />
```
git clone https://github.com/msls-bioinfo/CmiRClustFinder_v1.0.git
cd CmiRClustFinder_v1.0/
```
or 
<br/>
```
wget https://github.com/msls-bioinfo/CmiRClustFinder_v1.0/archive/refs/heads/main.zip
unzip main.zip
cd CmiRClustFinder_v1.0-main/
```
Assuming that you have downloaded the source code and it is in a directory `CmiRClustFinder/`, to install all dependencies follow the procedure: <br />

```
cd CmiRClustFinder/
sh install.sh
```
After the successful execution of `install.sh` you are ready to run the main pipeline script `CmiRClustFinder.r` which is located in `RScript/` directory

## Usage
The pipleline triggered by executing `Rscript CmiRclustFinder.r` which is present in `Rscripts` directory<br />
please navigate into the `Rscripts` directory to start the pipeline

```
Rscript CmiRclustFinder.r <TCGA cohort abbreviation> <BED file specifying user intrested genomic regions>
```
The first argument required to `Rscript` is TCGA cohort abbreviation, you can select from the list below

|Sr. No.| Cohort Abbreviation | Cohort Name |
|------|---------------------|--------------|
|1|TCGA-ACC|Adrenocortical carcinoma|
|2|TCGA-BLCA|Bladder Urothelial Carcinoma|
|3|TCGA-BRCA|Breast invasive carcinoma|
|4|TCGA-CESC|Cervical squamous cell carcinoma and endocervical adenocarcinoma|
|5|TCGA-CHOL|Cholangiocarcinoma|
|6|TCGA-COAD|Colon adenocarcinoma|
|7|TCGA-COADREAD|Colorectal adenocarcinoma|
|8|TCGA-DLBC|Lymphoid Neoplasm Diffuse Large B-cell Lymphoma|
|9|TCGA-ESCA|Esophageal carcinoma|
|10|TCGA-GBM|Glioblastoma multiforme|
|11|TCGA-GBMLGG|Glioma|
|12|TCGA-HNSC|Head and Neck squamous cell carcinoma|
|13|TCGA-KICH|Kidney Chromophobe|
|14|TCGA-KIPAN|Pan-kidney cohort(KICH+KIRC+KIRP)|
|15|TCGA-KIRC|Kidney renal clear cell carcinoma|
|16|TCGA-KIRP|Kidney renal papillary cell carcinoma|
|17|TCGA-LGG|Brain Lower Grade Glioma|
|18|TCGA-LIHC|Liver hepatocellular carcinoma|
|19|TCGA-LUAD|Lung adenocarcinoma|
|20|TCGA-LUSC|Lung squamous cell carcinoma|
|21|TCGA-MESO|Mesothelioma|
|22|TCGA-OV|Ovarian serous cystadenocarcinoma|
|23|TCGA-PAAD|Pancreatic adenocarcinoma|
|24|TCGA-PCPG|Pheochromocytoma and Paraganglioma|
|25|TCGA-PRAD|Prostate adenocarcinoma|
|26|TCGA-READ|Rectum adenocarcinoma|
|27|TCGA-SARC|Sarcoma|
|28|TCGA-SKCM|Skin Cutaneous Melanoma|
|29|TCGA-STAD|Stomach adenocarcinoma|
|30|TCGA-TGCT|Testicular Germ Cell Tumors|
|31|TCGA-THCA|Thyroid carcinoma|
|32|TCGA-THYM|Thymoma|
|33|TCGA-UCEC|Uterine Corpus Endometrial Carcinoma|
|34|TCGA-UCS|Uterine Carcinosarcoma|
|35|TCGA-UVM|Uveal Melanoma|

The second argument required to `Rscript` is BED file which contains the specific genomic regions, to check their co-localization with RCNV. <br />
Below is an example of BED file <br />
NOTE: The table header is for descriptive purposes, BED file should not have header

|CHROM|START|END|IDENTIFIER|
|-----|-----|---|----------|
|chr19|53666679|53706336|hsa-miR-526a-1/miR-512-1|
|chr14|101022066|101043062|hsa-miR-1185-1/miR-379|
|chr14|100869060|100884783|hsa-miR-136/miR-493|
|chrx|50003148|50014683|hsa-miR-502/miR-532|
|chr9|134849298|134850807|hsa-miR-3689f/miR-3689c|
|chr13|91350605|91351391|hsa-miR-92a-1/miR-17|
|chrx|134169378|134170278|hsa-miR-106a/miR-363|
|chrx|134540185|134546711|hsa-miR-424/miR-450b|
|chrx|145992750|146001131|hsa-miR-891b/miR-892c|
|chr20|63919449|63919939|hsa-miR-941-5/miR-941-1|

Know more about the bed file format : (http://genome.ucsc.edu/FAQ/FAQformat#format1)

## Examples
Sample datasets are included in the Examples folder. <br />
Run CmiRClustFinder on a sample dataset:
```

```


### Requirements
`CmiRClustFinder` requires an segmented somatic copy number altrations (sCNA) data for patients group from TCGA cohort.

## Output files
Results are saved to the run-specific folder inside the `Output` directory 





### Credits

1. TCGA Workflow: Analyze cancer genomics and epigenomics data using Bioconductor packages (https://f1000research.com/articles/5-1542)
2. TCGAbiolinks R package (https://bioconductor.org/packages/release/bioc/html/TCGAbiolinks.html)
3. BEDTools (https://bedtools.readthedocs.io/en/latest/)
4. UCSC liftOver (https://genome-store.ucsc.edu/)

",0,0.51,0.51,,,,,,0,1,,santacruzml,
790556716,R_kgDOLx7wLA,cse160a5a,koudav/cse160a5a,0,koudav,https://github.com/koudav/cse160a5a,"assignment 5a, cse 160, spring '24, ucsc",0,2024-04-23 05:15:02+00:00,2024-06-07 09:44:00+00:00,2024-06-07 09:43:57+00:00,,7596,0,0,JavaScript,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['koudav'],,,1,0.65,0.65,,,,,,0,1,,,
868715921,R_kgDOM8eNkQ,D1-Kitty-Click,d3adgoose/D1-Kitty-Click,0,d3adgoose,https://github.com/d3adgoose/D1-Kitty-Click,,0,2024-10-07 03:51:53+00:00,2024-10-30 23:21:55+00:00,2024-10-30 23:21:51+00:00,,47,0,0,TypeScript,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['d3adgoose'],,"This is the starter code project for the `Demo 1` project in CMPM 121, Game Development Patterns, at UC Santa Cruz (fall quarter, 2024). Students should incrementally modify this project, tracking their progress with git, to develop an incremental game of their own unique design.

Kaylee has updated the code to make it more of a cat clicker game that feeds the cat lover!

The source that helped me improve my game:
https://juicestine01.github.io/cmpm-121-demo-1/ ,
https://ishachury20.github.io/cmpm-121-demo-1/
",1,0.81,0.81,,,,,,0,1,,,
563056558,R_kgDOIY-Prg,Shoelace,jackckelly/Shoelace,0,jackckelly,https://github.com/jackckelly/Shoelace,"GM information and suggestions for Fatal Frequencies, a GUMSHOE One-2-One module",0,2022-11-07 20:19:24+00:00,2023-10-06 17:17:04+00:00,2024-01-11 09:01:24+00:00,,329,2,2,JavaScript,0,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,2,main,1,"['jackckelly', 'Will-Tate']",,"# Shoelace

See the latest build of the project at: https://www.devi-a.com/FatalFrequenciesFrontend

## About the project
This project was started as part of my work at the Expressive Intelligence Studio at the University of California, Santa Cruz. For the project, I was interested in exploring how we can use computational tools to help better visualize and provide suggestions for game masters of tabletop roleplaying games. This allows GMs to track what has happened in the game world, and uses knowledge of the current state (as well as a Prolog knowledge database) to provide suggestions for what can happen next.

This project uses the Fatal Frequencies scenario from the Gumshoe One-2-One system for this prototype. It features the characters, information, and clues from the game, as well as a sample of Prolog queries that could be useful to GMs. All text, including scene information and characters are taken from Fatal Frequencies, and are not my own creation. They are used here purely for research and demonstration purposes. To find more of this content and support the original authors, check out Cthulhu Confidential here: https://pelgranepress.com/product/cthulhu-confidential/

## How to use
Download this repository, then run a localhost server in the folder's directory to get it up and running. For instance, if you have Python3 installed, enter the command ``python -m http.server``, then navigate in your browser to ``http://localhost:8000/``. 

## database.prolog
The majority of the content from the module has been extracted and rewritten in Prolog. You can access and edit this file in order to see the information in the game, and to add new Prolog queries to the website. 

## Graphs
All graphs are rendered using Mermaid (https://mermaid-js.github.io/mermaid/#/). The initial starting graphs are preset based on the module's information, but can change based on input (for instance, nodes changing color when scenes are completed). The scene nodes are also clickable links. 

## Prolog queries
The GM suggestions screen features Prolog queries that pull out patterns of characters, scenes, and clues that match specific queries. While this includes some examples here, in order to add more, you will want to write the Prolog query in the ``database.prolog`` file and add the call to that query in the ``main.js`` file. 

",1,0.81,0.81,,,,,,0,0,,,
30938374,MDEwOlJlcG9zaXRvcnkzMDkzODM3NA==,OmicsIntegrator,fraenkel-lab/OmicsIntegrator,0,fraenkel-lab,https://github.com/fraenkel-lab/OmicsIntegrator,This repository is the working directory for the Garnet-Forest bundle of python scripts for analyzing diverse forms of 'omic' data in a network context.,0,2015-02-17 21:21:07+00:00,2023-12-05 06:32:29+00:00,2019-01-09 21:01:12+00:00,http://fraenkel.mit.edu/omicsintegrator,264666,31,31,Python,1,1,1,1,0,0,20,1,0,6,bsd-2-clause,1,0,0,public,20,6,31,master,1,"['agitter', 'AmandaKedaigle', 'aabaker99', 'jpgulliver', 'Mkebede', 'nafisahis', 'alexlenail']",1,"<center><img src=""http://fraenkel-nsf.csbi.mit.edu/omicsintegrator/omicsI_logo.png"" height=""40%"" width=""40%"" ></center>

## OmicsIntegrator has moved. See [OmicsIntegrator2](https://github.com/fraenkel-lab/OmicsIntegrator2). This codebase is not maintained. 

[![Build Status](https://travis-ci.org/fraenkel-lab/OmicsIntegrator.svg?branch=master)](https://travis-ci.org/fraenkel-lab/OmicsIntegrator)

Omics Integrator is a package designed to integrate proteomic data, gene expression data and/or epigenetic data using a protein-protein interaction network. It is comprised of two modules, Garnet and Forest.

Contact: Amanda Kedaigle [mandyjoy@mit.edu]

Copyright (c) 2015 Massachusetts Institute of Technology
All rights reserved.

Reference:
--------------------
[Network-Based Interpretation of Diverse High-Throughput Datasets through the Omics Integrator Software Package](http://dx.doi.org/10.1371/journal.pcbi.1004879)
Tuncbag N<sup>\*</sup>, Gosline SJC<sup>\*</sup>, Kedaigle A, Soltis AR, Gitter A, Fraenkel E. *PLoS Comput Biol* 12(4): e1004879. doi:10.1371/journal.pcbi.1004879.

For a step-by-step protocol for running this software:
[Discovering altered regulation and signaling through network-based integration of transcriptomic, epigenomic and proteomic tumor data](https://link.springer.com/protocol/10.1007/978-1-4939-7493-1_2)
Kedaigle A, and Fraenkel E. *Cancer Systems Biology: Methods in Molecular Biology*, 2018.

System Requirements:
--------------------
1. Python 2.6 or 2.7 (3.x version currently untested) and the dependencies
below. We recommend that users without an existing Python environment
install Anaconda (https://www.continuum.io/downloads) to obtain Python
2.7 and the following required packages:
  - numpy: http://www.numpy.org/
  - scipy: http://www.scipy.org/
  - matplotlib: http://matplotlib.org/
  - Networkx: http://networkx.github.io

2. msgsteiner package (version 1.3): [code](http://staff.polito.it/alfredo.braunstein/code/msgsteiner-1.3.tgz), [license](http://areeweb.polito.it/ricerca/cmp/code/bpsteiner)

3. Boost C++ library: http://www.boost.org

4. Cytoscape for viewing results graphically (tested on versions 2.8-3.2):
http://www.cytoscape.org

Features
--------

* Maps gene expression data to transcription factors using chromatin
  accessibility data

* Identifies proteins in the same pathway as `hits` using protein interaction
  network

* Integrates numerous high throughput data types to determine testable
  biological hypotheses

Installation:
--------------------
Omics Integrator is a collection of Python scripts and data files so can be
easily installed on any system. Steps 1 through 4 are only required for Forest,
and you may skip to step 5 if you will only be running Garnet.

1. Boost is pre-installed on many Linux distributions. If your operating system
does not include Boost, follow the [Boost getting started
guide](http://www.boost.org/doc/libs/1_59_0/more/getting_started/index.html) for
instructions on how to download the library and extract files from the archive.
To use the [Homebrew](http://brew.sh/) package manager for Mac simply type `brew install boost` to install the library.
2. Download `msgsteiner-1.3.tgz` from http://staff.polito.it/alfredo.braunstein/code/msgsteiner-1.3.tgz ([license](http://areeweb.polito.it/ricerca/cmp/code/bpsteiner))
3. Unpack files from the archive: `tar -xvf msgsteiner-1.3.tgz`
4. Enter the `msgsteiner-1.3` subdirectory and run `make`
  * See [this advice](./patches) on compiling the C++ code if you encounter problems and [this advice](https://github.com/fraenkel-lab/OmicsIntegrator/issues/22) regarding compilation issues on OS X.
  * Make a note of the path to the compiled msgsteiner file that was created, which you will use when running Forest.
  * In Linux, use `readlink -f msgsteiner` in the `msgsteiner-1.3` subdirectory to obtain the path.
5. Download the Omics Integrator package: [OmicsIntegrator-0.3.1.tar.gz](./dist/OmicsIntegrator-0.3.1.tar.gz)
6. Unpack files from the archive: `tar -xvzf OmicsIntegrator-0.3.1.tar.gz`
7. Make sure you have all the requirements using the pip tool by entering the
directory and typing: `pip install -r requirements.txt`
  * Some users have reported errors when using this command to install matplotlib. To fix, install matplotlib independently (http://matplotlib.org) or use Anaconda as indicated above.

Now Omics Integrator is installed on your computer and can be used to analyze
your data.

Examples
-----------------
We provide many scripts and files to showcase the various capabilities of Omics
Integrator.  To run this:

1. Download the [example files](./dist/OmicsIntegratorExamples.tar.gz)
2. Unpack by typing `tar -xvzf OmicsIntegratorExamples.tar.gz` in the `dist`
directory.
3. Move the unpacked files into the `example` directory.

For specific details about the examples, check out the [README
file](./example/README.md) in the example directory.

Running garnet.py
-----------------

Garnet is a script that runs a series of
smaller scripts to map epigenetic data to genes and then scan the genome to
determine the likelihood of a transcription factor binding the genome near that
gene.

```
Usage: garnet.py [configfilename]

  -s SEED, --seed=SEED  An integer seed for the pseudo-random number
                        generators. If you want to reproduce exact results,
                        supply the same seed. Default = None.


Options:
  -h, --help            show this help message and exit
  --outdir=OUTDIR       Name of directory to place garnet output. DEFAULT:none
  --utilpath=ADDPATH    Destination of chipsequtil library, Default=../src
```

Unlike Forest, the Garnet configuration file is a positional argument and must not
be preceded with `--conf=`.  The configuration file should take the following format:

### garnet input

```
[chromatinData]
#these files contain epigenetically interesting regions
bedfile = bedfilecontainingregions.bed
fastafile = fastafilemappedusinggalaxytools.fasta
#these two files are provided in the package
genefile = ../../data/ucsc_hg19_knownGenes.txt
xreffile = ../../data/ucsc_hg19_kgXref.txt
#distance to look from transcription start site
windowsize = 2000

[motifData]
#motif matrices to be used, data provided with the package
tamo_file = ../../data/matrix_files/vertebrates_clustered_motifs.tamo
#settings for scanning
genome = hg19
numthreads = 4
doNetwork = False
tfDelimiter = .

[expressionData]
expressionFile = tabDelimitedExpressionData.txt
pvalThresh = 0.01
qvalThresh =

[regression]
#for generating and saving regression plots
savePlot=False
```

#### Chromatin Data

Many BED-formatted (`bedfile`) and FASTA-formatted (`fastafile`) files are
included in the examples/ directory. `bedfile` can also be output from MACS
(with a `.xls` extension) or GPS/GEM (with a `.txt` extension).
To use your own epigenetic data, convert to BED and upload the
BED-file to http://usegalaxy.org and select `Fetch Alignments/Sequences` from the left
menu to click on `Extract Genomic DNA`. This will produce a FASTA-formatted file
that will work with garnet.  We have provided gene (`genefile`) and xref
(`xreffile`)  annotations for both hg19 and mm9 - these files can be downloaded
from http://genome.ucsc.edu/cgi-bin/hgTables if needed. The `windowsize`
parameter determines the maximum distance from a transcription start site to
consider an epigenetic event associated. 2kb is a very conservative metric.

#### motifData

We provide motif data in the proper TAMO format, the user just needs to enter
the genome used.  The default `numthreads` is 4, but the user can alter this
depending on the processing power of their machine. `doNetwork` will create a
NetworkX object mapping transcription factors to genes, required input for the
[SAMNet algorithm](http://github.com/sgosline/SAMNet).  `tfDelimiter` is an
internal parameter to tell Garnet how to handle cases when many transcription
factors map to the sam binding motif.

#### expressionData

If the user has expression data to evaluate, provide a tab-delimited file under
`expressionFile`.  File should have two columns, one containing the name of the
gene and the second containing the log fold change of that gene in a particular
condition. We recommend only including those genes whose change in expression is
statistically significant. P-value (`pvalThresh`) or Q-value (`qvalThresh`)
thresholds will be used to select only those transcription factors whose
correlation with expression falls below the provided threshold.

#### regression

Linear regression plots are placed in a subdirectory named `regression_plots` if
`savePlot=True` in the configuration file.

### Garnet output

Garnet produces a number of intermediate files that enable you
to better interpret your data or re-run a sub-script that may have failed. All
files are placed in the directory provided by the `--outdir` option of the
garnet script.

- **events_to_genes.fsa**: This file contains the regions of the fastafile
  provided in the configuration file that are within the specified distance to a
  transcription start site.

- **events_to_genes.xls**: This file contains each region, the epigenetic
  activity in that region, and the relationship of that region to the closest
  gene.

- **events_to_genes_with_motifs.txt**: This contains the raw transcription
  factor scoring data for each region in the fasta file.

- **events_to_genes_with_motifs.tgm**: This contains the transcription factor
  binding matrix scoring data mapped to the closest gene.

- **events_To_genes_with_motifs_tfids.txt**: Names of transcription factors (or
  columns) of the matrix.

- **events_to_genes_with_motifs_geneids.txt**: Names of genes (or rows) of the
  matrix.

- **events_to_genes_with_motifs.pkl**: A Pickle-compressed Python File
  containing a dictionary data structure that contains files 4-6 (under the keys
  `tgm`,`tfs`, and `genes`) respectively as well as a `delim` key that describes
  what delimiter was used to separate out TFs in the case where there are
  multiple TFs in the same family.

- **events_to_genes_with_motifsregression_results.tsv**: Results from linear
  regression.

- **events_to_genes_with_motifsregression_results_FOREST_INPUT.tsv**: Only those
  regression results that fall under the p-value or q-value significance
  threshold provided in the configuration file, e.g. p=0.05, are included.
  This file can be used as input to Forest, and the prizes are -log2(pval)
  or -log2(qval).

- **regression_plots**: An optional subdirectory that contains plots visualizing
  the transcription factor linear regression tests.

Running forest.py
-----------------

Forest **requires** the compiled msgsteiner package.

```
Usage: forest.py [options]

Find multiple pathways within an interactome that are altered in a particular
condition using the Prize Collecting Steiner Forest problem

Options:
  -h, --help            show this help message and exit
  -p PRIZEFILE, --prize=PRIZEFILE
                        (Required) Path to the text file containing the
                        prizes. Should be a tab delimited file with lines:
                        ""ProteinName PrizeValue""
  -e EDGEFILE, --edge=EDGEFILE
                        (Required) Path to the text file containing the
                        interactome edges. Should be a tab delimited file with
                        3 or 4 columns: ""ProteinA        ProteinB
                        Weight(between 0 and 1) Directionality(U or D,
                        optional)""
  -c CONFFILE, --conf=CONFFILE
                        Path to the text file containing the parameters.
                        Should be several lines that looks like:
                        ""ParameterName = ParameterValue"". Must contain values
                        for w, b, D. May contain values for optional
                        parameters mu, garnetBeta, noise, r, g. Default =
                        ""./conf.txt""
  -d DUMMYMODE, --dummyMode=DUMMYMODE
                        Tells the program which nodes in the interactome to
                        connect the dummy node to. ""terminals""= connect to all
                        terminals, ""others""= connect to all nodes except for
                        terminals, ""all""= connect to all nodes in the
                        interactome. If you wish you supply your own list of
                        proteins, dummyMode could also be the path to a text
                        file containing a list of proteins (one per line).
                        Default = ""terminals""
  --garnet=GARNET       Path to the text file containing the output of the
                        GARNET module regression. Should be a tab delimited
                        file with 2 columns: ""TranscriptionFactorName
                        Score"". Default = ""None""
  --musquared           Flag to add negative prizes to hub nodes proportional
                        to their degree^2, rather than degree. Must specify a
                        positive mu in conf file.
  --excludeTerms        Flag to exclude terminals when calculating negative
                        prizes. Use if you want terminals to keep exact
                        assigned prize regardless of degree.
  --msgpath=MSGPATH     Full path to the message passing code. Default =
                        ""<current directory>/msgsteiner""
  --outpath=OUTPUTPATH  Path to the directory which will hold the output
                        files. Default = this directory
  --outlabel=OUTPUTLABEL
                        A string to put at the beginning of the names of files
                        output by the program. Default = ""result""
  --cyto30              Use this flag if you want the output files to be
                        amenable with Cytoscape v3.0 (this is the default).
  --cyto28              Use this flag if you want the output files to be
                        amenable with Cytoscape v2.8, rather than v3.0.
  --noisyEdges=NOISENUM
                        An integer specifying how many times you would like to
                        add noise to the given edge values and re-run the
                        algorithm. Results of these runs will be merged
                        together and written in files with the word
                        ""_noisyEdges_"" added to their names. The noise level
                        can be controlled using the configuration file.
                        Default = 0
  --shuffledPrizes=SHUFFLENUM
                        An integer specifying how many times you would like to
                        shuffle around the given prizes and re-run the
                        algorithm. Results of these runs will be merged
                        together and written in files with the word
                        ""_shuffledPrizes_"" added to their names. Default = 0
  --randomTerminals=TERMNUM
                        An integer specifying how many times you would like to
                        apply your given prizes to random nodes in the
                        interactome (with a similar degree distribution) and
                        re-run the algorithm. Results of these runs will be
                        merged together and written in files with the word
                        ""_randomTerminals_"" added to their names. Default = 0
  --knockout=KNOCKOUT   A list specifying protein(s) you would like to ""knock
                        out"" of the interactome to simulate a knockout
                        experiment, i.e. ['TP53'] or ['TP53', 'EGFR'].
  -k CV, --cv=CV        An integer specifying the k value if you would like to
                        run k-fold cross validation on the prize proteins.
                        Default = None.
  --cv-reps=CV_REPS     An integer specifying how many runs of cross-
                        validation you would like to run. To use this option,
                        you must also specify a -k or --cv parameter. Default
                        = None.
  -s SEED, --seed=SEED  An integer seed for the pseudo-random number
                        generators. If you want to reproduce exact results,
                        supply the same seed. Default = None.

```

### Forest input files and parameters

#### Required inputs

The first two options (`-p` and `-e`) are required. You should record your
terminal nodes and prize values in a text file. The file
`example/a549/Tgfb_phos.txt` is an example of what this file should look like.
You should record your interactome and edge weights in a text file with 3 or 4
columns. The file `data/iref_mitab_miscore_2013_08_12_interactome.txt` is a
human interactome example (this interactome comes from iRefIndex v13, scored and
formatted for our code).

A sample configuration file, `a549/tgfb_forest.cfg` is supplied. The user can
change the values included in this file or can supply their own
similarly formatted file. Unlike Garnet, the Forest configuration file name must
be preceded with `-c` or `--conf=`.
If the `-c` argument is not included in the command line
the program will attempt to read the default `conf.txt`. The parameters `w`, `b`, and `D`
must be set in the configuration file. Optional parameters `mu`, `garnetBeta`, `noise`,
`g`, and `r` may also be included.  The `processes` and `threads` parameters
both provide parallelization.  By default, Forest parallelizes tasks
by running each network optimization task (e.g. for a different set of shuffled
prizes or edge noise values) in a different, single-threaded process.  If
you are not running Forest multiple times with cross validiation, shuffled
prizes, or noisy edges, you may set `processes = 1` and `threads` to the
number of processors on your computer to run msgsteiner in a multi-threaded
manner.


```
w = float, controls the number of trees
b = float, controls the trade-off between including more
    terminals and using less reliable edges
D = int, controls the maximum path-length from v0 to terminal nodes
mu = float, controls the degree-based negative prizes (defualt 0.0)
garnetBeta = float, scales the garnet output prizes relative to the
             provided protein prizes (default 0.01)
noise = float, controls the standard deviation of the Gaussian edge
        noise when the --noisyEdges option is used (default 0.333)
g = float, msgsteiner reinforcement parameter that affects the convergence of the
    solution and runtime, with larger values leading to faster convergence
    but suboptimal results (default 0.001)
r = float, msgsteiner parameter that adds random noise to edges,
    which is rarely needed because the Forest --noisyEdges option
    is recommended instead (default 0)
processes = int, number of processes to spawn when doing randomization runs
            (default to number of processors on your computer)
threads = int, number of threads to use during msgsteiner optimization
            (default 1)
```

For more details about the parameters, see our publication.


#### Optional inputs

The rest of the command line options are optional.

If you have run the garnet module to create scores for transcription factors,
you can include that output file with the `--garnet` option and use `garnetBeta` in the
configuration file to scale the garnet scores.

The `--dummyMode` option will change which nodes in the terminal are connected
to the dummy node in the interactome. We provide an example of this using
`a549/Tgfb_interactors.txt`. For an explanation of the dummy node, see
publication.

The `--musquared` option will apply negative prizes to nodes based on their
squared degree, as opposed to linear degree. This is helpful if the default
mu behavior is not strict enough to eliminate irrelevant hub nodes from your
network.

If the file `msgsteiner` is not in the same directory as
forest.py, the path needs to be specified using the `--msgpath` option, e.g.,
'--msgpath /home/msgsteiner-1.3/msgsteiner'.

If you would like the output files to be stored in a directory other than the
one you are running the code from, you can specify this directory with the
`--outpath` option. The names of the output files will all start with the word
`result` unless you specify another word or phrase, such as an identifying label
for this experiment or run, with the `--outlabel option`. The `--cyto30` and
`--cyto28` tags can be used to specify which version of Cytoscape you would like
the output files to be compatiable with.

We include three options, `--noisyEdges`, `--shuffledPrizes`, and
`--randomTerminals` to determine how robust your results are by comparing them
to results with slightly altered input values. To use these options, supply a
number for either parameter greater than 0. If the number you give is more than
1, it will alter values and run the program that number of times and merge the
results together. The program will add Gaussian noise to the edge values you
gave in the `-e` option, or shuffle the prizes around all the network proteins
in the `-p` option, or assign the prizes to network proteins with similar
degrees as your original terminals, according to which option you use. In
`--noisyEdges`, Gaussian noise with mean 0 and standard deviation specified by
the parameter `noise` in the configuration file (default 0.333) will be added
to the edge scores. The results from these runs will be stored in seperate files
from the results of the run with the original prize or edge values, and both
will be outputted by the program to the same directory.

The knockout option can be used if you would like to simulate a knockout
experiment by removing a node from your interactome. Specify your knockout
proteins in a list, i.e. ['TP53'] or ['TP53', 'EGFR'].

The `-k` and `--cv` options can be used if you would like to run k-fold cross
validation. This will partition the proteins with prizes into k equal
subsamples. It will run msgsteiner k times, leaving one subsample of prizes out
each time. The `--cv-reps` option can be used if you would like to run k-fold
cross validation multiple times, each time with a different random partitioning
of terminals. If you do not supply `--cv-reps` but do provide a k, cross
validation will be run once. Each time it is run, a file called
`<outputlabel>_cvResults_<rep>.txt` will be created. For each of the k
iterations, it will display the number of terminals held out of the prizes
dictionary, the number of those that were recovered in the optimal network as
Steiner nodes, and the total number of Steiner nodes in the optimal network.

The `-s` option will supply a seed option to the pseudo-random number generators
used in noisyPrizes, shuffledPrizes, randomTerminals, and the optimization in
msgsteiner itself. If you want to reproduce exact results, you should supply the
same seed every time. If you do not supply your own seed, system time is used a
seed.

### Running forest

Once you submit your command to the command line the program will run. It will
display messages as it completes, letting you know where in the process you are.
If there is a warning or an error it will be displayed on the command line. If
the run completes successfully, several files will be created. These files can
be imported into Cytoscape v.3.0 to view the results of the run.  These files
will be named first with the outputlabel that you provided (or `result` by
default), and then with a phrase identifying which file type it is.

### Forest output

- **info.txt** contains information about the algorithm run, including any error
  messages if there were any during the run.

- **optimalForest.sif** contains the optimal network output of the
  message-passing algorithm (without the dummy node). It is a Simple Interaction
Format file. To see the network, open Cytoscape, and click on File > Import >
Network > File..., and then select this file to open. Click OK.

- **augmentedForest.sif** is the same thing, only it includes all the edges in
  the interactome that exist between nodes in the optimal Forest, even those
edges not chosen by the algorithm. Betweenness centrality for all nodes was
calculated with this network.

- **dummyForest.sif** is the same as optimalForest.sif, only it includes the
dummy node and all edges connecting to it.

- **edgeattributes.tsv** is a tab-seperated value file containing information
  for each edge in the network, such as the weight in the interactome, and the
fraction of optimal networks this edge was contained in. To
import this information into Cytoscape, first import the network .sif file you
would like to view, and then click on File > Import > Table > File..., and
select this file. Specify that this file contains edge attributes, rather than
node attributes, and that the first row of the file should be interpreted as
column labels. Click OK.

- **nodeattributes.tsv** is a tab-seperated value file containing information
  for each node in the network, such as the prize you assigned to it and
betweenness centrality in the augmented network. To import this information into
Cytoscape, first import the network .sif file you would like to view, and then
click on File > Import > Table > File..., and select this file. Specify that
this file contains node attributes, rather than edge attributes, and that the
first row of the file should be interpreted as column labels. Click OK.

When the network and the attributes are imported into Cytoscape, you can alter
the appearance of the network as you usually would using VizMapper.

Testing
-----------------
See the `tests` directory for instructions on testing Omics Integrator.

Third Party Code
-----------------
See the 'LICENSE-3RD-PARTY' file for license information for:
[python-avl-tree by Pavel Grafov](https://github.com/pgrafov/python-avl-tree)
",0,0.51,0.51,,,,,,0,12,,,
151727361,MDEwOlJlcG9zaXRvcnkxNTE3MjczNjE=,Accessing_public_genomic_data,hbctraining/Accessing_public_genomic_data,0,hbctraining,https://github.com/hbctraining/Accessing_public_genomic_data,Tutorials on accessing public reference and genomic data,0,2018-10-05 13:47:30+00:00,2025-02-18 19:31:03+00:00,2025-02-18 19:30:59+00:00,https://hbctraining.github.io/Accessing_public_genomic_data,7900,31,31,HTML,1,1,1,1,1,0,16,0,0,2,,1,0,0,public,16,2,31,master,1,"['rkhetani', 'marypiper', 'mistrm82']",1,"## Accessing genomic reference and experimental sequencing data


| Audience | Computational skills required | Duration |
:----------|:---------------|:----------|
| Biologists | [Shell for Bioinformatics](https://hbctraining.github.io/Shell-for-bioinformatics/) | 3 hour workshop (~3 hours of trainer-led time)|


### Description

This repository has teaching materials for a **3 hour**, hands-on **Accessing genomic reference and experimental sequencing data** workshop led at a relaxed pace. 

For many types of sequencing analyses, we need access to public data stored in various databases and repositories. This workshop will discuss types of genomic reference data available through public databases such as Ensembl, NCBI, and UCSC, and step through how to find and download this data. The workshop will also explore how to find and download publicly available experimental data, such as data (FASTQ files and count matrices) from published papers, using GEO and the SRA repositories. While most of the workshop will access data using a web browser, downloading data from the SRA will require beginner knowledge of the command-line interface. 

### Learning Objectives

* Understanding what is a genome build
* Identifying differences in reference data available from Ensembl, NCBI, and UCSC
* Finding and downloading experiment-appropriate genome reference data
* Finding and downloading publically available experimental sequence data

> These materials are developed for a trainer-led workshop, but also amenable to self-guided learning.

### Lessons
- **[Click here](schedules/schedule_o2.md) for links to lessons and the suggested schedule using the _[HMS-RC](https://rc.hms.harvard.edu) O2 cluster_**
- **[Click here](schedules/schedule_odyssey.md) for links to lessons and the suggested schedule using the _[FAS-RC](https://www.rc.fas.harvard.edu) Odyssey cluster_**

### Dataset

We will be demonstrating how to access the data in the lessons.

### Installation Requirements

***Mac users:***
No installation requirements.

***Windows users:***
[GitBash](https://git-scm.com/download/win)

***
*These materials have been developed by members of the teaching team at the [Harvard Chan Bioinformatics Core (HBC)](http://bioinformatics.sph.harvard.edu/). These are open access materials distributed under the terms of the [Creative Commons Attribution license](https://creativecommons.org/licenses/by/4.0/) (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.*

",0,0.59,0.59,,,,,,0,4,,,
405227146,MDEwOlJlcG9zaXRvcnk0MDUyMjcxNDY=,corbett-lab.github.io,corbett-lab/corbett-lab.github.io,0,corbett-lab,https://github.com/corbett-lab/corbett-lab.github.io,,0,2021-09-10 22:26:18+00:00,2024-12-20 22:48:50+00:00,2024-12-20 22:48:46+00:00,,155412,4,4,HTML,1,1,1,1,1,0,11,0,0,0,mit,1,0,0,public,11,0,4,main,1,"['russcd', 'genicos', 'amkram', 'AlanZhangUCSC', 'jmcbroome', 'cademirch', 'abschneider', 'maxgenetti', 'bpt26', 'maracbaylis', 'gabepen', 'aanakamo', 'erikenbody', 'lilymaryam', 'bdesanctis', 'maxgenettiUCSC', 'ccondon894', 'nick33333']",1,"# corbett-detig-lab-website

![science](https://img.shields.io/badge/Corbett-Lab-brightgreen)

This is the official website of the Corbett-Detig lab at UCSC. 

## What it does?

Evolutionary Genomics

## Quick start

```yml
email: rucorbet@ucsc.edu
```

## Features

![IMG_5339](secret/newlab.jpg)


| name                    | default value          | google scholar    |  dog   |
| ----------------------- | ---------------------- | ----------------- |---------------|
| [`Russell Corbett-Detig`](/People/Current/russ/) | Principal Investigator |[Russ' Publications](https://scholar.google.com/citations?user=9sF4nOkAAAAJ&hl=en)| [Maeby](https://user-images.githubusercontent.com/10063921/132934537-0feab719-b8ec-4ec9-b300-68086de61ffa.png) |
| [`Erik Enbody`](/People/Current/erik/)           | Postdoctoral Researcher|[Erik's Publications](https://scholar.google.com/citations?user=3bBANnkAAAAJ&hl=en) | [Hallie](/assets/images/hallie.jpg) |
| [`Maximilian Genetti`](/People/Current/max/)    | Grad student           | [Max's Publications](https://pubmed.ncbi.nlm.nih.gov/?term=genetti+max%5Bau%5D&sort=pubdate) | [Lupita](https://user-images.githubusercontent.com/43384843/134749391-ee3e5a0c-5ae4-42f6-ac78-5d894d1e948c.jpeg)
| [`Alex Kramer`](/People/Current/alex/)           | Grad student           | [Alex's Publications](https://scholar.google.com/citations?hl=en&user=d8U1u-wAAAAJ)|
| [`Cade Mirchandani`](/People/Current/cade/)      | Grad student               |[Cade's Publications](https://scholar.google.com/citations?hl=en&user=9lQjfcEAAAAJ)                   | [Layla](https://user-images.githubusercontent.com/88911118/132076266-5dabc9a1-0893-4567-b528-29e324b3319b.png) |
| [`Nicolas Ayala`](/People/Current/nico/)         | Nico              |[Nico's Publications](https://scholar.google.com/citations?user=Q2nj73IAAAAJ&hl=en)| [Nova](https://user-images.githubusercontent.com/88911118/132076284-2b07b952-0f40-470a-ab0f-78611f94ab9f.jpg) |
| [`Gabriel Penunuri`](/People/Current/gabe/)       | Grad student |    |
| [`Lily Karim`](/People/Current/lily/)           | Grad student              |                   | |
| [`Jodie Jacobs`](/People/Current/jodie/)           | Friend of the lab              |                   | |
| [`Chris Condon`](/People/Current/chris/)           | Grad student              | [Chris' Publications](https://scholar.google.com/citations?user=NFpd0PsAAAAJ&hl=en&oi=ao) | |
| [`Shelbi Russell`](https://russellsymbiosislab.engineering.ucsc.edu/)     | Friend of the lab   |  | |
| [`Alan Zhang`](People/Current/alan/) | Grad student | | [Luna](People/Current/alan/luna.jpg) |
| [`Anne Nakamoto`](People/Current/anne/) | Grad student | [Anne's Publications](https://scholar.google.com/citations?user=d3Ndg64AAAAJ&hl=en&oi=ao) | |
| [`Bianca De Sanctis`](People/Current/bianca/) | Postdoc | [Bianca's Publications](https://scholar.google.com/citations?user=IvpdXzEAAAAJ&hl=en) | [Chloe](https://github.com/corbett-lab/corbett-lab.github.io/assets/15896948/c72bf3ab-d5b0-41af-9e62-6fd200c4efad) |
| [`Pratik Katte`](People/Current/pratik/) | Grad student | | |
| [`Justin Cullen`](People/Current/justin/) | Undergrad student | | |
| [`Cyrus Park`](People/Current/cyrus/) | Undergrad student | | |


## Social Media
  
[![Twitter](https://user-images.githubusercontent.com/10063921/136299975-547fe4c5-94b0-49cc-b66d-cbe1f5c12913.png ""Russ' Twitter"")](https://twitter.com/RussCorbett)  [![Instagram](https://user-images.githubusercontent.com/88911118/134778849-c25417c8-ee14-40d5-a3d0-a03e9d759c08.png ""Lab Instagram"")](https://www.instagram.com/corbettdetiglab/)

",1,0.76,0.76,,,,,,0,5,,,
137790168,MDEwOlJlcG9zaXRvcnkxMzc3OTAxNjg=,MutEnricher,asoltis/MutEnricher,0,asoltis,https://github.com/asoltis/MutEnricher,Somatic coding and non-coding mutation enrichment analysis for tumor WGS data,0,2018-06-18 18:21:04+00:00,2024-11-12 19:03:03+00:00,2021-06-16 02:06:41+00:00,,31350,10,10,Python,1,1,1,1,0,0,3,0,0,1,other,1,0,0,public,3,1,10,master,1,['asoltis'],,"# MutEnricher #
----------------

<img src='images/MutEnricher_Fig1_20190422_for_GitHub.png'>

Author: Anthony R. Soltis (anthony.soltis.ctr@usuhs.edu, anthonyrsoltis@gmail.com)

Institution: Uniformed Services University of the Health Sciences, Bethesda, MD

License: MIT License, see [License](https://github.com/asoltis/MutEnricher/blob/master/LICENSE.txt)

Version: 1.3.3

Introduction:
---------------

MutEnricher is a flexible toolset that performs somatic mutation enrichment analysis of both 
protein-coding and non-coding genomic loci from whole genome sequencing (WGS) data, implemented
in Python and **usable with Python 2 and 3.** 

**MutEnricher is now also available as a** [Docker image](https://hub.docker.com/r/asoltis/mutenricher).

MutEnricher contains two distinct modules:
1. coding - for performing somatic enrichment analysis of non-silent variation in protein-coding genes
2. noncoding - for performing enrichment analysis of non-coding regions

The main driver script is mutEnricher.py and each tool can be evoked from here, i.e.:
1. python mutEnricher coding ...
2. python mutEnricher noncoding ...

See help pages and associated documentation for methodological and run details. 

Citation:
---------
A [MutEnricher manuscript](https://rdcu.be/b51ka) is now published in BMC Bioinformatics. Please cite if using this software:

Soltis, A.R., Dalgard, C.L., Pollard, H.B., & Wilkerson, M.D. MutEnricher: a flexible toolset for somatic mutation enrichment analysis of tumor whole genomes. BMC Bioinformatics (2020). 20(1).

Info and User Guides:
---------------------

[Wiki](https://github.com/asoltis/MutEnricher/wiki)

[Quickstart guide](https://github.com/asoltis/MutEnricher/wiki/Quickstart-guide)

[Tutorial](https://github.com/asoltis/MutEnricher/wiki/Tutorial)

[Output file descriptions](https://github.com/asoltis/MutEnricher/wiki/Output-file-descriptions)

Installation:
---------------

See [Installation Guide](https://github.com/asoltis/MutEnricher/wiki/Installation-Guide) section on Wiki.

Additional utilities
----------------------

In the ""utilities"" sub-directory, we include two helper functions for generating covariate files for use with MutEnricher's 
covariate clustering functions:

    1. get_gene_covariates.py  
    2. get_region_covariates.py

See the help pages for example usage. (1) above requires GTF input (as for the coding module) and (2) requires and input BED (as for 
the noncoding module). Both also require a copy of an indexed genome FASTA file (e.g. for hg19/hg38 human genomes) as input.

Example data
--------------

We include various example files for testing MutEnricher on synthetic somatic data. See the ""example_data"" sub-folder. 

Several quickstart commands are provided in example_data/quickstart_commands.txt file. A sample quickstart command for coding analysis:

```
cd example_data
python ../mutEnricher.py coding annotation_files/ucsc.refFlat.20170829.no_chrMY.gtf.gz vcf_files.txt --anno-type nonsilent_terms.txt -o test_out_coding --prefix test_global
```

Files/folders contained in example_data:
    
1. example_data/annotation_files

    Contains example GTF and BED files for running MutEnricher's coding and noncoding modules. 
    - ucsc.refFlat.20170829.no_chrMY.gtf.gz
    - ucsc.refFlat.20170829.promoters_up2kb_downUTR.no_chrMY.bed

    NOTE: Input GTF (coding analysis) and BED files (noncoding analysis) can be gzip compressed or not. 

2. example_data/covariates

    Contains example covariate and covariate weights files for running the covariate clustering background method:

    For coding:
    - ucsc.refFlat.20170829.no_chrMY.covariates.txt
    - ucsc.refFlat.20170829.no_chrMY.covariate_weights.txt
    
    For noncoding:
    - ucsc.refFlat.20170829.promoters_up1kb_down200.no_chrMY.covariates.txt
    - ucsc.refFlat.20170829.promoters_up1kb_down200.no_chrMY.covariate_weights.txt

3. nonsilent_terms.txt

    Example non-silent terms file for use with coding module. This example is applicable to VCFs annotated with ANNOVAR refGene models
    (the sample VCFs are annotated in this way). Use with the --anno-type option in the coding module.

    NOTE: These same terms will be used if ""annovar"" is passed to the --anno-type option. 

4. precomputed_apcluster

    This folder provides pre-computed affinity propagation results for the datasets in (1) and (2) above. These directories can be
    supplied to MutEnricher via the --precomputed-covars option. 

    For coding (all genes):
    - coding.ucsc.refFlat.20170829.no_chrMY/all_genes
    
    For noncoding:
    - noncoding.ucsc.refFlat.20170829.promoters_up2kb_downUTR.no_chrMY/apcluster_regions

5. quickstart_commands.txt
    
    Sample execution commands (associated with quickstart guide).

6. vcf_files.txt

    Sample VCF input files list file. This file contains local paths and assumes working directory is ""example_data"" sub-directory.

7. vcfs

    Sub-directory containing 100 synthetic somatic VCF files (compressed with index .tbi files). These files were generated by randomly
    inserting ""somatic mutations"" at positions in the hg19 genome at a target rate of ~2 mutations/Mb. Three true positive cases 
    are included, two coding and one non-coding, whereby non-silent mutations were inserted into the TP53 and KRAS genes and somatic
    mutations were inserted into the TERT gene promoter region. 

# Change log #
---------------
06-15-2021
----------
- Version 1.3.3
- Updates:
    - Include VEP annotation parsing capabilities (via ""CSQ"" field) in coding module. 
    - Included missing function in coding analysis code to parse blacklist variant input file.

05-11-2021
----------
- Version 1.3.2
- Updates:
    - Included SnpEff annotation parsing capabilities (via ""ANN"" INFO field) in coding module. Set --anno-type options to 'SnpEff' to 
      use pre-set annotations compatible with this tool.
    - Improved error handling for interval files and regions in covariate utility scripts.

10-01-2020
----------
- Version 1.3.1
- Bug fix:
    - Update to coding module and gene covariate code to address incomplete merging of overlapping gene feature intervals (exons, CDS).

06-10-2020
----------
- Dockerfile added for creation of Docker image.
- No code updates.

10-23-2019
----------
- Version 1.3.0
- Major updates:
    - 'nsamples' (binomial testing method) is now default statistical testing (--stat-type) option.
    - Combined covariate clustering plus local background rate method implemented. When covariates are supplied and --use-local is also set,
      programs compute local backgrounds around features part of clusters during background calculations. 

10-10-2019
----------
- Version 1.2.1
- Minor update to local background method, whereby minimum search window is increased to 1 Mb.

09-13-2019
----------
- Version 1.2.0
- Major updates:
    - Code updated for compatibility with Python 3.
    - Included --stat-type option to select between original negative binomial test based on mutation counts (nmutations, default) or
      binomial test on number of mutated samples (nsamples).
- Minor updates:
    - Updated --anno-type preset options to better reflect various ANNOVAR gene annotations. 
    - Deprecated --repliseq-fns option in utilities code and updated to -i/--interval-files option

03-25-2019
----------
- Version 1.1.3
- Updates:
    - Noncoding code now produces <prefix>_region_WAP_hotspot_Fisher_enrichments.txt output file, which includes an overall combined
      Fisher's combined p-value for the overall region, WAP, and hotspot (if present) p-values.

02-12-2019
----------
- Version 1.1.2
- Updates:
    - In both coding and noncoding modules, new option --min-hs-samps included for setting minimum number of samples that must contain 
      mutations in a candidate hotspot region for subsequent testing. Default is set to 2; setting to 1 is equivalent to prior default 
      behavior. 

01-15-2019
-----------
- Version 1.1.1
- Updates/bug fixes:
    - Coding analysis code now produces output file with combined Fisher p-value for overall gene and hotspot(s) enrichments.
    - Updated method used to compute Fisher p-values for better numerical accuracy.
    - utilities/get_gene_covariates.py updated to read gzipped GTF files.
    - Fixed minor bug in coding analysis code associated with local background rate calculation method.
    - Updated coding analysis code to calculate gene background mutation rate from samples possessing at least one non-silent mutation.

06-15-2018 
-----------
- Initial release; The development of this Software was sponsored by the Uniformed Services University of the Health Sciences (USU); however, the information or content and conclusions do not necessarily represent the official position or policy of, nor should any official endorsement be inferred on the part of, USU, the Department of Defense, or the U.S. Government. 

",0,0.48,0.48,,,,,,0,0,,,
597546587,R_kgDOI53WWw,CruzHacksBeats,ahzengyang/CruzHacksBeats,0,ahzengyang,https://github.com/ahzengyang/CruzHacksBeats,,0,2023-02-04 21:46:26+00:00,2023-02-04 21:46:26+00:00,2023-02-05 11:32:29+00:00,,5,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,"['Fenchpress', 'ahzengyang']",,"# CruzHacksBeats
https://www.youtube.com/watch?v=tuIrd0GOjcs
## Inspiration
There was a time in Bryan Fenchel's life that he spent playing, composing, and producing music every waking moment. Bryan believed that someday some of this sonic brilliance would reach someone’s ears and It would have a profound impact.  Everyone wants to be heard; this is one thing we all have in common.  But when do we listen?  We listen when we want to connect with others.  When we are curious and seeking to connect with the world that we are all a part of.  Wouldn’t it be wonderful if small artists were able to connect with audiences all over the world in meaningful ways and reach beyond the screen to dance into the real world?

## What it does
This juxtaposition and synchronization of the real and virtual world creates strings that interweave one another, surpassing geographical obstacles. With the help of Niantic Lightship Visual Positioning System(VPS), we are building the foundations of a digital music wonderland; our first contribution to the real world meta-verse. The platform we will have started to build together, here at CruzHacks 2023, will allow users to step “through the looking glass” and experience a world of wonder together.

By tossing a virtual rose, a user can show their support triggering an audio visual augmented reality(AR) performance of Renee Harmoni, a rising star and artist on the label of my close friend and business partner, Donyea Gooding(Starchild Yeezo). We are grateful to have been given permission to use her unreleased song for our hack.  Though she is in Los Angeles, she is here today virtually on the Stevenson Stage alongside the beautiful grand piano.
https://www.tiktok.com/@officialreneeharmoni?_t=8YXL2GB3jeo&_r=1

## How we built it
We came together as a team, a joint effort of 2 CS students at USF and 2 students at UC Santa Cruz. We fleshed out the details and troubleshooted together, which resulted in a product much greater than which any of us had pictured.  Teamwork makes the dreamwork.  We started by attending the Niantic Lightship workshops to get a better understanding of what and who we would have the pleasure of working with.

## Challenges we ran into
Though it was difficult, the challenge was a well welcome pleasure to all of us. Though we encountered hindrances we were lucky and grateful to have the assistance of Niantic representatives to help us cross the finish line.

## Accomplishments that we're proud of
We are very proud of our team member Renier’s work on implementing the physics of the throwing of roses to make the toss more natural and varied.  It was a much more rigorous task than anticipated.  

## What we learned
It was challenging to learn new tools and languages that we all were not familiar with such as Niantic Lightship VPS, C#, Unity.  We also learned the importance of both strong leadership and the power of teamwork.

## What's next for Find The Beat
We are focused on the needs of creators and their audiences.  If we build something that can support multiple live AR performances in multiple locations, while supporting the exchange of NFT’s, and enabling artists to engage with fans in meaningful individualized, memorable ways unique to the AR/VR live performance experience; that would be something truly special. We are currently developing AI tools to enable any electronic musician to live stream a performance and instantly transform streaming live audio into a multi media interactive immersive live performance. What if these AI driven AR performances were linked to virtual performance locations in order to also support a VR audience? 
We aim to serve creators by giving them a safe place to perform and share there ideas and experiences with an audience of infinite size and scale in real and virtual places, limited only by the creators imagination and enhanced by the capability, opportunity and access of Web 3.0 technologies: An end to end Web 3 virtual performance solution built for everyone. 
We seek to foster meaningful connections with Niantic and continue to contribute to the real-world meta-verse.  
",1,0.73,0.73,,,,,,0,1,,,
407380575,MDEwOlJlcG9zaXRvcnk0MDczODA1NzU=,PoxiiPro,PoxiiPro/PoxiiPro,0,PoxiiPro,https://github.com/PoxiiPro/PoxiiPro,Config files for my GitHub profile.,0,2021-09-17 02:37:41+00:00,2023-08-10 02:39:15+00:00,2023-08-10 02:39:11+00:00,https://github.com/PoxiiPro,1,0,0,,0,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['PoxiiPro'],,"I am a University of California Santa Cruz computer science graduate with programming experience in artificial intelligence, machine learning, natural language processing, mobile apps, and full stack web apps looking for opportunities.

<!---
PoxiiPro/PoxiiPro is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
",1,0.73,0.73,,,,,,0,1,,,
394616856,MDEwOlJlcG9zaXRvcnkzOTQ2MTY4NTY=,UCSC_tracks,AiweiWu/UCSC_tracks,0,AiweiWu,https://github.com/AiweiWu/UCSC_tracks,,0,2021-08-10 10:48:31+00:00,2021-08-10 12:08:09+00:00,2021-08-10 12:08:07+00:00,,3,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['AiweiWu'],,"UCSC tracks
",0,0.79,0.79,,,,,,0,1,,,
4190105,MDEwOlJlcG9zaXRvcnk0MTkwMTA1,chromozoom,rothlab/chromozoom,0,rothlab,https://github.com/rothlab/chromozoom,"ChromoZoom is a fast, fluid web-based genome browser",0,2012-05-01 06:18:01+00:00,2024-12-09 12:38:12+00:00,2021-03-15 03:50:06+00:00,http://chromozoom.org,12490,46,46,JavaScript,1,1,1,1,0,0,5,0,0,5,agpl-3.0,1,0,0,public,5,5,46,master,1,"['powerpak', 'miha-skalic']",1,"# ChromoZoom

The goal of ChromoZoom is to make genome browsing online as effortless as navigating the world on Google Maps, while retaining superior data density and customizability, modeled off of the capabilities of [UCSC genome browser](http://genome.ucsc.edu/) and [IGV](http://software.broadinstitute.org/software/igv/).

All data is drawn directly in the browser using [canvas][] and [SVG][], similar to the approach of [igv.js][] and [pileup.js][]. There's a few substantial differences though:

- We placed a premium on fast navigation. You can zoom with the mousewheel and ""throw"" the display, just like Google Maps.
- You don't need to install software to a server or embed code into a webpage to use ChromoZoom. [Simply visit chromozoom.org](http://chromozoom.org), which is designed as a first-class genome browsing experience for nearly all of UCSC's tracks and genomes.
- It's easy to create and load custom genomes using the [IGB Quickload format][igbql].

[canvas]: http://en.wikipedia.org/wiki/Canvas_element
[SVG]: http://en.wikipedia.org/wiki/Scalable_Vector_Graphics
[pileup.js]: https://github.com/hammerlab/pileup.js/
[igv.js]: https://github.com/igvteam/igv.js
[bbbw]: https://www.ncbi.nlm.nih.gov/pubmed/20639541

## License

ChromoZoom is free for academic, nonprofit, and personal use.  The source code is licensed under the [GNU Affero General Public License v3](http://www.gnu.org/licenses/agpl-3.0.html).  In a nutshell, this license means that you are free to copy, redistribute, and modify the source code, but you are expected to provide the source for any code derived from ChromoZoom to anybody that receives the modified code or uses it over a computer network (e.g. as a web application).  ChromoZoom is not free for commercial use.  For commercial licensing, please contact the [Roth laboratory](http://llama.mshri.on.ca).

## Requirements

To host ChromoZoom or run the UCSC track scraper, you need either macOS or Linux. For Windows users, we suggest 
[usage of our virtual environment](#running-in-virtual-environment).

The web interface should work in any recent version of a modern HTML5-capable web browser (Chrome, Firefox, Safari, IE ≥11).

### To serve the ChromoZoom web interface

Out of the box, ChromoZoom is serves a web interface that can display data on top of genome layouts crossloaded from UCSC, or data in [IGB Quickload directories][igbql]. You will need:

- PHP 5.x + Apache (or another webserver that can run PHP scripts)
    - Note that [magic quotes][16] must be **disabled**.
- [libcurl bindings for PHP][10] (on macOS, this is included in the default PHP install)
- To support any of the binary track and genome formats, you will need the following on your `$PATH`, which during setup will be symlinked into a new directory in this repo called `bin/`:
    - [`tabix`][11], a generic indexer for TAB-delimited genome position files
    - [`samtools`][11], utilities for viewing for the Sequence Alignment/Map (SAM) and BAM (Binary SAM) formats
    - The following [Jim Kent binaries][12]:
        - `bigBedInfo`
        - `bigBedSummary`
        - `bigBedToBed`
        - `bigWigSummary`
        - `bigWigInfo`
        - `twoBitToFa`

Place a checkout of this repo somewhere in your webserver's DOCROOT.  To setup the aforementioned symlinks to binaries, run `rake check` from the command line at the root of the repo.  Files under `php/` and `index.php` will need to be executable by the webserver.  Access `index.php` from a web browser to view the ChromoZoom interface.

**Note:** To support HTTPS URLs for VCF/tabix or BAM files, you will need to compile `tabix` and `samtools` with `libcurl` support. See [below](#https-support-for-samtools) for details.

[10]: http://php.net/manual/en/book.curl.php
[11]: http://www.htslib.org/download/
[12]: http://hgdownload.cse.ucsc.edu/admin/exe/
[16]: http://php.net/manual/en/security.magicquotes.disabling.php
[igbql]: https://wiki.transvar.org/display/igbman/Sharing+data+using+QuickLoad+sites

### To scrape data from UCSC

We provide a pipeline to convert data from genomes hosted at UCSC into highly efficient [binary formats][bbbw] that make it simple to serve thousands of annotation tracks from flatfiles. This is the strategy used for [chromozoom.org](http://chromozoom.org).

The script is at `UCSC_tracks/get_tracks.py`. See the [README.md](https://github.com/rothlab/chromozoom/tree/master/UCSC_tracks) in that directory for full instructions on how to run the track scraper. You can target the scraper to specific UCSC genome assemblies using the `--org_prefix` switch.

### Running in virtual environment

Using virtualization ChromoZoom can run easily from any system. [VirtualBox](http://www.virtualbox.org/wiki/Downloads) and [Vagrant](http://www.vagrantup.com/downloads.html) must be installed. To set up your environment, run the following:

    $ cd path/to/this/repo
    $ vagrant up

Once set up, you can access ChromoZoom at `localhost:8080`.
        
## Development

In addition to the above, you'll need [node.js](https://nodejs.org/) and two [npm](https://www.npmjs.com/) packages:

    $ npm install -g browserify watchify

### Basic setup

    $ git clone https://github.com/rothlab/chromozoom.git
    $ cd chromozoom
    $ rake check

This will tell you if you're missing any of the previously mentioned binaries needed for hosting ChromoZoom or running the UCSC track scraper. You should then serve this directory from Apache + PHP (symlinking into your existing webroot usually works) and access `index.php`.

After making changes to the JavaScript in `js/`, you need to recompile the scripts in `build/`. When developing, use

    $ rake watchify

which will open three screen sessions and continuously recompile debug-friendly versions of the scripts (quit by pressing <kbd>Ctrl</kbd> + <kbd>A</kbd>, then type `:quit` + <kbd>Enter</kbd>.) To compile minified scripts for production, use

    $ rake browserify

which also runs right before you commit code to git, since `rake check` installs a pre-commit hook (see `git-hooks-pre-commit.sh`).

## Recommended Enhancements

None of the following components are strictly necessary for running ChromoZoom—however, they add useful capabilities, such as improved searching and track format support. Both of these upgrades were used for our main instance at [chromozoom.org](http://chromozoom.org).

1. Compiling [`bigBedSearch`][bbs], which allows prefix searching of bigBed fields
2. [HTTPS support](#https-support-for-samtools) for `samtools` and `tabix`

[bbs]: https://github.com/powerpak/bigBedSearch

### Compiling [`bigBedSearch`][bbs]

The [bigBed format][bbbw] can include extra B+ tree indices in the very last section of the file, which ChromoZoom can then use to search for features by the text content of various fields in the uncompressed BED data. e.g., if you want to search a gene track for gene names matching a certain prefix, these indices make such a search practical even if the track itself is large and somewhere else on the web.

I've created a binary that enables these prefix queries, which you can install if you have `gcc` and `make`:

    $ git clone https://github.com/powerpak/bigBedSearch.git
    $ cd bigBedSearch
    $ make

This should produce a `bigBedSearch` executable that you can copy to ChromoZoom's `bin/` directory so the web frontend can use it.

If you want HTTPS to work, either make sure `/usr/include/openssl` is available, or specify the equivalent SSL_DIR as an environment variable.

You can also use that source tree to produce customized versions of `bigBedInfo`, `bigBedSummary`, `bigBedToBed`, `bigWigInfo`, and `bigWigSummary`, if UCSC's binaries weren't compiled in the way you prefer. (e.g., HTTPS doesn't always seem to work in UCSC's macOS binaries.)

### HTTPS support for `samtools`

Current release versions for `samtools` and `tabix` don't support HTTPS, but `libcurl` is being merged into the next planned release so that this is possible. To get these features now, follow these instructions, which are largely cribbed from [this answer on BioStars](https://www.biostars.org/p/147772/), with a major change being that libcurl was already merged into the development branch for htslib.

You'll first need to have `gcc`, `autoconf`, and `zlib`, `libcurl`, `openssl`, and `ncurses` with development headers. On macs, `brew install autoconf` and you should already have the rest if you have Xcode. On most Linux distros, these are all easily found in your respective package manager.

Get the development version of htslib and setup the configure script:

    $ git clone https://github.com/samtools/htslib.git
    $ cd htslib/
    $ autoconf

If the last step fails with something about m4 macros, try being more forceful with `autoreconf --install`. Then configure with libcurl support and compile:

    $ ./configure --enable-libcurl
    $ make

(**Side note.** To get this to compile with a slightly older `libcurl`, such as the moderately ancient version 7.19.7 on [certain high-performance computing nodes](https://hpc.mssm.edu), you may have to remove the case statement about `CURLE_NOT_BUILT_IN` from `hfile_libcurl.c`.)

Once it works, you'll find `tabix` in this directory, along with `htsfile` (which is like `file`, for sequencing formats), both with HTTPS support. Test that it's working with

    $ ./htsfile https://hostname.example.com/path/to/some.bam

All good? Then get the source release for `samtools` 1.2:

    $ cd ..
    $ curl -LO https://github.com/samtools/samtools/releases/download/1.2/samtools-1.2.tar.bz2
    $ tar xzvf samtools-1.2.tar.bz2
    $ cd samtools-1.2

Although this includes htslib 1.2.1, you want to point it to the development version you just installed:

    $ rm -rf htslib-1.2.1
    $ ln -s ../htslib htslib-1.2.1
    $ make LDLIBS+=-lcurl LDLIBS+=-lcrypto

You should find `samtools` in this directory. Test it against some BAM file on an HTTPS server, and if you get back SAM data you're in good shape:

    $ ./samtools view https://hostname.example.com/path/to/some.bam 1:1-10000

(Note that this will spit out a `.bai` file into the current directory, which you can safely delete afterward.)",0,0.73,0.73,,,,,,0,9,,,
62345615,MDEwOlJlcG9zaXRvcnk2MjM0NTYxNQ==,CMPS104A,aidangadberry/CMPS104A,0,aidangadberry,https://github.com/aidangadberry/CMPS104A,UCSC - Fundamentals of Compiler Design I (CS 104A),0,2016-06-30 22:15:43+00:00,2020-06-18 20:12:03+00:00,2016-10-25 23:33:03+00:00,,443,3,3,C++,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,3,master,1,['aidangadberry'],,"# Fundamentals Of Compiler Design I

- Wesley Mackey - Spring 2016
- https://courses.soe.ucsc.edu/courses/cmps104a

An introduction to the basic techniques used in compiler design. Topics include compiler structure, symbol tables, regular expressions and languages, finite automata, lexical analysis, context-free languages, LL(1), recursive descent, LALR(1), and LR(1) parsing; and attribute grammars as a model of syntax-directed translation. Students use compiler building tools to construct a working compiler.
<br/>

- **Aidan Gadberry, agadberr@ucsc.edu**
",1,0.7,0.7,,,,,,0,1,,,
80238546,MDEwOlJlcG9zaXRvcnk4MDIzODU0Ng==,UCSC-Mens-VB-Stats,sportzwiz321/UCSC-Mens-VB-Stats,0,sportzwiz321,https://github.com/sportzwiz321/UCSC-Mens-VB-Stats,"A homemade interactive application that records and calculates statistics for the University of California, Santa Cruz Men's volleyball team in practices and match play",0,2017-01-27 19:24:42+00:00,2017-01-27 19:25:28+00:00,2017-01-27 19:24:46+00:00,,93,0,0,Java,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['sportzwiz321'],,,1,0.78,0.78,,,,,,0,1,,,
229527770,MDEwOlJlcG9zaXRvcnkyMjk1Mjc3NzA=,GhostWriters,nlakshmanan/GhostWriters,0,nlakshmanan,https://github.com/nlakshmanan/GhostWriters,Automated movie script generation,0,2019-12-22 06:29:09+00:00,2023-02-16 17:52:00+00:00,2020-05-02 23:51:50+00:00,,9976,3,3,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,3,master,1,['nlakshmanan'],,"# Automated Script Writing 

## Problem
- Films and shows are produced at high rates and Writer’s block can lead to lack of creativity
- Humans lack consistency and are error-prone, AI is here for help!
  Use AI to generate scripts based on emotions.

## Approach
![Approach](https://github.com/nlakshmanan/GhostWriters/blob/master/images/approach.png)

## Components
- Script Dataset
  - consists of 862 ﬁlm scripts from The Internet Movie Script Database (IMSDb), representing 7,400 characters, with a total of 664,000 lines of dialogue and 9,599,000 tokens (UC Santa Cruz - Baskin Engineering)
- IBM Watson Tone Analyzer
  - used for sentiment analysis to gather dataset of lines based on different emotions
  - limited to 3000 API calls
- nVidia Teslas
  - GPUs used for training large sets of texts, specifically labelled sets from tone analyzer
- OpenAI’s GPT-2 Model
  - text generating AI used to develop new scripts based on user’s input

## Architecture
 ![Architecture](https://github.com/nlakshmanan/GhostWriters/blob/master/images/architecture.png)

## Sample
[Automated Script with sentiment analysis](https://github.com/nlakshmanan/GhostWriters/blob/master/examples/Automated%20Script%20with%20Emotions.pdf)

[Automated Script without sentiment analysis](https://github.com/nlakshmanan/GhostWriters/blob/master/examples/Automated%20Script%20Before%20SA.pdf)

## Next steps
- Refining scripts 
 - removing repeated lines
 - format output text to a strict script structure

 - Maintaining context 
     - more coherent and better flow

 - Dialogues
   - script is mainly scenes and narration currently
",0,0.74,0.74,,,,,,0,1,,,
850429429,R_kgDOMrCF9Q,Demo2024,JavoCL/Demo2024,0,JavoCL,https://github.com/JavoCL/Demo2024,Proyecto Demo UCSC 2024-II,0,2024-08-31 18:39:38+00:00,2024-12-10 18:33:04+00:00,2024-12-10 18:33:00+00:00,,670011,1,1,ShaderLab,1,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,1,main,1,"['JavoCL', 'JavoInnod']",,"# Demo2024
 Proyecto Demo UCSC 2024-II
",0,0.72,0.72,,,,,,0,2,,,
175325418,MDEwOlJlcG9zaXRvcnkxNzUzMjU0MTg=,IMAP,jkimlab/IMAP,0,jkimlab,https://github.com/jkimlab/IMAP,,0,2019-03-13 01:42:37+00:00,2025-02-24 18:14:36+00:00,2021-01-11 06:06:38+00:00,,132555,6,6,Perl,1,1,1,1,0,0,3,0,0,1,,1,0,0,public,3,1,6,master,1,['jkimlab'],,"IMAP
====================
Chromosome-level genome assembler combining multiple de novo assemblies


System requirements (Tested versions)
-------------------
* Linux x64 (Tested in CentOS 7.5, Ubuntu 16.04 and Ubuntu 18.04)
* Perl >= 5.22 or higher
* Python (Python2: 2.4–2.7, and Python3: 3.2 and higher)
* JAVA (build 1.8)
* Perl modules
  - Switch
  - Parallel::ForkManager
  - Bio::TreeIO
  - YAML
  - ExtUtils::PkgConfig
  - GD
  - XML::Parser
  - XML::Parser::PerlSAX
  - XML::DOM
  - XML::DOM::XPath
  - XML::Twig
* GCC (version 4.8)
* BOOST (version 1.46.0)
* libgd (version 2.1.1)
* zlib (version 1.2.8)
* libbz2 (version 1.0.6)
* libncurses (version 6.0)

Installing IMAP from source codes
-------------------
To install IMAP,
 
    1. Download source
      git clone https://github.com/jkimlab/IMAP.git
    
    2. Check & install the required perl libraries
      - Check the required perl libraries
        ./build.pl --check
    
      - Install the required perl libraries
    
    3. Install IMAP package
        ./build.pl --install
        
To uninstall IMAP,

        ./build.pl --uninstall

Installing IMAP using docker
-------------------
To install IMAP,

    1. Install docker (https://docs.docker.com/install/linux/docker-ce/ubuntu)
        curl -fsSL https://get.docker.com/ | sudo sh
        sudo usermod -aG docker $USER         # adding user to the “docker” group
    
    2. Download source
      git clone https://github.com/jkimlab/IMAP.git
      
    3. Build image using Dockerfile 
      - Change to the directory where Dockerfile is located.
        docker build -t [image_name] .
  
    4. Run by docker
      - Run image and create container
        docker run -it [image_name] /bin/bash

Running IMAP with example dataset 
-------------------
* Required approximately 60GB empty disk space
       
       ./build.pl --example
        cd IMAP_EX
        bash CMD

Running IMAP
-------------------
To run IMAP, you need to prepare a parameter file. 

* parameter file

    You can use multiple sequencing read libraries, and one or more outgroup species.  
        
       ############################################################
       ## Sequencing read library information
       ############################################################
       ### Average insert sizes
       ### Standard deviation of insert sizes
       ### Path of read files (.fq(.gz))
       ##### [Paired-end reads] p1, p2
       ##### [Mate-pair reads] m1, m2
       [LIB]
       insertSize        [(integer) insert size]
       insertSizeSD        [(integer) SD of insert size]
       p1        path of forward read          (ex. [path]/read1.1.fq)
       p2        path of reverse read          (ex. [path]/read1.2.fq)
       p1        path of forward read          (ex. [path]/read2.1.fq)
       p2        path of reverse read          (ex. [path]/read2.2.fq)

       [LIB]
       insertSize        [(integer) insert size]
       insertSizeSD        [(integer) SD of insert size]
       m1        path of forward read          (ex. [path]/read3.1.fq.gz)
       m2        path of reverse read          (ex. [path]/read3.2.fq.gz)

       ############################################################
       ## General assembly parameters
       ############################################################
       ## Minimum length of contigs
       MinContigLength        [(integer) minimum length of contigs]
       ## Kmer size for de novo assembly
       Kmer        [(integer) kmer]
       ### Maximum read length for SOAPdenovo2
       MaxReadLength        [(integer) maximum read length]

       ############################################################
       ## RACA & DESCHRAMBLER parameters
       ############################################################
       ## You can use the outrgroup more than one, but the names must be different.
       Reference        [(string) name]        [path of sequence file (.fa)]          (ex. S288C  [path]/S288C.fa)
       Outgroup        [(string) name]        [path of sequence file (.fa)]          (ex. dairenensis [path]/Saccharomyces_dairenensis.fa)
       ### Tree must contain the names of a reference, target(s) and outgroup(s) (newick format)
       TREE        [path of tree (must be in newick format)]          (ex. [path]/tree.nwk)
       ### Synteny resolution
       Resolution        [(integer) synteny resolution]

       ############################################################
       ## Error correction parameters 
       ############################################################
       IterationNumber        [(integer) the number of iteration]


Then, you can use the 'IMAP' perl script.

    Usage:  ./IMAP.pl -t [threads] -p [parameter file] -o [out directory]

    Options:
        --threads|-t <integer> Number of threads (default: 1)
        --params|-p <filename> Parameter file
        --outdir|-o <filename> Output directory (default: ./IMAP_RESULT)
        --help|-h Print usages
        
    Simple examples:
        ./IMAP.pl -t 40 -p param.txt -o ./IMAP_RESULT
         
Included third party tools
-------------------
* BWA (http://bio-bwa.sourceforge.net/)
* LASTZ (http://www.bx.psu.edu/~rsharris/lastz/)
* MaSuRCA (http://www.genome.umd.edu/masurca.html)
* SPAdes (http://bioinf.spbau.ru/spades)
* SOAPdenovo2 (https://github.com/aquaskyline/SOAPdenovo2)
* GapCloser (http://soap.genomics.org.cn/soapdenovo.html)
* RACA (https://github.com/ma-compbio/RACA)
* DESCHRAMBLER (https://github.com/jkimlab/DESCHRAMBLER)
* Pilon (https://github.com/broadinstitute/pilon)
* GATK (https://software.broadinstitute.org/gatk/)
* Picard (https://github.com/broadinstitute/picard)
* SAMtools (http://samtools.sourceforge.net/)
* Kent utilities (http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/)


How to cite
-----------
Song G, Lee J, Kim J, Kang S, Lee H, Kwon D, Lee D, Lang GI, Cherry JM, Kim J. Integrative Meta-Assembly Pipeline (IMAP): Chromosome-level genome assembler combining multiple de novo assemblies. PLoS One. 2019 Aug 27;14(8):e0221858. doi: 10.1371/journal.pone.0221858.


Contact
-------------------  
bioinfolabkr@gmail.com
",0,0.67,0.67,,,,,,0,1,,,
54077659,MDEwOlJlcG9zaXRvcnk1NDA3NzY1OQ==,HTML5,susansico/HTML5,0,susansico,https://github.com/susansico/HTML5,UCSC Extension HTML5 Class Projects.,0,2016-03-17 01:06:02+00:00,2016-03-17 01:09:04+00:00,2016-03-17 01:07:39+00:00,,816,0,0,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['susansico'],,,1,0.66,0.66,,,,,,0,1,,,
245018960,MDEwOlJlcG9zaXRvcnkyNDUwMTg5NjA=,freepdk-45nm,mflowgen/freepdk-45nm,0,mflowgen,https://github.com/mflowgen/freepdk-45nm,ASIC Design Kit for FreePDK45 + Nangate for use with mflowgen,0,2020-03-04 22:40:07+00:00,2025-03-03 07:27:26+00:00,2020-03-08 23:04:41+00:00,,17762,163,163,Verilog,1,1,1,1,0,0,36,0,0,4,,1,0,0,public,36,4,163,master,1,['ctorng'],1,"README
==========================================================================
- Author : Christopher Torng
- Date   : June 7, 2019

This is a 45nm ASIC design kit for mflowgen, a modular ASIC/FPGA flow
generator:

- https://github.com/cornell-brg/mflowgen

This kit uses FreePDK45 and the NanGate Open Cell Library.

- FreePDK45: https://www.eda.ncsu.edu/wiki/FreePDK45:Contents
- NanGate: https://projects.si2.org/openeda.si2.org/help/group_ld.php?group=63}

The FreePDK45 kit is an open-source generic process design kit (PDK)
(i.e., does not correspond to any real process and cannot be fabricated)
that allows researchers and students to experiment with designing in a
modern technology node without signing restrictive non-disclosure
agreements or paying for licenses. The PDK allows you to use commercial
full-custom layout tools (e.g., Cadence Virtuoso) to design both analog
and digital circuits.

The Nangate Open Cell Library is a generic open-source digital
standard-cell library designed using the FreePDK45 kit.

This package was assembled from these kits to be used as an ASIC design
kit for designs built with mflowgen. See more information about mflowgen
at its github repo.

--------------------------------------------------------------------------
Modifications -- 03/08/2020
--------------------------------------------------------------------------

Changes to make Calibre LVS pass more robustly:

- calibre-lvs.rule

We added case sensitivity to the commands:

    SOURCE CASE YES
    LAYOUT CASE YES

The tools sometimes create wires like ""n52"" and ""N52"" in the same
module, and they are meant to be separate nets. If case sensitivity
options are set to ""NO"", this turns into multiple drivers from
Calibre's viewpoint.

This change reduces the number of unexpected LVS issues for most
designs.

--------------------------------------------------------------------------
Modifications -- 03/04/2020
--------------------------------------------------------------------------

Changes to allow a small combination design (GcdUnit) to pass Calibre LVS:

-  adk.tcl
-  rtk-stream-out.map
-  stdcells-bc.lib
-  stdcells-wc.lib
-  stdcells.cdl
-  stdcells.gds
-  stdcells.lef
-  stdcells.lib
-  stdcells.v

New WELLTAP_X1 stdcell added. The rtk-stream-out.map now has layer
numbers that match the Calibre DRC/LVS rule decks and stdcells.gds
layer numbers.

--------------------------------------------------------------------------
Modifications -- 03/04/2020
--------------------------------------------------------------------------

Change:

- rtk-tech.lef: One-line change to set MANUFACTURINGGRID to 0.0025 instead
  of 0.0050

OpenRAM is an open-source memory compiler:

- https://openram.soe.ucsc.edu/

To interact nicely with OpenRAM, we modified the routing technology LEF
(i.e., rtk-tech.lef) such that ""MANUFACTURINGGRID"" is ""0.0025"" instead of
""0.0050"" (a one line change). Otherwise Cadence Innovus complains that the
SRAM LEF has pins that are not on the manufacturing grid.

Since this is a fake technology, we think this is okay.

Actually, the DRC rule deck also says that the grid should be 2.5nm:

    Grid.1 {
    @All shapes must be on a 2.5 nm grid
    OFFGRID active 5 5
    }

--------------------------------------------------------------------------
Modifications -- 12/17/2019
--------------------------------------------------------------------------

Change:

- calibre-lvs.rule: One-line change to set precision to 10000 instead of
  2000

In order to run LVS, the precisions have to match between the LVS rule
file precision and the GDS database precision. Otherwise Calibre says
this:

    ERROR: Rule file precision 2000 is not consistent with database
    precision 10000 in input file design_merged.gds

The stdcells.gds has a precision of 10000. For example, here is a message
printed from calibredrv when reading the gds for the minimum size
inverter:

    % calibredrv -a layout filemerge -in stdcells.gds -topcell INV_X1
                 -out dummy.gds

    Output file precision will be 10000, based on the lowest common
    multiple of the input file precision(s).

To run LVS successfully, our solution is just to make the LVS rule deck
expect a precision of 10000.

--------------------------------------------------------------------------
Modifications -- 06/07/2019
--------------------------------------------------------------------------

Initial commit

Base set of files from FreePDK45 and NanGate Open Cell Library assembled
into an ASIC design kit (ADK) for use with mflowgen

",1,0.67,0.67,,,,,,0,7,,,
128812862,MDEwOlJlcG9zaXRvcnkxMjg4MTI4NjI=,Lines_Queueing,elip12/Lines_Queueing,0,elip12,https://github.com/elip12/Lines_Queueing,Otree experiment,0,2018-04-09 17:58:05+00:00,2020-09-18 17:03:14+00:00,2020-09-18 17:03:11+00:00,,627,0,0,JavaScript,1,1,1,1,0,0,5,0,0,0,mit,1,0,0,public,5,0,0,master,1,"['fenixzhao1', 'elip12', 'ryanluk4', 'LeepsLab', 'jeffreyyang3', 'itsdwang', 'gramorgan']",,"# Lines_Queueing
Otree experiment written for the LEEPS Lab at the University of California, Santa Cruz.


## Description
This is a real-time economics experiment written in Otree and Otree-redwood (UCSC LEEPS Lab otree websocket extension).
It simulates a queue of people who are waiting to go into a 'service room', where they accumulate money. There are different
modes of play, but in general players are incentivized to enter the service room as soon as possible. They have the ability
to try to swap places with other players in the queue. Sometimes, they are able to offer a portion of their payoff
to the players with whom they want to swap.

models.py defines the data fields that the experiment facilitator will collect after the experiment completes (that is,
the fields that each player will unknowingly enter data into via their actions - attempt to trade or not, accept trades
or not, etc). It also defines a state machine that serves as the backend. For example, when a player enters the service room,
a message is sent to the state machine saying their time in the service room has started. The state machine updates the positions
of all other players in the queue (advancing them by 1), then reboradcasts the new state to every player.

pages.py defines the different screens participants navigate through in the experiment.

templates/Lines_Queueing/ holds the html for each screen (also known as a page). The queue page is written with Vue.js.
When a message is sent from models.py over websockets to the HTML queue page, the Vue machine updates the html, changing
what players see. Following the previous example, each players screen would change to show them having advanced one position
in the queue.

Otree documentation: https://otree.readthedocs.io/en/latest/
Demo server where you can play this a configuration of this app: leeps-otree.ucsc.edu:8000/demo. Click 'lines fork'.

## License
This software is licensed under the MIT license.
Copyright 2019 Eli Pandolfo

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
documentation files (the ""Software""), to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,
and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial
portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT
LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
",1,0.78,0.78,,,,,,0,1,,,
625329296,R_kgDOJUXEkA,hse_hw3_chromhmm,LanaShhh/hse_hw3_chromhmm,0,LanaShhh,https://github.com/LanaShhh/hse_hw3_chromhmm,,0,2023-04-08 19:21:24+00:00,2023-04-09 13:18:34+00:00,2023-04-09 15:32:38+00:00,,21086,0,0,Jupyter Notebook,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['LanaShhh'],,"# hse_hw3_chromhmm

## Отчет

### Содержание файлов и папок
- cellmarkfiletable.txt 
- data.zip - архив с данными, полученные с помощью ChromHMM
- HW3.ipynb - ноутбук, содержащий все запущенные команды
- HW3_bonus.ipynb - ноутук с кодом бонуса
- new_bed.zip - архив с *dense.bed файлом, у которого эпигенетические типы переименованы (результат бонуса)
- pics - папка с картинками ChromHMM
- genome_browser_pics - картинка с участками генома из GenomeBrowser

### Список гистоновых меток

В HW2 использовались данные по HL-60, но тут оказалось мало данных по модификациям, поэтому в HW3 использовалась **клеточная линия HMEC**.

**Файл контроля - Control.bam**, ссылка - http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneHmecControlStdAlnRep1.bam

Гистоновая метка | Имя файла в проекте | Ссылка на файл для скачивания
---              | ---                 | ---
H3k27ac          | H3k27ac.bam         | http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneHmecH3k27acStdAlnRep1.bam
H3k27me3         | H3k27me3.bam        | http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneHmecH3k27me3StdAlnRep1.bam
H3k36me3         | H3k36me3.bam        | http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneHmecH3k36me3StdAlnRep1.bam
H3k4me1          | H3k4me1.bam         | http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneHmecH3k4me1StdAlnRep1.bam
H3k4me2          | H3k4me2.bam         | http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneHmecH3k4me2StdAlnRep1.bam
H3k4me3          | H3k4me3.bam         | http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneHmecH3k4me3StdAlnRep1.bam
H3k9ac           | H3k9ac.bam          | http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneHmecH3k9acStdAlnRep1.bam
H3k09me3         | H3k9me3.bam         | http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneHmecH3k09me3AlnRep1.bam
H4k20me1         | H4k20me1.bam        | http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneHmecH4k20me1StdAlnRep1.bam
Ctcf             | Ctcf.bam            | http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneHmecCtcfStdAlnRep1.bam


### Картинки ChromHMM

![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/pics/HMEC_15_RefSeqTES_neighborhood.png)

![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/pics/HMEC_15_RefSeqTSS_neighborhood.png)

![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/pics/HMEC_15_overlap.png)

![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/pics/emissions_15.png)

![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/pics/transitions_15.png)

### Эпигенетические типы

Номер | Название                  | Описание | Картинка 
---   | ---                       | ---      | ---
1     | Active promoter           | Выражен в H3k9me3. Чаще всего находится на ядерной ламине. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/1.png)
2     | Weak promoter             | Чаще всего находится на ядерной ламине. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/2.png)
3     | Inactive/poised Promoter  | Выражен в H3k27me3. Чаще всего находится на ядерной ламине, RefSeqTES, RefSeqGene. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/3.png)
4     | Strong enhancer           | Выражен в H3k27me3, H3k4me1, H3k4me2. Чаще всего находится на CpG-островках, RefSeqExon, RefSeqTSS, RefSeqTES, RefSeqGene. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/4.png)
5     | Strong enhancer           | Выражен в H3k4me1, H3k4me2. Чаще всего находится на ядерной ламине, RefSeqGene. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/5.png)
6     | Weak/poised enhancer      | Выражен в H3k4me1, H3k4me2. Чаще всего находится на RefSeqTSS2kb, RefSeqTES. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/6.png)
7     | Weak/poised enhancer      | Выражен в H3k4me1, H3k4me2, H3k4me3. Чаще всего находится на CpG-островках, RefSeqExon, RefSeqTSS. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/7.png)
8     | Insulator                 | Выражен в H3k4me2, H3k4me3, H3k9ac, H3k27ac. Чаще всего находится на RefSeqExon, RefSeqTSS, RefSeqTSS2kb, CpG-островках. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/8.png)
9     | Transcriptional transition| Выражен в H3k4me1, H3k4me2, H3k27ac. Чаще всего находится на ядерной ламине, RefSeqGene. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/9.png)
10    | Transcriptional elongation| Выражен в H3k4me1, H3k4me2, H3k27ac. Чаще всего находится на ядерной ламине, RefSeqGene. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/10.png)
11    | Weak transcribed          | Выражен в H3k4me1, H3k27ac. Чаще всего находится на ядерной ламине, RefSeqGene. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/11.png)
12    | Polycomb-repressed        | Выражен в H3k4me1. Чаще всего находится на ядерной ламине, RefSeqGene. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/12.png)
13    | Heterochromatin; low signal| Выражен в H3k4me1, H3k36me3, H4k20me1. Чаще всего находится на RefSeqTES, RefSeqGene, RefSeqExon. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/13.png)
14    | Repetitive/Copy Number Variation| Выражен в H3k36me3. Чаще всего находится наRefSeqTES, RefSeqGene, RefSeqExon. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/14.png)
15    | Repetitive/Copy Number Variation| Выражен в Ctcf. Чаще всего находится на ядерной ламине. | ![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/15.png)

### Все запущенные команды находятся в файле HW3.ipynb. Код бонуса находится в файле HW3_bonus.ipynb

### Результат бонуса 

![](https://github.com/LanaShhh/hse_hw3_chromhmm/blob/main/genome_browser_pics/bonus_res.png)




",0,0.51,0.51,,,,,,0,1,,,
64594703,MDEwOlJlcG9zaXRvcnk2NDU5NDcwMw==,UCSC,ChrisOcon/UCSC,0,ChrisOcon,https://github.com/ChrisOcon/UCSC,,0,2016-07-31 13:17:08+00:00,2016-07-31 13:17:08+00:00,2016-07-31 13:17:09+00:00,,0,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,[],,,-1,0.81,0.81,,,,,,0,0,,,
66306910,MDEwOlJlcG9zaXRvcnk2NjMwNjkxMA==,PA4,gita-vahdatinia/PA4,0,gita-vahdatinia,https://github.com/gita-vahdatinia/PA4,Programming assignment in CS12B UCSC,0,2016-08-22 20:47:44+00:00,2016-08-22 20:48:31+00:00,2016-08-22 20:48:29+00:00,,8,0,0,Java,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['gita-vahdatinia'],,"//---------------------------------------------------------------
// Guita Vahdatinia
// gvahdati
// 12B
// 5/13/16
// list of all the files 
// README 
// ----------------------------------------------------------------
README
Simulation.java
QueueInterface.java
QueueEmptyException.java
Queue.java
QueueTest.java
Job.java
Makefile
",1,0.7,0.7,,,,,,0,0,,,
528298510,R_kgDOH30yDg,NLP-PhD-Application-In-The-World,RZFan525/NLP-PhD-Application-In-The-World,0,RZFan525,https://github.com/RZFan525/NLP-PhD-Application-In-The-World,The information of NLP PhD application in the world.,0,2022-08-24 06:42:21+00:00,2025-02-15 03:37:16+00:00,2024-08-27 16:44:51+00:00,,15,36,36,,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,36,main,1,['RZFan525'],,"# NLP Ph.D application in the world
The information of NLP Ph.D application in the world.

![](https://img.shields.io/badge/build-welcome%20to%20contribute!-blue) [![GitHub stars](https://img.shields.io/github/stars/RZFan525/NLP-PhD-Application-In-The-World)](https://github.com/RZFan525/NLP-PhD-Application-In-The-World/stargazers)

[Towards applying to CS Ph.D. programs](https://blog-ruipan-xyz.translate.goog/blog/towards-applying-to-cs-ph.d.-programs?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN)

[CS PhD Statements of Purpose](https://cs-sop.notion.site/CS-PhD-Statements-of-Purpose-df39955313834889b7ac5411c37b958d)

[Computer Science Professor Profile](https://drafty.cs.brown.edu/csprofessors?src=csopendata)
## United States

---

### [Massachusetts Institute of Technology (MIT)](https://www.mit.edu/) (Cambridge, Massachusetts)
* **English:** TOEFL(100); IELTS(7.0); GRE(optional)

---

### [Stanford University (Stanford)](https://www.stanford.edu/) (Stanford, California)
* **English:** TOEFL(100); GRE(optional)

---

### [Carnegie Mellon University (CMU)](https://www.cmu.edu/) (Pittsburgh, Pennsylvania)
* **English:** TOEFL(100); IELTS(7.0); Duolingo(105); GRE(optional)

---

### [University of California, Berkeley (UCB)](https://www.berkeley.edu/) (Berkeley, California)
* **English:** TOEFL(90); IELTS(7.0); GRE(optional)

---

### [University of Illinois Urbana-Champaign (UIUC)](https://illinois.edu/) (Champaign ,Illinois)
* **English:** TOEFL(S24); IELTS(S8.0); GRE(optional)

---

### [University of Washington (UW)](https://www.washington.edu/) (Seattle, Washington)
* **English:** TOEFL(92); IELTS(7.0); Duolingo(120); GRE(optional)

#### [AllenNLP](https://allenai.org/allennlp)
* **Faculty:** [Noah Smith](https://nasmith.github.io/), [Hannaneh Hajishirzi](https://www.semanticscholar.org/author/Hannaneh-Hajishirzi/2548384)

#### xlab
* **Faculty:** [Yejin Choi](https://homes.cs.washington.edu/~yejin/)

#### LZ's Group
* **Faculty:** [Luke Zettlemoyer](https://www.cs.washington.edu/people/faculty/lsz)

#### [TsvetShop](https://tsvetshop.github.io/)
* **Faculty:** [Yulia Tsvetkov](https://homes.cs.washington.edu/~yuliats/)

#### [Wangsheng's Group (AI for Medicine)](https://homes.cs.washington.edu/~swang/group.html)
* **Faculty:** [Sheng Wang](https://homes.cs.washington.edu/~swang/)

---

### [Cornell University (Cornell)](https://www.cornell.edu/) (Ithaca, New York)
* **English:** TOEFL(S22, R20, L15, W20); IELTS(7.0); GRE(optional)

---

### [University of Pennsylvania (Upenn)](https://www.upenn.edu/) (Philadelphia, Pennsylvania)
* **English:** ?


---

### [Princeton University (Princeton)](https://www.princeton.edu/) (Princeton, New Jersey)
* **English:** ?

---

### [University of Michigan (Umich)](https://umich.edu/) (Ann Arbor, Michigan)
* **English:** TOEFL(84); IELTS(6.5); GRE(optional)

---

### [Georgia Institute of Technology (Gatech)](https://www.gatech.edu/) (Atlanta, Georgia)
* **English:** TOEFL(90); IELTS(7.0); GRE(optional)

---

### [Johns Hopkins University (JHU)](https://www.jhu.edu/) (Baltimore, Maryland)
* **English:** TOEFL(100); IELTS(7.0); GRE(optional)

---

### [Columbia University (Columbia)](https://www.columbia.edu/) (New York, New York)
* **English:** TOEFL(101); IELTS(7.0); GRE(optional)

---

### [University of Chicago (UChicago)](https://www.uchicago.edu/) (Chicago, Illinois)
* **English:** TOEFL(90); IELTS(7.0); GRE(optional)

---

### [Harvard University (Harvard)](https://www.harvard.edu/) (Cambridge, Massachusetts Hall)
* **English:** TOEFL(80); IELTS(6.5); GRE(optional)

---

### [University of California San Diego (UCSD)](https://ucsd.edu/) (San Diego, California)
* **English:** TOEFL(85); IELTS(7.0); GRE(optional)

---

### [University of California Los Angeles (UCLA)](https://www.ucla.edu/) (Los Angeles, California)
* **English:** TOEFL(87 [W25, S24, R21, L17]); IELTS(7.0); GRE(optional)

---

### [California Institute of Technology (Caltech)](https://www.caltech.edu/) (Pasadena, California)
* **English:** ?

---

### [Yale University (Yale)](https://www.yale.edu/) (New Haven, Connecticut)
* **English:** TOEFL(S25); IELTS(S7.5); GRE(optional)

---

### [University of Southern California (USC)](https://www.usc.edu/) (Los Angeles, California)
* **English:** TOEFL(100 & 20 in each section); IELTS(7.0); GRE(optional)

#### [The Intelligence and Knowledge Discovery (INK) Lab](https://inklab.usc.edu/index.html)
* **Faculty:** [Xiang Ren](https://shanzhenren.github.io/)

#### JiaRobin's Group
* **Faculty:** [Robin Jia](https://robinjia.github.io/)

#### ZhaoJieyu's Group
* **Faculty:** [Jieyu Zhao](https://jyzhao.net/index.html)

---

### [University of California, Santa Barbara (UCSB)](https://www.ucsb.edu/) (Santa Barbara, L.A., California)
* **English:** TOEFL(100); IELTS(7.0); GRE(optional)

#### [UCSB NLP Group](http://nlp.cs.ucsb.edu/index.html)
* **Faculty:** [William Wang](https://sites.cs.ucsb.edu/~william/), [Lei Li](https://sites.cs.ucsb.edu/~lilei/), [Xifeng Yan](https://sites.cs.ucsb.edu/~xyan/), [Kyle Mahowald
](https://mahowak.github.io/), [Simon Todd](https://sjtodd.github.io/), [Shiyu Chang](https://code-terminator.github.io/)

---

### [New York University (NYU)](https://www.nyu.edu/) (New York, New York)
* **English:** TOEFL(100); IELTS(7.0); GRE(optional)


---

### [University of Maryland, College Park (UMD)](https://umd.edu/) (College Park, Maryland)
* **English:** TOEFL(96 [S22, L24, R26, W24]); IELTS(7.0 [S6.5, L7, R7, W7]); GRE(optional)

---

### [The University of Texas at Austin (UT Austin)](https://www.utexas.edu/) (Austin, Texas)
* **English:** TOEFL(79); IELTS(6.5); GRE(optional)

---

### [University of Wisconsin–Madison (Wisc)](https://www.wisc.edu/) (Madison, Wisconsin)
* **English:** TOEFL(92); IELTS(7.0); GRE(optional)

---

### [Purdue University (Purdue)](https://www.purdue.edu/) (West Lafayette, Indiana)
* **English:** TOEFL(100 & 22 in each section); IELTS(7.5 & 7.0 in each section); GRE(optional)

---

### [Brown University (Brown)](https://www.brown.edu/) (Providence, Rhode Island)
* **English:** TOEFL(105); IELTS(7.0); GRE(optional)

---

### [Duke University (Duke)](https://duke.edu/) (Durham, North Carolina)
* **English:** TOEFL(90); IELTS(7.0); GRE(optional)

---

### [Northeastern University (Northeastern)](https://www.northeastern.edu/) (Boston, Massachusetts)
* **English:** TOEFL(100); IELTS(7.5); GRE(optional)

---

### [Northwestern University (Northwestern)](https://www.northwestern.edu/) (Evanston, Illinois)
* **English:** TOEFL(90); IELTS(7.0); GRE(optional)

---

### [University of Massachusetts Amherst (UMass)](https://www.umass.edu/) (Amherst, Massachusetts)
* **English:** TOEFL(80); IELTS(6.5); GRE(optional)

---

### [The Ohio State University (OSU)](https://www.osu.edu/) (Columbus, Ohio)
* **English:** TOEFL(79); IELTS(7.0); GRE(optional)

---

### [University of Illinois at Chicago (UIC)](https://www.uic.edu/) (Chicago, Illinois)
* **English:** TOEFL(don't known); IELTS(don't known); GRE(optional)

#### Cornelia Caragea‘s Group
* **Faculty:** [Cornelia Caragea](https://www.cs.uic.edu/~cornelia/)

---

### [University of California, Santa Cruz (UCSC)](https://www.ucsc.edu/) (Santa Cruz, California)
* **English:** TOEFL(100); IELTS(7.0); GRE(optional)

#### [ERIC Lab](http://eric-lab.soe.ucsc.edu/home)
* **Faculty:** [Xin (Eric) Wang](https://eric-xw.github.io/index.html)

---

### [University of California, Riverside (UCR)](https://www.ucr.edu/) (Riverside, California)
* **English:** TOEFL(80); IELTS(7.0); GRE(required)

#### DongYue's Group
* **Faculty:** [Yue Dong](https://yuedongcs.github.io/)

---

### [University of Texas at Dallas (UTD)](https://www.utdallas.edu/) (Richardson, Texas)
* **English:** TOEFL(80); IELTS(6.5); Duolingo(105); GRE(required)

#### [Human Language Technology Research Institute (HLT)](https://www.hlt.utdallas.edu/)
* **Faculty:** [Xinya Du](https://xinyadu.github.io/index.html)

---

### [George Mason University (GMU)](https://www.gmu.edu/) (Fairfax, Virginia)
* **English:** TOEFL(88 & 20 in each section); IELTS(7.0 & 6.5 in each section); Duolingo(120); GRE(optional)

#### [George Mason Natural Language Processing Group (GMNLP)](https://nlp.cs.gmu.edu/)
* **Faculty:** [Ziyu Yao](https://ziyuyao.org/)

---

### [Illinois Institute of Technology](https://www.iit.edu/) (Chicago, Illinois)
* **English:** TOEFL(90); IELTS(6.5); GRE(optional)

#### Shukai's Group
* **Faculty:** [Kai Shu](http://www.cs.iit.edu/~kshu/)

---

### [Temple University](https://www.temple.edu/) (Philadelphia, Pennsylvania)
* **English:** TOEFL(79); IELTS(6.5); Duolingo(110); GRE(310)

#### [LanguageX](https://sites.google.com/site/yinwenpeng1987/languagex-lab)
* **Faculty:** [Wenpeng Yin](https://sites.google.com/site/yinwenpeng1987/home)


## Hong Kong
### [Hong Kong Baptist University (HKBU)](https://www.hkbu.edu.hk/)
* **English:** TOEFL(79); IELTS(6.0)

#### MaJing's Group
* **Faculty:** [Jing Ma](https://majingcuhk.github.io/)

### [Chinese University of Hong Kong, Shenzhen (CUHKSZ)](https://www.cuhk.edu.cn/zh-hans)
* **English:** TOEFL(79); IELTS(6.5)

#### BenyouWang's Group
* **Faculty:** [Benyou Wang](https://wabyking.github.io/old.html)

---

## Singapore
### [National University of Singapore (NUS)](https://www.nus.edu.sg/)
* **English:** TOEFL(90); IELTS(6.0); GRE(320+3.5)

#### XieQizhe's Group
* **Faculty:** [Qizhe Xie](https://www.qizhexie.com/)
",0,0.68,0.68,,,,,,0,4,,,
731880600,R_kgDOK5-cmA,CSE20,Aradhya2005/CSE20,0,Aradhya2005,https://github.com/Aradhya2005/CSE20,The coding projects I wrote in Python for my CSE-20 class at UC Santa Cruz.,0,2023-12-15 05:12:31+00:00,2023-12-17 04:43:14+00:00,2023-12-17 04:44:17+00:00,,11,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['Aradhya2005'],,,1,0.6,0.6,,,,,,0,1,,,
874611967,R_kgDONCGE_w,Wordrange,mokimochiii/Wordrange,0,mokimochiii,https://github.com/mokimochiii/Wordrange,,0,2024-10-18 06:32:09+00:00,2024-10-18 06:37:52+00:00,2024-10-18 06:37:49+00:00,,6742,0,0,C++,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['mokimochiii'],,"# Wordrange

README for Wordrange
Vince Moreno, 19 May 2024



Description
The Wordrange folder contains the ""wordrange"" executable which allows a user
input a file of words or numbers. The program stores and sorts the values in
the file in an AVL tree. The user can then find how many values are between
two given values in the AVL tree (lexigrahically if strings). The Tests folder 
contains input and output files to test the code on.



Files
Code Files: Makefile, wordrange.cpp, avl.cpp
Input/Output Files: simple-input.txt, simple-output.txt
Extra files: README

The Tests folder contains input and output .txt files.
Input Files: allwords-basic.txt, simple-input.txt, allwords-more-range.txt
Output Files: allwords-basic-output.txt, simple-output.txt, allwords-more-range-output.txt



Usage
In main folder:
  1. Run ""make"", to get executable ""wordrange""
  2. Run ""./wordrange <INPUT FILE> <OUTPUT FILE>""



Input/Output
Each line of the input file will be of the following two forms:
i <STRING>
or
r <STRING1> <STRING2>

The first line means to insert a string into the AVL tree
The second line above means: count the number of strings 
(currently stored) that are lexicographically between STRING1 and STRING2.
The count is printed to the <OUTPUT FILE> provided as an argument



AVL Trees (avl.cpp)
The cpp file wordrange.cpp uses the AVL tree implemented by avl.h and avl.cpp
to efficienty store and traverse through the heap of words given by the input
file. An AVL tree is more efficient than a normal Binary Search tree in that it
keeps the left and right subtrees balanced. Each node in the tree contains a 
balance factor which is equal to the height of the left node minus the height
of the right node. If the balance factor of a node is greater than 1 or less
than -1, the tree will reorder the nodes to balance itself out. It does the
process of balancing through four different types of rotations: Right, Left,
RightLeft, LeftRight. Each rotation is used for a specific case depending on the
nodes' balance factors. Each time a node is inserted, the tree recursively balances
the tree nodes which allows the time complexity to traverse the tree to remain at
O(log n). This allows for fast traversing when doing the range query operations --
it allows for over 1 million range operations in under 20 seconds. The function 
countInRange() gives the answer to the range queries in O(log n) time through 
recursion. Each node also contains an int value that says how many child nodes 
it stores. Using this value allows the function to make less recursive calls.

Sources I used to understand AVL trees
https://www.youtube.com/watch?v=zP2xbKerIds&ab_channel=MaanethDeSilva
https://www.youtube.com/watch?v=jDM6_TnYIqE&ab_channel=AbdulBari
Lecture recordings from CSE101 in University of California, Santa Cruz
",1,0.62,0.62,,,,,,0,1,,,
219910992,MDEwOlJlcG9zaXRvcnkyMTk5MTA5OTI=,Bit-Vector,darrelllong/Bit-Vector,0,darrelllong,https://github.com/darrelllong/Bit-Vector,"Variable-length bit vectors in C, based on (8, 16, 32, 64)-bit integers",0,2019-11-06 04:21:51+00:00,2021-08-17 05:50:58+00:00,2021-08-17 05:50:55+00:00,,11,0,0,C,1,1,1,1,0,0,0,0,0,0,bsd-2-clause,1,0,0,public,0,0,0,master,1,['darrelllong'],,"# Variable length bit vectors in C

Simple bit vectors in C, with the usual create, delete, get, set, clear and length operations. This implementation *does not* automatically extend the bit 
vector if you go past the end, but that is a simple addition.

For some reason, students these days struggle with the ideas of bits and numbers in bases other than 10. Variable length strings of bits is one of the things
that gives them a difficult time. But once they realize their utility, they are pleased.

Originally written for the students of CSE 13S at the University of California, Santa Cruz.
",1,0.61,0.61,,,,,,0,1,,,
597613650,R_kgDOI57cUg,Toaster-Oven,nghiapham110702/Toaster-Oven,0,nghiapham110702,https://github.com/nghiapham110702/Toaster-Oven,,0,2023-02-05 04:19:58+00:00,2024-02-26 00:46:43+00:00,2023-02-05 04:42:12+00:00,,76,1,1,C,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,main,1,['nghiapham110702'],,"Nghia Pham

 - Deepened programming skills in state machines and reactive systems through hands-on experience with programming a simple toaster
 
 - Acquired a deeper understanding of finite state machines and event-driven programming through the use of button press duration in decision-making processes
 
 - Highlighted key learning outcomes include expertise in event-driven programming and the ability to effectively implement state machine diagrams.

This is the Finite State Machine diagram for Toaster Oven

![FMS](https://user-images.githubusercontent.com/103624639/216801828-ea5e3f8b-18c0-43cc-9f69-65eaa5c1479c.PNG)

Credit to professor Steve mcGuire from UC Santa Cruz

This is the output of the oven on the pickit Uno 32 equipment

![Result](https://user-images.githubusercontent.com/103624639/216801913-e1fb6634-b87c-45e8-9136-4174e31aa4e7.PNG)

 
",1,0.72,0.72,,,,,,0,1,,,
842844995,R_kgDOMjzLQw,ucsc,Amashagit/ucsc,0,Amashagit,https://github.com/Amashagit/ucsc,,0,2024-08-15 08:06:17+00:00,2024-08-15 09:08:52+00:00,2024-08-15 09:08:49+00:00,,1,0,0,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['Amashagit'],,,0,0.83,0.83,,,,,,0,1,,,
122769168,MDEwOlJlcG9zaXRvcnkxMjI3NjkxNjg=,UCSC_spectral_pipeline,msiebert1/UCSC_spectral_pipeline,0,msiebert1,https://github.com/msiebert1/UCSC_spectral_pipeline,,0,2018-02-24 18:48:48+00:00,2024-11-07 00:17:33+00:00,2024-11-07 00:17:27+00:00,,413913,5,5,Python,1,1,1,1,0,0,8,0,0,16,,1,0,0,public,8,16,5,master,1,"['msiebert1', 'brojonat', 'astrophpeter', 'Majoburo', 'tiarahung', 'stinyanont']",,"UCSC Spectral Reduction Pipeline
================================

This is the 0.1 version of a quicklook spectral reduction pipeline. It has been tested extensively on Kast and LRIS spectral images (SOAR still in development). 

The pipeline is a collection of python scripts (pre_reduction_dev.py, QUICKLOOK.py, and cal.py), utilising an implementation of IRAF called, not surprisingly, pyRAF. It is heavily influenced from Stefano Valenti's work on similar pipelines for NNT (EFOSC) and LCO (FLOYDS) pipelines.

### Documentation

https://ucsc-spectral-pipeline.readthedocs.io/en/latest/",0,0.84,0.84,,,,,,0,6,,,
939250941,R_kgDON_vU_Q,FLAT,UCSC-REAL/FLAT,0,UCSC-REAL,https://github.com/UCSC-REAL/FLAT,[ICLR 2025] FLAT: LLM Unlearning via Loss Adjustment with Only Forget Data,0,2025-02-26 08:40:24+00:00,2025-02-28 05:56:09+00:00,2025-02-26 08:52:31+00:00,,74199,4,4,Python,1,1,1,1,0,0,0,0,0,0,apache-2.0,1,0,0,public,0,0,4,main,1,[],1,"<div align=""center"">

# FLAT: LLM Unlearning via Loss Adjustment with Only Forget Data 

<a href='https://github.com/UCSC-REAL/FLAT'><img src='https://img.shields.io/badge/Project-Page-Green'></a>
<a href='https://www.arxiv.org/pdf/2410.11143'><img src='https://img.shields.io/badge/Paper-PDF-orange'></a> 

FLAT is a ""flat"" loss adjustment approach which addresses these issues by maximizing f-divergence between the available template answer and the forget answer only w.r.t. the forget data. The variational form of the defined f -divergence theoretically provides a way of loss adjustment by assigning different importance weights for the learning w.r.t. template responses and the forgetting of responses subject to unlearning. Empirical results demonstrate that our approach not only achieves superior unlearning performance compared to existing methods but also minimizes the impact on the model’s retained capabilities, ensuring high utility across diverse tasks, including copyrighted content unlearning on Harry Potter dataset and MUSE Benchmark, and entity unlearning on the TOFU dataset.

</div>


## 🎉🎉 News 
- [x] [2025.01] 👏👏 Accepted by **ICLR 2025**.
- [x] [2024.10] 🚀🚀 Release the paper of [**FLAT**](https://www.arxiv.org/pdf/2410.11143).

## Installation

```
conda create -n flat python=3.10
conda activate flat
conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia
conda install -c ""nvidia/label/cuda-11.8.0"" cuda-toolkit
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
pip install natsort
pip install sacrebleu
pip install sentencepiece
```

## Task 1: Unlearning on Harry Potter

### Finetune the original model

```
# OPT-2.7b lr=1e-5
python finetune.py --model_name facebook/opt-2.7b

# finetune llama2-7b
python finetune.py --model_name meta-llama/Llama-2-7b-hf
```

After obtaining the finetuned model, we need to change the `hp_ft_model_path` in model_config.yaml. When unlearning, the code will load the finetuned model as the original model.

### Unlearn
```
master_port=18765
model=llama2-7b
lr=2e-7
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=$master_port unlearn.py --config-name=forget.yaml batch_size=4 gradient_accumulation_steps=4 model_family=${model} lr=${lr}
```

### Metric & Evaluation

Unlearning efficacy: 

- BLEU on Harry Potter completion
- Rouge-L on Harry Potter completion

Utility:

- Perplexity on Wikitext
- Zero-shot Accuracy on benchmarks
- Zero-shot Accuracy on TruthfulQA

```
python evaluate.py --method_name $your_name --model_save_dir $your_model_path
```

## Task 2 & Task 3: Unlearning on TOFU and MUSE-NEWS
Please refer to [**TOFU**](https://github.com/locuslab/tofu) and [**MUSEBENCH**](https://github.com/swj0419/muse_bench).

## Citing FLAT

If you find our codebase and dataset beneficial, please cite our work:
```
@article{wang2024llm,
  title={LLM Unlearning via Loss Adjustment with Only Forget Data},
  author={Wang, Yaxuan and Wei, Jiaheng and Liu, Chris Yuhao and Pang, Jinlong and Liu, Quan and Shah, Ankit Parag and Bao, Yujia and Liu, Yang and Wei, Wei},
  journal={arXiv preprint arXiv:2410.11143},
  year={2024}
}
```

## Thanks
We thank the codebase from [**TOFU**](https://github.com/locuslab/tofu), [**SOUL**](https://github.com/OPTML-Group/SOUL), and [**MUSEBENCH**](https://github.com/swj0419/muse_bench).
",1,0.7,0.7,,,,,,0,1,,,
79531214,MDEwOlJlcG9zaXRvcnk3OTUzMTIxNA==,GeneFuse,OpenGene/GeneFuse,0,OpenGene,https://github.com/OpenGene/GeneFuse,Gene fusion detection and visualization,0,2017-01-20 06:12:55+00:00,2025-03-03 03:01:58+00:00,2022-02-21 08:07:06+00:00,,404,121,121,C,1,1,1,1,0,0,59,0,0,35,mit,1,0,0,public,59,35,121,master,1,"['sfchen', 'ZKai0801']",1,"[![install with conda](
https://anaconda.org/bioconda/genefuse/badges/version.svg)](https://anaconda.org/bioconda/genefuse)
# GeneFuse
A tool to detect and visualize target gene fusions by scanning FASTQ files directly. This tool accepts FASTQ files and reference genome as input, and outputs detected fusion results in TEXT, JSON and HTML formats.

# Take a quick glance of the informative report
* Sample HTML report: http://opengene.org/GeneFuse/report.html
* Sample JSON report: http://opengene.org/GeneFuse/report.json
* Dataset for testing: http://opengene.org/dataset.html  Please download the paired-end FASTQ files for GeneFuse testing (Illumina platform)

# Get genefuse program
## install with Bioconda
[![install with conda](
https://anaconda.org/bioconda/genefuse/badges/version.svg)](https://anaconda.org/bioconda/genefuse)
```shell
conda install -c bioconda genefuse
```
## download binary
This binary is only for Linux systems, http://opengene.org/GeneFuse/genefuse
```shell
# this binary was compiled on CentOS, and tested on CentOS/Ubuntu
wget http://opengene.org/GeneFuse/genefuse
chmod a+x ./genefuse
```
## or compile from source
```shell
# get source (you can also use browser to download from master or releases)
git clone https://github.com/OpenGene/genefuse.git

# build
cd genefuse
make

# Install
sudo make install
```

# Usage
You should provide following arguments to run genefuse
* the reference genome fasta file, specified by `-r` or `--ref=`
* the fusion setting file, specified by `-f` or `--fusion=`
* the fastq file(s), specified by `-1` or `--read1=` for single-end data. If dealing with pair-end data, specify the read2 file by `-2` or `--read2=`
* use `-h` or `--html=` to specify the file name of HTML report
* use `-j` or `--json=` to specify the file name of JSON report
* the plain text result is directly printed to STDOUT, you can pipe it to a file using a `>`

## Example
```shell
genefuse -r hg19.fasta -f genes/druggable.hg19.csv -1 genefuse.R1.fq.gz -2 genefuse.R2.fq.gz -h report.html > result
```

## Reference genome
The reference genome should be a single whole FASTA file containg all chromosome data. This file shouldn't be compressed. For human data, typicall `hg19/GRch37` or `hg38/GRch38` assembly is used, which can be downloaded from following sites:
* `hg19/GRch37`: https://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz
* `hg38/GRch38`: https://hgdownload.cse.ucsc.edu/goldenpath/hg38/bigZips/hg38.fa.gz  
Remember to decompress hg19.fa.gz/hg38.fa.gz since it is gzipped and is not supported currently.

## Fusion file
The fusion file is a list of coordinated target genes together with their exons. A sample is:
```CSV
>EML4_ENST00000318522.5,chr2:42396490-42559688
1,42396490,42396776
2,42472645,42472827
3,42483641,42483770
4,42488261,42488434
5,42490318,42490446
...

>ALK_ENST00000389048.3,chr2:29415640-30144432
1,30142859,30144432
2,29940444,29940563
3,29917716,29917880
4,29754781,29754982
5,29606598,29606725
...
```
The coordination system should be consistent with the reference genome.  
### Fusion files provided in this package
Four fusion files are provided with `genefuse`:
1. `genes/druggable.hg19.csv`: all druggable fusion genes based on `hg19/GRch37` reference assembly.
2. `genes/druggable.hg38.csv`: all druggable fusion genes based on `hg38/GRch38` reference assembly.
3. `genes/cancer.hg19.csv`: all COSMIC curated fusion genes (http://cancer.sanger.ac.uk/cosmic/fusion) based on `hg19/GRch37` reference assembly.
4. `genes/cancer.hg38.csv`: all COSMIC curated fusion genes (http://cancer.sanger.ac.uk/cosmic/fusion) based on `hg38/GRch38` reference assembly.

Notes:
* `genefuse` runs much faster with `druggable` genes than `cancer` genes, since `druggable` genes are only a small subset of `cancer` genes. Use this one if you only care about the fusion related personalized medicine for cancers.
* The `cancer` genes should be enough for most cancer related studies, since all COSMIC curated fusion genes are included.
* If you want to create a custom gene list, please follow the instructions given on next section.
### Create a fusion file based on hg19 or hg38
If you'd like to create a custom fusion file, you can use `scripts/make_fusion_genes.py`   
As the script uses `refFlat.txt` file to determine genomic coordinates of exons, you need to download a `refFlat.txt` file from UCSC Genome Browser in advance. Of course, the choice of using either hg19 or hg38 is up to you.

- For hg19: http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/refFlat.txt.gz
- For hg38: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/refFlat.txt.gz

Please make sure unzip the file to txt format before you continue

As for the input gene list file, all genes should be listed in separate lines.  By default, the longest transcript will be used. However, you can specify a different transcript by adding the transcript ID to the end of a gene. The gene and its transcript should be separated by a tab or a space. Please note that each gene should be the HGNC official gene symbol, and each transcript should be NCBI RefSeq transcript ID. 

An example of gene list file:

```
BRCA2        NM_000059
FAM155A
IRS2
```

When both input gene list file (`gene_list.txt`) and `refFlat.txt` file are prepared, you can use following command to generate a user-defined fusion file (`fusion.csv`):

```shell
python3 scripts/make_fusion_genes.py gene_list.txt -r /path/to/refflat -o fusion.csv
```

# HTML report
GeneFuse can generate very informative and interactive HTML pages to visualize the fusions with following information:
* the fusion genes, along with their transcripts.
* the inferred break point with reference genome coordinations.
* the inferred fusion protein, with all exons and the transcription direction.
* the supporting reads, with all bases colorized according to their quality scores.
* the number of supporting reads, and how many of them are unique (the rest may be duplications)
## A HTML report example
![image](http://www.opengene.org/GeneFuse/eml4alk.png)  
See the HTML page of this picture: http://opengene.org/GeneFuse/report.html

# All options
```
options:
  -1, --read1       read1 file name (string)
  -2, --read2       read2 file name (string [=])
  -f, --fusion      fusion file name, in CSV format (string)
  -r, --ref         reference fasta file name (string)
  -u, --unique      specify the least supporting read number is required to report a fusion, default is 2 (int [=2])
  -d, --deletion    specify the least deletion length of a intra-gene deletion to report, default is 50 (int [=50])
  -h, --html        file name to store HTML report, default is genefuse.html (string [=genefuse.html])
  -j, --json        file name to store JSON report, default is genefuse.json (string [=genefuse.json])
  -t, --thread      worker thread number, default is 4 (int [=4])
  -?, --help        print this message
```

# Cite GeneFuse
If you used GeneFuse in you work, you can cite it as: 

Shifu Chen, Ming Liu, Tanxiao Huang, Wenting Liao, Mingyan Xu and Jia Gu. GeneFuse: detection and visualization of target gene fusions from DNA sequencing data. International Journal of Biological Sciences, 2018; 14(8): 843-848. doi: 10.7150/ijbs.24626
",0,0.54,0.54,,,,,,0,13,,,
177789865,MDEwOlJlcG9zaXRvcnkxNzc3ODk4NjU=,appbiblioteca,vvilche1/appbiblioteca,0,vvilche1,https://github.com/vvilche1/appbiblioteca,aplicacion para biblioteca ucsc,0,2019-03-26 13:03:35+00:00,2019-03-26 13:03:38+00:00,2019-03-26 13:03:36+00:00,,5,0,0,,1,1,1,1,0,0,0,0,0,0,apache-2.0,1,0,0,public,0,0,0,master,1,['vvilche1'],,"# appbiblioteca
aplicacion para biblioteca ucsc
",0,0.74,0.74,,,,,,0,0,,,
323996216,MDEwOlJlcG9zaXRvcnkzMjM5OTYyMTY=,yep-trackhubs,seqcode/yep-trackhubs,0,seqcode,https://github.com/seqcode/yep-trackhubs,UCSC genome browser trackhubs for YEP project,0,2020-12-23 20:31:43+00:00,2020-12-23 20:36:09+00:00,2020-12-23 20:36:07+00:00,,83,0,0,HTML,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['shaunmahony'],1,"# YEP UCSC genome browser trackhubs

Trackhubs: [http://genome.ucsc.edu/cgi-bin/hgTracks?db=sacCer3&hubUrl=https://seqcode.github.io/yep-trackhubs/yep/hub.txt]
",0,0.64,0.64,,,,,,0,1,,,
474461592,R_kgDOHEe1mA,CSE110A-sp2022,SorensenUCSC/CSE110A-sp2022,0,SorensenUCSC,https://github.com/SorensenUCSC/CSE110A-sp2022,class website for CSE110A spring 2022 at UCSC,0,2022-03-26 20:40:54+00:00,2023-10-08 11:09:06+00:00,2023-04-03 01:35:54+00:00,,38074,2,2,HTML,1,1,1,1,1,0,1,0,0,0,,1,0,0,public,1,0,2,main,1,"['tyler-utah', 'xuyanwen2012', 'SorensenUCSC']",,"# CSE110A-sp2022
class website for CSE110A spring 2022 at UCSC
",1,0.74,0.74,,,,,,0,3,,,
49785190,MDEwOlJlcG9zaXRvcnk0OTc4NTE5MA==,CNView,RCollins13/CNView,0,RCollins13,https://github.com/RCollins13/CNView,Visualization and annotation of CNVs from population-scale whole-genome sequencing data,0,2016-01-16 18:20:08+00:00,2025-02-10 21:54:50+00:00,2018-01-08 21:03:14+00:00,,12624,70,70,R,1,1,1,1,0,0,11,0,0,3,mit,1,0,0,public,11,3,70,master,1,['RCollins13'],,"# CNView
Visualization, quantitation, and annotation of CNVs from population-scale whole-genome sequencing data.

**Contact:** Ryan Collins (rcollins@chgr.mgh.harvard.edu)

All content copyright (c) 2016 Ryan Collins and is distributed under terms of the MIT license.  

---  
![CNView: Example CNV Visualization](/ExamplePlots/CNView.BannerExample.jpg?raw=true ""CNView: Example CNV Visualization"")  

---  
## Table of Contents  
#### [General Information](https://github.com/RCollins13/CNView#general-information-1)  
- [CNView Summary](https://github.com/RCollins13/CNView#cnview-summary)
- [Accessing Reference Libraries](https://github.com/RCollins13/CNView#accessing-reference-libraries)  
- [Citing CNView](https://github.com/RCollins13/CNView#citing-cnview)

#### [Code documentation](https://github.com/RCollins13/CNView#code-documentation-1)  
- [CNView.R](https://github.com/RCollins13/CNView#cnviewr)  
  
#### [Example Usage](https://github.com/RCollins13/CNView#example-usage-1)  
- [Getting Started](https://github.com/RCollins13/CNView#getting-started-1)
- [Example A: Canonical Deletion, Single Sample](https://github.com/RCollins13/CNView#example-a)  
- [Example B: Multiple Samples on the Same x-Axis](https://github.com/RCollins13/CNView#example-b)  
- [Example C: Multiple Highlighted Intervals](https://github.com/RCollins13/CNView#example-c)  
- [Example D: Disabled UCSC Annotations](https://github.com/RCollins13/CNView#example-d)  

---  
## General Information
### CNView Summary  
CNView is a low-profile visualization tool for read depth in batches of next-generation sequencing libraries and, more specifically, for visually inspecting sites of [copy-number variation (CNV)](https://en.wikipedia.org/wiki/Copy-number_variation).  It also implements a framework for estimating CNV probabilities and annotating CNV intervals.  

We are open to tailoring applciations of CNView for specific needs or to customize plots. CNView is also under active development. Contact us at [rcollins@chgr.mgh.harvard.edu](mailto:rcollins@chgr.mgh.harvard.edu) if you have any questions or requests. Please post any issues to GitHub.

### Accessing Reference Libraries  
Like many depth-based CNV tools, CNView does not work on individual libraries. Instead, CNView jointly models multiple libraries simultaneously, normalizing both within and across libraries to reduce systematic coverage biases. Thus, while technically CNView will run successfully from just two libraries, generally CNVs become clearly resolvable with at least 20 total samples jointly modelled.  

If you do not have access to an appropriate cohort of normative reference samples, thousands of standard Illumina WGS libraries are available from the [1,000 Genomes Project](http://www.1000genomes.org/) and many hundreds of other libraries are available through other public repositories like the [European Nucleotide Archive (ENA)](http://www.ebi.ac.uk/ena) or the [Sequence Read Archive (SRA)](http://www.ncbi.nlm.nih.gov/sra).  

### Citing CNView  
If you use CNView, please cite our preprint ( [Collins et al., 2016](http://biorxiv.org/content/early/2016/04/20/049536) ).  

---  
## Code Documentation
### CNView.R  
Performs joint normalization of binned coverage values across a batch of WGS libraries and facilitates visualization. Also interfaces with UCSC Genome Browser to underlay several annotation tracks.  
```
Usage: CNView.R [options] chr start end samples.list covmatrix.bed outfile


Options:
        -c INTEGER, --compression=INTEGER
                compression scalar for rebinning, if desired [default 'NULL']

        -i CHARACTER, --highlight=CHARACTER
                tab-delimited list of coordinate pairs for intervals to highlight and color as third column; NULL disables highlighting [default NA]

        -w INTEGER, --window=INTEGER
                distance to append to both sides of input interval for viewing [default 61.8% of plot interval]

        --ymin=INTEGER
                minimum value for y axis [default NULL]

        --ymax=INTEGER
                maximum value for y axis [default NULL]

        -n INTEGER, --normDist=INTEGER
                distance outside region to use for normalization (both sides) [default 5000000]

        -s INTEGER, --subsample=INTEGER
                truncate coverage matrix to [s] samples; useful for very large cohorts [default 200]

        --gcex=INTEGER
                scalar applied to all fonts and legend [default 1]

        --names=CHARACTER
                list of custom names to be applied to each plot panel (e.g. 'mother', 'father', 'child', rather than actual sample IDs) [default NA]

        -t CHARACTER, --title=CHARACTER
                custom title for plot [default NULL]

        -p, --probs
                add CNV probabilities below each higlighted interval [default FALSE]

        -u, --noUCSC
                disable UCSC track plotting [default FALSE]

        -G, --nogenesymbols
                disable gene symbol printing below gene bodies in UCSC tracks [default FALSE]

        --tabix
                use tabix to index into coverage matrix [default FALSE]

        --noUnix
                disable use of unix coreutils [default FALSE]

        -q, --quiet
                disable verbose output [default FALSE]

        -l, --nolegend
                disable legend on plot [default TRUE]

        -h, --help
                Show this help message and exit
```
**Usage Notes:**  
1. Several base-level options (such as ```returnData``` or ```plot```) are not wrapped by the Rscript implementation. To access those features, import the R function directly and run from the R command prompt.  
2. By default, CNView will randomly subsample an input matrix to include only 200 libraries (including those specified to plot). This is a useful parameter to reduce computational requirements (both memory and runtime) for large cohorts, but can be modulated with the ```-s```/```--subsample``` option.  
3. Use the ```--tabix``` option to improve speed when reading from very large coverage matrices. However, your input matrix must be bgzipped and tabix-indexed; see the [Samtools documentation](http://www.htslib.org/doc/tabix.html) for details.   

---  
## Example Usage
### Getting Started  
**Required Input Data**  
The input data for CNView is a tab-delimited [bed-style](https://genome.ucsc.edu/FAQ/FAQformat.html#format1) matrix of binned coverage values, which can be generated by ```bedtools coverage``` followed by ```bedtools unionbedg``` ([bedtools documentation](http://bedtools.readthedocs.org/en/latest/)). Alternatively, you can use [```binCov.py```](https://github.com/RCollins13/WGD#bincovpy) from the CNView sister repository, [WGD](https://github.com/RCollins13/WGD). Generally, 100bp-1kb sequential bins provide a reasonable tradeoff between resolution and modeling speed on a standard 8GB two-core laptop.

An example coverage matrix at 100bp binsize for human libraries aligned to reference genome hg19 would look something like this:  
```
Chr  Start     End       SampleA  SampleB  SampleC  ...  SampleZ
1    0         100       89       56       217      ...  141
1    100       200       98       60       230      ...  132
1    200       300       102      59       202      ...  142
...  ...       ...       ...      ...      ...      ...  ...
Y    59373200  59373300  79       48       207      ...  133
Y    59373300  59373400  89       51       196      ...  138
Y    59373400  59373500  93       68       198      ...  129
```  

CNView has been tested on libraries ranging from 1X to >300X coverage simultaneously and appears to perform relatively consistently irrespective of the ranges of coverage between individual libraries in the same batch.  


### Example A  
**Canonical Deletion Plotted in a Single Sample**  
The basic use-case for CNView is to visualize a predefined CNV locus, which can be predicted from whole-genome sequencing data with many different algorithms, such as [cn.MOPS](http://www.ncbi.nlm.nih.gov/pubmed/22302147), [CNVnator](http://www.ncbi.nlm.nih.gov/pubmed/21324876), or [GenomeSTRiP](http://www.ncbi.nlm.nih.gov/pubmed/25621458).  Once a putative CNV locus is defined, visualization can be performed right out of the box with CNView by invoking ```CNView.R``` with all default parameters.  
![Canonical Deletion Plotted in a Single Sample](/ExamplePlots/CNView.ExamplePlotA.jpg?raw=true ""Canonical Deletion Plotted in a Single Sample"")  
**Example code to generate the above plot:**
```
bash$ ./CNView.R 2 178714141 178760307 SFARI_d12529p1 \
                 ~/cov_matrix.bed \
                 ./ExamplePlots/CNView.ExamplePlotA.pdf \
                 --title ""Example Plot A: Canonical Deletion, Single Sample \
                 --probs""
```  
This example is visualizing a 46kb deletion of two exons from *PDE11A*. The first three positional arguments were the coordinates of the deletion (the highlighted region), while the other positional arguments were as follows:  
- ```SFARI_d12529p1``` is the ID of the sample being plotted, which has to exactly match one of the names of the columns in the coverage matrix.  
- ```~/cov_matrix.bed``` is the path to the input coverage matrix, like the [example](https://github.com/RCollins13/CNView#getting-started-1) provided above.  
- ```./ExamplePlots/CNView.ExamplePlotA.pdf``` is the path to the desired output file (always will be pdf).  
- ```--title``` overrides the default title with the subsequently supplied string in quotes.  
- ```--probs``` prints the FDR-corrected probability of the highlighted window being deleted or duplicated. The q-value is calculated by evaluating the t-score at each bin overlapping the highlighted window, combining those p-values with [Fisher's Method](https://en.wikipedia.org/wiki/Fisher%27s_method), then correcting for false discovery rate (FDR) with the [Benjamini-Hochberg procedure](https://en.wikipedia.org/wiki/False_discovery_rate#Benjamini.E2.80.93Hochberg_procedure).  

Running the above code will also print some runtime diagnostics to stdout, which can alternatively be silenced with ```-q```/```--quiet```:  
```
+-------------------+
| CNView Visualizer |
|     (c) 2016      |
+-------------------+
Sample ID file 'SFARI_d12529p1' not found, assuming single sample ID provided
Attempting to connect to UCSC Genome Browser... Complete
Filtering & loading coverage matrix... Complete
Compressing coverage matrix [1,000 bp bins]...  Complete
Performing intra-sample normalization... Complete
Performing inter-sample normalization... Complete
Plotting samples to ./ExamplePlots/CNView.ExamplePlotA.pdf... Complete
Appending UCSC tracks... Complete

** FINISHED ON Thu Mar 31 14:12:43 2016 **
```
---  
### Example B  
**Multiple Samples on the Same x-Axis**  
CNView can plot up to 300 samples stacked atop the same shared x-axis. This is accomplished by passing a text file with the IDs of all samples to be plotted in place of the sample ID.  
![Multiple Samples on the Same x-Axis](/ExamplePlots/CNView.ExamplePlotB.jpg?raw=true ""Multiple Samples on the Same x-Axis"")  
**Example code to generate the above plot:**  
```
bash$ ./CNView.R 19 47636953 47687179 \
                 ~/sampleIDs.txt
                 ~/cov_matrix.bed \
                 ./ExamplePlots/CNView.ExamplePlotB.pdf \
                 --title ""Example Plot B: Multiple Samples on the Same x-Axis"" \
                 -w 50000 \
                 --ymin -6 \
                 --ymax 10 \
                 --probs
```
In this example, we're visualizing a 51kb duplication of *SAE1* that occurred *de novo* in the child (top panel, sample SFARI_d13874p1) and is present in neither the father (middle panel, SFARI_d13874fa) nor the mother (bottom panel, SFARI_d13874mo). We're also using a few more options to illustrate how to control the plot parameters:  
- ```~/sampleIDs.tx``` is the path to the text file containing all sample IDs to be plotted (max of 300 samples/plot, which is an R system default, although this can be overwritten if needed).  The contents of the file for this plot were:  
```
bash$ cat ~/sampleIDs.txt
SFARI_d13874p1
SFARI_d13874fa
SFARI_d13874mo
```  
- ```-w 50000``` tells CNView to pad 50kb of additional space to the plotting window on both sides of the specified interval. By default, the plotting window will be padded with [61.8%](https://en.wikipedia.org/wiki/Golden_ratio) of the size of the interval.  
- ```--ymim``` and ```--ymax``` override the default y axes, which is most useful when plotting multiple samples simultaneously to get a relative.  

And here's the expected stdout text:  
```
+-------------------+
| CNView Visualizer |
|     (c) 2016      |
+-------------------+
Sample ID file 'SFARI_d12529p1' not found, assuming single sample ID provided
Attempting to connect to UCSC Genome Browser... Complete
Filtering & loading coverage matrix... Complete
Compressing coverage matrix [1,000 bp bins]...  Complete
Performing intra-sample normalization... Complete
Performing inter-sample normalization... Complete
Plotting samples to ./ExamplePlots/CNView.ExamplePlotA.pdf... Complete
Appending UCSC tracks... Complete

** FINISHED ON Thu Mar 31 14:12:43 2016 **
```
---  
### Example C  
**Multiple Highlighted Intervals**  
CNView also allows for customization of the highlighted interval(s), which can be useful if you have multiple regions of interest being visualized in the same plot.  
![Multiple Highlighted Intervals](/ExamplePlots/CNView.ExamplePlotC.jpg?raw=true ""Multiple Highlighted Intervals"")    
**Example code to generate the above plot:**  
```
bash$ ./CNView.R 3 8820980 8860556 \
                 ~/sampleIDs_2.txt \
                 ~/cov_matrix.bed \
                 ./ExamplePlots/CNView.ExamplePlotC.pdf \
                 --highlight ~/highlights.txt \
                 --title ""Example Plot C: Multiple Highlighted Intervals"" \
                 --ymin -3 \
                 --ymax 7  \
                 --probs  
```
Here, we're visualizing a 39.6kb [paired-duplication inversion, a.k.a. ""dupINVdup""](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4571023/figure/fig1/), wherein two duplications flank the ends of an underlying inversion.  The distal (left-most) duplication is very small (611bp; highlighted in orange), so is only marginally visible at our 1kb bin resolution, while the proximal (right-most) duplication is quite clear (36.9kb; highlighted in blue).  The custom highlighting is achieved by passing CNView the ```--highlight``` option with a corresponding three-column, tab-delimited file. Here, the contents of that file, ```~/highlights.txt```, were:  
```
bash$ cat ~/highlights.txt 
8820980        8821591        darkorange
8823650        8860556        blue
```  
The first two columns correspond to the intervals to highlight, and the third column corresponds to the R color to shade the interval. Also note that, if ```-p```/```--probs``` is specified in conjunction with ```-h```/```--highlight```, separate probabilites are computed for each interval in the highlight parameters file.  



Also, like before we've plotted a full family trio (child/father/mother), but unlike [Example B](https://github.com/RCollins13/CNView#example-b) this variant is evidently inherited from the mother (child is top panel and mother is bottom).  The contents of the sample file, ```~/sampleIDs_2.txt```, were:  
```
bash$ cat ~/sampleIDs_2.txt
SFARI_d14637p1
SFARI_d14637fa
SFARI_d14637mo
```  

The output to stdout for this call should be:  
```
+-------------------+
| CNView Visualizer |
|     (c) 2016      |
+-------------------+
Reading sample IDs from /Users/collins/sampleIDs_2.txt
Attempting to connect to UCSC Genome Browser... Complete
Filtering & loading coverage matrix... Complete
Compressing coverage matrix [10,000 bp bins]...  Complete
Performing intra-sample normalization... Complete
Performing inter-sample normalization... Complete
Plotting samples to ./ExamplePlots/CNView.ExamplePlotC.pdf... Complete
Appending UCSC tracks... Complete

** FINISHED ON Thu Mar 31 14:13:22 2016 **
```
---  
## Example D  
**Disabled UCSC Annotations**  
If you like to keep things low-profile or don't have an active internet conenction, you can disable the UCSC annotations with ```-u```/```-noUCSC``` and tell the verbose output to shut up with ```-q```/```-quiet```.  
![Disabled UCSC Annotations](/ExamplePlots/CNView.ExamplePlotD.jpg?raw=true ""Disabled UCSC Annotations"")    
**Example code to generate the above plot:**  
```
bash$ ./CNView.R 2 178714141 178760307 SFARI_d12529p1 \
                 ~/cov_matrix.bed \
                 ./ExamplePlots/CNView.ExamplePlotD.pdf \
                 --title ""Example Plot D: Disabled UCSC Annotations"" \
                 --noUCSC \
                 --quiet
```  
This example is an identical repeat of [example A](https://github.com/RCollins13/CNView#example-a), just slimmed down to save on UCSC connections and stdout spam.
",0,0.55,0.55,,,,,,0,3,,,
81385821,MDEwOlJlcG9zaXRvcnk4MTM4NTgyMQ==,ucsc_pslpostarget,Yating-L/ucsc_pslpostarget,0,Yating-L,https://github.com/Yating-L/ucsc_pslpostarget,,0,2017-02-08 23:12:55+00:00,2018-05-16 22:45:21+00:00,2018-05-16 22:45:20+00:00,,77,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['Yating-L'],,"Galaxy wrapper for UCSC pslPosTarget tool (version: v340)
==========================================================

flip psl strands so target is positive and implicit

usage:
-------

   $ pslPosTarget inPsl outPsl

options:

Source code:
-------------

http://hgdownload.cse.ucsc.edu/admin/exe/

Licence
-------
Please note that commercial download and installation of the Blat and In-Silico PCR software may be licensed through Kent Informatics (http://www.kentinformatics.com).
",0,0.79,0.79,,,,,,0,1,,,
135633470,MDEwOlJlcG9zaXRvcnkxMzU2MzM0NzA=,graphs,a-windisch/graphs,0,a-windisch,https://github.com/a-windisch/graphs,Coding project for C++ for C Programmers course on coursera,0,2018-05-31 20:43:14+00:00,2021-06-21 16:12:40+00:00,2018-05-31 20:44:01+00:00,,10,1,1,C++,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,master,1,['a-windisch'],,"#  Dijkstra's shortest path and Jarnik-Prim minimum spanning tree
### course project 'C++ for C Programmers Part A', University of California Santa Cruz, provided through coursera 
### Andreas Windisch, andreas.windisch@yahoo.com 

## 1. Files contained in this repository

- README.md - This file.
- Makefile - Makefile to produce the executables
- shortes_path.cpp - This program implements Dijkstra's shortes path and demonstrates the algorithm by applying it to an example. 
- minimum_spanning_tree.cpp - This program implements the Jarnik-Prim algorithm for computing the MST for a (connected, undirected) graph. 
- mst_data.dat - A datafile containing a graph. This file is used by the MST program to compute the MST on this graph.

## 2. Short description of the project

This is a naive implementation of Dijkstra's shortest path and the Jarnik-Prim minimum spanning tree. I also started implementing Kruskal's algorithm, but the implementation is incomplete. Feel free to use to code as you please, EXCEPT for getting course credit on coursera. If you have any questions regarding this problem, feel free to contact me using andreas.windisch (at) yahoo.com.

## 3. Building the program

On a linux machine, just open a console, cd into the directory where the files are strored, and run:
```bash
$ make
```
This should produce two executables, 'shortest_path' and 'minimum_spanning_tree'.
To run the two (independent) programs type:
```bash
$ ./shortest_path
```
and/or
```bash
$ ./minimum_spanning_tree
```



",1,0.68,0.68,,,,,,0,0,,,
844516597,R_kgDOMlZM9Q,42-CPP_Modules,itislu/42-CPP_Modules,0,itislu,https://github.com/itislu/42-CPP_Modules,My beginnings with C++ - 12th project in the 42 Common Core Curriculum,0,2024-08-19 12:25:27+00:00,2025-03-04 18:36:15+00:00,2025-03-04 18:36:11+00:00,,487,0,0,C++,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['itislu'],,"# Table of Contents

- [**💡 Mini Articles**](#-mini-articles)
- [**📚 Resources**](#-resources)
- [**⚙️ Automatic Commit Labels**](#%EF%B8%8F-automatic-commit-labels)

---

# 💡 Mini Articles

- ### [Copy-and-Swap Idiom](cpp04#copy-and-swap-idiom)
- ### [Ever wondered why there is `delete` AND `delete[]`?](cpp04#ever-wondered-why-there-is-delete-and-delete)

---

# 📚 Resources

| Description                                                                                                                                                                            | Type                          | Link                                                                                                                                   |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| **Learn C++** <br> A site with well structured lessons for absolute beginners. Goes over all concepts.                                                                                 | ~300 short Lessons            | [learncpp.com](https://www.learncpp.com)                                                                                               |
| **C++ Overview using the C++98 Standard** <br> One of the few tutorials using the C++98 standard.                                                                                      | ~ 25 medium length Articles   | [cplusplus.com](https://cplusplus.com/doc/oldtutorial/)                                                                                |
| **C++ Super-FAQ** <br> A great FAQ about C++ ranging from newbie questions, learning C++ when coming from C, basic and advanced features of C++, coding standards to language updates. | FAQ (100s of questions)       | [isocpp.org](https://isocpp.org/faq)                                                                                                   |
| **C++ Core Guidelines written by Bjarne Stroustrup himself** <br> Very useful to read about certain features of the language and for best practices, dos and don'ts.                   | Knowledge Base (~25 sections) | [isocpp.github.io](https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines)                                                       |
| **42 Intra eLearning Videos** <br> Teaches the concepts needed for all modules. One video for each concept.                                                                            | ~40 Videos                    | [elearning.intra.42.fr](https://elearning.intra.42.fr/tags/38/notions)                                                                 |
| **Harvard C++ Quick Overview over New Features compared to C** <br> Concise introduction to C++ concepts with really clear code examples.                                              | 1 Article                     | [read.seas.harvard.edu](https://read.seas.harvard.edu/cs161/2021/doc/cplusplus/)                                                       |
| **C++ tutorial for C users** <br> A tutorial for C programmers who wish to learn C++, showing basic principles of C++. Lots of small code snippets.                                    | 1 Article                     | [ericbrasseur.org](http://www.ericbrasseur.org/cppcen.html)                                                                            |
| **C++ for C Programmers - University of California, Santa Cruz** <br> A university professor goes over the benefits of C++ compared to C in a slow and easy to understand way.         | Video (8h 44min)              | [youtube.com](https://www.youtube.com/watch?v=Mkg2fpKD1KI)                                                                             |
| **C++ Programming Wikibook** <br> Easy to read, Wikipedia-style book. Great start when researching a new concept or feature.                                                           | Online-Book <br> (~650 pages) | [wikibooks.org](https://en.wikibooks.org/wiki/C%2B%2B_Programming)                                                                     |
| **A Tour of C++ - Book** <br> A short book that serves as an overview of all of standard C++, written by Bjarne Stroustrup.                                                            | Book <br> (~200 pages)        | [PDF](https://github.com/Kikou1998/textbook/blob/master/A%20Tour%20of%20C%2B%2B%20(2nd%20Edition)%20(C%2B%2B%20In-Depth%20Series).pdf) |
| **C++ Primer (5th Edition) - Book** <br> In-depth tutorial and reference book.                                                                                                         | Book <br> (~900 pages)        | [PDF](https://github.com/yanshengjia/cpp-playground/blob/master/cpp-primer/resource/C%2B%2B%20Primer%20(5th%20Edition).pdf)            |
| **The C++ Programming Language - Book** <br> Hardcore reference book written by Bjarne Stroustrup.                                                                                                                                                                         | Book <br> (~1300 pages)       | [PDF](https://chenweixiang.github.io/docs/The_C++_Programming_Language_4th_Edition_Bjarne_Stroustrup.pdf)                                              |
| **C++ Weekly With Jason Turner** <br> Weekly videos about C++ (and Computer Science in general).                                                                                       | 400+ short-form Videos        | [youtube.com](https://www.youtube.com/@cppweekly)                                                                                      |

---

# ⚙️ Automatic Commit Labels

This alias automatically puts the numbers of the current CPP module and the current exercise at the very front as the scope of the commit. The numbers are extracted from the current directory.

**Example:**<br>
You are in the directory `cpp00/ex00` -> the scope would be `00/00`.

The alias also conveniently formats your commit messages according to the [Conventional Commits](https://www.conventionalcommits.org/) - first a keyword, then the message.<br>
It puts the first argument as the type of the commit, everything else as the message. The type is usually one of the following: `feat`, `fix`, `refactor`, `style`, `test`, `docs`, `chore`. The message is a short description of the commit.

### Usage

This is how you can use the alias:

```bash
git cml feat Convert program arguments to uppercase
```

**Result:**

```
[00/00] feat: Convert program arguments to uppercase
```

### Setup

To set this alias up globally for your git configuration, run the following command.<br>
If you prefer to set the alias up just for your CPP repository (so not globally across all repositories), simply remove the `--global` option.

```bash
git config --global alias.cml '!f() {
    local exercise_path=$(echo ""$GIT_PREFIX"" | sed '""'""'s/^..\///'""'""' | grep -oE '""'""'.*(/|^)ex[0-9]{2}(/|$)'""'""');
    local module=$(dirname ""$exercise_path"" | xargs basename | sed '""'""'s/^\.$//'""'""');
    local exercise=$(basename ""$exercise_path"");
    if [ -z ""$module"" ] && [ -z ""$exercise"" ]; then
        module=$(basename ""$GIT_PREFIX"");
    fi;
    local module_num=$(echo ""$module"" | grep -oE '""'""'[0-9]+'""'""');
    local exercise_num=$(echo ""$exercise"" | grep -oE '""'""'[0-9]{2}'""'""');
    local type=$1;
    shift;
    local message=""$@"";
    local scope="""";
    if [ -n ""$module_num"" ] && [ -n ""$exercise_num"" ]; then
        scope=""[$module_num/$exercise_num] "";
    elif [ -n ""$module"" ]; then
        scope=""[$module] "";
    elif [ -n ""$exercise"" ]; then
        scope=""[$exercise] "";
    fi;
    if [ -n ""$message"" ]; then
        git commit -m ""$scope$type: $message"";
    else
        git commit -m ""$scope$type"";
    fi;
}; f'
```

## Pre-populate the scope in the commit message file

If you prefer to use `git commit` to edit your commit messages in your editor, you can use a git hook to automatically pre-populate the scope in the commit message file.

This means when you `git commit`, the file that opens already has the scope of your commit in the first line.

### Setup

1. **Make sure you are in the root of your repository.**

2. Run the following command:
   ```bash
   cat << 'EOF' >> .git/hooks/prepare-commit-msg
   #!/bin/sh

   COMMIT_MSG_FILE=$1
   COMMIT_SOURCE=$2
   SHA1=$3

   # Remove the ""# Please enter the commit message..."" help message.
   /usr/bin/perl -i.bak -ne 'print unless(m/^. Please enter the commit message/..m/^#$/)' ""$COMMIT_MSG_FILE""

   # Only use if it's a regular commit without -m or -F
   case ""$2,$3"" in
           ,|template,)
                   # Prepend the scope to the commit message file
                   exercise_path=$(echo ""$GIT_PREFIX"" | sed 's/^..\///' | grep -oE '.*(/|^)ex[0-9]{2}(/|$)')
                   module=$(dirname ""$exercise_path"" | xargs basename | sed 's/^\.$//')
                   exercise=$(basename ""$exercise_path"")
                   if [ -z ""$module"" ] && [ -z ""$exercise"" ]; then
                           module=$(basename ""$GIT_PREFIX"")
                   fi
                   module_num=$(echo ""$module"" | grep -oE '[0-9]+')
                   exercise_num=$(echo ""$exercise"" | grep -oE '[0-9]{2}')
                   scope=""""
                   if [ -n ""$module_num"" ] && [ -n ""$exercise_num"" ]; then
                           scope=""[$module_num/$exercise_num] ""
                   elif [ -n ""$module"" ]; then
                           scope=""[$module] ""
                   elif [ -n ""$exercise"" ]; then
                           scope=""[$exercise] ""
                   fi
                   sed -i ""1i$scope"" ""$COMMIT_MSG_FILE""
                   ;;
           *) ;;
   esac
   EOF
   chmod +x .git/hooks/prepare-commit-msg
   ```
",0,0.63,0.63,,,,,,0,1,,,
701580867,R_kgDOKdFGQw,ucsc-snowflake,erangalds/ucsc-snowflake,0,erangalds,https://github.com/erangalds/ucsc-snowflake,,0,2023-10-07 01:14:08+00:00,2024-09-27 12:43:42+00:00,2024-09-27 12:43:39+00:00,,16654,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['erangalds'],,,0,0.85,0.85,,,,,,0,1,,,
358315829,MDEwOlJlcG9zaXRvcnkzNTgzMTU4Mjk=,slugstore-ecommerce,breteo/slugstore-ecommerce,0,breteo,https://github.com/breteo/slugstore-ecommerce,The UCSC Bookstore Revamped,0,2021-04-15 15:57:31+00:00,2024-03-29 03:06:26+00:00,2021-06-05 23:38:00+00:00,,2687,0,0,JavaScript,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,"['breteo', 'eMar2000', 'vgupta-1', 'ankurgupta16']",,"# ecom
By: Brendan Teo, Vipul Gupta, Eric Mar


IMPORTANT INSTRUCTIONS FOR TAs: 

-Note that for CSE 183, we are submitting our py4web zipped file as our submission in the Google form, as we were not able to upload to the cloud. Download and use this zip file from the google form we will submit, as its database is prepopulated with two users and ten specific ebook entries with their own unique information per ebook. Here are some important steps for the TAs for running this project in their own py4web server to test out the site with the zip file that will be submitted in the Google form:

1. Download the zip file

2. Open it and place it in the py4web apps folder.

3. EXTREMELY IMPORTANT: type the command ""pip install stripe"" in the directory of this project folder. It might not matter where you put the command, as long as you install stripe entering this command in your terminal. 

4. Reload py4web in your editor, or terminal. 

5. go to the url using the project name and the py4web server, and the site should appear to the login page, where you can login and test the site.

*Emphasis on installing stripe using ""pip install stripe"", this is so the transaction function works. If you do not do this, simply trying to access the site will give you a 404 not found error.




This project is an ebook ecommerce site, from which users can make purchases for selected ebooks. This website was developed using the py4web server, with the back end being in python using sqlite for the databases, and the front end using html and javascript.


The main pages featured on this site are a home page, an index/browsing page, a shopping cart page, a wishlist page, and an info page for each ebook. There is also a purchase button that takes the user to a transaction page that is from the Stripe API.


Important features of our website using vue: 

1. The search bar uses vue, and will show suggested titles from the database based on the substring entered in the search bar. Clicking on the title will redirect to the ebook's specific info page.

2. The review and ratings system works so that the site can have general reviews and ratings on the home page and each individual ebook's reviews and ratings show up on its own specific info page. This info page with display the correct info based on ebook clicked on or title searched for in the search bar. The full review can be deleted by the user who submits the review and the rating is only editable by the user who submitted the rating. All ratings and review are visible to all users.

3. The purchase button uses the Stripe API in order to conduct a transaction for buying ebooks. The shopping cart is cleared once a transaction is complete. Only Stripe testing credit card numbers can be used currently, because the transaction is in testing mode currently. Here is a link to testing credit cards that can be used in the stripe page: https://stripe.com/docs/testing


Overall, our goal was to build an ebook ecommerce website and we feel like we have created something that very closely resembles that with this py4web site. 


Useful Resources: Unit 15, 17, 20, and doing hw5 for the CSE 183 class at UCSC really helped us implement a lot of the major pieces of our site. We started with starter code from this class and built our pages from there. The starter code basically allows users to log in and create profiles. 


Testing: Testing for this project was done on mac and windows, using the py4web server and the browsers Firefox, Google Chrome, and Microsoft edge.



Work Allocation:

Eric: Front end

Brendan: Back end

Vipul: Back end 


Also small note from Vipul Gupta: commits made by github account ankurgupta16 were actually by me. The git config settings in my laptop were set to my brother's account as my laptop was set up with a backup of his laptop. Corrected this midway through project, which can be seen in the commit history.

",1,0.78,0.78,,,,,,0,2,,,
165684596,MDEwOlJlcG9zaXRvcnkxNjU2ODQ1OTY=,csmd,liuyu8721/csmd,0,liuyu8721,https://github.com/liuyu8721/csmd,A computational pipeline for high-resolution profiling of low abundance microbiome in clinical samples using whole genome shotgun sequencing,0,2019-01-14 15:26:08+00:00,2021-10-11 07:23:25+00:00,2019-05-30 07:32:07+00:00,,53549,7,7,R,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,7,master,1,['liuyu8721'],,"Computation Subtraction-based Microbiome Discovery (CSMD)
====
CSMD is a computational pipeline for high-resolution profiling of low abundance microbiome in clinical samples using whole genome shotgun sequencing.
## 1. Pre-requisite software
Table 1. List of pre-requisite software and the available information

|No.            |Software       |Version        | Availability |
| ------------- |-------------|-------------| -----       |
|1|BWA|0.7.15|http://bio-bwa.sourceforge.net/|
|2|SAMtools|1.4|http://www.htslib.org/|
|3|bedtools|2.26.0|https://bedtools.readthedocs.io/en/latest/|
|4|seqtk|1.2|https://github.com/lh3/seqtk|
|5|RepeatMasker|4.0.7|http://repeatmasker.org/|
|6|PathoScope 2.0|0.02|https://sourceforge.net/projects/pathoscope/|
|7|R|3.3.3|https://www.r-project.org/|
|8|countreg|0.2-0|https://r-forge.r-project.org/projects/countreg/|
|9|Bowtie2|2.2.1|https://sourceforge.net/projects/bowtie-bio/files/bowtie2/|
|10|fastq-tools|0.8|https://github.com/dcjones/fastq-tools|
|11|blast|2.6.0+|https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastDocs&DOC_TYPE=Download|
## 2. Database availability
Table 2. List of pre-download databases and the available information

|No.|Databases|Availability|
|----|----|----|
|1|Human reference genome (hg38)|https://genome.ucsc.edu/cgi-bin/hgGateway?db=hg38|
|2|Three assembled human genomes available on NCBI: HuRef, YH and BGIAF.|ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/002/125/GCA_000002125.2_HuRef<br/> ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/004/845/GCA_000004845.2_YH_2.0<br/> ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/005/465/GCA_000005465.1_BGIAF</br>|
|3|Repbase|https://www.girinst.org/|
|4|Ensembl Homo sapiens cDNA database|ftp://ftp.ensembl.org/pub/current_fasta/homo_sapiens/cdna/|
|5|NCBI Homo sapiens RNA database|ftp://ftp.ncbi.nih.gov/genomes/H_sapiens/RNA/|
|6|NCBI BLAST human genome database|ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/|
|7|NCBI non-redundant nucleotide sequences (nt)|ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/|
|8|NCBI RefSeq Bacteria database|ftp://ftp.ncbi.hlm.nih.gov/genomes/refseq/bacteria|
|9|Taxonomy files|https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz|
## 3. Computational pipeline
### Step 3.1: A four-phase human-derived sequence subtraction.
* Phase I: Subtract reads from standard human reference genome (hg38)
* Phase II: Subtract reads from three additional assembled human genomes (AHG), including HuRef, YH and BGIAF
* Phase III: Subtract low complexity reads (LCR)
* Phase IV: Subtract reads from three extra human sequence databases (EHG), including Ensembl Homo sapiens cDNA database, NCBI Homo sapiens RNA database, and NCBI BLAST human genome database
### Step 3.2: A four-phase microbiome discovery procedure.
* Phase I: Generate an initial redundant reference library that may include all the species genomes of interest.
* Phase II: Identify a list of alternatives of candidate genomes through fast similarity search against the initial library.
* Phase III: Screen out genomes with significantly insufficient coverage and do the species correction for possibly misidentified genomes using BLAST analysis.
* Phase IV: Perform genome refinement through analyzing their coverage structure.
### Step 3.3: Per-sample microbiome profiling.
## 4.Installation
### 4.1 Download
Download the code from https://github.com/liuyu8721/CSMD. 
You could issue the following command to extract the files: ""unzip csmd-master.zip"".

### 4.2 Configuration
1. Make sure the pre-requisite software listed in Table 1 has been installed and available in the runtime environment. They should be first configured in 'config.sh', for example:
```shell
export PATH=/public/software/bwa/v0.7.15/bin:$PATH
```
2. Change all *.sh and csmd file to be executable: 
```shell
chmod +x *.sh; chmod +x csmd
```
3. Add csmd into your PATH available: 
```shell
export CSMD_HOME=/public/users/liuyu/csmd_test/code/csmd-master 
export PATH=$PATH:${CSMD_HOME}
```
### 4.3 Databases
CSMD will work with a series of libraries listed in Table 2, including human-related genomes or sequences (21G) and all RefSeq bacteria genomes (150G, as of November 2018). The build process will then require approximately 500GB of additional disk space and 200GB of RAM. These genomes or sequences can be found in DBPATH/hg38/SEQ, DBPATH/AHG/SEQ, DBPATH/EHG/SEQ and DBPATH/RefSeq/bacteria/SEQ, respectively. And the indexed files will be saved in DBPATH/hg38, DBPATH/AHG, DBPATH/EHG and DBPATH/RefSeq/bacteria, respectively.
```shell
csmd --download-library libname --db DBPATH
```
NOTE:<BR/>
--download-library libname: Permissible libname includes “hg38”, “AHG”, “EHG”, “nt”, or “RefSeqBac”.
       --db DBPATH: the store path for the download.
```shell
csmd --build-library libname --db DBPATH
```
NOTE: <BR/>
--build-library: Permissible libname includes “hg38”, “AHG”, “EHG”, “nt”, or “RefSeqBac”. This command generates csmd needed indexed databases, BWA index for hg38 and AHG databases, Bowtie2 index for EHG and RefSeq representative species databases, and blast index for nt and representative genomes databases. To obtain RepBase, go to http://www.girinst.org.<BR/>
--db DBPATH: the store path for the databases, as described above.
### 4.4 Taxonomy files
Taxonomy files include the taxonomic lineage of taxa, information on type strains and material, and host information. This command will download the taxonomy files from NCBI and re-build it according to csmd running. These files can be found in DBPATH/taxonomy/taxdump/ and the re-build file will be saved in DBPATH/taxonomy/taxtree.txt. The files taxdb.tar.gz (ftp://ftp.ncbi.nlm.nih.gov/blast/db/taxdb.tar.gz) for blast and GenBank2RefSeq.txt in the CSMD download page(/data) for GenBank acceccion number translation should also be saved in DBPATH/taxonomy.
```shell
csmd --download-taxonomy --db DBPATH
csmd --build-taxonomy --db DBPATH
```
NOTE:<BR/>
--download-taxonomy: download taxdump.tar.gz from NCBI.<BR/>
--build-taxonomy: re-organize the taxonomy tree.
## 5. Running CSMD
### 5.1 CS step
This CS procedure performs sensitive and specific computational subtraction of human DNA from the clinical samples. Post-QC paired-end fastq data are assumed as the initial input and indexed human-related genomes or sequences are assumed to be available.
```
csmd --cs hg38Removal --ref REFNAME --thread NUMBER --r1 SAMPLE.R1.fastq --r2 SAMPLE.R2.fastq --output SAMPLE.hg38removal.fastq
csmd --cs ahgRemoval --ref REFNAME --thread NUMBER --input SAMPLE.hg38removal.fastq --output SAMPLE.hg38removal.AHGremoval.fastq
csmd --cs lcrRemoval --ref REFNAME --thread NUMBER --input SAMPLE.hg38removal.AHGremoval.fastq --output SAMPLE.hg38removal.AHGremoval.LCRremoval.fasta
csmd --cs ehgRemoval --ref REFNAME --thread NUMBER \
--input SAMPLE.hg38removal.AHGremoval.LCRremoval.fasta --output SAMPLE.hg38Removal.AHGRemoval.LCRRemoval.EHGRemoval.fasta

```
NOTE:<BR/>
--cs: cs step detail name. Permissible step name includes “hg38Removal”, “ahgRemoval”, “lcrRemoval”, or “ehgRemoval”.<BR/>
--ref: the reference name with the path used for human-derived reads alignment and removal.<BR/>
--thread: number of threads (CPUs) to use.<BR/>
--r1, --r2, --input: the input file with the path. For hg38Removal, the input should be paired-end reads using the parameters “--r1” and “--r2”, but for others, using the parameter “--input"". In the phase of hg38Removal, paired-end reads will be combined into single-end data after finishing hg38 reads alignment and removal. The format of the input files is illustrated as the examples.<BR/>
--output: the output file with the path. The format of the output files is illustrated as the examples.<BR/>
### 5.2 MD step
This MD procedure develops a comprehensive and minimally non-redundant reference database using pooled data from the study samples. To overcome the limitations of microbial identification from samples with low microbial biomass and to maximize detection power, all putatively non-human reads in the study group are combined as the input in this step. It includes three key sub-steps to make the microbiome finding as accurate as possible: species finding, species correction and species refinement.
```shell
csmd --md finding --ref REFNAME --thread NUMBER --input pooled_nonHuman.fasta --outdir OUTDIR
csmd --md correction --sam csmd_finding.sam --report csmd_finding_report.tsv \
-–cutoff 25 --thread NUMBER --nt NTNAME --refseq RSNAME --taxdir TAXDIR --outdir OUTDIR
csmd --md refinement --seqdir SEQPATH --seqlist DBLIST --thread NUMBER --input pooled_nonHuman.fasta --outdir OUTDIR
```
### 5.3 Profile step
This procedure provides accurate taxonomy classification for each sample based on a mapping of metagenomic reads against the comprehensive and minimally non-redundant reference database generated in the MD step.
```shell
csmd --pf dbsetup --seqdir SEQPATH --seqlist DBUPDATE --outdir OUTDIR
csmd --pf profile --ref REFNAME --thread NUMBER \
--input SAMPLE.hg38Removal.AHGRemoval.LCRRemoval.EHGRemoval.fasta --sam SAMPLE.csmd.sam –-report SAMPLE.csmd.profile.report
```
NOTE: --pf: profile step detail name. Permissible step name includes “dbsetdup” for the indexed CSMD database and “profile” for single sample microbiome profiling.<BR/>
--seqdir, --seqlist: see MD step.<BR/>
--sam: reads alignment detail in SAM format.<BR/>
--report: pathoscope style report in tsv format.<BR/>
",0,0.59,0.59,,,,,,0,2,,,
16149522,MDEwOlJlcG9zaXRvcnkxNjE0OTUyMg==,MIPGEN,shendurelab/MIPGEN,0,shendurelab,https://github.com/shendurelab/MIPGEN,One stop MIP design and analysis,0,2014-01-22 19:16:12+00:00,2024-12-13 06:54:46+00:00,2020-09-30 05:55:39+00:00,,16613,23,23,C++,1,1,1,1,1,0,19,0,0,5,other,1,0,0,public,19,5,23,master,1,"['augustboyle', 'aaronmck']",1,"MIPGEN
======

One stop MIP design and analysis

Use MIPgen to design custom mip panels for target enrichment of moderate to high complexity DNA targets ranging from 120 to 250bp in size

To compile MIPgen, you will need a C++ compiler, such as GCC (http://gcc.gnu.org/). That's it! For running design and analysis, see below for dependencies

-----
BUILD MIPGEN
-----

To build MIPgen from source, simply enter the MIPGEN directory and type 'make'

The 'mipgen' executable can be used to perform designs

-----
MIPGEN PARAMETERS
-----

MIPgen accepts many options intended to maximize the flexibility of your designs

The only required parameters are the minimum and maximum sizes for the captured sequence, a prefix for output files and a BED file consisting of the coordinates of the desired targets, and the bwa- and samtools-indexed reference genome from which to pull the sequences

Other options provide control over tiling, scoring and oligo design behavior

MIPgen offers three scoring methods based on two methods of scoring: SVR and logistic regression. Both methods offer similar performance, with slight improvements seen with the SVR scoring. The 'mixed' scoring option will perform designs primarily with the faster logistic regression score and finish with the more accurate SVR scoring scheme.

Run ./mipgen -doc to see the full documentation or read the text below

-----
MIPGEN PITFALLS
-----

-It is not recommended to include more than 8bp total of degenerate smMIP tags. Longer tags reduce specificity and complicate MIP rebalancing, leading to less complete coverage of targeted sites.

-Priority scores (used by default) modulate tiling behavior and density of tiling of low scoring regions. To reduce density, lower priority scores. In particular, for interrogation of very short loci (<10bp), priority scores should be set to 0 to prevent splitting the region into two MIP targets.

-Regions that do not map uniquely to the provided reference are not tiled by default. These and other bases that cannot be tiled will be reported in the <project_name>.coverage_failed.bed file.

-SVR scores below 1.4 and logistic scores below 0.7 rarely perform adequately. It is recommended to remove these probes from designed assays.

-The failure flags in the design files are a series of three bits. They represent, in order, non-uniqueness of the MIP target with respect to the provided genome sequence, inability to design SNP MIPs to any/all of the variants provided in the SNP file, and tandem repeats being detected in the MIP arms using Tandem Repeats Finder. SNP failures will be included in final probe sets. The others will not, assuming non-uniqueness is selected and a TRF executable is provided

-MIPgen does not remove candidate MIPs with overly high targeting arm copy numbers if there are no suitable alternatives to cover the desired regions. Be advised that copy numbers above 200 are likely to poisson your assay and should be removed unless you are specifically interrogating all such low complexity sites.

-----
OTHER NOTES
-----

-the subset of Boost required for compilation is included in the directory

-the makefile should compile the source without any external dependencies, but the executable that is generated makes system calls to SAMtools, BWA, Tabix, and, optionally, Tandem Repeats Finder

-the tools subdirectory has scripts for parsing fastqs prior to mapping and collapsing tag-defined read groups (TDRGs) and trimming targeting arms after mapping (Cython required)

-summary information is also printed as part of the collapsing process and requires NumPy and SciPy

-if there is at least 10bp of overlap between forward and reverse reads, it is recommended to merge read pairs into fr-reads using PEAR ( http://bioinformatics.oxfordjournals.org/content/early/2013/11/10/bioinformatics.btt593.long ) prior to processing and to utilize single end options

-see the HemoMIPs 2020 analysis pipeline for analyzing MIP sequencing data with html reports and full genotyping results: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007956
 
-----
GETTING STARTED
-----

Below is an example for getting MIPgen up and running and designing probes to EGFR, TERT and BRAF. 
The output generated from these commands is included in the repository (mipgen_example.tgz). 
A copy of the human reference genome (GRCh37, also known as hg19 in UCSC) is available at the 1000Genomes FTP site:

ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz

The file used in the example to pull out gene exons for the hg19 human reference (refGene.txt) can be downloaded here: 

http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/


First extract the tar file to make a MIPgen directory

    tar -xf mipgen.tgz

Enter the directory to compile the source code

    cd MIPGEN/

Use make to compile -- make sure you have a C++ compiler

    make

Make a directory for practicing designs

    mkdir ../mipgen_practice
    cd ../mipgen_practice

Create a new file and type a list of gene symbols you would like to tile
with MIPs

    vim practice_genes.txt

You can use one of the scripts in the tools subdirectory and a refGene text
file listing gene exons to pull out the genomic coordinates of your genes of
interest

    sh ../MIPGEN/tools/extract_coding_gene_exons.sh practice_genes.txt ../refGene.txt > practice_genes.bed

Check to make sure the coordinates are what you expect and that no errors
occurred

    less practice_genes.bed

Now you can perform designs with MIPgen!
Here is a design with very basic options.
Set the path to the genome reference and change the name of the FASTA file if yours is different.
Make sure you have the dependencies installed or accessible through a given
path (BWA, tabix, samtools)!

    ../MIPGEN/mipgen -regions_to_scan practice_genes.bed -project_name practice_design -min_capture_size 162 -max_capture_size 162 -bwa_genome_index /<path to genome reference fasta file>/human_g1k_v37.fa

The final selection of MIPs is located in the picked MIPs file
review the scores to make sure the MIPs stand a good chance of success
(logistic scores below 0.6 are unlikely to provide usable data)

    less practice_design.picked_mips.txt

By default all tested MIPs are output; this is a lot of output! (Turn it off with the silent_mode option)

    rm practice_design.all_mips.txt

sai, fq (and sam) files are not deleted automatically and can also take up
space

    rm -f *.sai *.fq

Generate a UCSC track with another tools script to visualize online

    python ../MIPGEN/tools/generate_ucsc_track.py practice_design.picked_mips.txt practice_ucsc_track

Look at the other files in this directory to see designs for TERT, BRAF and
EGFR (we have not tested these designs experimentally so we cannot precisely
assess predicted performance for these probes)

-----
DESCRIPTION OF ALL OPTIONS
-----
    usage: mipgen (<-parameter_name> <parameter_value>)*
    Created by Evan Boyle (boylee@uw.edu)
    Required parameters:

    -project_name                   prefix for output
                                    ex: /my/output/directory/batch_1
    -bwa_genome_index               samtools- and bwa-indexed genome reference file path
    -regions_to_scan                BED file of positions GUARANTEED to be captured
                                    regions will be merged as needed
    -min_capture_size               integer value for length of targeting arms plus insert region to be captured
                                    tested down to 120
    -max_capture_size               integer value for length of targeting arms plus insert region to be captured
                                    tested up to 250
    Oligo control:

    -arm_lengths                    manual setting of individual possible ligation and extension arm lengths
                                    format: <extension arm length 1>:<ligation arm length 1>,<extension arm length 2>:<ligation arm length 2>,...
                                    ex: 16:24,16:25,16:26
    -arm_length_sums                setting of arm lengths to include all pairs of arm lengths adding to a certain length
                                    default is 40,41,42,43,44,45
                                    format: <sum of extension and ligation arm lengths 1>,<sum of extension and ligation arm lengths 2>,...
                                    ex: 40,45
    -ext_min_length                 minimum length of extension arm
                                    default is 16
    -lig_min_length                 minimum length of ligation arm
                                    default is 18
    -tag_sizes                      specifies degenerate tag (Ns) to place in extension arm and ligation arm
                                    more than 8 bases severely reduces on-target rate of capture
                                    default is 5,0
                                    format: <extension arm tag size>,<ligation arm tag size>
                                    ex: 4,4
    -masked_arm_threshold           fraction of targeting arms having masked bases (floating point number from 0 to 1)
                                    to tolerate before optimizing with respect to masking (requires TRF executable)
                                    default is 0.5
    -target_arm_copy                threshold over which minimizing copy number to the reference takes priority
                                    default is 20
    -max_arm_copy_product           maximum permissible product of targeting arm copy number to the reference
                                    default is 75
    Tool dependencies:

    -tabix                          tabix build
                                    default is ""tabix""
    -bwa                            bwa build
                                    default is ""bwa""
    -trf                            tandem repeats finder executable
                                    default is ""off""
                                    providing a TRF executable enables filtering of simple repeats in probe arms
    Input options:
    -genome_dir                     genome reference directory helpful for large jobs
                                    ex: /my/genome/split/by/chromosomes/
    -snp_file                       path to vcf file of snps to avoid during design
    -common_snps                    providing ""off"" will disable loading of common SNPs to avoid from NCBI using Tabix
                    DEPRECATED:ONLY SNP FILE OPTION NOW SUPPORTED
    -file_of_parameters             file containing any of the above parameters in the following format:
                                            -first_parameter_name first_parameter_value
                                            -second_parameter_name second_parameter_value
                                            <etc.>
                                            lines that do not start with '-' are ignored
    Tiling control:

    -feature_flank                  integer value for extending BED coordinates
                                    default is 0
    -capture_increment              integer step size for examining capture sizes
                                    default is 5
                                    starts at maximum and descends until minimum is reached or
                                    acceptable score found, whichever is first
    -logistic_heuristic             providing ""off"" will remove assumptions to reduce search space
    -max_mip_overlap                integer specifying the maximum number of nucleotides of overlap
                                    to test before selecting a MIP
                                    default is 30
    -starting_mip_overlap           integer specifying the starting number of nucleotides of overlap
                                    to use when linearly tiling with MIPs
                                    default is 0
    -check_copy_number              providing ""off"" enables targeting of non-unique sites in the reference
    -seal_both_strands              providing ""on"" disallows targeting arms occupying the same base
                                    positions on opposite strands
    -half_seal_both_strands         providing ""on"" disallows a second targeting arm from occupying more than half
                                    of the positions of a targeting arm placed on the other strand
    -double_tile_strand_unaware     providing ""on"" will ensure that all positions in targeted regions will be tiled twice
    -double_tile_strands_separately providing ""on"" will ensure that all positions in targeted regions will be tiled twice,
                                    at least once on each strand
    Scoring parameters:

    -score_method                   ""logistic"" will use the logistic model for scoring (default)
                                    ""svr"" will switch behavior to score with the svr model
                                    ""mixed"" will first filter with logistic scoring and finalize with the svr model
    -logistic_optimal_score         value of score metric at which to stop optimizing MIP score and
                                    accept current MIP, lower scores lead to fewer outliers
                                    default is 0.98
    -svr_optimal_score              analogous to logistic_optimal_score
                                    default is 2.2
    -logistic_priority_score        value of score metric below which MIPs are placed nonlinearly to
                                    optimize score and SNPs are tolerated
                                    default is 0.9,lower values enable more efficient linear tiling
    -svr_priotity_score             analogous to logistic_priority_score
                                    default is 1.5 for SVR
    Miscellaneous:

    -silent_mode                    providing ""on"" will reduce volume of text output
    -download_tabix_index           providing ""on"" will force redownload of common snp tbi file
                                        DEPRECATED
    -bwa_threads                    make use of BWA's multithreading option (-t _)
                                    default is 1

-----
TERMS OF USE AND LICENSING INFORMATION
-----

Copyright 2014 University of Washington.
 
Permission is hereby granted, to You to use, copy, modify, merge the MIPgen software and associated documentation files (the ""Software"") for the use in Your non-commercial academic and research activities. This includes electronic distribution for the use of client software, but only within Your organization. You may not otherwise distribute the Software, including any modified versions. You may not sublicense any rights under this license and You may not sell copies of the software.
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
 
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,  DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 
Contact license@uw.edu for any uses not covered under this license.

© University of Washington 2014
",0,0.52,0.52,,,,,,0,21,,,
84258079,MDEwOlJlcG9zaXRvcnk4NDI1ODA3OQ==,rugby,nguyenkevin16/rugby,0,nguyenkevin16,https://github.com/nguyenkevin16/rugby,A rails/react app for the UCSC Men's Rugby Team. (In Progress),0,2017-03-07 23:52:41+00:00,2017-06-14 07:24:27+00:00,2017-06-14 07:26:43+00:00,,1686,0,0,Ruby,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['nguyenkevin16'],,"# UCSC Rugby

[UCSC Rugby][rugby] is a mockup for the UC Santa Cruz Men's Rugby Club,
and is built on Ruby on Rails and React.js/Redux. It serves as an example for
the current template site hosted at [ucscrugby.com](ucscrugby.com).

[rugby]: http://ucsc-rugby.herokuapp.com/#/

![main-sceenshot](./docs/rugby-main.png)",1,0.83,0.83,,,,,,0,0,,,
120686464,MDEwOlJlcG9zaXRvcnkxMjA2ODY0NjQ=,UCSC_Python,jmiw/UCSC_Python,0,jmiw,https://github.com/jmiw/UCSC_Python,UCSC_PythonBeginnerCourse,0,2018-02-07 23:47:14+00:00,2018-02-08 00:41:44+00:00,2018-02-08 04:57:43+00:00,,2,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['jmiw'],,,1,0.76,0.76,,,,,,0,0,,,
19685683,MDEwOlJlcG9zaXRvcnkxOTY4NTY4Mw==,BallotBox,mscorca/BallotBox,0,mscorca,https://github.com/mscorca/BallotBox,"BallotBox, CMPS 121 Project, UCSC",0,2014-05-12 04:19:33+00:00,2014-06-10 00:14:38+00:00,2014-06-10 00:14:38+00:00,,5876,0,0,Java,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,"['mscorca', 'b-aston']",,"BallotBox
=========

BallotBox, CMPS 121 Project, UCSC
",1,0.77,0.77,,,,,,0,3,,,
514971461,R_kgDOHrHXRQ,Shuangjie-Zhang,shuang-jie/Shuangjie-Zhang,0,shuang-jie,https://github.com/shuang-jie/Shuangjie-Zhang,,0,2022-07-17 22:41:57+00:00,2022-07-17 22:42:54+00:00,2022-08-29 22:49:55+00:00,,20058,0,0,TeX,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,0,main,1,"['gcushen', 'github-actions[bot]', 'shuang-jie', 'Anish-D', 'cfroehli', 'jpawlowski', 'mrueg', 'michaelaye', 'napulen']",,"
### Hi there, I'm [Shuangjie Zhang!](https://shuangjiezhang.netlify.app)

I am pursuing a Ph.D. in Statistics surpervised by [Juhee Lee](https://engineering.ucsc.edu/people/juheelee) at [University of California Santa Cruz](https://www.ucsc.edu/).

**I’m currently working on** 
- 1. Bayesian high-dimensional modeling
- 2. Multivariate count table
- 3. Application to microbiome study and ecological study. 

**How to reach me**
- Email: szhan209 [AT] ucsc.edu

**News & Travel**:

- **Sep 2022**: O'Bayes at UCSC
- **August 2022**: I am going to attend JSM 2022. Although not presenting, hope to meet you there!
",1,0.72,0.72,,,,,,0,1,,,
293459391,MDEwOlJlcG9zaXRvcnkyOTM0NTkzOTE=,MBDseq_DnmtKO_mm10,mbk0asis/MBDseq_DnmtKO_mm10,0,mbk0asis,https://github.com/mbk0asis/MBDseq_DnmtKO_mm10,BigWig files for UCSC upload,0,2020-09-07 07:52:19+00:00,2020-12-14 06:35:25+00:00,2020-09-09 06:15:30+00:00,,404038,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['mbk0asis'],,# MBDseq_DnmtKO_mm10,0,0.46,0.46,,,,,,0,1,,,
813839489,R_kgDOMII0gQ,Enhanced_TransUNet-Integrating-Dual-Attention-and-CutMix-for-Medical-Image-Segmentation,marzianizam/Enhanced_TransUNet-Integrating-Dual-Attention-and-CutMix-for-Medical-Image-Segmentation,0,marzianizam,https://github.com/marzianizam/Enhanced_TransUNet-Integrating-Dual-Attention-and-CutMix-for-Medical-Image-Segmentation,"A class project for CSE244C at UCSC, which is focused on enhancing the TransUnet model for improved medical image segmentation. This project aims to integrate advanced features like Dual Attention mechanisms and CutMix data augmentation to improve medical image segmentation.",0,2024-06-11 21:04:07+00:00,2024-06-28 20:29:48+00:00,2024-06-28 20:29:45+00:00,,6782,0,0,Jupyter Notebook,1,1,1,0,0,0,0,0,0,0,apache-2.0,1,0,0,public,0,0,0,main,1,['marzianizam'],,"# Enhanced TransUNet : Integrating Dual-Attention-and-CutMix for Medical Image Segmentation

This repository is part of a final project for the ""Deep Learning for Advanced Computer Vision 224C"" course at the University of California, Santa Cruz. It reproduces and enhances the TransUNet model, as detailed in the paper [""TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation""](https://arxiv.org/pdf/2102.04306). The project's initial phase involved training a baseline TransUNet model, followed by targeted modifications to improve its segmentation performance.

_Authors : Marian Zlateva (mzlateva@ucsc.edu) and Marzia Binta Nizam(manizam@ucsc.edu)_
<br><br>
_Details of our experiments can be found [here](https://github.com/marzianizam/Enhanced_TransUNet-Integrating-Dual-Attention-and-CutMix-for-Medical-Image-Segmentation/blob/main/Enhanced_TransUNet___Integrating_Dual_Attention_and_CutMix_for_Medical_Image_Segmentation.pdf)_

# Overview
This study focuses on enhancing the TransUNet model for medical image segmentation by improving its generalization capabilities. Initially, the baseline TransUNet model achieved a Mean Dice score of 0.769 and a median Hausdorff Distance of 32.87. By incorporating Channel Attention, Dual Attention mechanisms, and CutMix data augmentation, significant improvements were made, culminating in a Mean Dice score of **0.823** and a reduced median Hausdorff Distance of **19.74**. These strategic modifications have enhanced the model's ability to accurately segment complex anatomical structures, advancing the application of medical imaging.

## Results Summary

The following table summarizes the performance improvements across various organs:

| Model                          | Average Dice | Median HD95 | Aorta  | Gallbladder | Kidney (L) | Kidney (R) | Liver  | Pancreas | Spleen | Stomach |
|--------------------------------|--------------|-------------|--------|-------------|------------|------------|--------|----------|--------|---------|
| TransUNet  (Baseline)           | 0.769        | 32.87       | 0.868  | 0.596       | 0.814      | 0.740      | 0.945  | 0.542    | 0.873  | 0.778   |
| TransUNet  (ours)               | **0.823**        | **19.74**       | **0.882**  | **0.631**       | **0.860**      | **0.831**      | **0.946**  | **0.693**   | **0.907**  | **0.833**   |

*Note: All values are rounded to three decimal places for clarity.*


Below are the segmentation visualizations comparing the baseline and enhanced models:

![Segmentation Comparison](https://github.com/marzianizam/Enhanced_TransUNet-Integrating-Dual-Attention-and-CutMix-for-Medical-Image-Segmentation/blob/main/Result_Viz.png)

*Figure 1: Visual comparison of segmentation performance between baseline TransUNet and TransUNet with Dual Attention and CutMix.*


# Environment Setup

Please prepare an environment with python=3.7, and then use the command (following the original TransUnet repo)

```bash
pip install -r requirements.txt
```

You can view our environment specification [here](https://github.com/marzianizam/Enhanced_TransUNet-Integrating-Dual-Attention-and-CutMix-for-Medical-Image-Segmentation/blob/main/our_environment_spec.txt).

# Download Google pre-trained ViT models

Download the Google pre-trained ViT models following the original repository's instruction.

```bash
wget https://storage.googleapis.com/vit_models/imagenet21k/{MODEL_NAME}.npz &&
mkdir ./model/vit_checkpoint/imagenet21k &&
mv {MODEL_NAME}.npz ./model/vit_checkpoint/imagenet21k/{MODEL_NAME}.npz
```

# Data

The experiments were conducted on the Synapse multi-organ segmentation dataset. Please refer to the [original repository](https://github.com/Beckschen/TransUNet/blob/main/datasets/README.md) for the data preparation. 

 # Train

 Run the train script on synapse dataset. We used batch size 8 due to our limited GPU access, but the original code supports multiple GPUs as well.

 ```bash
CUDA_VISIBLE_DEVICES=0 python train.py --dataset Synapse --vit_name R50-ViT-B_16
```

# Test

Run the test script on synapse dataset. It supports testing for both 2D images and 3D volumes. 

 ```bash
python test.py --dataset Synapse --vit_name R50-ViT-B_16 --is_savenii
```
You can download our trained model from [here](https://drive.google.com/file/d/15dHKTUUtyOZbAIW83FfIuDbIIEhTWxnw/view?usp=sharing) to test. 

# Visualization

Please refer to this [notebook](https://github.com/marzianizam/Enhanced_TransUNet-Integrating-Dual-Attention-and-CutMix-for-Medical-Image-Segmentation/blob/main/visualization.ipynb) for visualizing the predictions.

# Reference

* [TransUNet](https://github.com/Beckschen/TransUNet/tree/main)
* [Google ViT](https://github.com/google-research/vision_transformer)
* [ViT-pytorch](https://github.com/jeonsworld/ViT-pytorch)
* [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch)
* [DA-TransUnet](https://github.com/SUN-1024/DA-TransUnet/tree/main)


# Acknowledgement

We appreciate the developers of [TransUNet](https://github.com/Beckschen/TransUNet/tree/main) and the provider of the [Synapse multi-organ segmentation](https://www.synapse.org/#!Synapse:syn3193805/wiki/217789) dataset. We are grateful to Professor [Yuyin Zhou](https://campusdirectory.ucsc.edu/cd_detail?uid=yzhou284) for her invaluable guidance and insightful suggestions throughout the duration of this project. :smiley: :smiley:


",1,0.74,0.74,,,,,,0,1,,,
512979826,R_kgDOHpNzcg,Courses,thanhtcptit/Courses,0,thanhtcptit,https://github.com/thanhtcptit/Courses,Courses resources,0,2022-07-12 03:02:00+00:00,2024-10-11 00:10:57+00:00,2023-06-21 08:56:20+00:00,,215596,3,3,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,3,main,1,['thanhtcptit'],,"# Courses Resources

## Course List
- [Bayesian Statistics Specialization - University of California, Santa Cruz](https://www.coursera.org/specializations/bayesian-statistics)
- [Building Cloud Computing Solutions at Scale Specialization - Duke University](https://www.coursera.org/specializations/building-cloud-computing-solutions-at-scale)
- [DevOps on AWS Specialization - Amazon Web Services](https://www.coursera.org/specializations/aws-devops)
- [Digital Signal Processing Specialization - EPFL](https://www.coursera.org/specializations/digital-signal-processing)
- [Mathematics for Machine Learning Specialization - Imperial College London](https://www.coursera.org/specializations/mathematics-machine-learning)
- [Natural Language Processing Specialization - DeepLearning.ai](https://www.coursera.org/specializations/natural-language-processing)
- [Parallel, Concurrent, and Distributed Programming in Java Specialization - Rice University](https://www.coursera.org/specializations/pcdp)
- [Practical Time Series Analysis - The State University of New York](https://www.coursera.org/learn/practical-time-series-analysis)
- [Recommender Systems Specialization - University of Minnesota](https://www.coursera.org/specializations/recommender-systems)
- [Statistics with Python Specialization - University of Michigan](https://www.coursera.org/specializations/statistics-with-python)
- [Mathematical Logic - JAIST](https://dlc-lms.jaist.ac.jp/moodle/course/view.php?id=2722)
- [Information Theory - JAIST](https://dlc-lms.jaist.ac.jp/moodle/course/view.php?id=2714)
- [System Optimization - JAIST](https://dlc-lms.jaist.ac.jp/moodle/course/view.php?id=2628)
- [Discrete Signal Processing - JAIST](https://dlc-lms.jaist.ac.jp/moodle/course/view.php?id=2630)
- [Functional Programming - JAIST](https://dlc-lms.jaist.ac.jp/moodle/course/view.php?id=2591)
- [Computer Network - JAIST](https://dlc-lms.jaist.ac.jp/moodle/course/view.php?id=2577)
",0,0.59,0.59,,,,,,0,1,,,
402163269,MDEwOlJlcG9zaXRvcnk0MDIxNjMyNjk=,ca-mpa,NCEAS/ca-mpa,0,NCEAS,https://github.com/NCEAS/ca-mpa,OPC California MPA project,0,2021-09-01 18:25:54+00:00,2025-03-01 16:48:23+00:00,2025-03-01 16:48:19+00:00,,660272,4,4,HTML,1,1,1,1,0,0,1,0,0,2,,1,0,0,public,1,2,4,main,1,"['joshgsmith', 'lopazanski', 'cfree14', 'brunj7', 'jacobeurich']",1,"# Decadal Review of California's Marine Protected Area (MPA) Network

This is the GitHub repository for an [NCEAS](https://www.nceas.ucsb.edu/) Working Group leading the [Decadal Review of California's Marine Protected Area (MPA) Network](https://wildlife.ca.gov/Conservation/Marine/MPAs/Management/Decadal-Review). This work is funded by the OPC and CDFW.

## Repository structure

The GitHub repository contains all of the code associated with the project and the figures produced by this code. 

The data for the project are kept in a Google Drive folder outside the GitHub repository. Non-confidential data will be made available in a public data repository upon project completion.

The GitHub repository has the following structure

* **data:** This folder contains subfolders for each dataset used in the project. The subfolders contain scripts that clean and visualize the data. Figures for visualizing each dataset are contained within a *figures* folder inside each subfolder.
* **analyses:** This folder contains subfolders that contain the analyses conducted for each paper. Paper subteams may structure the contents of these subfolders in whatever manner is best for them.

## Repository best practices

We ask that contributors adhere to the following best practices when working in the repository:

1. Use lower case for all folders and file names
2. Use underscores for spaces (don't use hyphens or spaces)

## Working group members

The Working Group is led by Jenn Caselle and Kerry Nickols and is a collaboration among the following members and institutions:

* Joshua Smith, Nat'l. Center for Ecol. Anal. & Synth. (NCEAS)
* Chris Free, UC Santa Barbara
* Clarissa Anderson, UC San Diego
* Cori Lopazanski, UC Santa Barbara
* David Gill, Duke University
* David Mouillot, U. Montpellier
* Jacob Eurich, EDF
* Jenn Caselle, UC Santa Barbara
* Jenny Dugan, UC Santa Barbara
* Joachim Claudet, French National Centre for Scientific Research
* Julien Brun, UC Santa Barbara
* Kerry Nickols, CSU Northridge
* Kristin Kaschner, U. Freiburg
* Mark Carr, UC Santa Cruz
* Peter Raimondi, UC Santa Cruz
* Rick Starr, Moss Landing Marine Laboratories
* Scott Hamilton, Moss Landing Marine Laboratories
* Shelby Ziegler, U. Georgia
* Tessa Francis, U. Washington
",1,0.82,0.82,,,,,,0,8,,,
86613873,MDEwOlJlcG9zaXRvcnk4NjYxMzg3Mw==,thesis,adamnovak/thesis,0,adamnovak,https://github.com/adamnovak/thesis,My Ph.D. thesis for UCSC,0,2017-03-29 18:05:07+00:00,2018-09-27 16:27:13+00:00,2017-06-07 19:15:04+00:00,,2598,0,0,TeX,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['adamnovak'],,"# Adam Novak's Thesis

This is my thesis for my Ph.D. in bioinformatics. If you want to make a pull 
request to write it for me, go right ahead.

",1,0.67,0.67,,,,,,7,1,,,
124591355,MDEwOlJlcG9zaXRvcnkxMjQ1OTEzNTU=,BioFeatureFinder,kbmlab/BioFeatureFinder,0,kbmlab,https://github.com/kbmlab/BioFeatureFinder,"BioFeatureFinder: Flexible, unbiased analysis of biological characteristics associated with genomic regions",0,2018-03-09 20:45:34+00:00,2024-11-12 17:23:04+00:00,2019-09-17 18:39:34+00:00,,377490,7,7,Python,1,1,1,1,0,0,1,0,0,3,,1,0,0,public,1,3,7,master,1,"['fciamponi', 'mlovci']",1,"# BioFeatureFinder: Flexible, unbiased analysis of biological characteristics associated with genomic regions

## Description.

BioFeatureFinder is an algorithm desiged with the goal of uncovering latent biological relationships from sets of genomic coordinates obtained from modern high-throughtput sequencing experiments and subsequent analysis pipelines (i.e. coordinates for protein binding sites or alternatively spliced exons). Designed with flexibility in mind, this algorithm can be used with any species which has a genomic sequence available (gene annotation is highly recommended, but not required) and is compatible with both Python 2.7 and 3.4 in most UNIX-based systems. Due to the modular structure of the algorithm, it can also be easily modified to include novel sources of data and/or analytical functions on demand by reasearchers.

## Scripts:

* ./biofeatures/scripts/analyze_features.py

Main script used for analyzing biological features associated with groups of exons. Uses .bed files of exon coordinates to compare with annotated exons in the data matrix (created by build_datamatrix.py), runs KS statistic to filter non-significant differences and then uses GradientBoost classifier to determine wich features are more “important” in group separation (input vs background).

* ./biofeatures/scripts/build_datamatrix.py

Script used for creating the data matrix for biological features associated with exons and their neighouring regions. Uses a .GTF annotation, genome FASTA, .BW and .BED files as input for BioFeatures. Can be modified to include other functions on demand.

* ./biofeatures/extract_gtf_regions.py

Script used to extract each region (CDS, UTR, Splice Site, Exon, Intron and etc...) from the refference annotation file. By default, it uses the 3rd column of the GTF (Feature) to identify which regions are present in the annotation and extract them. For extracting intronic annotation, it requires the existence of both ""gene"" and ""exon"" features in the annotation.

* ./biofeatures/scripts/analyze_gtf_regions.py

Script used to compare an input BED annotation with multiple GTF references, to indentify preferential regions.

## Dependency Installation

### With anaconda (recommended):

    conda install argparse glob2 matplotlib numpy pandas pybedtools pysam rpy2 scipy seaborn setuptools scikit-learn system
    
### External dependencies (must be available on PATH)
    
    * EMBOSS - http://emboss.sourceforge.net/download/
    * ViennaRNA - https://www.tbi.univie.ac.at/RNA/
    * QGRS Mapper - https://github.com/freezer333/qgrs-cpp
    * BEDTools - http://bedtools.readthedocs.io/en/latest/

## Scripts installation

### Install scripts

    pip install .

### Development install, local changes are reflected in command-line calls (recommended)

    pip install -e .

## Authors

    Felipe E. Ciamponi (GitHub: Fciamponi)
    Michael T. Lovci (GitHub: mlovci)
    Pedro R. S. Cruz
    Katlin B. Massirer

## Funding

BioFeatureFinder was funded by the São Paulo Research Foundation (FAPESP grants 12/00195-3, 13/50724-5, 14/25758-6, 15/25134-5, 16/25521-1).
    
## Testing dataset

For testing of the algorith, we've included a sub-sample of the RBFOX2 RNA-binding protein eCLIP dataset (approx. 10% of the raw dataset). For the purpose of this tutorial, we will use only the phastCons scores for multiple alignments of 99 vertebrate genomes to the human genome (100way), which can be obtained directly in bigWig format (for obtaining and processing the 46way dataset, please follow the ""Converting multiple wig into single bigWig"" section).
    
    #Enter the ""hg19_data"" inside the ""test_data"" folder
    cd /full/path/to/BioFeatureFinder/test_data/hg19_data/
    
    #Download the hg19 (GRCh37.p13) from GENCODE ftp website
    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_19/GRCh37.p13.genome.fa.gz
    
    #Download the 100way phastCons scores from UCSC goldenPath
    wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/phastCons100way/hg19.100way.phastCons.bw
    
    #(OPTIONAL) Enter the ""hg19_annotations"" folder and extract 
    #the GTF regions from the Homo sapiens GRCh37.p13 annotation 
    #from Ensembl (with chameleon for UCSC-style)
    cd ./hg19_annotations
    extract_gtf_regions.py -g ./Homo_sapiens.GRCh37.p13.chameleonUCSC.gtf.gz -o grch37 --intron  
    
    #Go back to ""test_data"" folder
    cd /full/path/to/BioFeatureFinder/test_data/    
    
    #Run region analysis to identify preferential occurence region for input dataset
    analyze_gtf_regions.py \
    -i ./rbfox2_sample.bed \
    -r ./hg19_data/hg19_annotations/grch37.* \
    -l 3pss 3utr 5pss 5utr cds intron \
    -o rbfox2_test_regions
    
    #Run the ""build_datamatrix"" script to create extract biological features from the files
    build_datamatrix.py \ #load the main script
    -i ./rbfox2_sample.bed \ #set your input set of bed coordinates
    -gen ./hg19_data/GRCh37.p13.genome.fa \ #Load the genomic sequence
    -g ./hg19_data/hg19_annotations/grch37.intron.gtf.gz \ #Load the GTF file 
    -cs ./hg19_data/hg19.100way.phastCons.bw \
    -var ./hg19_data/hg19_var/* \
    --fasta \
    -k 4 5 6 \
    --rnafold \
    --qgrs \
    --keepBED \
    --keepTEMP \
    -o rbfox2.test
    
    #Run the ""analyze_features.py"" script to identify significant features 
    analyze_features.py \
    -i ./rbfox2_sample.bed \
    -m ./rbfox2.test.datamatrix/rbfox2.test.datamatrix.tsv \
    -o rbfox2.test \
    -p bonferroni \
    -mi 0.01 \
    -c 0.8 \
    -s



## Converting multiple wig into single bigWig

After installing BioFeatureFinder with ""pip install -e ."" and any dependencies, please also install the following binaries from UCSC utilities directory (http://hgdownload.soe.ucsc.edu/admin/exe/):

    wigToBigWig 
    bigWigMerge 
    bedGraphToBigWig 

Input files for BioFeatureFinder test dataset include the human genome sequence (hg19 assembly) and bigWig files with phastCon scores for conservation. Both of these files are too large to be included in GitHub, so we need to download and process these files to be used by BFF. This can either be done manually (following the steps below) or using some type of automated script.

1 - Download chromosome sizes from UCSC:

    wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.chrom.sizes

2 - Download the md5 checksum for the vertebrate conservation files:

    wget -O md5.vertebrate.txt http://hgdownload.soe.ucsc.edu/goldenPath/hg19/phastCons46way/vertebrate/md5sum.txt

3 - Download each wig file from the md5 and extract them:
    
    cut -f 3 -d "" "" md5.vertebrate.txt |xargs -P 1 -L 1 -I % wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/phastCons46way/vertebrate/% 
    gunzip ./*.wigFix.gz

4 - Remove files without data (can be identified with something like ""ls -lah""):
    
    rm chrUn_gl000226.phastCons46way.wigFix

5 - Convert wig files to bigWig:
    
    ls *.wigFix | xargs -P 1 -L 1 -I % wigToBigWig ./% ./hg19.chrom.sizes ./%.bw 

6 - Merge the multiple bigWig files in a single bedGraph and sort it by coordinates:
    
    bigWigMerge ./*.bw hg19.phastCons46way.vertebrates.bg
    bedtools sort -i ./hg19.phastCons46way.vertebrates.bg > ./hg19.phastCons46way.vertebrates.srt.bg
    rm ./*.bw

7 - Convert the bedGraph back into a bigWig binary file and clean up:
    
    bedGraphToBigWig ./hg19.phastCons46way.vertebrates.srt.bg ./hg19.chrom.sizes ./hg19.phastCons46way.vertebrates.bw
    rm ./*.bg

8 - Repeat steps 2 to 7 for other levels of conservation (placentalMammals and primates).
    
## Change logs

* v1.1.4 - analyze_features.py now plots additional visualization options (total of 4: CDF, violin plot, histogram and KDE) in a single multi-page PDF file; analyze_gtf_regions now outputs a clustermap with rudimentary hierarchical clustering using centroid algorithm.
* v1.1.3 - Added early-stopping to GradientBoostClassifier.
* v1.1.2 - Added mutual information score (MI) and linear correlation as feature selection tools.
* v1.1.1 - Fixed issue with hyperparameter tuning when using 10-fold cross validation.
* v1.1.0 - Re-structure of main scripts to support both Python 3.4 and 2.7.
* v1.0.3 - Added test dataset and readme file.
* v1.0.2 - Fixed bug in classifier metrics barchart plotting.
* v1.0.1 - Fixed bug in KS test.
* v1.0.0 - Original BioFeatureFinder algorithm.
",0,0.57,0.57,,,,,,0,3,,,
78124025,MDEwOlJlcG9zaXRvcnk3ODEyNDAyNQ==,mend_qc,UCSC-Treehouse/mend_qc,0,UCSC-Treehouse,https://github.com/UCSC-Treehouse/mend_qc,Count Mapped Exonic Non-Duplicate (MEND) reads in an RNA Seq bam file,0,2017-01-05 15:21:19+00:00,2024-12-13 17:13:02+00:00,2025-01-13 17:16:05+00:00,,4423,4,4,R,1,1,1,1,0,0,0,0,0,3,mit,1,0,0,public,0,3,4,master,1,"['hbeale', 'rcurrie']",1,"# MEND QC
Calculates the number of Mapped Exonic Non-Duplicate (MEND) reads in a bam file containing RNA-Seq data.

## Overview
[samblaster](https://github.com/GregoryFaust/samblaster) is used to mark duplicates and [sambamba](http://lomereiter.github.io/sambamba/) is used to sort. Then [RSeqQC](http://rseqc.sourceforge.net/) calculates the reads distribution over exons skipping reads marked qc_failed, PCR duplicate, Unmapped, Non-primary (or secondary). The [MEND qc](https://github.com/UCSC-Treehouse/mend_qc) script parseReadDist.R estimates the number of MEND reads by counting tags in CDS exons, 5' UTR exons and 3' UTR exons and multiplying by reads per tag. 

## Output
* readDist.txt: The output of RSeqQC read_distribution.py (~1kb)
* bam_mend_qc.tsv: MND, MEND and treehouse_compendium_qc (PASS/FAIL)
* bam_mend_qc.json: Same as bam_mend_qc.tsv but in json format
* sortedByCoord.md.bam: BAM with duplicates marked sorted by coordinate
* sortedByCoord.md.bam.bai: Index for sortedByCoord.md.bam

## Running 
via Docker:

```
docker run --rm \
  -v <path to bam file>:/inputs/sample.bam \
  -v <path to output>:/outputs \
  -v <path to tmp space>:/tmp \
  ucsctreehouse/bam-mend-qc \
    /inputs/sample.bam \
    /outputs
```

Optionally, specify a bed file as the third argument (after ""/outputs""). The bed file needs to be formatted as specified by RSeQC. If not specified, is uses a bed file containing exon definitions specified by GENCODE_v23_basic in hg38 coordinates.

Note: Intermediate bam files are created under /tmp within the docker container.

Directly:

```
run.sh <path to bam> <path to output folder> <optionally: path to bed file>
```

NOTE: See Dockerfile for installation of required libraries

## Example expected stdout
```
Sorting by name...
Marking duplicates...
samblaster: Version 0.1.24
samblaster: Inputting from stdin
samblaster: Outputting to stdout
samblaster: Loaded 195 header sequence entries.
samblaster: Marked 1 of 999 (0.10%) read ids as duplicates using 1340k memory in 0.005S CPU seconds and 0S wall tim
e.
Sorting by coordinate...
Writing sorted chunks to temporary directory...
[==============================================================================]
Counting reads...
processing /ref/hg38_GENCODE_v23_basic.bed ... Done
processing /tmp/TEST.bam.sortedByCoord.md.bam ... Finished

[1] ""analyzing /data/readDist.txt""
Read 3 items
```
",1,0.58,0.58,,,,,,0,1,,,
134286709,MDEwOlJlcG9zaXRvcnkxMzQyODY3MDk=,MosaicForecast,douym/MosaicForecast,0,douym,https://github.com/douym/MosaicForecast,A mosaicSNV detecting software based on phasing and random forest,0,2018-05-21 15:18:40+00:00,2022-05-07 03:44:38+00:00,2022-09-10 06:49:28+00:00,,2014788,3,3,Python,0,1,1,1,0,0,3,0,0,0,mit,1,0,0,public,3,0,3,master,1,"['douym', 'danielmsk']",,"# MosaicForecast
A machine learning method that leverages read-based phasing and read-level features to accurately detect mosaic SNVs (SNPs, small indels) from NGS data. It builds on existing algorithms to achieve a multifold increase in specificity.

![MF_pipeline](https://user-images.githubusercontent.com/8002850/55032948-61016880-4fe8-11e9-8cf8-343fd3cdd26e.png)


## Dependency:
### Required Interpreter Versions:
* Python version 3.6+
* R version 3.5+
### Installation of Dependencies:
1. We have created a docker image with all dependencies installed:  
	https://hub.docker.com/r/yanmei/mosaicforecast  
	Usage:  
		docker image pull yanmei/mosaicforecast:0.0.1  
		docker run -v ${your_local_directory}:/MF --rm -it yanmei/mosaicforecast:0.0.1 /bin/bash  
		gunzip hs37d5.fa.gz  
		Phase.py /MF/demo/ /MF/demo/phasing hs37d5.fa /MF/demo/test.input 20 k24.umap.wg.bw 4 
```
Please note that ""${your_local_directory}:/MF"" is the absolute path of your local mosaicforecast directory. After attaching your local MF directory to the docker image, you would be able to read and write from that directory in your docker image. The attached directory in the docker image would be ""/MF"".
```
		
	 
2. You could also install conda first (https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh), and then create an environment using conda through this command:  
conda env create --name MF --file environment.yaml  
The environment 'MF' could be activated through this command:  
conda activate MF  
Other dependencies and resources could be downloaded though running:  
bash downloads.sh 

### Python packages (set up the environment mannually, not recommended):
* collections
* itertools
* subprocess
* multiprocessing
* regex
* uuid
* math 
* numpy (1.16.1)
* pandas (0.25.0)
* pyfaidx (0.5.3)
* pysam (0.15.2)
* pysamstats (1.1.2)
* scipy (1.2.1)
### R packages (set up the environment mannually, not recommended):
* caret (6.0-80)
* e1071 (1.7-0)
* glmnet (2.0-16)
* nnet (7.3-12)
* mlr (2.13)
* RColorBrewer (1.1.2)
* ggplot2 (2.3.0.0)
### Other softwares:
* samtools (1.9): https://sourceforge.net/projects/samtools/files/samtools/1.9/
* wigToBigWig (v4):  
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/wigToBigWig  
chmod +x wigToBigWig
* bigWigAverageOverBed (v2):  
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bigWigAverageOverBed  
chmod +x bigWigAverageOverBed  
* fetchChromSizes:  
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/fetchChromSizes  
chmod +x fetchChromSizes  

## Resources:
#### Human reference genome: 
* GRCh37/hg19: 
``` 
wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz   
wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz.fai   
```
* GRCh38/hg38: 
```
wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa 
wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa.fai 
```
#### Mappability score: 
`The wigToBigWig command line below take ~30GB memory, please make sure you requires enought memory before running.`

* Umap score (k=24, GRCh37/hg19):   
```
wget https://bismap.hoffmanlab.org/raw/hg19.umap.tar.gz  
tar -zxvf hg19.umap.tar.gz  
cd hg19  
fetchChromSizes hg19> hg19.chrom.sizes  
wigToBigWig <(zcat k24.umap.wg.gz) hg19.chrom.sizes k24.umap.wg.bw  
```
  
* Umap score (k=24, GRCh38/hg38):   
```
wget https://bismap.hoffmanlab.org/raw/hg38.umap.tar.gz  
tar -zxvf hg38.umap.tar.gz  
cd hg38  
fetchChromSizes hg38> hg38.chrom.sizes  
wigToBigWig <(zcat k24.umap.wg.gz) hg38.chrom.sizes k24.umap.wg.bw  
```

#### Regions to filter out:
* GRCh37/hg19: 

Segmental Duplication regions (should be removed before calling all kinds of mosaics):  
```
wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/genomicSuperDups.txt.gz
```
Regions enriched for SNVs with >=3 haplotypes (should be removed before calling all kinds of mosaics):  
```
wget https://raw.githubusercontent.com/parklab/MosaicForecast/master/resources/predictedhap3ormore_cluster.GRCh37.bed
```
Simple repeats (should be removed before calling mosaic INDELS):  
```
wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/simpleRepeat.txt.gz
```
* GRCh38/hg38 (please note that our model is trained under GRCh37 and the file ""predictedhap3ormore_cluster.GRCh38.bed"" is simply a liftover from the GRCh37 file above, hence could be un-optimized for GRCh38):  

Segmental Duplication regions (should be removed before calling all kinds of mosaics):  
```
wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/genomicSuperDups.txt.gz
```
Regions enriched for SNVs with >=3 haplotypes (should be removed before calling all kinds of mosaics):  
```
wget https://raw.githubusercontent.com/parklab/MosaicForecast/master/resources/predictedhap3ormore_cluster.GRCh38.bed
```

Simple repeats (should be removed before calling mosaic INDELS):  
```
wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/simpleRepeat.txt.gz
```
#### Population allele frequency
* gnomAD datasets (recommend to remove variants with population MAF>0.001%):  
https://gnomad.broadinstitute.org/downloads 
* from GATK bundle:
```
ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/
```

## How to run Mutect2-PON:
* Please refer to FAQ.md, we also have a Snakemake pipeline.

# Usage:
## Phasing:
**Usage:** 

python Phase.py bam_dir output_dir ref_fasta  input_positions min_dp_inforSNPs Umap_mappability(bigWig file,k=24) n_threads_parallel sequencing_file_format(bam/cram)

**Note:** 

1. Name of bam files should be ""sample.bam"" under the bam\_dir, and there should be index files under the same directory (samtools index sample.bam).   
2. There should be a fai file under the same dir of the fasta file (samtools faidx input.fa).
3. File format of the input\_positions: chr pos-1 pos ref alt sample, sep=\t 
4. The ""min\_dp\_inforSNPs"" is the minimum depth of coverage of trustworthy neaby het SNPs, can be set to 20.
5. The program to extract mappability score: ""bigWigAverageOverBed"" should be downloaded and installed, and its path should be added to the PATH environment variable.

**Demo:**
```
python Phase.py demo demo/phasing ${human_g1k_v37_decoy.fasta} demo/test.input 20 ${k24.umap.wg.bw} 2
```
**Output:**
```
output_dir/all.phasing
```
| sample | chr | pos | ref | alt | phasing | conflicting_reads | mappability | variant_type |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| test | 12 | 52644508 | C | T | hap=3 | 0 | 1.0 | SNP |
| test | 15 | 75918044 | G | A | hap=3 | 0 | 1.0 | SNP |
| test | 1 | 1004865 | G | C | hap=3 | 0 | 1.0 | SNP |
| test | 1 | 2591769 | AG | A | hap>3 | 1 | 0.0 | DEL |
| test | 1 | 33801576 | TTTGTTG | T | hap=2 | 0 | 0.583333 | DEL |

```
hap=2: likely het variants
hap=3: likely mosaic variants
hap>3: likely cnv/repeat
conflicting_reads: number of read pairs supporting both ref and alt alleles.

Intermediate files:
1. output_dir/all.merged.inforSNPs.pos: all nearby inforSNPs of candidate mosaics.
2. output_dir/all_2x2table: 2x2 tables by all nearby inforSNPs.
3. output_dir/all.phasing_2by2: Phasing results of mosaics and all nearby inforSNPs (2x2 table).
4. output_dir/multiple_inforSNPs.log: Phasing results of different pairs of inforSNPs.

```


## Extraction of read-level features:
**Usage:**

python ReadLevel_Features_extraction.py input.bed output_features bam_dir ref.fa Umap_mappability(bigWig file,k=24) n_jobs_parallel sequencing_file_format(bam/cram)

**Note:**
1. Names of bam files should be ""sample.bam"" under the bam_dir, and there should be index files under the same directory (samtools index sample.bam). Cram files are also supported.
2. There should be a fai file under the same dir of the fasta file (samtools faidx input.fa)
3. File format of the input.bed: chr pos-1 pos ref alt sample, sep=\t 
4. We did not use gnomad population AF as an feature (instead we use it to filter), but you can use it to train your model if you have interest in common variants
5. The program to extract mappability score: ""bigWigAverageOverBed"" should be downloaded and installed, and its path should be added to the PATH environment variable.

**Demo:**
```
python ReadLevel_Features_extraction.py demo/test.input demo/test.features demo ${ref.fa} ${k24.umap.wg.bw} 2 bam  
```
**Output:**
```
A list of read-level features for each input site.
```

| id | dp_p | conflict_num | mappability | type | length | GCcontent | ref_softclip | alt_softclip | querypos_p | leftpos_p | seqpos_p | mapq_p | baseq_p | baseq_t | ref_baseq1b_p | ref_baseq1b_t | alt_baseq1b_p | alt_baseq1b_t | sb_p | context | major_mismatches_mean | minor_mismatches_mean | mismatches_p | AF | dp | mosaic_likelihood | het_likelihood | refhom_likelihood | althom_likelihood | mapq_difference | sb_read12_p | dp_diff |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| test\~11\~40316580\~C\~T | 0.3183008162818 | 0 | 1 | SNP | 0 | 0.476190476190476 | 0.0240384615384615 | 0 | 0.1582 | 0.16521 | 0.68821 | NA | 0.91657 | -0.57364 | 0.98911 | 0.21893 | 0.67576 | -0.8528 | 0.69934 | GGA | 0.00878 | 0.0144466666666667 | 0.29396 | 0.028 | 214 | 0.999414559235067 | 3.90999117967593e-49 | 0.000585440764932926 | 0 | 0 | 0.69142 | -12.8571 |
| test\~12\~52644508\~C\~T | 0.197545792452075 | 0 | 1 | SNP | 0 | 0.571428571428571 | 0.0208333333333333 | 0 | 0.19325 | 0.20057 | 0.88251 | NA | 0.11764 | -0.95448 | 0.31536 | 0.6827 | 0.31601 | 0.58756 | 0.13401 | CGC | 0.01236 | 0.0127266666666667 | 0.17424 | 0.054 | 203 | 0.999999999985368 | 5.11687178601205e-39 | 1.46319954019795e-11 | 0 | 0 | 0.36124 | -12.8571 |

```
1. id: uniq ID of the input candidate sites.
2. mappability: UMAP mappability score at the candidate site (k=24).
3. type: type of the candidate mutation (SNP, MNP, INS or DEL).
4. length: difference of base pair lengh of ref and alt allele for candidate sites.
5. GCcontent: 20-bp local GCcontent.
6. ref_softclip: proportion of soft-clipped reads for ref reads.
7. alt_softclip: proportion of soft-clipped reads for alt reads.
8. querypos_p: p-value or effect size by wilcoxon's rank sum test of base query positions of ref and alt alleles.
9. leftpos_p: p-value or effect size by wilcoxon's rank sum test of left-most positions of ref and alt reads.
10. seqpos_p: p-value or effect size by wilcoxon's rank sum test of base sequencing cycles of ref and alt alleles.
11. baseq_p: p-value or effect size by Wilcoxon's rank sum test of base qualities of ref and alt alleles.
12. baseq_t: The test statistic under the large-sample approximation that the rank sum statistic is normally distributed (wilcox rank sum test of base qualites of alt alleles vs. ref alleles).
13. ref_baseq1b_p: p-value or effect size by Wilcoxon's rank sum test of base qualities from ref reads at mutant position, compared with base qualities from ref reads at 1bp downtream of the mutant position.
14. ref_baseq1b_t: The test statistic under the large-sample approximation that the rank sum statistic is normally distributed (wilcox rank sum test of base qualities from ref reads at mutant position, compared with base qualities from ref reads at 1bp downtream of the mutant position).
15. alt_baseq1b_p: p-value or effect size by Wilcoxon's rank sum test of base qualities from alt reads at mutant position, compared with base qualities from alt reads at 1bp downtream of the mutant position.
16. alt_baseq1b_t: The test statistic under the large-sample approximation that the rank sum statistic is normally distributed (wilcox rank sum test of base qualities from alt reads at mutant position, compared with base qualities from alt reads at 1bp downtream of the mutant position).
17. context: three-nucleotide base context on the reads surrounding the mutant position.
18. major_mismatches_mean: average mismatches per ref reads.
19. minor_mismatches_mean: average mismatches per alt reads.
20. mismatches_p: p-value or effect size by Wilcoxon's rank sum test of mismatches per ref reads vs. mismatches per alt reads.
21. sb_p: p-value or effect size by Fisher's exact test of strand bias for ref and alt alleles.
22. sb_read12_p: p-value or effect size by Fisher's exact test of read1/read2 bias for ref and alt alleles.
23. mosaic_likelihood: mosaic genotype likelihood calculated (assuming uniform distribution of mosaics allele fraction from 0-1).
24. het_likelihood: Genotype likelihood of the variant being germline heterozygous.
25. refhom_likelihood: reference-homozygous genotype likelihood.
26. mapq_p: p-value or effect size by Wilcoxon's rank sum test of mapping qualities of ref and alt reads.
27. mapq_difference: difference of average map quality per alt reads vs. average map quality per ref reads.
28. AF: variant allele fraction.
29. dp: read depth at mutant position.
30. dp_diff: difference of average read depths of local (<200bp) and distant (>2kb) regions.
31. dp_p: p-value or effect size by Wilcoxon's rank sum test of read depths sampled within 200bp window surrounding the mutant position vs. read depths sampled in distant regions from the mutant position (>2kb).
32. conflict_num: number of read pairs supporting both ref and alt alleles.
```

## Genotype Prediction:

**Usage:**

Rscript Prediction.R input\_file(feature\_list) model\_trained model\_type(Phase|Refine) output\_file(predictions)

**Note:**
1. The ""input\_file"" is a list of read-level features obtained in the last step.
2. The ""model\_trained"" is the pre-trained RF model to predict genotypes.
3. If you trained model with refined-genotypes (mosaic, het, refhom, repeat), then the ""model\_type"" is ""Refine""; otherwise if you trained model with Phasing (hap=2, hap=3, hap>3), then the ""model\_type"" is ""Phase"". 
4. We also added annotations of additional filtrations: Predicted mosaics with extra-high read depths (>=2X), sites with >=1.5X read depths and >=20% AF were marked as ""low-confidence""; predicted mosaics with only one alt allele and <1% AF were marked as ""cautious"".  

> You may use our models trained with brain WGS data for SNPs (paired-end read at 50-250X read depths, we train our models based on Mutect2-PON callings. To our experience, the models were pretty robust across different depths, but the best strategy would be using a model with similar depth with your data):
>
> * models\_trained/50xRFmodel\_addRMSK\_Refine.rds
> * models\_trained/100xRFmodel\_addRMSK\_Refine.rds
> * models\_trained/150xRFmodel\_addRMSK\_Refine.rds
> * models\_trained/200xRFmodel\_addRMSK\_Refine.rds
> * models\_trained/250xRFmodel\_addRMSK\_Refine.rds
>
> We also pre-trained a model for mosaic deletions (using paired-end read at 250X, with phasing information):
> * models\_trained/deletions\_250x.RF.rds

**Demo:**
```
Rscript Prediction.R demo/test.SNP.features models_trained/250xRFmodel_addRMSK_Refine.rds Refine demo/test.SNP.predictions   
Rscript Prediction.R demo/test.DEL.features models_trained/deletions_250x.RF.rds Phase demo/test.DEL.predictions
```
**Output:**
```
Genotype predictions for all input sites.
```

| id | AF | dp | prediction | het | mosaic | refhom | repeat |
| --- | --- | --- | --- | --- | --- | --- | --- |
| test\~11\~40316580\~C\~T | 0.028 | 214 | mosaic | 0.002 | 0.958 | 0 | 0.04 |
| test\~12\~52644508\~C\~T | 0.054 | 203 | mosaic | 0.002 | 0.982 | 0 | 0.016 |
| test\~15\~75918044\~G\~A | 0.036 | 193 | mosaic | 0.006 | 0.812 | 0 | 0.182 |
| test\~1\~1004865\~G\~C | 0.085 | 212 | mosaic | 0.006 | 0.988 | 0 | 0.006 |

```
1. prediction: genotype predictions including refhom, het, mosaic and repeat.
2. het/mosaic/refhom/repeat: genotyping probabilities for each genotype.
```

## You could also train RF models using your own data:
**Usage:**

Rscript Train_RFmodel.R input(trainset) output(prediction_model) type_model(Phase|Refine) type_variant(SNP|INS|DEL)

**Note:** 

1. You could choose to train your model based on Phasing (hap=2, hap=3, hap>3, type in ""Phase"") or Refined genotypes (""mosaic"",""het"",""refhom"",""repeat"", type in ""Refine"").
2. The input file should be a list of pre-generated read-level features, adding a column termed ""phase"" (Phase model) or ""phase\_model\_corrected"" (Refined genotypes model). 
3. We strongly recommend using Refined genotypes instead of Phasing genotypes, since ~50% of hap=3 sites were validated as ""repeat"" variants in our dataset:  
![website_pie](https://user-images.githubusercontent.com/8002850/57800167-be7b8100-771e-11e9-9773-0e39249698a6.png)


In case you don't have experimentally-evaluated sites, it's ok to manually-check ~100 hap=3 sites with igv, and mark the sites in messy regions as ""repeat"". Here are some examples of ""hap=3"" sites experimentally evaluated as ""repeat"":  
   
![hap3_wrong_examples](https://user-images.githubusercontent.com/8002850/55192738-9094aa00-517b-11e9-8809-e67d548b009f.png)

and here are some examples of ""hap=3"" sites experimentally evaluated as true positive mosaics:
	
![hap3_right](https://user-images.githubusercontent.com/8002850/55193553-a905c400-517d-11e9-8ce1-a0eaeb30d312.png)

**Demo:**
```
Rscript Train_RFmodel.R demo/trainset demo/Phase_model.rds Phase SNP  
Rscript Train_RFmodel.R demo/trainset demo/Refine_model.rds Refine SNP  
Rscript Train_RFmodel.R demo/deletions_trainset demo/Deletions_Refine_model.rds Phase DEL 
```
**Output:**
```
Random Forest prediction model
```

## Convert phasing to four-category genotypes based on experimental data:

#### (Recommended when you have >=100 orthogonally-evaluated or manually-checked sites with hap=3)
**Usage:**

Rscript PhasingRefine.R input(trainset) output1(model) output2(converted genotypes) read_length(int) pdf(plot) variant_type(SNP|INDEL)

**Note:**

1. The input file should be a list of pre-generated read-level features for all sites including phasable and non-phasable ones, adding a column termed ""phase"", containing the pre-generated haplotype number for each site (hap=2, hap=3, hap>3, notphased), and a column termed ""validation"", containing the orthogonally validation results. The un-evalulated sites should be ""NA"" in the ""validation"" column.
2. The output1 is the multinomial regression model, the output2 is the extraplolated four-category genotypes for all phasable sites.
3. The ""hap=3"" sites could contain ~50% FP sites, mostly ""repeat"" sites. When you don't have experimentally evaluated sites, it's ok to check ~100 hap=3 sites manually, converting the ""hap=3"" sites present in messy regions as ""repeat"" and covert ""hap=3"" sites present in clean regions as ""mosaic"".

**Demo:**
```
Rscript PhasingRefine.R demo/trainset demo/model_phasingcorrection.rds demo/phasable_sites_convertedgenotypes 150 demo/phasable_sites_Refine.pdf SNP 
```
**Output:**
```
A list of extrapolated genotypes based on Phasing, Readlevel features and orthogonal validations.
```

| id | phase | validation | phase_model_corrected | pc1 | pc2 | pc3 | pc4 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1465\~7\~64358306\~C\~T | hap=3 | repeat | repeat | 1.76125132511822 | -0.0974980761360579 | 0.040830632773886 | 1.76651681595286 |
| 1465\~2\~10704065\~T\~C | hap=3 | repeat | repeat | -0.0124184653486693 | -0.289637541460141 | -2.01395435019693 | 1.5135587692184 |
| 1465\~10\~42529522\~G\~C | hap=3 | NA | repeat | 0.707159292549739 | -3.91325643368487 | 1.54152147288395 | -1.10846627458624 |
| 1465\~7\~122592074\~G\~T | hap=3 | mosaic | mosaic | -0.104370300644773 | 2.33641117703566 | -0.0299470543274239 | 1.40486521150486 |
| 1465\~X\~61712742\~A\~G | hap=3 | repeat | repeat | 0.574318366975694 | 1.16511088082416 | 1.24290479319458 | 1.24880945486403 |
| 1465\~10\~42544320\~C\~T | hap=3 | NA | repeat | 0.544602308768669 | 1.64954352441225 | 0.28095361817584 | 0.744802179936821 |

```
1. phase_model_corrected: Four-category genotypes extrapolated based on phasing and read-level features.
2. pc1/pc2/pc3/pc4/pc5: the first five PCA components constructed with read-level features.
3. demo/phasable_sites_Refine.pdf: A plot showing the genotype extrapolation from phasing to 4-category genotypes.
```
![phasing_refine](https://user-images.githubusercontent.com/8002850/55196676-7ad8b200-5186-11e9-8324-b0a16f80e6de.png)


## Contact:
If you have any questions please contact us:

Yanmei Dou: yanmei_dou@westlake.edu.cn, douyanmei@gmail.com  
Peter J Park: peter_park@hms.harvard.edu



",0,0.67,0.67,,,,,,0,4,,,
216146819,MDEwOlJlcG9zaXRvcnkyMTYxNDY4MTk=,Github-Git-Cheat-Sheet,UCSC-Mozilla-Club/Github-Git-Cheat-Sheet,0,UCSC-Mozilla-Club,https://github.com/UCSC-Mozilla-Club/Github-Git-Cheat-Sheet,,0,2019-10-19 04:04:34+00:00,2021-10-31 11:08:46+00:00,2019-10-22 15:00:02+00:00,,9,4,4,,1,1,1,1,0,0,28,0,0,1,,1,0,0,public,28,1,4,master,1,"['hiranthaR', 'PrabhaniN']",1,"# GitHub Git Cheat Sheet

Git  is  an open  source  distributed  version  control  system  that  facilitates  GitHub  activities  on  your  laptop  or 
desktop. This cheat sheet will help you with commonly used Git command line instructions for quick reference.

### Catalog

- **[Install Git](#install-git)**
- **[How to use Git](#how-to-use-git)**
- **[Configure tooling](#configure-tooling)**
- **[Create repositories](#create-repositories)**
- **[Make changes](#make-changes)**
- **[Group changes](#group-changes)**
- **[Refactor filenames](#refactor-filenames)**
- **[Suppress tracking](#suppress-tracking)**
- **[Save fragments](#save-fragments)**
- **[Review history](#review-history)**
- **[Redo commits](#redo-commits)**
- **[Synchronize changes](#synchronize-changes)**

## INSTALL GIT

### For Windows users
1. Download [Git for Windows Setup](https://git-scm.com/download/win)
2. Install Git through the setup. Usually all configurations can be left in default settings.

### For macOS users
Download [Git for Mac](https://git-scm.com/download/mac)

### For Linux users
#### Debian/Ubuntu
For the latest stable version for your release of Debian/Ubuntu
```bash
$ sudo apt-get update
$ sudo apt-get get install git
```
For Ubuntu, this PPA provides the latest stable upstream Git version
```bash
$ sudo add-apt-repository ppa:git-core/ppa
$ sudo apt update
$ sudo apt install git
```

#### Fedora
For releases up to Fedora 21
```bash 
$ yum install git
```
For Fedora 22 and later
 ```bash
$ dnf install git
```

#### Arch Linux
```bash
$ pacman -S git
```

## HOW TO USE GIT

**Windows**<br>
Right click on any location and click `git bash`.

**Linux and Mac**<br>
Open `Terminal` to use git.

## CONFIGURE TOOLING

**Configure user information for all local repositories**<br>

Set the name you want attached to your commit transactions
```bash
$ git config --global user.name ""[name]""
```

Set the email address you want attached to your commit transactions
```bash
$ git config --global user.email ""[email address]""
```

## CREATE REPOSITORIES

**Start a new repository or obtain one from an existing URL**

Create a new local repository with the specified name
```bash
$ git init [project-name]
```

Download a project and its entire version history
```bash
$ git clone [url]
```

## MAKE CHANGES

**Review edits and craft a commit transaction**

List all new or modified files to be commited
```bash
$ git status
```

Show file differences not yet staged
```bash
$ git diff
```

Snapshot a file in preparation for versioning
```bash
$ git add [file]
```
Snapshot all files of the current directory in preparation for versioning
```bash
$ git add .
```

Unstage the file (reset), but preserve its contents
```bash
$ git reset [file]
```

Record file snapshots permanently in version history
```bash
$ git commit -m ""[descriptive message]""
```

## GROUP CHANGES

**Name a series of commits and combine completed efforts**

List all local branches in the current repository
```bash
$ git branch
```

Create a new branch
```bash
$ git branch [branch-name]
```

Switch to the specified branch and updates the working directory
```bash
$ git checkout [branch-name]
```

Combine the specified branch's history into the current branch
```bash
$ git merge [branch-name]
```

Delete the specified branch
```bash
$ git branch -d [branch-name]
```

## REFACTOR FILENAMES

**Relocate and remove versioned files**

Delete the file from the working directory and stages the deletion
```bash
$ git rm [file]
```

Remove the file from version control but preserves the file locally
```
$ git rm --cached [file]
```

Change the file name and prepares it for commit
```
$ git mv [file-original] [file-renamed]
```

## SUPPRESS TRACKING

**Exclude temporary files and paths**

List all ignored files in this project
```
$ git ls-files --other --ignored --exclude-standard
```

A text file named .gitignore suppresses accidental versioning of files and paths matching the specified paterns
```
*.log
build/
temp-*
```

## SAVE FRAGMENTS

**Shelve and restore incomplete changes**

Temporarily store all modified tracked files
```
$ git stash
```

List all stashed changesets
```
$ git stash list
```

Restore the most recently stashed files
```
$ git stash pop
```

Discard the most recently stashed changeset
```
$ git stash drop
```

## REVIEW HISTORY

**Browse and inspect the evolution of project files**

List version history for the current branch
```
$ git log
```

List version history for a file, including renames
```
$ git log --follow [file]
```

Show content differences between two branches
```
$ git diff [first-branch]...[second-branch]
```

Output metadata and content changes of the specified commit
```
$ git show [commit]
```

## REDO COMMITS

**Erase mistakes and craf replacement history**

Undo all commits afer [commit], preserving changes locally
```
$ git reset [commit]
```

Discard all history and changes back to the specified commit
```
$ git reset --hard [commit]
```

## SYNCHRONIZE CHANGES

**Register a repository bookmark and exchange version history**

Download all history from the repository bookmark
```
$ git fetch [bookmark]
```
Combine bookmark’s branch into current local branch
```
$ git merge [bookmark]/[branch]
```
Upload all local branch commits to GitHub
```
$ git push [alias] [branch]
```
Download bookmark history and incorporates changes
```
$ git pull
```
",1,0.76,0.76,,,,,,0,1,,,
759624858,R_kgDOLUb0mg,UCSC-NLP-SemEval-2024-Task-5,devashat/UCSC-NLP-SemEval-2024-Task-5,0,devashat,https://github.com/devashat/UCSC-NLP-SemEval-2024-Task-5,Codebase supporting NLP at UCSC's submission to Semeval 2024 Task 5,0,2024-02-19 02:38:28+00:00,2024-02-20 01:43:02+00:00,2024-02-21 01:55:19+00:00,,25,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,"['devashat', 'samyak24jain']",,"# NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation using Few-Shot Multi-Choice QA

This repository contains the code for our submission for [SemEval-2024 Task 5: The Legal Argument Reasoning Task in Civil Procedure](https://trusthlt.github.io/semeval24/). Our submission secured the 7th place on the leaderboard.

## Contained Files

**Note: When we refer to data as being in ""binary classification format"", we mean that it is in the format provided to us by the task organizers. When we refer to it being in ""multi-choice format"", we mean that we have run the reformatting script on the data provided to us to create a new dataset.**

### GPT

- `binary_few_shot.py`: Performs few shot prompting for either GPT-3.5 or GPT-4 on the test data. Data must be in binary classification format.
- `multi_choice_few_shot.py`: Performs few shot prompting for either GPT-3.5 or GPT-4 on the test data. Data must be in multi-choice format.
- `reformat_test_data.py`: Converts test data from binary classification format to multi-choice format.

### BERT

- `inference.py`: Generates predictions for the intended BERT model.
- `train.py`: Runs training on the intended BERT model. 


## How to Run

### GPT

- Make sure the environment you are running the scripts in has an OpenAI access key defined so that the API can be accessed. 
- To run few shot prompting with binary classification, use `binary_few_shot.py`.
- To run few shot prompting with multi-choice classification, first reformat the data using `reformat_test_data.py`. Then, use `multi_choice_few_shot.py`.
- For both scripts, set the `model` variable to the desired version of GPT-3.5 or GPT-4, then you may run the script.

### BERT

- There is a collection of bash scripts that when run, call either `train.py` or `inference.py` for either vanilla BERT or Legal BERT.
- If need be, change the `--dataset` flag in the bash script to the appropriately named dataset you have stored locally.
",1,0.77,0.77,,,,,,0,2,,,
176298552,MDEwOlJlcG9zaXRvcnkxNzYyOTg1NTI=,RiboseQC,ohlerlab/RiboseQC,0,ohlerlab,https://github.com/ohlerlab/RiboseQC,"Pipeline for Quality Control of Ribo-Seq data, selection of P-site offsets, and codon usage statistics.",0,2019-03-18 14:02:33+00:00,2025-01-07 06:41:53+00:00,2023-08-09 14:02:56+00:00,,24784,17,17,R,1,1,1,1,0,0,16,0,0,12,,1,0,0,public,16,12,17,master,1,"['zslastman', 'lcalviell', 'ggvillamil', 'Tim-Yu', 'Emeerdink', 'ohlerlab', 'janoppelt']",,"# Ribo-seQC
A comprehensive analysis tool for Ribo-seq and small RNA-seq data


**Ribo-seQC** (*RiboseQC*) is an R package (to be submitted to *Bioconductor*) that performs quality control analysis of small RNA-seq data, with a focus on Ribo-seq and related techniques. Thanks to syntax and functions present in *Bioconductor* packages like *GenomicFeatures*, *rtracklayer* or *BSgenome*, this package can perform comprehensive analyses on a variety of genomic regions. In addition, Ribo-seQC allows to automatically generate an html report for each analyzed sample, allowing for quick and interactive comparison of multiple samples at once.

This tools focuses on the analysis of different read lengths, taking into account the genomic regions they map to (e.g. coding sequence, UTRs, non-coding RNAs, mitochondria or chloroplasts, etc...). Other useful features, such as automatic P-sites position calculation or analysis the top mapping positions, are available in the Ribo-seQC package, and we encourage to donwload and have a look at the vignette (**RiboseQC.html** https://htmlpreview.github.io/?https://github.com/lcalviell/Ribo-seQC/blob/master/RiboseQC.html ), our manual (**RiboseQC-manual.pdf**), and our manuscript (to be added soon...).

An example Ribo-seQC html report for Ribo-seq data from Arabodopsis roots and shoots (Hsu *et al*, PNAS 2016) is available here, and can be downloaded and opened using a browser like Chrome, Firefox or others (*Warning*, file size is ~120Mb):

https://drive.google.com/open?id=13OBuS4CG0GA6j3Dt68zRtC8KBzNkmO7J

Here another report created using data from a TCP-seq (Archer *et al*, Nature 2016) experiment in yeast (*Warning*, file size is ~110Mb): 

https://drive.google.com/open?id=1568rBoFgYBREhrNeHQr1-66nQVKA8WEZ



To install Ribo-seQC:

```
library(""devtools"")
install_github(repo = ""lcalviell/Ribo-seQC"")
library(""RiboseQC"")

```

Two simple steps are required to use Ribo-seQC on your data:
```
?prepare_annotation_files
```
parses a *.gtf* and a *.2bit* file. (this need to be done once per each annotation-genome combination, a .2bit file can be obtained from a fasta file using the *faToTwoBit* software from UCSC: https://genome.ucsc.edu/goldenpath/help/twoBit.html - http://hgdownload.soe.ucsc.edu/admin/exe/ )


and
```
?RiboseQC_analysis
```

the master function used to perform the entire analysis workflow.
Please check the vignette for some example workflows.


For any question, please email:

calviello.l.bio@gmail.com, dominique.sydow@posteo.de (analysis and data visualization), Dermot.Harnett@mdc-berlin.de (package mantainer), Uwe.ohler@mdc-berlin.de (project supervisor).


Enjoy!


",0,0.58,0.58,,,,,,464,3,,,
715733247,R_kgDOKqk4_w,lkduran,lkduran/lkduran,0,lkduran,https://github.com/lkduran/lkduran,Config files for my GitHub profile.,0,2023-11-07 18:13:20+00:00,2023-11-07 18:13:20+00:00,2023-11-30 07:11:02+00:00,https://github.com/lkduran,1,0,0,,0,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['lkduran'],,"- 👋 Hi, I’m @lkduran
- 🌱 I’m currently learning computer shit
- 📫 How to reach me: lkduran@ucsc.edu
- 😤 Pronouns: She/they

<!---
lkduran/lkduran is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
",1,0.75,0.75,,,,,,0,1,,,
76442064,MDEwOlJlcG9zaXRvcnk3NjQ0MjA2NA==,CMPM120,chphuynh/CMPM120,0,chphuynh,https://github.com/chphuynh/CMPM120,,0,2016-12-14 08:56:36+00:00,2016-12-14 09:03:41+00:00,2016-12-14 09:03:39+00:00,,36786,0,0,JavaScript,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['chphuynh'],,"# CMPM 120 - Game Design Experience
- Arnav Jhala - Spring 2016
- https://courses.soe.ucsc.edu/courses/cmpm120

- The objective of this course is to teach the concrete programming and collaboration skills associated with making a digital game, from start to finish. This includes but is not limited to: establishing a team, concepting, storyboarding, prototyping, producing and testing a game for release. Students are organized into groups and work together to create and produce a playable game. The class is taught in conjunction with ARTG 120 which covers the skills required to design and critique digital games.

<br/>

- **Christopher Huynh, chphuynh@ucsc.edu**
",1,0.86,0.86,,,,,,0,0,,,
117022114,MDEwOlJlcG9zaXRvcnkxMTcwMjIxMTQ=,bigdata-ucsc,divyagopavarapu/bigdata-ucsc,0,divyagopavarapu,https://github.com/divyagopavarapu/bigdata-ucsc,,0,2018-01-10 23:08:45+00:00,2018-09-24 19:12:57+00:00,2018-01-19 22:20:23+00:00,,1732,1,1,Jupyter Notebook,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,master,1,['divyagopavarapu'],,# bigdata-ucsc,1,0.72,0.72,,,,,,0,0,,,
236133256,MDEwOlJlcG9zaXRvcnkyMzYxMzMyNTY=,sinhala-hate-speech-detection,chashikajw/sinhala-hate-speech-detection,0,chashikajw,https://github.com/chashikajw/sinhala-hate-speech-detection,This is a hate speech detection approach for Sinhala language,0,2020-01-25 06:13:12+00:00,2024-03-21 03:47:15+00:00,2020-03-24 19:24:04+00:00,,385,2,2,Jupyter Notebook,1,1,1,1,0,0,3,0,0,0,mit,1,0,0,public,3,0,2,master,1,['chashikajw'],,"# sinhala-hate-speech-detection

This is a hate speech detection algorithm using bag of words approach and linear regression model.


Dataset is provided by: UCSC LTRL. 

Sinhala stopwords and suffixes reference: [http://ltrl.ucsc.lk/download-3](http://ltrl.ucsc.lk/download-3/)

You can use any dataset and apply the algorithm.


",0,0.68,0.68,,,,,,0,1,,,
917687064,R_kgDONrLLGA,C-course,gabri-1910/C-course,0,gabri-1910,https://github.com/gabri-1910/C-course,This repository contains the source code for programming exercises and projects completed during the Coursera C Programming Specialization.,0,2025-01-16 13:12:44+00:00,2025-01-16 13:35:29+00:00,2025-01-16 13:35:27+00:00,,3,0,0,C,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['gabri-1910'],,"# C for Everyone: Programming Fundamentals - University of California, Santa Cruz
This repository contains the source code for programming exercises and projects completed during the Coursera C Programming Specialization.   

**Key Features:**

* Includes solutions to various C programming challenges and assignments.
* Demonstrates fundamental C concepts such as:
    * Data types (int, float, char, etc.)
    * Operators (arithmetic, relational, logical)
    * Control flow (if-else, loops, switch)
    * Functions (defining, calling, passing arguments)
    * Arrays, pointers, and structures
    * File I/O 
* Code is well-commented and easy to understand.
* May include supplementary materials like documentation or README files for specific projects.

**Note:**

* You can customize this description further by:
    * Adding the specific name of the Coursera specialization.
    * Mentioning any unique aspects of your projects.
    * Including a brief overview of the learning objectives achieved.


I hope this helps! Let me know if you'd like me to refine this further. 
",1,0.61,0.61,,,,,,0,1,,,
36488392,MDEwOlJlcG9zaXRvcnkzNjQ4ODM5Mg==,CMPM146-FINAL,SamReha/CMPM146-FINAL,0,SamReha,https://github.com/SamReha/CMPM146-FINAL,"A Rogue-like game that uses context-free grammars as the basis of its level generation system, plus a system for authoring new grammars automatically.",0,2015-05-29 07:25:27+00:00,2017-11-08 12:40:32+00:00,2017-01-24 22:28:19+00:00,,188,4,4,Java,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,4,master,1,"['SamReha', 'deipfei', 'Nicojaw', 'dylfaust']",,"# CMPM146-FINAL
A Rogue-like game that uses context-free grammars as the basis of its level generation system, plus a system for authoring new grammars automatically.

In this project, we demonstrate Roag, a Rogue-like game that uses context-free grammars to generate new dungeon content. We also demonstrate a grammar generator capable of intelligently authoring new grammars for use by Roag.

Context free grammars can be a very useful tool for level generation in video games, despite their current lack of use. Grammars are great at generating distinct artifacts that all exhibit common structures and patterns. This makes them ideal for our domain, Rogue-like dungeons, because they allow you to define a possibility space of dungeons wherein each dungeon feels different, but all dungeons follow similar rules, like always having a start and ending or other high-level structural elements. Because we are using the context-free grammar engine Tracery.JS (galaxykate), our grammars are defined as JSON documents, which makes them human-readable and human-editable as well. 

Our grammar generator is capable of authoring new, Roag-compatible dungeon grammars automatically by referencing a set of ""atomic elements"", essentially a dictionary of symbols representing the basic elements of whatever game will be using the created grammars. These are things like floor types, enemy types, and pickups types. When a designer is using our grammar generation system, they must hand-author the atomic elements, but the grammar generator does the rest of the work for you. Again, because our grammars are human-readable and human-editable, the automatically produced grammars can be easily tweaked and tuned by hand.

## Usage:
To play Roag, compile the game with the following command:
javac RoagGame/Frame.java RoagGame/Roag.java

And run Roag with this command:
java RoagGame/Frame.java

Roag's current grammar is located in grammar_generator/dungeon.json. If there's no grammar here, one will need to be provided in order for Roag to be playable.

The grammar generator can be run with the following command:
python grammar_generator/generator

It will look for an atoms file in grammar\_generator/atoms.json. If there's no atoms file here, one will need to be provided. It will write the new grammar to grammar_generator/dungeon.json.
The grammar generator can be configured by editing the settings object in grammar_generator/atoms.json

## Known Issues:
Tracery.JS cannot currently be run in a Node-friendly way, as such our project lacks a grammar engine. Roag will run using a random grammar from a pre-authored list regardless of the dungeon.json file present.

## Future Plans:

## Contact Us:
 - Samuel Reha (sreha@ucsc.edu)
 - Nico Williams
 - Dustin Pfeiffer
",1,0.8,0.8,,,,,,0,3,,,
287436318,MDEwOlJlcG9zaXRvcnkyODc0MzYzMTg=,PLASTER,bahlolab/PLASTER,0,bahlolab,https://github.com/bahlolab/PLASTER,Nextflow pipeline for long amplicon typing of PacBio SMRT sequencing data,0,2020-08-14 03:40:18+00:00,2024-07-17 07:12:12+00:00,2022-05-17 08:11:46+00:00,,44209,2,2,R,1,1,1,1,0,0,3,0,0,0,mit,1,0,0,public,3,0,2,master,1,['jemunro'],1,"# **PLASTER**: Phased Long Allele Sequence Typing with Error Removal

PLASTER is a comprehensive data processing pipeline for allele typing from long amplicon sequencing data generated on the PacBio SMRT platform. Inputs are PacBio subreads in BAM format, as well as sample barcodes and target amplicon details. Outputs are phased BAMs for each sample amplicon and variant calls in VCF format. Additionally the pipeline supports Pharmacogenomic star alelle assignment using the PharmVar database, and gene fusion detection for CYP2D6 and CYP2D7 fusion alleles.

The pipeline is built using [Nextflow](https://nextflow.io/), a workflow tool to run tasks across multiple compute infrastructures in a  portable and efficient manner. Included is a [Docker](https://www.docker.com/) container, making installation trivial and results highly reproducible. 

## Pipeline Overview
<p align=""center""><img src=""doc/diagram.png""/></p>

## Prerequisites

* [Nextflow](https://nextflow.io/) (version ≥ 20.07.1)
* [Singularity](https://sylabs.io/guides/3.0/user-guide/index.html) or [Docker](https://www.docker.com/)
* Job compute nodes require an internet connection to download data from PharmVar and Ensembl VEP

## Usage

### Pre-processing

* **Running the test dataset**
  ```
  nextflow run bahlolab/PLASTER -profile preproc,test,singularity
  ```
  This command will download the pipeline from GitHub and run the pre-processing stage on a minimal test dataset using singularity to run the software container. Replace ""singularity"" with ""docker"" to use docker instead. Note that nextflow pipelines are run in the current working directory, so make sure your terminal is in the appropriate directory first.  
  
  Profiles are provided for executors [SLURM](https://slurm.schedmd.com/documentation.html) and [PBS/torque](http://en.wikipedia.org/wiki/Portable_Batch_System), to use these append either `slurm` or `pbs` to the end of the profile specification (e.g., `-profile preproc,test,singularity,slurm`). Additional executors may be specified in a custom nextflow configuration, see [Nextflow executor documentation](https://www.nextflow.io/docs/latest/executor.html).
* **Running your own dataset**
  ```
  nextflow run bahlolab/PLASTER -profile preproc,singularity -c <my_dataset.config>
  ```
  where `my_dataset.config` is a Nextflow config file specifying the following required parameters:
  ```Nextflow
  params {
    subreads_bam = '/PATH/TO/MY/subreads.bam'
    barcodes_fasta = '/PATH/TO/MY/barcodes.fasta'
    amplicons_json = '/PATH/TO/MY/amplicons.json'
    ref_fasta = 'ftp://hgdownload.cse.ucsc.edu/goldenPath/hg38/chromosomes/chr22.fa.gz'
  }
  ```
  See [Pre-processing Parameters](doc/preproc.md) for more details, including details on running in single-sample mode (no barcodes).
* **Resuming a failed run**  
  Adding the `-resume` option to the Nextflow run command will use cached results from any pipeline steps where the inputs remain the same:
  ```
  nextflow run bahlolab/PLASTER -profile preproc,singularity -c <my_dataset.config> -resume
  ```
* **Outputs**  
  * **`./output/bam/`**  
    Directory containing aligned CCS BAM files for each sample-amplicon.
  * **`./output/sample_amplicon_bam_manifest.csv`**  
    CSV file with columns ""sample"", ""amplicon"", ""n_reads"", ""bam_file"" - to be used as input for the Allele-typing stage (see below).
  * **`./output/pre_processing_report.html`**  
    HTML report with various summary statistics and plots.

### Allele-typing

* **Running the test dataset**
  ```
  nextflow run bahlolab/PLASTER -profile typing,test,singularity
  ```
  This command will download the pipeline from GitHub and run the allele-typing stage on a minimal test dataset using singularity to run the software container. Replace ""singularity"" with ""docker"" to use docker instead. Note that nextflow pipelines are run in the current working directory, so make sure your terminal is in the appropriate directory first.
  
  Profiles are provided for executors [SLURM](https://slurm.schedmd.com/documentation.html) and [PBS/torque](http://en.wikipedia.org/wiki/Portable_Batch_System), to use these append either `slurm` or `pbs` to the end of the profile specification (e.g., `-profile preproc,test,singularity,slurm`). Additional executors may be specified in a custom nextflow configuration, see [Nextflow executor documentation](https://www.nextflow.io/docs/latest/executor.html).
* **Running your own dataset**
  ```
  nextflow run bahlolab/PLASTER -profile typing,singularity -c <my_dataset.config>
  ```
  where `my_dataset.config` is a file specifying the following required parameters:
  ```Nextflow
  params {
    manifest = '/PATH/TO/MY/manifest.csv'
    amplicons_json = '/PATH/TO/MY/amplicons.json'
    ref_fasta = 'ftp://hgdownload.cse.ucsc.edu/goldenPath/hg38/chromosomes/chr22.fa.gz'
  }
  ```
  See [Allele-typing Parameters](doc/typing.md) for more details.
* **Resuming a failed run**  
  Adding the `-resume` option to the Nextflow run command will use cached results from any pipeline steps where the inputs remain the same:
  ```
  nextflow run bahlolab/PLASTER -profile preproc,singularity -c <my_dataset.config> -resume
  ```
* **Outputs**  
  * **`./output/low_read_count.csv`**  
    Details of any sample amplicons excluded due to having too few reads. If no sample amplicons are excluded for this reason then this file won't be created.
  * **`./output/low_phased_read_count.csv`**  
    Details of any sample amplicons excluded due to having too few phased reads. If no samples amplicons are excluded for this reason then this file won't be created.
  * **`./output/fusion_calls.csv`**  
    Details of fusions calls made. Only produced if fusion detection is enabled.
  * **`./output/fusion_report.html`**  
    Report summarising fusion calling. Only produced if fusion detection is enabled.
  * **`./output/<amplicon>.phase_copy_num.csv`**  
    Details of copy number assigned to each sample-amplicon-phase.
  * **`./output/AmpPhaseR/`**  
    Directory containing AmpPhaseR plots showing read phasing, denoising and chimera removal.
  * **`./output/<amplicon>.vep.vcf.gz`**  
  VCF file with sample-amplicon-phase calls and VEP variant annotation.
  * **`./output/<amplicon>.sample_phase_alleleles.csv`**  
  Star allele assignment for sample phases. Only produced is star allele assignment is enabled.
  * **`./output/<amplicon>.allele_definition.csv`**  
  Star allele definitions. Only produced is star allele assignment is enabled.
    
    

## Implementation

### Pre-processing

* **CCS** - [PacificBiosciences/ccs](https://github.com/PacificBiosciences/ccs) implemented in [`pb_ccs.nf`](nf/preproc/pb_ccs.nf)
* **Barcoding** - [PacificBiosciences/barcoding](https://github.com/PacificBiosciences/barcoding) implemented in [`pb_lima.nf`](nf/preproc/pb_lima.nf)
* **Alignment** - [PacificBiosciences/pbmm2](https://github.com/PacificBiosciences/pbmm2) implemented in [`pb_mm2.nf`](nf/preproc/pb_mm2.nf)
* **Trim** - Python script [`bam_annotate_amplicons.py`](bin/bam_annotate_samples.py) based on [pysam](https://github.com/pysam-developers/pysam)
* **Split** - Python scripts [`bam_annotate_samples.py`](bin/bam_annotate_samples.py) and [`bam_split_sample_amplicons.py`](bin/bam_split_sample_amplicons.py) based on [pysam](https://github.com/pysam-developers/pysam)
* **Report** - Rmarkdown document [`preproc-report.Rmd`](bin/preproc-report.Rmd)

### Allele-typing

* **Fusions** - R script [`fusion_call.R`](bin/fusion_call.R) and Rmarkdown document [`fusion_report.Rmd`](bin/fusion_report.Rmd)
* **Call SNPs** - [GATK HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/4404604697243-HaplotypeCaller) and [GATK GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/articles/4404607598875-GenotypeGVCFs) implemented in [`gatk.nf`](nf/typing/gatk.nf)
* **Phase** - [BCFtools](http://samtools.github.io/bcftools/bcftools.html) and R package [AmpPhaseR](AmpPhaseR) implemented in [`phase.nf`](nf/typing/phase.nf)
* **Call Variants** - [GATK HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/4404604697243-HaplotypeCaller) and [GATK GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/articles/4404607598875-GenotypeGVCFs) implemented in [`gatk.nf`](nf/typing/gatk.nf)
* **VEP** - [Ensembl Variant Effect Predictor](https://www.ensembl.org/info/docs/tools/vep/index.html) implemented in [`vep.nf`](nf/typing/vep.nf)
* **Star Alleles** - R script [`pharmvar_star_allele.R`](bin/pharmvar_star_allele.R) using [PharmVar](https://www.pharmvar.org/) database

## Contributing
* Contributions are welcome, feel free to fork this repository and make a pull request.
* The pipeline could be extended to support Nanopore data as well PacBio data, but would require an alternate pre-processing stage. Please contact us if you are interested in working on this.

## Copy Number Analysis
* The folder `CopyNumAnalysis` contains raw data and the analysis Rmarkdown document associated with the qPCR copy number analysis presented in the PLASTER manuscript.

## Citation
* [Charnaud, S., Munro, J.E., Semenec, L. et al. PacBio long-read amplicon sequencing enables scalable high-resolution population allele typing of the complex CYP2D6 locus. Commun Biol 5, 168 (2022)](https://doi.org/10.1038/s42003-022-03102-8)
",0,0.64,0.64,,,,,,0,2,,,
471281854,R_kgDOHBcwvg,MOPline,stat-lab/MOPline,0,stat-lab,https://github.com/stat-lab/MOPline,Detection and genotyping of structural variants,0,2022-03-18 08:00:12+00:00,2025-02-26 05:49:50+00:00,2025-02-26 05:49:47+00:00,,305216,18,18,Perl,1,1,1,1,0,0,5,0,0,7,mit,1,0,0,public,5,7,18,main,1,['stat-lab'],,"# MOPline
Detection and Genotyping of Structural Variants
## Table of Contents

- [Introduction](#introduction)
- [Requirements](#requirements)
- [Citation](#citation)
- [Install](#install)
- [Composition of Required Directories](#composition)
- [Human and Non-human Data](#hdata)
- [General Usage](#gusage)
        - [\[Step-0\] Preparation of input files](#step0)
                - [(1) Run SV detection tools](#run_sv)
                        - [(a) Run user-installed tools using wrapper scripts](#user_install)
                        - [(b) Run using Singularity](#singularity)
                           - [(c) Batch runs of SV callers with multiple samples](#batch)
                        - [Notes on SV calling and input bam](#notes)
                - [(2) Create coverage files](#create_cov)
        - [\[Step-1\] Select overlap calls \(high-confidence calls\) from SV call sets](#step1)
        - [\[Step-2\] Add alignment statistics to SV sites](#step2)
        - [\[Step-3\] Merge vcf files from multiple samples \(joint-call\)](#step3)
        - [\[Step-4\] Genotype and SMC](#step4)
        - [\[Step-5\] Annotate](#step5)
        - [\[Step-6\] Filter](#step6)
- [Data Required for Non-Human Species](#nhdata)
- [For single sample](#single)
- [Quick Start with Sample Data](#quick)
        - [A: Human SV datasets of 6 samples](#hsample)
        - [B: Yeast WGS bam files of 10 samples](#ysample)

## Introduction

MOPline accurately and sensitively detects structural variations (SVs) in whole genome sequencing (WGS) data from a single to thousands (and sometimes tens of thousands) of human or non-human samples. MOPline first selects overlap SV calls (high-quality calls) of each SV type and size-ranges from SV call sets to generate a single vcf file for a sample. The SV call sets are multiple vcf files generated with existing SV detection algorithms (tools). The package is pre-programed to select overlap calls with 6~9 algorithms selected by us, but the user is free to choose any combinations of algorithms. In the second step, alignment statistics on coverage and split reads are added to the SV sites in the vcf files for each sample, providing materials for the next SV genotyping. The third step involves a joint call to merge the vcf files of multiple samples. In the fourth step, genotyping of each SV allele is performed based on multinominal logistic regression using the alignment statistics. In this step, reference alleles are also genotyped with the method described above to recover SVs that were missed due to the selection of SVs with high confidence in the first step. We call this recovery method Supplementing Missing Call (SMC). Finally, SVs are filtered and/or annotated for gene-overlapping SVs.

## Requirements

perl 5 or later versions  
R 3.0 or later (required library: [nnet](https://cran.r-project.org/web/packages/nnet))  
[samtools](https://github.com/samtools/samtools) (v1.13 or later is required for GRIDSS v2.13-)  
[vcftools](https://vcftools.github.io/index.html) (v0.1.15 or later)  
java 1.8 or later (for GRIDSS and MELT)

#### General input file
- Reference fasta file & its index file  
- Alignment bam/cram file & its index file (only bam file is permitted for several SV detection tools)  
- Gender list file (optional, 1st column: sample name, 2nd column: M|F, separated by tab)

#### SV detection tools (algorithms)  
The tools required depend on the tool presets used in MOPline or the tool sets customized by the user. The necessary SV detection tools must be installed by the user, but it is possible to use tools other than MELT using the singularity definition file provided in this package, as described below.  
The presets customized in this MOPline package are follows:  
- **Preset: 8tools (MOPline-8t)**  
        - [CNVnator](https://github.com/abyzovlab/CNVnator)  
        - [GRIDSS](https://github.com/PapenfussLab/gridss)  
        - [inGAP](https://sourceforge.net/projects/ingap/files/ingap)  
        - [Manta](https://github.com/Illumina/manta)  
        - [MATCHCLIP](https://github.com/yhwu/matchclips2)  
        - [MELT](https://melt.igs.umaryland.edu)  
        - [Wham](https://github.com/zeeev/wham)
         - [INSurVeyor](https://github.com/kensung-lab/INSurVeyor)  
- **Preset: 7tools (MOPline-7t)**  
        - INSurVeyor is excluded from 8tools
- **Preset: 7tools_1 (MOPline-7t-1)**  
        - GRIDSS is excluded from 8tools  
- **Preset: 7tools_2 (MOPline-7t-2)**  
        - MELT is excluded from 8tools  
- **Preset: 6tools_1 (MOPline-6t-1)**  
        - GRIDSS and INSurVeyor are excluded from 8tools  
- **Preset: 6tools_2 (MOPline-6t-2)**  
        - MELT and INSurVeyor are excluded from 8tools  
- **Preset: 9tools (MOPline-9t)**  
        - DELLY, Lumpy, SoftSV are added to 6tools_1

## Citation

Please cite the following paper when using MOPline for your publication.  

Kosugi et al. Detection of trait-associated structural variations using short-read sequencing. Cell Genomics 3, 100328 (2023). https://doi.org/10.1016/j.xgen.2023.100328

## Install

```
git clone https://github.com/stat-lab/MOPline
```
The Data folder in the MOPline folder contains parameter files, multinomial logistic regression-based model files for genotyping (R.nnet.models), and annotated data files for the human build 37/38/T2T reference. Do not change the name of the files/directories (except config.txt) and the directory structure in the MOPline folder.  
  
MOPline and additional 10 SV detection tools (except for MELT and INSurVeyor) can be executed using [Singularity](#singularity).

### Sample data
[Sample datasets](http://jenger.riken.jp/static/SVs_bykosugisensei_20220329/Sample_data.tar.gz) (or available from https://drive.google.com/drive/folders/1bIEtaaM3xx8POIAf96kV-ImNXTHWPwQQ?usp=sharing) include human SV call sets from 6 individuals and yeast 10 WGS data. The datasets also include output data created with MOPline.

## <a name=""composition""></a>Directory structure required (sample directory and tool directory)

MOPline assumes a directory structure of sample_directory/tool_directory under the working directory, where the sample directory has the sample name or sample ID, Under the sample directory, there are the tool directories with the names of algorithms, such as Manta and inGAP, which is case-sensitive. Under each sample directory, there are also should be bam and its index files. For convenience, when running against a sample (sample name: Sample_ID1), the working directory should contain a Sample_ID1 directory, under which the Sample_ID1.bam and Sample_ID1.bam.bai or their symbolic links should exist. When running with the 7tools preset, seven tool directories (CNVnator, GRIDSS, inGAP, Manta, MATCHCLIP, MELT, and Wham) must exist under the Sample_ID1 directory. In the tool directories, the runs of the corresponding tools are performed. In addition, Cov and Merge_7tools folders will be created under each sample directory in Step-0 and Step-1, as described below.  
![image](https://github.com/stat-lab/MOPline/files/10089842/Dir_Struc.pdf)

## <a name=""hdata""></a>Human and Non-human Data

By default, MOPline treats WGS alignment data (bam/cram) and SV call data (vcf) generated based on the human build 37 reference (GRCh37 or GRCh37d5). When using the data based on the human build 38 reference or T2T-CHM13.v2.0, run several MOPline scripts using the option ‘--build 38’ or '--build T2T. By specifying the --build option, specific data files associated with the human reference will be automatically selected from the Data folder. For non-human species, run several MOPline scripts using the option ‘-nh 1’ and, in some cases, also using other options specifying some reference-specific annotation files for gap, repeat, and gene regions ([for detail](#nhdata)).

## <a name=""gusage""></a>General Usage

### <a name=""step0""></a>[Step-0]  Preparation of input files

### <a name=""run_sv""></a>(1) Run SV detection tools

The preset algorithms are a combination of tools that have been evaluated for precision and recall, but the user may use a different combination or use other algorithms not listed here. The final output file from each algorithm must be converted to a MOPline-specific vcf file using the conversion script in ‘run_SVcallers’ folder of this package. The converted vcf file should contain ‘SVTYPE’, ‘SVLEN’, and ‘READS’ keys in the INFO field, where the READS key represents RSS (reads supporting SV). Because many short read-based SV detection algorithms, including MOPline, cannot determine the length of INSs, the length of the INS is recorded as 0 or 1 in many INS calls. The conversion scripts, the corresponding algorithms, and their output files are indicated in the table below.

**Table 1.** Conversion scripts for 10 SV detection algorithms
|Algorithm|Command using a conversion script (for a sample name, AB)                      |
| :------ | :---------------------------------------------------------------------------- |
|CNVnator |convert_CNVnator_vcf.pl AB.out gap.bed > CNVnator.AB.vcf                       |
|DELLY    |convert_DELLY_vcf.pl AB.out $nh > DELLY.AB.vcf                               |
|GRIDSS   |convert_GRIDSS_vcf.pl AB.vcf.gz $nh > GRIDSS.AB.vcf                                |
|inGAP    |convert_inGAP_vcf.pl AB.chr1.out .... AB.chrY.out > inGAP.AB.vcf               |
|Lumpy    |convert_Lumpy_vcf.pl AB.vcf $nh > Lumpy.AB.vcf                                     |
|Manta    |convert_Manta_vcf.pl results/variants/diploidSV.vcf.gz $nh > Manta.AB.vcf          |
|MATCHCLIP|convert_MATCHCLIP_vcf.pl AB.matchclip.out $nh > MATCHCLIP.AB.vcf                   |
|MELT     |convert_MELT_vcf.pl ALU.final_comp.vcf LINE1.final_comp.vcf SVA.final_comp.vcf <br>HERVK.final_comp.vcf > MELT.AB.vcf|
|SoftSV   |convert_SoftSV_vcf.pl deletions_small.txt insertion_small.txt tandems_small.txt<br> inversions_small.txt deletions.txt tandems.txt inversions.txt $nh > SoftSV.AB.vcf
|Wham     |convert_Wham_vcf.pl AB.vcf $nh > Wham.AB.vcf                                       |
|INSurVeyor|convert_INSurVeyor_vcf.pl AB.vcf $nh > INSurVeyor.AB.vcf                       |

If the data is for non-human species, specify 1 for $nh. If the data is human, specify 0 for $nh or nothing. In the case of CNVnator, the second argument of the convert_CNVnator_vcf.pl script must be a gap bed file that indicates the gap regions (a stretch of ‘N’ bases) in the reference genome. The gap.bed file for human is located in the Data folder in the package. For non-human species, a gap file can be obtained at [UCSC](https://hgdownload.soe.ucsc.edu/downloads) for some species or created manually, but this file can be omitted.

### <a name=""user_install""></a>(a) Run user-installed tools using wrapper scripts

For convenience, we have provided wrapper scripts to run the selected SV detection tools in the ‘run_SVcallers’ folder. The tools are expected to be properly installed by the user. A run of the script with the ‘-h’ option informs us the required arguments and options. We also provide a script for sequential execution of multiple algorithms for a single sample (run_single.pl) and batch scripts for multiple samples using Slurm and LSF job managers (run_batch_slurm.pl, run_batch_LSF.pl). To run these three scripts, specify a configure file that describes the parameters for each algorithm, using a template configure file (MOPline/scripts/run_SVcallers/config.txt) (an example config file is also provided in the sample data [Sample_data_output/yeast_run]). A tool directory with the name of the algorithm specified in the configure file is automatically created in the sample directories under the working directory, and the algorithm is executed under the tool directory.  
  
Example using run_single.pl  
```
run_single.pl -b <input bam> -c <config file> -sd <sample directory>
```
This command creates SV calling results from the SV detection algorithms specified in the config file, and the results are in the tool directories under the sample directory specified with the -sd option.  

### <a name=""singularity""></a>(b) Run using Singularity

Other than MELT and INSurVeylor (MELT has license restrictions) the above 9 SV detection algorithms can be run using a Singularity image container built using the Definition file (mopline.def or mopline-rkylx89.def.txt) included in the package. We expect to be run INSurVeylor using Singularity image file (https://github.com/kensung-lab/INSurVeyor/releases) specific to INSurVeylor or installed using conda   
An image file (e.g., mopline.sif) can be obtained using one of the following:  
```
sudo singularity build mopline.sif mopline.def
```
```
singularity build --fakeroot mopline.sif mopline.def
```
Alternatively, a mopline.sif (v1.8.3.1: 1.74 GB) can be obtained at [Jenger site](http://jenger.riken.jp/static/SVs_bykosugisensei_20220329/mopline.sif) :(http://jenger.riken.jp/static/SVs_bykosugisensei_20220329/mopline.sif).  
  
To run CNVnator with singularity  
```
singularity exec <mopline.sif file path> /opt/local/tools/MOPline/scripts/run_SVcallers/run_CNVnator.pl -cp <cnvnator command path> -b <input bam> -r <ref directory> -rd <ROOT install directory> -p <output prefix>   # the path of run_CNVnator.pl is the path in the singularity container.
```
To run the run_single_singularity.pl script for a single sample, install this MOPline package on the appropriate location and run as follows:  
```
~/MOPline_v1.#/scripts/SV_callers/run_single_singularity.pl -s <mopline.sif file path> -b <input bam> -c <config_singularity.txt> -td <temp direcory> -bd <bind directories> -sd <sample directory> 
```
The sif file must be specified as an absolute path. The config_singularity.txt provided in this package can be used for the template, but the lines 'Do not change' indicating the path in the singularity container, must not be changed. Input files (reference.fasta, alignment bam) on the host must be specified with the absolute path. The temp directory corresponds to /tmp on the host or a temporary directory specified with the environmental variable TMPDIR. The bind directories are the paths on the host to add to the singularity container. If the input files used by run_single_singularity.pl are located in a directory other than the working directory, the path to the corresponding directory must be specified with the -bd option (multiple paths separated by commas).

### <a name=""batch""></a>(c) Batch runs of SV callers with multiple samples

Users who have access to a job management system that includes Slurm and LSF Job Manager can use batch scripts (run_batch_slurm.pl, run_batch_LSF.pl) for multiple samples.  
If you do not use the job management system, the script run_SVcallers_batch.pl can be used for multiple samples. To use this script, split multiple samples into several batches (e.g., 10 samples x 10 batches for 100 samples), depending on the CPU cores and memory capacity of the computational environment. For each batch、create sample list files showing the sample name and the corresponding bam/cram file path in each line. If the input alignment files are in cram format, the script automatically converts cram files to bam files because some SV callers only accept bam format as input. The --add_MC (-am) option can be used to add MC/MQ tags to bam files, which may be useful for reducing the runtime of the MELT tool. In these cases, a new bam file will be created under the sample directory. A typical usage of this script is as follows:
```
run_SVcallers_batch.pl -sl <sample list file> -r <reference fasta> -sf <mopline.sif singularity file if -c is specified> -c <config file for singularity-based run> -td <temp direcory> -bd <bind directories> -c2 <config file for MELT, INSurVeyor, and/or others> (-am if adding MC/MQ tag in bam file)
(Sample directories must exist under the working directory.)
```
This command executes the SV callers for each sample sequentially. Multiple parallel executions of this command with the split sample list files will reduce the overall runtime. Both the -c and -c2 options can be specified, or just one of them. If the mopline.sif singularity container is not used and a user-installed SV caller is used, define the SV caller parameters in the configuration file specified with -c2.

### <a name=""notes""></a>Notes on SV calling and input bam

#### inGAP-sv
inGAP requires sam files and reference fasta files split by chromosome as input files. inGAP is highly sensitive to PCR-duplicates in input sam files. When a sam file containing 10% PCR-duplicates is used, inGAP often generates more than twice as many calls compared with sam without PCR-duplicates.
#### GRIDSS
In GRIDSS, accidental errors can occur, primarily due to the java heap memory size allocations. To solve this problem, it is often set the java maximum heap size (export JAVA_TOOL_OPTIONS=""-XX:+UseSerialGC -Xmx15g -Xms15g"") and the GRIDSS options, ‘15g’ for both, --jvmheap and --otherjvmheap. It is advisable to link the reference fasta and bwa files in the GRIDGSS working directory. GRIDSS is very sensitive to reads with low-quality terminal regions, so it is recommended to trim off low-quality regions of reads before alignment if may fractions of reads contain low-quality regions.
#### Manta
When an input bam is generated with a reference containing many decoy sequences such as GRCh38+decoy, it often takes longer time to complete run. In this case, runtime can be shortened by using the –callRegions option, which specifies a bgzip-compressed bed file indicating only target chromosomes (i.e., chr1, … chrY).
#### MELT
MELT often takes much longer, especially for GRCh38-based alignment data, without MC/MQ tag in the input bam. Also, when using a reference that contains many decoy sequences such as GRCh38+decoy, the -b option can be used a list of chromosomes to exclude (> 1 Mb chr) to reduce execution time. For non-human species, MELT can be run by preparing species-specific mobile element data, as described in the MELT documentation.
#### INSurVeylor
INSurVeylor needs an input BAM/CRAM file containing MC and MQ tags, which can be added to BAM/CRAM using samtools fixmate or picard.jar FixMateInformation command.

#### Input bam
Trimming the read sequence in the fastq file (e.g. using Trimomatic) or removing PCR duplicates in the bam file is recommended, especially if the reads are of low quality. If there are 'N' bases or low quality bases in the terminal regions of the reads, these reads often generate soft clipping alignments in the bam file, which can cause some algorithms to call the wrong breakpoints. Also, when PCR-duplicate reads are present, some algorithms may call wrong SVs. GRIDSS is an SV detection algorithm that is highly sensitive to split-read alignments. if GRIDSS generates tens of thousands of SV calls and takes several days to complete execution (usually completed within a day for 30x human WGS data), it would be an indication that the sequencing data contains many low-quality reads.

The samtools fixmate command can be used to add MC and MQ tags, which is useful for reducing MELT run time, running INSurVeylor, and removing PCR duplicates. Example commands using bwa and samtools fixmate are shown below.
(bwa → bam with MC/MQ tags)
```
bwa mem <ref index> <read_1> <read_2> -M -R “read group header specification” -t <num threads> | samtools view -uShb - | samtools fixmate -um - [out.fm.bam]
```
(sort by coordinate)
```
samtools sort -@ 4 <out.fm.bam> -o [out.sr.bam]
```
(sort by coordinate & remove PCR-duplicates)
```
samtools sort -@ 4 <out.fm.bam> | samtools markdup -ur - [out.rm.sr.bam]
```
### <a name=""create_cov""></a>(2) Create coverage files with each sample bam file

In Step-2, MOPline uses coverage files recording read depth and the number of soft-clipped ends for each 50-bp window for each sample.  
Create a coverage file using the create_coverage_file_bam.pl script as follows:
```
mopline create_cov -b <bam_list> -r <reference_fasta> -rl <read_length> -n <num_threads>
```
(-nh 1 if sample is a non-human species)  
**bam_list:** bam/cram list file specifying bam or cram file name per line. The bam_list can also be a list of sample names if the bam file name is `${sample_name}.bam` and exists in the `${sample_name}` directory.  
**read_length:** Mean read length in the bam file

The above command creates a Cov directory under the sample directory, which contains the coverage files for each chromosome (`${sample_name}.chr*.cov.gz`).

For batch jobs, we provide create_coverage_file_bam_single.pl and run_create_cov_batch.pl scripts, the former of which can be used to submit a single bam file job using a job manager such as Slurm and LSF. The run_create_cov_batch.pl script can be used with sample list files, which were split into several batches (e.g., 10 samples x 10 batches for 100 samples), depending on the CPU cores and memory capacity of the computational environment. A sample list file describes sample name in each line. A typical usage of this script is as follows:
```
run_create_cov_batch.pl -s <sample list file> -r <reference fasta file> -rl <read length (bp)>
(Sample directories must exist under the working directory and each sample directory must have a bam file, named ${sample_name}.bam.)
```
This command executes the script create_coverage_file_bam_single.pl sequentially for each sample with a single thread. Multiple parallel executions of this command with the split sample list files will reduce the overall runtime.

### <a name=""step1""></a>[Step-1]  Select overlap calls (high-confidence calls) from SV call sets

Once the MOPline-specific vcf files for each algorithm and for each sample are created, selection of overlapping SV calls is first performed. Use the merge_SV_vcf.*tools.pl script in the scripts folder to select overlap calls and high-confidence calls from the SV call sets (vcf files) generated in Step-0 and to merge the selected calls of each SV type and size-ranges for each sample. For high coverage (30x or more) bam files, use mopline subcommands, merge_7tools, merge_6tools_1, merge_6tools_2, or merge_9tools. When using bam files with ~20x coverage, use the subcommand with a ‘_lc’ at the end.
```
mopline merge_7tools -s <sample-list-file or a sample name> -rl <read length> 
```
(-nh 1 if sample is a non-human species)

This command generates a `${sample_name}.Merge.ALL.vcf` file in the ‘Merge_7tools’ folder (default for 7tools preset) created under the sample directory. The INFO field of each SV line displays a TOOLS key indicating which algorithm called the corresponding SV.

### [Using custom algorithm set]

When using a custom algorithm set instead of the preset, users can create their own merge script (corresponding to the merge_SV_vcf.7tools.pl script) using the make_merge_SV_vcf_script.pl script as follows:
```
make_merge_SV_vcf_script.pl -t <algorithm list, comma-separated> -tc <tool-config file> -p <prefix name of output script>
```
(use -h for detailed explanation)  
If the -tc option is not specified in the above command, the tool configuration file (Data/SVtool_pairs_config.txt) is automatically selected. This file specifies the favorable pairs of tools and minimum RSSs of overlap call selection for each of 14 algorithms we have chosen. If additional algorithms are used, the SVtool_pairs_config.txt file can be modified to specify the preferred pairs minimum RSSs for each newly added algorithm for each SV type and size range.

### <a name=""step2""></a>[Step-2]  Add alignment statistics to SV sites

In this step, alignment statistics such as DPR, SR, and DPS are added to each SV site in every sample. DPR is the ratio of the depth of the region inside the SV to the adjacent depth, and DPS is the deviation rate of DPR measured in a 50-bp window. SR is the ratio of soft-clipped read ends around the breakpoint to the outside area. To measure these values, a coverage file must first be created for each sample (see [create coverage files](#create_cov)), recording the read depth and the number of soft-clipped ends for each 50-bp window.

Using the coverage files you created, add alignment statistics to each SV site in the `${sample_name}.Merge.ALL.vcf` file created in Step-1. This step also adds reliable genotypes from the genotypes called from several algorithms in Step-0 to some of the SV sites. The command with the add_GT_DPR_vcf.pl script is as follows:
```
mopline add_cov -s <sample_list> -ts <tool_set> -vd <vcf_directory> --build 37 -n <num_threads> 
```
(--build 38 for human build38, -nh 1 -r <ref.index> -gap <gap_bed> for non-human species)  
**sample_list:** A sample list file showing sample names per line. A sample directory with the same names as the specified sample list must exist under the working directory.  
**tool_set:** Algorithm preset or list file showing algorithm names per line [default: 7tools]  
**vcf_directory:** The name of the directory containing the input vcf files in the sample directories [default: Merge_7tools]

The above command updates a `${sample_name}.Merge.ALL.vcf` file in the vcf_directory and rename the original vcf file as `${sample_name}.Merge.ALL.noAdd.vcf`. Alignment statistics are added to the FORMAT/SAMPLE fields with DR, DS, and SR keys in the updated vcf file.

For batch jobs, we provide a add_GT_DPR_vcf_single.pl script to submit a single vcf file job using a job manager such as Slurm and LSF.

### <a name=""step3""></a>[Step-3]  Merge vcf files from multiple samples (joint-call)

Joint calling is performed with the merge_SV_calls_ALLtype.pl script as follows:
```
mopline joint_call -s <sample_list> -md <merge_dir> -od <out_dir> -p <out_prefix> --build 37
```
(--build 38 for human build 38, -nh 1 -gap <gap_bed> for non-human species)  
**sample_list:** A sample list file showing sample names per line. A sample directory with the same name as the specified sample list must exist under the working directory. If the sample directory name and the sample name are different, specify them by separating each line with a comma, such as `${sample_directory_name},${sample_name}`. It is assumed that the input vcf files (`${sample_name}.Merge.ALL.vcf`) exist in `${sample_directory}/${merge_dir}`.  
**merge_dir:** Name of the directory containing the input vcf files under the sample directories [default: Merge_7tools]  
**out_dir:** Name of the directory where the output vcf file will be generated [default: the same name as merge_dir]  
**out_prefix:** Prefix name of the output vcf file  
**gap_bed:** A bed file of reference gap regions. For human, the file is automatically selected from the MOPline package. For non-human species, obtain from [UCSC](https://hgdownload.soe.ucsc.edu/downloads) or create manually.

The above command outputs `${out_prefix}.vcf` under `${out_dir}` directory.

### <a name=""step4""></a>[Step-4]  Genotype and SMC

In this step, all SV alleles are genotyped based on multinominal logistic regression (model data are in the Data/R.nnet.models folder) and reference alleles are re-genotyped to recover missing SV calls by SMC. This step requires files showing repeat regions in the reference, which are provided for human (Data/simpleRepeat.txt.gz, Data/genomicSuperDups.txt.gz) and are automatically selected.  
Before the run of this step, confirm whether the R nnet package has been installed as follows:  
```
R
>library(nnet)
```
The command using the genotype_SV_SMC_7.4.pl script is as follows:
```
mopline smc -v <input_vcf> -ts <tool_set> -od <out_dir> -p <out_prefix> --build 37 -n <num_threads>
```
(--build 38 for human build 38, -nh 1 -sr <STR_file> -sd <SD_file> -r <ref_index> for non-human species)  
**input_vcf:** An input vcf file from Step-3  
**tool_set:** Algorithm preset name or a list file showing algorithm names per line [default: 7tools]  
**out_dir:** Name of the directory where the output vcf file will be generated  
**out_prefix:** Prefix name of the output vcf file  
**STR_file:** A simple/short tandem repeat file from [UCSC](https://hgdownload.soe.ucsc.edu/downloads) or [Tandem Repeats Finder](https://tandem.bu.edu/trf/trf.html) output (only for non-human species, can be unspecified)  
**SD_file:** A segmental duplication file from [UCSC](https://hgdownload.soe.ucsc.edu/downloads) (only for non-human species, can be unspecified)  
**ref_index:** A reference fasta index file (mandatory for non-human species)  

The Step-4 corrects the genotypes (given with GT tag) and adds a new tag, MC, to the FORMAT/SAMPLE fields of the output vcf file. The MC tag represents the level of SMC; the lower the MC value, the higher the confidence level. The genotyping step requires a parameter file indicating the minimum SRR for each SV type and algorithm. MOPline provides a default parameter file for 14 pre-selected algorithms, which is automatically selected during this step. If additional algorithms are to be used that are not listed in this parameter file, the user can edit the parameter file by adding parameters of those algorithms (if not edited, all algorithms not present in the parameter file will have a minimum RSS of 3).

This step takes longer to perform and requires more memory as the sample size increases and the genome size increases. For human samples larger than 1,500, it is recommended that this step is performed for each chromosome, which can be done using the -c option (default: ALL).

### <a name=""step5""></a>[Step-5]  Annotate (optional)

Step-5 adds gene name/ID and gene region that overlap the SV to the INFO filed (with SVANN key) of the vcf file. The gene region includes exon/CDS (All-exons if the SV completely overlaps all exons), 5’-/3’-UTR, intron, 5’-/3’-flanking regions. Two ranges of the flanking regions are specified by default (5 Kb and 50 Kb); these lengths can be changed with the options, -c5, -c3, -f5, and -f3. These annotations are also added to the FORMAT AN subfield for each sample in an additional output vcf file. For human, the gff3 gene annotation files (Homo_sapiens.GRCh37.87.gff3.gz, Homo_sapiens.GRCh38.104.gff3.gz, or Homo_sapience.T2T-chm13v2.0.ensemble.gff3.gz), downloaded from Ensembl (ftp://ftp.ensembl.org/pub/grch37/release-87/gff3/homo_sapiens9), is selected by default. For non-human species, a gff3 annotation file obtained from the Ensembl site must be specified with the -r option. Any input SV vcf file with SVTYPE and SVLEN keys in the INFO field may be used. As input vcf file, an output vcf file from step2, step3, step4, or step6 can be used. The annotate command can be done as follows:
```
mopline annotate -v <input_vcf> -p <out_prefix> --build 37 -n <num_threads>
```
(--build 38 for human build 38, -nh 1 -r <gff3_file> for non-human species)  
This command generates two output vcf files, `${out_prefix}.annot.vcf` and `${out_prefix}.AS.annot.vcf`. The latter contains annotations for each sample in the FORMAT AN subfield.

### <a name=""step6""></a>[Step-6]  Filter (optional)

This step, using the filter_MOPline.pl script, filters out DEL/DUPs with inconsistent DPRs. DUPs associated with gap regions and DUPs overlapping segmental duplications also filtered out based on several criteria (see our paper for detail). For human samples, gap bed and segmental duplication files are automatically selected from the Data directory. For non-human samples, these files can be specified with the -gap and -segdup options. The -ex option can also be used to specify a bed file that indicates regions to exclude SVs. This package provides bed files for human reference builds 37, 38, and T2T-CHM13 that indicate regions where SVs are always indeterminately called due to low quality alignments of short reads. The total lengths of the excluding regions of build 37 and 38 are 26 Mb and 16 Mb, respectively. In addition, SVs within the centromere regions for the human builds 38 and T2T-CHM13 are removed. By default, the bed files of these regions are automatically selected for human but user-provided files can be specified with the -ex and -ec options. If you do not prefer to use these filtering, specify any letter (e.g., -ex 0 or -ec 0) for the options. The input vcf can be from a single sample or from multiple samples but must have the keys DPR, SR, and DPS in the INFO field. As input vcf file, an output vcf file from step2, step3, step4, or step5 can be used.
```
mopline filter -v <input vcf> --build 37 > [output vcf]
```
(--build 38 for human build 38, -nh 1 -g <gap_bed> -sd <segmental duplication file> for non-human species)

## <a name=""nhdata""></a>Data Required for Non-Human Species

For non-human species, the following files are required at specific MOPline steps (for human, these files are provided in this package and automatically selected at each step). Human sample data files are present in the Data directory of this package.

**Table 2.** Files required for non-human species, which must be supplied by users
|Name  |Description                   |Sample file                   |Available site|Required step|
| :--- | :--------------------------- | :--------------------------- | :------ | :------- |
|Gap   |BED file of 'N' regions in the reference fasta|gap.bed     |[UCSC](https://hgdownload.soe.ucsc.edu/downloads)|steps-0,2,3,6
|STR   |Simple tandem repeats (required columns: 2,3,4)|simpleRepeat.txt.gz |[UCSC](https://hgdownload.soe.ucsc.edu/downloads)|step-4
|SegDup|Segmental duplications (required columns: 2,3,4)|genomicSuperDups.txt.gz|[UCSC](https://hgdownload.soe.ucsc.edu/downloads)|steps-4,6
|Cen   |BED file of centromere regions|hg38.centromere.bed|[UCSC](https://hgdownload.soe.ucsc.edu/downloads)|step-6
|Gene  |Gene annotation GFF3 file   |Homo_sapiens.GRCh37.87.gff3.gz|[Ensembl](https://asia.ensembl.org/index.html)|step-5
|Rindex|Reference fasta index       |hs37.fa.fai                   |samtools faidx|steps-1,2

### For data unavailable at the public sites

Gap bed and STR file can be custom created. But MOPline can be executed at any step, even if Gap, STR, SegDup, or Cen file is not specified.  

**Gap bed:** A gap bed file can be created using scripts/gap_fasta_to_bed.pl in this package.  
```
gap_fasta_to_bed.pl [input genome.fasta] [minimum length of gap, default: 1000 (bp)] > [output bed]
```
**STR (SimpleRepeat) file:** STR/SimpleRepeat file can be created using the [TRF](https://github.com/Benson-Genomics-Lab/TRF) and scripts/convert_TRF_data_to_simpleRepeat.pl.  
  
(TRF run)  
```
trf409.linux64 [input genome.fasta] 2 7 7 80 10 50 500 -d -l 1
```
(Convert TRF data to UCSC SimpleRepeat format)  
```
convert_TRF_data_to_simpleRepeat.pl [TRF output *.data file] > [output SimpleRepeat-like file]
```
## <a name=""single""></a>For single sample

For MOPline with a single sample, all the steps can be performed, but the required directory structure (sample and tool directories) is required for completion. If SV genotyping is not needed, the steps-3 and -4 can be omitted.

## <a name=""quick""></a>Quick Start with Sample Data

The [sample data](http://jenger.riken.jp/static/SVs_bykosugisensei_20220329/Sample_data.tar.gz) (or available from https://drive.google.com/drive/folders/1bIEtaaM3xx8POIAf96kV-ImNXTHWPwQQ?usp=sharing) provided includes two sample datasets: human and yeast (*Saccharomyces cerevisiae*) data. The human data includes SV datasets generated with high coverage WGS datasets of 6 individuals from a 1000 Genomes CEU population (ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/1000G_2504_high_coverage/data). The yeast data includes WGS bam files for 10 yeast isolates (Peter J. et al., Nature 556, pages 339–344 (2018)) and associated files including reference fasta and annotation files. An [output dataset](http://jenger.riken.jp/static/SVs_bykosugisensei_20220329/Sample_data_output.tar.gz) generated with this dataset using MOPline-7t is also available.

### <a name=""hsample""></a>A: Using Human SV datasets of 6 samples

This sample SV data was generated using the 7tools preset (MOPline-7t) with 6 human WGS data (bam files of 30×, 150 bp paired-end sequencing data aligned against the GRCh37 reference), and only SVs corresponding to chromosome 17 were extracted. Each sample directory contains 7 tool directories, and each tool directory contains an SV vcf file generated with the conversion scripts shown in Table 1. Each sample directory contains a Cov directory, which contains a coverage file of chromosome 17 that records alignment statistics from the bam file.
```
cd Human_chr17_b37_6samples
export PATH=$PATH:${MOPline-install-directory-path}
(e.g., export PATH=$PATH:/home/tools/MOPline_v1.7)
```
#### [Step-1] Select overlap calls from SV call sets
```
mopline merge_7tools -s sample_list.txt -rl 150 -d Merge_7tools
```
The command adds alignment statistics and genotype data to `${sample}.Merge.ALL.vcf` for each sample. Check the values given by the DR, DS, and SR tags in the FORMAT/SAMPLE fields of the vcf files.

#### [Step-2] Add alignment statistics to SV sites
Step 2-(1) ‘Create coverage files’ is omitted for this sample data because the Cov directory in each sample director already contains coverage files.
```
mopline add_cov -s sample_list.txt -ts 7tools -vd Merge_7tools -n 4 
```
The command adds alignment statistics and genotype data to `${sample}.Merge.ALL.vcf` for each sample. Check the values given by the DR, DS, and SR tags in the FORMAT/SAMPLE fields of the vcf files.

#### [Step-3] Merge vcf files from multiple samples
```
mopline joint_call -s sample_list.txt -md Merge_7tools -od JointCall -p MOPline
```
The command generates a MOPline.All-samples.vcf in the JointCall directory.

#### [Step-4] Genotype and SMC
```
mopline smc -v JointCall/MOPline.All-samples.vcf -ts 7tools -od MOPline_SMC -p MOPline.smc -n 4
```
A vcf file named MOPline.smc.vcf has been created in the MOPline_SMC directory, with new MC tags in the FORMAT/SAMPLE fields and AF, AC, DPR, SR, and DPS keys in the INFO field.

#### [Step-5] Annotate
```
cd MOPline_SMC
mopline annotate -v MOPline.smc.vcf -p MOPline -n 4
```
This will generate two annotated vcf files in the working directory, MOPline.annot vcf and MOPline.AS.annot.vcf. The latter vcf file contains sample-level gene-overlap annotations in the FORMAT/SAMPLE fields with AN tags.

#### [Step-6] Filter
```
mopline filter -v MOPline.AS.annot.vcf > MOPline.AS.annot.filt.vcf
```

### <a name=""ysample""></a>B: Using Yeast WGS bam files of 10 samples

This sample data contains yeast bam alignment files of 30×, 101 bp paired-end WGS data aligned using bwa mem against the S288C *S. cerevisiae* reference for 10 yeast strains. The dataset includes the S288C reference fasta file, *S. cerevisiae* gene annotation gff3 file (ftp://ftp.ensembl.org/), STR repeat file (https://genome.ucsc.edu), andTY1/TY3 retroelement reference files for MELT. The TY1/TY3 MELT reference files were generated according to the [MELT documentation](https://melt.igs.umaryland.edu/manual.php). This tutorial begins with SV calling using the 7tools preset. The commands for all steps have “-nh 1” since the yeast is a non-human species.
```
export PATH=$PATH:${MOPline-install-path}/scripts/run_SVcallers
mkdir yeast_run
cd yeast_run
ln -s ../Yeast_10samples/S288C.fa
ln -s ../Yeast_10samples/S288C.fa.fai
ln -s ../Yeast_10samples/Sc.R64-1-1.104.ensemble.gff3
ln -s ../Yeast_10samples/Sc_simpleRepeat.txt
ln -s ../Yeast_10samples/MELT_lib
ln -s ../Yeast_10samples/bam_list.txt
create_bam_link.pl -b bam_list.txt -bd ../Yeast_10samples
```

#### [Step-0] Run SV detection tools (working directory: yeast_run)

(Single sample mode [sample: CBS457])  
```
mkdir CBS457
cd CBS457
run_single.pl -c ../config.yeast.txt -b CBS457.bam -sn CBS457
```
A sample config file (config.yeast.txt) is contained in the Sample_data_output/yeast_run folder.
A run with this sample config file will generate SV call results of 7 tools in 7 tool directories.
        
(Batch mode using slurm)  
```
run_batch_slurm.pl -c config.yeast.txt -b bam_list.txt -a <account> -p <partition>
```
A run with this sample config file will generate SV call results of 7 tools in 7 tool directories in each of 10 sample directories.

#### [Step-1] Select overlap calls from SV call sets
```
mopline merge_7tools -s bam_list.txt -rl 101 -d Merge_7tools -nh 1 -r S288C.fa.fai
```
The command generates a vcf files named `${sample}.Merge.ALL.vcf` in the Merge_7tools directory generated in each sample directory. For non-human species, a reference index file should be specified with the -r option.

#### [Step-2] Add alignment statistics to SV sites
```
mopline create_cov -b bam_list.txt -r S288C.fa -rl 101 -n 4 -nh 1
```
The command creates a Cov directory under the sample directory, which contains the coverage files for each chromosome. A reference fasta file must be specified with the -r option.
```
mopline add_cov -s bam_list.txt -ts 7tools -vd Merge_7tools -n 4 -nh 1 -r S288C.fa.fai
```
The command adds alignment statistics and genotype data to `${sample}.Merge.ALL.vcf` for each sample. Check the values given in the DR, DS, and SR tags in the FORMAT/SAMPLE fields of the vcf files. For non-human species, a reference index file should be specified with the -r option.

#### [Step-3] Merge vcf files from multiple samples
```
mopline joint_call -s bam_list.txt -md Merge_7tools -od JointCall -p MOPline -nh 1
```
This generates a MOPline.All-samples.vcf in the JointCall directory.

#### [Step-4] Genotype and SMC
```
mopline smc -v JointCall/MOPline.All-samples.vcf -ts 7tools -r S288C.fa.fai -od MOPline_SMC -p MOPline.smc -n 4 -nh 1 -sr Sc_simpleRepeat.txt
```
A vcf file named MOPline.smc.vcf has been created in the MOPline_SMC directory, with new MC tags in the FORMAT/SAMPLE fields and AF, AC, DPR, SR, and DPS keys in the INFO field. For non-human species, the -sr option specifies a simple repeat file.

#### [Step-5] Annotate
```
cd MOPline_SMC
mopline annotate -v MOPline.smc.vcf -p MOPline -n 4 -nh 1 -r Sc.R64-1-1.104.ensemble.gff3
```
This will generate two annotated vcf files in the working directory, MOPline.annot vcf and MOPline.AS.annot.vcf. The latter vcf file contains sample-level gene-overlap annotations with AN tags in the FORMAT/SAMPLE fields. For non-human species, a gff3 annotation file must be specified with the -r option.

#### [Step-6] Filter
```
mopline filter -v MOPline.AS.annot.vcf -nh 1 > MOPline.AS.annot.filt.vcf
```
",0,0.42,0.42,,,,,,56,1,,,
162495695,MDEwOlJlcG9zaXRvcnkxNjI0OTU2OTU=,ucsc-hub-js,GMOD/ucsc-hub-js,0,GMOD,https://github.com/GMOD/ucsc-hub-js,read and write UCSC track and assembly hub files in node or the browser,0,2018-12-19 22:05:12+00:00,2025-01-22 13:11:37+00:00,2025-01-22 13:12:41+00:00,,1066,5,5,TypeScript,1,1,1,1,0,0,2,0,0,6,mit,1,0,0,public,2,6,5,master,1,"['garrettjstevens', 'cmdcolin', 'greenkeeper[bot]', 'rbuels']",1,"# ucsc-hub-js

read and write UCSC track and assembly hub files in node or the browser

## Status

[![Build Status](https://img.shields.io/github/actions/workflow/status/GMOD/ucsc-hub-js/push.yml?branch=master)](https://github.com/GMOD/ucsc-hub-js/actions)
[![NPM version](https://img.shields.io/npm/v/@gmod/ucsc-hub.svg?logo=npm\&style=flat-square)](https://npmjs.org/package/@gmod/ucsc-hub)
[![Coverage Status](https://img.shields.io/codecov/c/github/GMOD/ucsc-hub-js/master.svg?logo=codecov\&style=flat-square)](https://codecov.io/gh/GMOD/ucsc-hub-js/branch/master)

## Usage

Read about hub.txt, genomes.txt, and trackDb.txt files here:
<https://genome.ucsc.edu/goldenpath/help/hgTrackHubHelp.html>

## API

<!-- Generated by documentation.js. Update this documentation by updating the source code. -->

#### Table of Contents

*   [GenomesFile](#genomesfile)
    *   [Parameters](#parameters)
*   [HubFile](#hubfile)
    *   [Parameters](#parameters-1)
*   [RaFile](#rafile)
    *   [Parameters](#parameters-2)
    *   [Properties](#properties)
*   [RaStanza](#rastanza)
    *   [Parameters](#parameters-3)
*   [SingleFileHub](#singlefilehub)
    *   [Parameters](#parameters-4)
*   [TrackDbFile](#trackdbfile)
    *   [Parameters](#parameters-5)
    *   [settings](#settings)
        *   [Parameters](#parameters-6)

### GenomesFile

**Extends RaFile**

Class representing a genomes.txt file.

#### Parameters

*   `genomesFile` **([string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String) | [Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array)<[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)>)** A genomes.txt file as a string (optional, default `[]`)

<!---->

*   Throws **[Error](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Error)** Throws if the first line of the hub.txt file doesn't start
    with ""genome \<genome\_name>"" or if it has invalid entries

### HubFile

**Extends RaStanza**

Class representing a hub.txt file.

#### Parameters

*   `hubFile` **([string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String) | [Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array)<[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)>)** A hub.txt file as a string (optional, default `[]`)

<!---->

*   Throws **[Error](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Error)** Throws if the first line of the hub.txt file doesn't start
    with ""hub \<hub\_name>"", if it has invalid entries, or is missing required
    entries

### RaFile

Class representing an ra file. Each file is composed of multiple stanzas, and
each stanza is separated by one or more blank lines. Each stanza is stored in
a Map with the key being the value of the first key-value pair in the stanza.
The usual Map methods can be used on the file. An additional method `add()`
is available to take a raw line of text and break it up into a key and value
and add them to the class. This should be favored over `set()` when possible,
as it performs more validity checks than using `set()`.

#### Parameters

*   `raFile` **([string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String) | [Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array)<[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)>)** An ra file, either as a single
    string or an array of strings with one stanza per entry. Supports both LF
    and CRLF line terminators. (optional, default `[]`)
*   `options` **[object](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Object)**&#x20;

    *   `options.checkIndent` **[boolean](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean)** \[true] - Check if a the stanzas within
        the file are indented consistently and keep track of the indentation

#### Properties

*   `nameKey` **([undefined](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/undefined) | [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String))** The key of the first line of all the
    stanzas (`undefined` if the stanza has no lines yet).

<!---->

*   Throws **[Error](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Error)** Throws if an empty stanza is added, if the key in the first
    key-value pair of each stanze isn't the same, or if two stanzas have the same
    value for the key-value pair in their first lines.

### RaStanza

Class representing an ra file stanza. Each stanza line is split into its key
and value and stored as a Map, so the usual Map methods can be used on the
stanza.

#### Parameters

*   `stanza` **([string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String) | [Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array)<[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)>)**  (optional, default `[]`)
*   `options` **{checkIndent: [boolean](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean)?, skipValidation: [boolean](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean)?}?**&#x20;

### SingleFileHub

Class representing a ""single-file"" hub.txt file that contains all the
sections of a hub in a single file.

#### Parameters

*   `hubText` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)**&#x20;

### TrackDbFile

**Extends RaFile**

Class representing a genomes.txt file.

#### Parameters

*   `trackDbFile` **([string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String) | [Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array)<[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)>)** A trackDb.txt file as a string (optional, default `[]`)
*   `options` **any?**&#x20;

<!---->

*   Throws **[Error](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Error)** Throws if ""track"" is not the first key in each track or if a
    track is missing required keys

#### settings

Gets all track entries including those of parent tracks, with closer
entries overriding more distant ones

##### Parameters

*   `trackName` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)** The name of a track

<!---->

*   Throws **[Error](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Error)** Throws if track name does not exist in the trackDb

## License

MIT © [Generic Model Organism Database Project](http://gmod.org/wiki/Main_Page)
",0,0.81,0.81,,,,,,0,14,,,
655467911,R_kgDOJxGlhw,dbTitleBook27,limalewat11/dbTitleBook27,0,limalewat11,https://github.com/limalewat11/dbTitleBook27,dbTitleBook27 - Book Title List,0,2023-06-19 01:02:50+00:00,2023-06-19 01:04:20+00:00,2023-06-19 01:04:16+00:00,,2076,0,0,HTML,1,1,1,1,1,0,0,0,0,1,,1,0,0,public,0,1,0,main,1,['limalewat11'],,,0,0.56,0.56,,,,,,0,1,,,
7008871,MDEwOlJlcG9zaXRvcnk3MDA4ODcx,coursework,dysfungi/coursework,0,dysfungi,https://github.com/dysfungi/coursework,Organized School Coursework Repository,0,2012-12-04 23:31:09+00:00,2017-06-08 05:57:07+00:00,2017-04-12 18:44:59+00:00,,47530,3,3,C,1,1,1,1,0,0,8,0,0,0,,1,0,0,public,8,0,3,master,1,[],,"Id: README,v 1.1 2012-12-04 15:35:19-08 dmfrank - $
Derek Frank
Readme

This repository is used to display coursework I have done.

Directories:
  Coursework
  |- cs101-Algorithms_and_ADTs
  |  |- asg1-Java-List_ADT
  |  |- asg2-C-List_ADT
  |  |- asg3-Java-Sparse_Matrix_ADT
  |  |- asg4-C-BFS_and_Shortest_Paths_in_Graphs
  |  |- asg5-C-DFS-and_Strong_Components_of_Graphs
  |- cs104a-Fundamentals_of_Compiler_Design_I
  |  |- asg1-C-Stringtable_ADT
  |  |- asg2-C-Scanner
  |  |- asg3-C-Parser
  |- cs109-Advanced_Programming
  |  |- asg1-C++-Shell_and_Function_Pointers
  |  |- asg2-C++-DC_and_Bigint_Class
  |  |- asg3-C++-Draw_and_Inheritance
  |  |- asg4-C++-Listmap_Templates
  |  |- asg5-Java-Dining_Philosophers
  |- cs111-Operating_Systems
  |  |- asg1-C-Shell
  |  |- asg2-C-Synchronization
  |  |- asg3-C-Memory_Allocation
  |  |- asg4-C-Encryption
  |- cs112-Comparative_Programming_Languages
  |  |- asg2-Scheme-Silly_Basic_Interpreter
  |  |- asg3-OCaml-DC
  |  |- asg5-Prolog-Airline
  |- cs140-Artificial_Intelligence
  |  |- asg2-Lisp-RPS_Safari_Agent
  |  |- asg3-Lisp-RPS_Safari_Agent
  |  |- asg4-Lisp-RPS_Safari_Agent
  |- ce150-Computer_Networks
  |  |- project-C-Client_Server_Voting
  |- ams147-Computational_Methods_and_Application
  |  |- asg1-Matlab-Fixed_Point_Iterative_Method
  |  |- asg2-Matlab-Golden_Search_Method
  |  |- asg3-Matlab-Newton_Method
  |  |- asg4-Matlab-Numerical_Integration
  |  |- asg5-Matlab-Error_Estimation_and_Euler
  |  |- asg6-Matlab-Runge_Kutta_Method
  |  |- asg7-Matlab-Interpolation
  |  |- asg8-Matlab-Fast_Fourier_Transforms
  |  |- project-Matlab-Three_Body_System
  |- ams114-Dynamical_Systems
  |  |- asg5-Octave-Stability
  |  |- asg6-Octave-Limit_Cycles
  |  |- asg7-Octave-Bifurcation
  |  |- asg8-Octave-Lorenz_Equations
  |  |- final-Octave-Nonlinear_Dynamics_and_Chaos

Courses:
  Computer Science:
    Artificial Intelligence (CS 140)
    Computational Models (CS 130)
    Comparative Programming Languages (CS 112)
    Introduction to Operating Systems (CS 111)
    Advanced Programming (CS 109)
    Fundamentals of Compiler Design I (CS 104A)
    Introduction to Analysis of Algorithm (CS 102)
    Algorithms & Abstract Data Types (CS 101)
    Introduction to Data Structures (CS 12B/M)
    Intermediate Programming (CS 11)
    Introduction to Computer Science (CS 10)
    Introduction to Programming in Java (CS 5J)
  Computer Engineering:
    Introduction to Computer Networks (CE 150/L)
    Computer Architecture (CE 110)
    Probability & Statistics for Engineers (CE 107)
    Introduction to Networking & the Internet(CE 80N)
    Applied Discrete Mathematics (CE 16)
    Computer Systems & Assembly Language (CE 12/L)
  Applied Mathematics & Statistics:
    Computational Methods & Applications (AMS 147)
    Introduction to Dynamical Systems (AMS 114)
    Mathematical Methods for Engineers I (AMS 10)
    Mathematical Methods for Engineers II (AMS 20)
  Mathematics:
    Partial Differential Equations (Math 107)
    Multivariable Calculus A (Math 23A)
    Multivariable Calculus B (Math 23B)
    Calculus for Science, Engineering, and Mathematics B (Math 19B)
  Physics:
    Introduction to Physics I: Mechanics (Phys 6A/L)
    Introduction to Physics III: Electricity & Magnetism (Phys 6C/N)
  Miscellaneous:
    Introductory Macroeconomics (Econ 2)
    Introduction to Psychology (Psyc 1)
    American Popular Music (Musc 11C)
    Literary Interpretation (Lit 1)
    Rhetoric & Inquiry (Writ 2)
    Self & Society (Stev 80A)
    Self & Society II (Stev 81A)

",1,0.78,0.78,,,,,,0,0,,,
25018238,MDEwOlJlcG9zaXRvcnkyNTAxODIzOA==,ucscore,d-linkboy/ucscore,0,d-linkboy,https://github.com/d-linkboy/ucscore,,0,2014-10-10 03:04:31+00:00,2014-10-10 03:13:25+00:00,2017-08-10 09:27:17+00:00,http://d-linkboy.github.io/ucscore,94,0,0,,1,1,1,1,1,0,0,0,0,0,gpl-2.0,1,0,0,public,0,0,0,master,1,['d-linkboy'],,"ucscore
=======
",0,0.82,0.82,,,,,,0,1,,,
671657338,R_kgDOKAiteg,MFC_Modeling,jlab-sensing/MFC_Modeling,0,jlab-sensing,https://github.com/jlab-sensing/MFC_Modeling,,0,2023-07-27 20:41:45+00:00,2024-10-15 21:59:39+00:00,2024-10-15 21:59:36+00:00,,2411,0,0,Jupyter Notebook,1,1,1,0,0,0,1,0,0,0,,1,0,0,public,1,0,0,main,1,"['adunlop621', 'cjosephson']",1,"# MFC_Modeling
This is the Github repository for the code used in the paper [Deep Learning for Predicting Microbial Fuel Cell Energy Output](https://dl.acm.org/doi/10.1145/3674829.3675358). In this paper, we explore ways to improve the viability of intermittenly powered computing systems, with a focus on Soil Microbial Fuel Cells (SMFCs). SMFCs are an energy source which generate power from natural microbial interactions within soil, and they are a promising energy source for many types of environmental sensing tasks, such as smart farming and wildfire prevention. Very little work currently exists that attempts to model the relationship between soil conitions and SMFC energy generation, and this work is the first to use machine learning to do so.


All relevant datasets and models are stored in Hugging Face at https://huggingface.co/datasets/adunlop621/Soil_MFC/tree/main

In order for the code to run properly, it is neccesary to download the datasets from Hugging Face and place them in the same directory as the code with the following file names. ```Dataset 1```, collected from a deployment at Stanford University, is stored in ```stanfordMFCDataset.zip```, which expands into the directory ```rocket4```. ```Dataset 2```, collected from a deployment at UC Santa Cruz, is stored in the ```ucscMFCDataset directory```. In order to load the pretrained models used in the paper, it is neccesary to download the ```trained_models``` directory. 

If you want to just load and evaluate pretrained models instead of training from scratch, use the code in `Pretrained.ipynb`.

We define two main types of models in this work, based off the datasets used to train them: ```Type 1 models``` are trained on ```Dataset 1```, and ```Type 2 models``` are trained on ```Dataset 2```. 

[Type1.ipynb](https://github.com/jlab-sensing/MFC_Modeling/blob/main/Type1.ipynb), [Type2.ipynb](https://github.com/jlab-sensing/MFC_Modeling/blob/main/Type2.ipynb), and [SNN.ipynb](https://github.com/jlab-sensing/MFC_Modeling/blob/main/SNN.ipynb) contain the code used to train all all the models in the paper, as well as load pretrained models and run time series rolling validation. 

Pretrained models have the following naming conventions: model type (Type 1, 2, 1A, 1B, etc.), time horizon (3min, 5min, etc.), and quantile (quant5, quant50, etc.). For example, a Type 1A model predicting lower bound values in time intervals of 15 minutes would be called ```type1A_15min_quant5```.

The same naming convention holds for SNN models, with the model type simply being snn, ie. ```snn_15min_quant50```.






",1,0.76,0.76,,,,,,0,3,,,
736534426,R_kgDOK-afmg,crispr-offtarget-uncertainty,furkanozdenn/crispr-offtarget-uncertainty,0,furkanozdenn,https://github.com/furkanozdenn/crispr-offtarget-uncertainty,Learning to quantify uncertainty in off-target activity for CRISPR guide RNAs,0,2023-12-28 06:58:47+00:00,2024-11-13 12:08:39+00:00,2024-08-17 12:58:51+00:00,,9444,3,3,Python,1,1,1,1,0,0,2,0,0,2,mit,1,0,0,public,2,2,3,main,1,['furkanozdenn'],,"# Learning to quantify uncertainty in off-target activity for CRISPR guide RNAs

> crispAI is a deep learning-based tool that predicts the off-target cleavage activity for a given single guide RNA (sgRNA) and target DNA sequence pair and quantifies the associated uncertainty.
> crispAI-aggregate is an uncertainty aware genome-wide specificity score for sgRNA's.

> <a href=""https://en.wikipedia.org/wiki/Deep_learning"" target=""_blank"">**Deep Learning**</a>,  <a href=""https://www.nature.com/articles/s41467-018-04252-2"" target=""_blank"">**CRISPR-based genome editing**</a>, <a href=""https://link.springer.com/content/pdf/10.1007/978-3-319-54339-0.pdf"" target=""_blank"">**Uncertainty Quantification**</a>


---

## Authors

<a href=""https://www.cs.ox.ac.uk/people/furkan.ozden/"" target=""_blank"">**Furkan Ozden**</a> and <a href=""https://www.cs.ox.ac.uk/people/peter.minary/"" target=""_blank"">**Peter Minary**</a>

---

## Questions & comments 

[firstauthorname].[firstauthorsurname]@cs.ox.ac.uk

- [Installation](#installation)
- [Usage](#usage)
- [License](#license)

---


## Installation

### Step-1: Download and untar USCS chroms required for Cas-OFFinder (if you do not plan to use crispAI-aggregate score you can skip this step).

Download target organism's chromosome FASTA files.

- http://hgdownload.soe.ucsc.edu/downloads.html (UCSC genome sequences library)

Extract all FASTA files in a directory and name it 'ucsc_chroms'.

For example (human chromosomes, in POSIX environment):

    $ wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz
    $ mkdir -p /var/chromosome/human_hg19
    $ tar zxf chromFa.tar.gz -C /var/chromosome/human_hg19
    $ ls -al /var/chromosome/human_hg19
      drwxrwxr-x.  2 user group      4096 2013-10-18 11:49 .
      drwxrwxr-x. 16 user group      4096 2013-11-12 12:44 ..
      -rw-rw-r--.  1 user group 254235640 2009-03-21 00:58 chr1.fa
      -rw-rw-r--.  1 user group 138245449 2009-03-21 01:00 chr10.fa
      -rw-rw-r--.  1 user group 137706654 2009-03-21 01:00 chr11.fa
      -rw-rw-r--.  1 user group 136528940 2009-03-21 01:01 chr12.fa
      -rw-rw-r--.  1 user group 117473283 2009-03-21 01:01 chr13.fa
      -rw-rw-r--.  1 user group 109496538 2009-03-21 01:01 chr14.fa

Move obtained directory 'ucsc_chroms' to project folder ./crispAI_score/casoffinder/

    $ mv ucsc_chroms ./crispAI_score/casoffinder

### Step-2: Use conda to create environment with required packages.
    
    $ conda env create -f env/crispAI_env.yml
    $ conda activate crispAI_env

### Step-3: Install required R packages for NuPoP library using R_environment.csv (optionally run restore_environment.R).
- Note: crispAI uses R version 4.2 to annotate target sites. You should have R version 4.2 installed on your system.

    $ cd env/
    $ Rscript restore_environment.R


### Step-4: Test installation by running on example input data, this will run with default parameters.

    $ python crispAI.py --input_file example_offt_input.txt --mode offt-score


---

## Usage 

### offt-score mode: Off-target cleavage activity prediction for sgRNA-target pairs.

- Input: See `example_offt_input.txt` file for example input.
- Command: 
```bash
python crispAI.py --mode offt-score --input_file example_offt_input.txt --N_samples 1000 --N_mismatch 4 --O crispAI_output.csv --gpu -1
```
- Arguments:
    - `--mode`: Mode of operation. Default is 'offt-score'.
    - `--input_file`: Input file name. Default is 'input.csv'.
    - `--N_samples`: Number of samples to draw from posterior distribution. Default is 1000. Range is [100, 2000].
    - `--N_mismatch`: Number of mismatches to search for off-target sites. Default is 4.
    - `--O`: Output file name. Default is 'crispAI_output.csv'.
    - `--gpu`: CUDA device number for GPU support. Default is -1 for CPU.

- Output: The output will be a CSV file with the following columns: `sgRNA`, `chr`, `start`, `end`, `strand`, `target_sequence`, `mean`, `samples`, `std`. Here is an example output row:

| sgRNA | chr | start | end | strand | target_sequence | mean | samples | std |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GGGTGGGGGGAGTTTGCTCCNGG | chr6 | 43769554 | 43769576 | - | GGGTGGGGGGAGTTTGCTCCTGG | 62.71200180053711 | 5,86,53,0,0,220,0,0,0,16,217 | 108.10479736328125 |

![Offt-Score Example](img/offt.jpg)


### agg-score mode: Aggregate off-target cleavage activity prediction for sgRNAs.

- Input: See `example_agg_input.txt` file for example input.
- Command: 
```bash
python crispAI.py --mode agg-score --input_file example_agg_input.txt --N_samples 1000 --N_mismatch 4 --O crispAI_aggregate_output.csv --gpu -1 --plot-agg
```
- Arguments:
    - `--mode`: Mode of operation. Default is 'offt-score'.
    - `--input_file`: Input file name. Default is 'input.csv'.
    - `--N_samples`: Number of samples to draw from posterior distribution. Default is 1000. Range is [100, 2000].
    - `--N_mismatch`: Number of mismatches to search for off-target sites. Default is 4.
    - `--O`: Output file name. Default is 'crispAI_output.csv'.
    - `--gpu`: CUDA device number for GPU support. Default is -1 for CPU.
    - `--plot-agg`: Flag to plot aggregate score distribution for sgRNAs. 

Replace the values in the command with your actual values when running the program. The values provided in the command are the default values. If you want to use the default values, you can omit them from the command. For example, if you want to use the default value for `--N_samples`, you can omit `--N_samples 1000` from the command. The program will automatically use the default value. If you want to use GPU, replace `-1` with your actual CUDA device number. If you want to plot the aggregate score distribution for sgRNAs in the agg-score mode, add `--plot-agg` flag.

- Output: The output will be a CSV file with the following columns: `sgRNA`, `aggregate_score_mean`, `aggregate_score_median`, `aggregate_score_std`, `N-samples`. Here is an example output row:

| sgRNA | aggregate_score_mean | aggregate_score_median | aggregate_score_std | N-samples |
| --- | --- | --- | --- | --- |
| GTCCCCTGAGCCCATTTCCTNGG | 3.7147998809814453 | 3.2274999618530273 | 1.9692000150680542 | 1.85,1.9,7.16,2.97,2.57,4.05,3.81,1.96,4.08,2.32,4.09,5.75 |

<img src=""img/agg.jpg"" alt=""Agg-Score Example"" width=""400""/>

## License

- **[CC BY-NC-SA 2.0](https://creativecommons.org/licenses/by-nc-sa/2.0/)**
- Copyright 2023 © crispAI.
",0,0.58,0.58,,,,,,0,1,,,
174021402,MDEwOlJlcG9zaXRvcnkxNzQwMjE0MDI=,towerofpysa,maxpereira/towerofpysa,0,maxpereira,https://github.com/maxpereira/towerofpysa,python scraper for ucsc pisa class search,0,2019-03-05 21:12:12+00:00,2019-10-01 02:31:53+00:00,2019-10-01 02:31:51+00:00,https://devilworship.net/pisa,219,0,0,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['maxpereira'],,"# towerofpysa
Quick and dirty python script plus web frontend in order to display all UCSC courses available for the current academic term.

Setup instructions will be added in the near future.

## Known Issues
The parser cannot handle courses that have multiple locations.
",1,0.75,0.75,,,,,,0,1,,,
512145185,R_kgDOHoa3IQ,ucsc,kaviabey/ucsc,0,kaviabey,https://github.com/kaviabey/ucsc,,0,2022-07-09 09:27:44+00:00,2022-07-13 02:27:58+00:00,2022-07-13 07:38:52+00:00,,1,0,0,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['kaviabey'],,,1,0.79,0.79,,,,,,0,1,,,
338515815,MDEwOlJlcG9zaXRvcnkzMzg1MTU4MTU=,Credible-sample-elicitation,weijiaheng/Credible-sample-elicitation,0,weijiaheng,https://github.com/weijiaheng/Credible-sample-elicitation,"[AISTATS2021] Official implementation of ""Sample Elicitation""",0,2021-02-13 06:53:15+00:00,2025-01-17 16:07:08+00:00,2021-04-12 19:03:39+00:00,,3472,29,29,Python,1,1,1,1,0,0,6,0,0,0,mit,1,0,0,public,6,0,29,main,1,['weijiaheng'],,"# Sample elicitation

This code is the official implementation of our paper ""[Sample Elicitation](https://arxiv.org/abs/1910.03155)"" accepted by AISTATS2021.

## Utilities

📋 (1) Syntheric Experiment: this folder is the Matlab implementation of our porposed mechanism w.r.t. synthetic data (as an example, we adopt the synthetic data drawn from 2-D Gaussian distributions).

📋 (2) MNIST Experiment: this folder is the Python implementation of our porposed mechanism given image data (as an example, we adopt MNIST test dataset).

Note: details of reproducing our experiment results are mentioned in the ``README"" file in each folder.

## Citation

If you use our code, please cite the following paper:

```
@inproceedings{wei2021sample,
  title={Sample Elicitation},
  author={Wei, Jiaheng and Fu, Zuyue and Liu, Yang and Li, Xingyu and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2692--2700},
  year={2021},
  organization={PMLR}
}
```


## Contact

If you have any concerns on the impletation, feel free to send me an e-mail (jiahengwei@ucsc.edu) or report an issue.


## References

The codes of  `fid.py`,  `fid_peer.py` are based on **https://github.com/mseitzer/pytorch-fid/blob/master/pytorch_fid**



## Thanks for watching!
",1,0.68,0.68,,,,,,0,3,,,
471616975,R_kgDOHBxNzw,ucsc_big_data,marilsoncampos/ucsc_big_data,0,marilsoncampos,https://github.com/marilsoncampos/ucsc_big_data,,0,2022-03-19 06:56:23+00:00,2024-05-07 19:42:09+00:00,2024-05-07 19:42:04+00:00,,64520,0,0,Rich Text Format,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['marilsoncampos'],,"# ucsc_big_data
UC Santa Cruz materials from my class
",1,0.9,0.9,,,,,,0,1,,,
81371739,MDEwOlJlcG9zaXRvcnk4MTM3MTczOQ==,Tour,hellorichardpham/Tour,0,hellorichardpham,https://github.com/hellorichardpham/Tour,(Android) Official Virtual Tour app currently in use with the Jack Baskin School of Engineering at UC Santa Cruz,0,2017-02-08 20:16:42+00:00,2017-04-18 00:17:02+00:00,2017-04-12 22:59:12+00:00,,34407,0,0,Java,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['hellorichardpham'],,"# Tour
Virtual Tour app for use with the Jack Baskin School of Engineering at UC Santa Cruz

Author: Richard Willie Pham
Email: rwpham@ucsc.edu

Find this app on the Google Play Store under the name BSOE Virtual Tour. 
An iOS version is currently under development and will be released by June 2017. 

This app is currently being used by the Jack Baskin School of Engineering for prospective freshmen and graduate student tours.
Enhancements and bug fixes are still underway. 
",1,0.8,0.8,,,,,,0,1,,,
205427234,MDEwOlJlcG9zaXRvcnkyMDU0MjcyMzQ=,AtacWorks,NVIDIA-Genomics-Research/AtacWorks,0,NVIDIA-Genomics-Research,https://github.com/NVIDIA-Genomics-Research/AtacWorks,Deep learning based processing of Atac-seq data,0,2019-08-30 17:22:49+00:00,2025-02-21 14:46:06+00:00,2023-02-16 22:29:51+00:00,https://clara-parabricks.github.io/AtacWorks/,41068,129,129,Jupyter Notebook,1,1,1,1,1,0,23,0,0,26,other,1,0,0,public,23,26,129,dev-v0.4.0,1,"['avantikalal', 'ntadimeti', 'ohadmo', 'GPUtester', 'foertter']",1,"# AtacWorks

AtacWorks is a deep learning toolkit for coverage track denoising and peak calling from low-coverage or low-quality ATAC-Seq data.

![AtacWorks](data/readme/atacworks_slides.gif)

## Installation

### System requirements

* Ubuntu 16.04+
* CUDA 9.0+
* Python 3.7.0+
* GCC 5+
* (Optional) A conda or virtualenv setup
* Any NVIDIA GPU that supports CUDA 9.0+

**AtacWorks training and inference currently does not run on CPU.**

### PyPI installation
To install atacworks in your environment, run the following in your terminal
```
pip install atacworks==0.3.0
```

### Docker Installation
If you'd like to skip all installation and use a pre-installed docker image instead,
follow the instructions [here](Dockerfile.md), section ""Pre-installed AtacWorks"".

If you'd like to pull a docker image that contains AtacWorks source code, then 
follow the instructions [here](Dockerfile.md), section ""AtacWorks from Source"".

## Build from Source
Follow the instructions below if you are interested in running AtacWorks tutorial notebooks outside
of docker.

### 1. Clone repository

#### Latest released version
This will clone the repo to the `master` branch, which contains code for latest released version
and hot-fixes.

```
git clone --recursive -b master https://github.com/clara-genomics/AtacWorks.git
```
#### Latest development version
This will clone the repo to the default branch, which is set to be the latest development branch.
This branch is subject to change frequently as features and bug fixes are pushed.

```
git clone --recursive https://github.com/clara-genomics/AtacWorks.git
```

### 2. Install dependencies

* Download `bedGraphToBigWig` and `bigWigToBedGraph` binaries and add $PATH to your bashrc.
    ```
    rsync -aP rsync://hgdownload.soe.ucsc.edu/genome/admin/exe/linux.x86_64/bedGraphToBigWig <custom_path>
    rsync -aP rsync://hgdownload.soe.ucsc.edu/genome/admin/exe/linux.x86_64/bigWigToBedGraph <custom_path>
    export PATH=""$PATH:<custom_path> >> ~/.bashrc""
    ```

* Install pip dependencies

    ```
    cd AtacWorks && pip install -r requirements.txt
    ```
* Optional -- Install macs2.
  Only required if you want to use macs2 subcommands to call peaks based on peak probabilities generated by AtacWorks.

    ```
    pip install macs2==2.2.4
    ```

* Install atacworks

    ```
    pip install .
    ```

 
### 3. Tests

Run unit tests to verify that installation was successful

    ```
    python -m pytest tests/
    ```

## Workflow
AtacWorks trains a deep neural network to learn a mapping between noisy (low coverage/low cell count/low quality) ATAC-seq data and matching clean (high coverage, high cell count, and/or high quality) ATAC-seq data. Both noisy and clean data should be from the same cell type or tissue. Once this mapping is learned, the trained model can be applied to improve other noisy ATAC-Seq datasets.

### 1. Training an AtacWorks model

See [Tutorial 1](tutorials/tutorial1.md) for a workflow detailing the steps of model training and how to modify the parameters used in these steps.

### 2. Denoising and peak calling using a trained AtacWorks model

See [Tutorial 2](tutorials/tutorial2.md) for an advanced workflow detailing the prediction using a trained model, and how to modify the parameters used in these steps.

### 3. RAPIDS + AtacWorks Visualization of Single-cell Chromatin Accessibility
See [Tutorial here](tutorials/rapids_examples.md) for a demonstration of using Atacworks in conjunction with RAPIDS, to visualize cell type-specific chromatin accessibility and generate cell type-specific peak calls from single-cell ATAC-seq data. 

## FAQ
1. What's the preferred way for setting up the environment?
    > A virtual environment, conda installation or docker is preferred for running atacworks. Follow the instructions of setting up your preferred platforms. Once the env is setup, you can follow the Installation section above to install all the necessary dependencies.


## Contributing to AtacWorks
This section is only for developers of atacworks. If you would like to contribute to atacworks codebase,
take a look at the development guidelines [here](https://clara-parabricks.github.io/development.html#contributing-to-clara-omics).
You can read about our Continuous Integration (CI) test flow [here](https://clara-parabricks.github.io/development.html#ci-testing).

### Running CI Tests Locally
When a PR is submitted to the github, CI tests are triggered by github CI. If you would like to run those tests locally,
follow the instructions below.
!!CAUTION!!
Please note, your git repository will be mounted to the container, any untracked files will be removed from it.
Before executing the CI locally, stash or add them to the index.

Requirements:
1. [docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)
2. [nvidia-docker](https://github.com/NVIDIA/nvidia-docker)
3. [nvidia-container-runtime](https://github.com/NVIDIA/nvidia-container-runtime)

Run the following command to execute the CI build steps inside a container locally:

```
bash ci/local/build.sh -r <Atacworks repo path>
```
ci/local/build.sh script was adapted from [rapidsai/cudf](https://github.com/rapidsai/cudf/tree/branch-0.11/ci/local)
The default docker image is **clara-genomics-base:cuda10.1-ubuntu16.04-gcc5-py3.6**.

Other images from [gpuci/clara-genomics-base](https://hub.docker.com/r/gpuci/clara-genomics-base/tags) repository can be used instead, by using -i argument as shown below:

```
bash ci/local/build.sh -r <Atacworks repo path> -i gpuci/clara-genomics-base:cuda10.0-ubuntu18.04-gcc7-py3.6
```

## Citation

Please cite AtacWorks as follows:

Lal, A., Chiang, Z.D., Yakovenko, N. et al. Deep learning-based enhancement of epigenomics data with AtacWorks. Nat Commun 12, 1507 (2021). https://doi.org/10.1038/s41467-021-21765-5
",0,0.23,0.23,,,,,,0,8,,,
69304806,MDEwOlJlcG9zaXRvcnk2OTMwNDgwNg==,LocusZooms,Geeketics/LocusZooms,0,Geeketics,https://github.com/Geeketics/LocusZooms,Make LocusZoom-like plots with your own LD matrix.,0,2016-09-27 00:41:33+00:00,2025-03-03 14:06:28+00:00,2022-07-17 23:41:28+00:00,,12286,39,39,HTML,1,1,1,1,0,0,21,0,0,1,,1,0,0,public,21,1,39,master,1,"['Geeketics', 'rikutakei', 'mandyphippsgreen']",,"[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5154379.svg)](https://doi.org/10.5281/zenodo.5154379)

# LocusZoom-like Plots for GWAS Results
Authors: Tanya J Major and Riku Takei

## Make LocusZoom-like plots with your own LD matrix.

This package allows the user to create regional Manhattan plots from p-values, log(p-values), or log(Bayes Factors) with points coloured according to LD and genes annotated beneath. The LD input can be generated from the users own data (e.g. for a non-reference population). The package comes with a number of reference files for gene annotation, but is not limited to the use of these files.

This package was created for use with human SNP data, but can be used to plot non-human data.

This script creates an R function to create LocusZoom-like plots. Three example input files are included for test purposes, along with an example .jpg output.

This script has one package dependency: `scales`

  - Example.assoc.linear: A file of PLINK association results (only the ""CHR"", ""SNP"", ""BP"", and ""P"" columns are essential)
  - Example.ld: A file of the LD between the SNP to be labelled (top-hit / SNP of interest) and the SNPs included in the PLINK results file
    - this file MUST have a column called ""SNP_B"" (containing a list of all the SNPs in the results file) and a column called ""R2"" (containing the R^2 LD value of each SNP). The SNP names MUST match the names in the SNP column of the results file.
    - this file can be created for you by the locus_zoom.R script IF you have access to the Biochem servers and have rsIDs in your results file, if you are running this script on your local machine you will also need to ensure bcftools and plink2 are installed
  - Example.genes: A file of the genes within the region for use in the annotation step. This file must have five columns, ""Gene"", ""Chrom"", ""Start"", ""End"", and ""Coding"". The `{Gencode,UCSC}_GRCh37_Genes_UniqueList{2017,2021}.txt` files can be used for this file.

### Example locus.zoom run:

```
# load necessary files into R
Example.assoc.linear <- read.delim(""Example.assoc.linear"", stringsAsFactors = FALSE, header = TRUE)
Example.ld <- read.table(""Example.ld"", stringsAsFactors = FALSE, header = TRUE)
Unique.genes <- read.delim(""Gencode_GRCh37_Genes_UniqueList2021.txt"", stringsAsFactors = FALSE, header = TRUE)

# load the locuszoom function into R
source(""functions/locus_zoom.R"")

# create a LocusZoom-like plot
locus.zoom(data = Example.assoc.linear,                                    # a data.frame (or a list of data.frames) with the columns CHR, BP, SNP, and P
           region = c(16, 53340000, 54550000),                             # the chromosome region to be included in the plot
           offset_bp = 0,                                                  # how many basepairs around the SNP / gene / region of interest to plot
           ld.file = Example.ld,                                           # a file with LD values relevant to the SNP specified above
           genes.data = Unique.genes,			                   # a file of all the genes in the region / genome
           plot.title = ""Association of FTO with BMI in Europeans"",        # the plot title
           file.name = ""Example.jpg"",                                      # the name of the file to save the plot to
           secondary.snp = c(""rs1121980"", ""rs8060235""),                    # a list of SNPs to label on the plot
           secondary.label = TRUE)                                         # TRUE/FALSE whether to add rsIDs of secondary SNPs to plot
```

![](Example.jpg)

### Compulsory flags:

One of `snp`, `gene`, or `region` must be specified to create the plot:

 - `snp`: specify the SNP to be annotated (you must also include `ignore.lead = TRUE` if choosing this option)
 - `gene`: specify the Gene to make the plot around
 - `region`: specify the chromsome region you want to plot (must be specified as `c(chr, start, end)`

As well as each of the following:

 - `data`: specify the data.frame (or a list of data.frames) to be used in the plot (requires the columns ""CHR"", ""BP"", ""SNP"", and either ""P"" or ""logBF"")
 - `genes.data`: specify a data.frame with gene locations to plot beneath the graph (requires the columns ""Gene"", ""Chrom"", ""Start"", ""End"", and ""Coding"") - the Gencode or UCSC `{Gencode,UCSC}_GRCh37_Genes_UniqueList{2017,2021}.txt` files in this repo can be used for this
 - `plot.title`: specify a title to go above your plot
 - `file.name`: specify a filename for your plot to be saved to

### Optional flags:

 - `ld.file`: specify a data.frame with LD values relevant to the SNP specified by `snp` (requires the columns ""SNP_B"" and ""R2"") 
 - `offset_bp`: specify how far either side of the `snp`, `gene`, or `region` you want the plot to extend (defaults to 200000)
 - `psuedogenes`: when using one of the three gene lists in this repo you can specify whether you want to plot the pseudogenes (defaults to FALSE)
 - `RNAs`: when using one of the two gene lists created in 2021 in this repo you can specify whether you want to plot lncRNA and ncRNA genes (defaults to FALSE)
- `plot.type`: specify the file format of the plot (defaults to ""jpg"", options are ""jpg"", ""svg"", or ""view_only"" which will not save the plot, but output it to RStudio Viewer instead)
 - `nominal`: specify the nominal significance level to draw on the plot (in -log10(_P_), default is 6 or _P_ = 1e-6)
 - `significant`: specify the significance level to draw on the plot (in -log10(_P_), default is 7.3 or _P_ = 5e-8) 
 - `secondary.snp`: provide the list of secondary SNP IDs (must match IDs in results file) to be highlighted on the plot
 - `secondary.label`: specify whether to label the secondary SNPs on the plot (defaults to FALSE)
 - `secondary.circle`: specify whether to add a red circle around the secondary SNPs on the plot (defaults to TRUE)
 - `genes.pvalue`: specify a data.frame of p-values (e.g. MAGMA results) associated with each gene (requires the columns ""Gene"" and ""P"") 
 - `colour.genes`: specify whether to colour genes based on a p-value provided in gene.pvalue (defaults to FALSE)
 - `population`: specify the 1000 genomes population to use when calculating LD if ld.file = NULL (defaults to ""EUR"", options are ""AFR"", ""AMR"", ""EAS"", ""EUR"", ""SAS"", ""TAMA"", and ""ALL"")
 - `sig.type`: specify whether the y-axis should be labelled as -log10(_P_) or log10(BF) (defaults to ""P"", options are ""P"", ""logP"", or ""logBF""). For the ""P"" option an additional -log10 conversion of the input ""P"" column will be performed.
 - `nplots`: specify whether multiple results plots will be saved into your jpeg file (e.g. plot two GWAS results one above another; defaults to FALSE)
 - `ignore.lead`: specify whether to ignore the SNP with the smallest P and use the SNP specified by 'snp' to centre the plot (defaults to FALSE)
 - `rsid.check`: specify whether to check if the SNPs are labelled with rsIDs - should only matter if script is calculating LD for you (defaults to TRUE)
 - `nonhuman`: specify whether the data to plot has come from a non-human sample-set (defaults to FALSE) - if the data going in is from a non-human species make sure the chromosome column is only numbers (e.g. 1 instead of chr1, 23 instead of X). 

## Secondary Example:

_This is not reproducible from the example data._

```
locus.zoom(data = EUR_meta_full1_clean_rsid.nfiltered_chr7,
           gene = ""MLXIPL"",
           offset_bp = 500000,
           genes.data = Gencode_GRCh37_Genes_UniqueList2021,
           plot.title = ""Association of MLXIPL with gout in Europeans"",
           file.name = ""alternateExample.jpg"",
           genes.pvalue = MAGMA_EUR_meta_full_Gencode2021,
           colour.genes = TRUE,
           psuedogenes = FALSE,
           RNAs = TRUE)
```

![](alternateExample.jpg)

",1,0.68,0.68,,,,,,0,2,,,
139642596,MDEwOlJlcG9zaXRvcnkxMzk2NDI1OTY=,SafetyPal,alhooman/SafetyPal,0,alhooman,https://github.com/alhooman/SafetyPal,,0,2018-07-03 22:23:30+00:00,2018-07-26 16:45:12+00:00,2018-07-26 16:45:11+00:00,,16712,0,0,Java,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,"['smesparza', 'alhooman']",,"*************************************************************
*     SafetyPal
*
* Evan Blank      eblank@ucsc.edu
* Susana Esparza  smesparz@ucsc.edu
* Rami Alrwais    raalrwai@ucsc.edu
* Ali Hooman      alhooman@ucsc.edu
*
*************************************************************
*
* All sprint plans and reviews can be found in the 
* CS115_ReleaseDocuments/SprintPlansAndReview folder.
*
* SystemAndUnitTestReport.pdf in in CS115_ReleaseDocuments/
*
* Testing.pdf is in CS115_ReleaseDocuments/
*
* WorkingPrototype.pdf is in CS115_ReleaseDocuments/
*
* A copy of our final presentation is also included.
*
* SafetyPal.apk is the final build of our application
*
* Usage: Run in an Android emulator or on an 
*        Android device
*
*************************************************************
",1,0.77,0.77,,,,,,0,0,,,
145650406,MDEwOlJlcG9zaXRvcnkxNDU2NTA0MDY=,python_ucsc,dorozcos/python_ucsc,0,dorozcos,https://github.com/dorozcos/python_ucsc,Git Test Python Class,0,2018-08-22 03:21:16+00:00,2018-08-22 03:59:50+00:00,2018-08-22 03:59:49+00:00,,3,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['dorozcos'],,"# python_ucsc
Git Test Python Class
",1,0.82,0.82,,,,,,0,0,,,
17950456,MDEwOlJlcG9zaXRvcnkxNzk1MDQ1Ng==,BulkMetadataEditor,UCSCLibrary/BulkMetadataEditor,0,UCSCLibrary,https://github.com/UCSCLibrary/BulkMetadataEditor,Plugin for Omeka Classic - Bulk metadata search and edit,0,2014-03-20 16:48:15+00:00,2024-04-17 13:20:18+00:00,2023-11-29 02:15:39+00:00,,421,13,13,PHP,1,1,1,1,0,0,6,0,0,2,,1,0,0,public,6,2,13,master,1,"['DBinaghi', 'Daniel-KM', 'NedHenry', 'jajm', 'kloor']",1,"Bulk Metadata Editor (plugin for Omeka)
=======================================

[Bulk Metadata Editor] is an [Omeka] plugin intended to expedite the process of
editing metadata in Omeka collections of digital objects by providing tools for
administrators to edit many Items at once based on prespecified rules.

If you use this plugin, please take a moment to submit feedback about your
experience, so we can keep making Omeka better: [User Survey].

The editing options available are:
- Search and replace text
- Add a new metadatum in the selected field
- Prepend text to existing metadata in the selected fields
- Append text to existing metadata in the selected fields
- Remove text from ends of existing metadata in the selected fields
- Convert to uppercase or lowercase existing metadata in the selected fields
- Explode metadata with a separator in multiple elements in the selected fields
- Deduplicate and join metadata in the selected fields
- Deduplicate and remove empty metadata in the selected fields
- Deduplicate files of selected items by hash
- Delete all existing metadata in the selected fields
- Copy/Move content from the selected field to another

Installation
------------

Uncompress files and rename plugin folder ""BulkMetadataEditor"".

Then install it like any other Omeka plugin.

You may have to set the php cli path in `application/config/config.ini`,
according to your server if it is not automatically detected:

```
background.php.path = ""/usr/bin/php-cli""
```

In order to get messages about the process, you may have to set the logger:

```
log.errors = true
log.priority = Zend_Log::INFO
```

The log file is `application/logs/errors.log`, that must be writeable.


Notes
-----

- After a successful process, records must be reindexed in order to find them
  via the quick search field.
- The change ""*Append text to existing metadata in the selected fields*"" appends
  text only if there is already a metadata.
- The preview may fail when there is a lot of fields or changes to prepare.
  The true process will still work fine even with a huge number of items
  and fields, because it is done in the background, without the limit set by the
  server. Nevertheless, it's recommended to avoid too large updates.


Warning
-------

Use it at your own risk.

It's always recommended to backup your files and your databases and to check
your archives regularly so you can roll back if needed.


Troubleshooting
---------------

See online issues on the [plugin issues] page on GitHub.


License
-------

This plugin is published under [GNU/GPL v3].

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free Software
Foundation; either version 3 of the License, or (at your option) any later
version.

This program is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along with
this program; if not, write to the Free Software Foundation, Inc.,
51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.


Copyright
---------

* Copyright 2014 UCSC Library Digital Initiatives
* Copyright Daniel Berthereau, 2015-2017 (see [Daniel-KM] on GitHub)
* Copyright Julian Maurice for BibLibre, 2015 (see [jajm] on GitHub)
* Copyright Daniele Binaghi, 2020-2023 (see [DBinaghi] on GitHub)

[Bulk Metadata Editor]: https://github.com/UCSCLibrary/BulkMetadataEditor
[Omeka]: https://omeka.org
[User Survey]: https://docs.google.com/forms/d/1sfct41zxTelXFlyBwtsT1u33nRl7GGofSTt06d1SDMQ/viewform?usp=send_form
[plugin issues]: https://github.com/UCSCLibrary/BulkMetadataEditor/issues
[GNU/GPL v3]: https://www.gnu.org/licenses/gpl-3.0.html
[Daniel-KM]: https://github.com/Daniel-KM
[jajm]: https://github.com/jajm
[DBinaghi]: https://github.com/DBinaghi
",1,0.81,0.81,,,,,,4040,5,,,
395774526,MDEwOlJlcG9zaXRvcnkzOTU3NzQ1MjY=,20bryan,20bryan/20bryan,0,20bryan,https://github.com/20bryan/20bryan,Config files for my GitHub profile.,0,2021-08-13 19:40:23+00:00,2022-01-13 03:57:55+00:00,2022-01-24 02:46:10+00:00,https://github.com/20bryan,11,0,0,,0,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['20bryan'],,"### Hi there 👋

Thanks for visiting my profile! Here, you'll find pins to some of the open-source codebases I've worked on over the years (others are private or not publicly accessible.) 

I've also completed ground-up projects in operating systems and data structures & algorithms featuring the following:
- a six-part operating systems kernel w/ file system, inter-process management, etc.
- data compression algorithms
- an autocompletion search engine
- Wordnet implementation using a digraph

A little bit more about me -- I recently completed a summer internship at Zoom's inaugural talent program working on backend server development. 
Before that, I worked on cosmological galaxy simulations at UC Santa Cruz, publishing research at [Oxford](https://academic.oup.com/mnras/article/501/4/4948/6047185) and [NASA](https://ui.adsabs.harvard.edu/abs/2019AAS...23335507W/abstract). I also worked at UChicago's Center for Spatial Data Science, where I helped published an [open source research package](https://access.readthedocs.io/en/latest/) in the [Python Spatial Analysis Library](https://pysal.org/) (pinned below)! 

<!--
**20bryan/20bryan** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- 🔭 I’m currently working on ...
- 🌱 I’m currently learning ...
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...
-->
",1,0.59,0.59,,,,,,0,1,,,
29112802,MDEwOlJlcG9zaXRvcnkyOTExMjgwMg==,UCSCDining,nickyivyca/UCSCDining,0,nickyivyca,https://github.com/nickyivyca/UCSCDining,Android app for viewing UCSC dining menus,0,2015-01-12 01:13:23+00:00,2024-04-24 18:47:31+00:00,2024-04-24 18:47:27+00:00,,8751,4,4,Java,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,4,master,1,['nickyivyca'],,"# UCSCDining
Android app for viewing UCSC dining menus

Scrapes dining hall menus and nutrition info from the UCSC dining site. I am not affiliated with UC Santa Cruz; I graduated in 2018.

# Privacy Policy
This app does not store data except for cached menus and your preferences. This app does not upload any data anywhere, it is all kept locally to your phone. Google may back up these preferences if you have backups turned on.
",1,0.77,0.77,,,,,,0,2,,,
175423535,MDEwOlJlcG9zaXRvcnkxNzU0MjM1MzU=,UCSC,Ranindu15/UCSC,0,Ranindu15,https://github.com/Ranindu15/UCSC,codes for beginners,0,2019-03-13 13:11:23+00:00,2019-10-06 00:16:54+00:00,2019-11-02 16:36:49+00:00,,1,0,0,C,1,1,1,1,0,0,1,0,0,3,,1,0,0,public,1,3,0,scalcal,1,['Ranindu15'],,,0,0.78,0.78,,,,,,0,0,,,
45513674,MDEwOlJlcG9zaXRvcnk0NTUxMzY3NA==,bittorrent_amia15,ablwr/bittorrent_amia15,0,ablwr,https://github.com/ablwr/bittorrent_amia15,"an upcoming talk, very exciting",0,2015-11-04 03:45:55+00:00,2015-11-08 03:49:52+00:00,2015-11-20 17:10:25+00:00,,13139,0,0,JavaScript,1,1,1,1,1,0,0,0,0,0,mit,1,0,0,public,0,0,0,master,1,['ablwr'],,"# Seeding and Leeching: Collaborative Preservation using BitTorrent

Justin Mckinney, Independent Consultant
Mark Simon Haydn, Collections Manager
Rick Prelinger, UC Santa Cruz and Internet Archive
Ashley Blewer, New York Public Library

Using private tracker communities circulating film material as a model, this panel explores the potential for BitTorrent to work as a preservation tool. Studying the development of film sharing sites, and exploring the standards imposed on content and users, this discussion will evaluate the suitability of BitTorrent for creating an accessible, sustainable, stable network to aid the preservation of significant digital and digitized materials. Presenters will discuss the benefits and limitations of the protocol, and the work that can be done in the field to make decentralized hosting a feasible option for archives.",1,0.62,0.62,,"## Contributing

Please keep the [issue tracker](http://github.com/hakimel/reveal.js/issues) limited to **bug reports**, **feature requests** and **pull requests**.


### Personal Support
If you have personal support or setup questions the best place to ask those are [StackOverflow](http://stackoverflow.com/questions/tagged/reveal.js).


### Bug Reports
When reporting a bug make sure to include information about which browser and operating system you are on as well as the necessary steps to reproduce the issue. If possible please include a link to a sample presentation where the bug can be tested.


### Pull Requests
- Should follow the coding style of the file you work in, most importantly:
  - Tabs to indent
  - Single-quoted strings
- Should be made towards the **dev branch**
- Should be submitted from a feature/topic branch (not your master)


### Plugins
Please do not submit plugins as pull requests. They should be maintained in their own separate repository. More information here: https://github.com/hakimel/reveal.js/wiki/Plugin-Guidelines
",,,,0,1,,,
121416258,MDEwOlJlcG9zaXRvcnkxMjE0MTYyNTg=,UCSCTrackTools,chaochaowong/UCSCTrackTools,0,chaochaowong,https://github.com/chaochaowong/UCSCTrackTools,"Tools to make coverage or junction files (*.bed, *.bedGraph, *.bigWig) for RNA-seq and ChIP-seq data",0,2018-02-13 17:58:18+00:00,2019-05-08 18:18:20+00:00,2019-05-08 18:18:18+00:00,,24,0,0,R,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['chaochaowong'],,,0,0.67,0.67,,,,,,0,0,,,
482038860,R_kgDOHLtUTA,rocket_patrol_mods,jjam02/rocket_patrol_mods,0,jjam02,https://github.com/jjam02/rocket_patrol_mods,cmpm120 assignment to m the rocket patrol assignment,0,2022-04-15 17:44:43+00:00,2022-04-15 17:47:43+00:00,2022-04-19 03:31:12+00:00,,1785,0,0,JavaScript,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['jjam02'],,"# rocket_patrol
this is for cmpm 120 rocket patrol mod assignment
jalva100@ucsc.edu
Jonathan Alvarez
",1,0.77,0.77,,,,,,0,1,,,
795189260,R_kgDOL2WgDA,UCSC-Genome-API-Wrapper,mgardos01/UCSC-Genome-API-Wrapper,0,mgardos01,https://github.com/mgardos01/UCSC-Genome-API-Wrapper,"Local .NET API Proxy + Generated API SDKs for C#, Go, Java, PHP, Python, Ruby, and TypeScript for the UCSC Genome REST API (https://genome.ucsc.edu/goldenPath/help/api.html)",0,2024-05-02 18:56:01+00:00,2024-05-15 08:55:46+00:00,2024-05-15 08:55:42+00:00,,64,0,0,C#,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['mgardos01'],,"# Genome Data API Client SDKs
DISCLAIMER:BASICALLY A PROOF OF CONCEPT RIGHT NOW

This repository contains the source code for a wrapper around UCSC's Genome Browser REST API. 

By creating a local .NET 8 web API that mirrors requests to https://api.genome.ucsc.edu/, we can leverage .NET 8's [automatic Swagger documentation](https://learn.microsoft.com/en-us/aspnet/core/tutorials/web-api-help-pages-using-swagger?view=aspnetcore-8.0#swagger-ui) to automatically generate an OpenAPI spec (```Swagger/UCSCGenomeAPI.json```) on build that models *most* of the behavior of the Genome Browser's API. 

```bash
Swagger
└── UCSCGenomeAPI.json
```

Then, we can use this spec to automatically generate API SDKs with [Kiota](https://learn.microsoft.com/en-us/openapi/kiota/).
The client SDKs are already built and organized in the `Client` directory, categorized by programming language.

```bash
Client/
├── csharp
├── go
├── java
├── php
├── python
├── ruby
└── typescript
```

Refer to the [Kiota Quickstart](https://learn.microsoft.com/en-us/openapi/kiota/quickstarts/) to learn how to use each SDK with your language of choice.


# Local Development

This section is intended for developers who wish to run and build the C# API locally. Read on if you are setting up the Genome Data API for local development or customization.

## Prerequisites

- [.NET 8 SDK](https://dotnet.microsoft.com/download/dotnet/8.0): Ensure you have the latest version of the .NET 8 SDK installed on your machine. 

## Getting Started

To get started with this project, clone the repository to your local machine and navigate into the project directory:

```bash
git clone https://github.com/mgardos01/UCSC-Genome-API-Wrapper.git
cd UCSC-Genome-API-Wrapper
```

### Restore and Build

Before running the application, restore the NuGet packages and build the project:

```bash
dotnet restore
dotnet build [-p:GenerateSDKs=true]
```

These commands will install all necessary dependencies and compile the project for use. SDKs aren't regenerated on build by default. 

## Running the Application

To run the application locally, use the following command:

```bash
dotnet run
```

This command will start the proxy server hosted at `http://localhost:5099`. You can access the Swagger UI to interact with the API at `http://localhost:5099/swagger`. You can change this configuration in ```Properties/launchSettings.json```. 

",0,0.79,0.79,,,,,,0,1,,,
718942893,R_kgDOKtoyrQ,cse130_asgn,RealDummy/cse130_asgn,0,RealDummy,https://github.com/RealDummy/cse130_asgn,assignments for ucsc cse130,0,2023-11-15 05:25:40+00:00,2023-11-15 05:35:12+00:00,2023-11-15 05:40:22+00:00,,909,0,0,C,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['RealDummy'],,"asgn4: multithreaded web server

Serves the files that are in the directory the excecutable is in.
Does basically the bare minimum in order to display html. Also will write arbitrary files
to your computer with PUT. So don't use this (on your computer at least lol). Also there were strikes
so I never actually turned in this assignment. It needed a file opening manager in order to prevent
WriteWrite, ReadWrite, or WriteRead collisions. I was part of the way through finishing it.
",1,0.75,0.75,,,,,,0,1,,,
754517558,R_kgDOLPkGNg,web-ucsc,Kavirubc/web-ucsc,0,Kavirubc,https://github.com/Kavirubc/web-ucsc,,0,2024-02-08 08:18:01+00:00,2024-05-29 08:32:01+00:00,2024-02-08 08:18:05+00:00,,66,0,0,JavaScript,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['Kavirubc'],,"This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.js`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.
",0,0.85,0.85,,,,,,0,1,,,
206128315,MDEwOlJlcG9zaXRvcnkyMDYxMjgzMTU=,CARE,UCSC-Treehouse/CARE,0,UCSC-Treehouse,https://github.com/UCSC-Treehouse/CARE,Treehouse Comparative Analysis of RNA Expression,0,2019-09-03 16:58:48+00:00,2022-09-08 23:16:55+00:00,2023-05-08 23:01:30+00:00,,195,2,2,Jupyter Notebook,1,1,1,1,0,0,0,0,0,4,apache-2.0,1,0,0,public,0,4,2,master,1,"['e-t-k', 'hbeale']",1,"<table><tr><td><img src=""/docs/CARE-asklepion-v2.png""></td>
 <td><h1> Treehouse Comparative Analysis of RNA Expression (CARE)</h1><br>
 <a href=""http://treehousegenomics.ucsc.edu"">UCSC Treehouse Childhood Cancer Initiative</a>
</td></tr></table>

## Documentation on the docker used in the [Vaske 2019 comparative tumor RNA-Seq analysis publication](https://treehousegenomics.ucsc.edu/p/vaske-2019-comparative-tumor-RNA/) is available [here](docs/Vaske-2019-comparative-tumor-RNA.md).

Treehouse Comparative Analysis of RNA Expression (CARE) is a dockerized pipeline of Jupyter notebooks that performs outlier analysis on an RNA-Seq expression file generated from a FASTQ by the UCSC
[UCSC Treehouse Toil RNA-Seq expression pipeline](https://github.com/UCSC-Treehouse/pipelines), vs a background compendium of samples. It generates two HTML documents summarizing the results that you can populate with your sample's clinical information.

## Requirements
```
Linux
Git
Make
Docker
6.5 Gb of disk space, approximately, for background compendium and reference files
200 Mb disk space per focus sample for combined input+output
```

## Usage
Clone:
```
git clone https://github.com/UCSC-Treehouse/CARE.git
cd CARE
```

Checkout the [latest release](https://github.com/UCSC-Treehouse/CARE/releases):

```git checkout 0.17.1.0```

Use Make to run the docker:
```make run```

This will:
 - Pull the docker
 - Download the background compendium and reference files if not already present. (May take several minutes; compendium download is ~1.5GB).
 - Download a test sample
 - Run CARE on the test sample

### Manual downloads
If you prefer, you can manually download the input tgz files from the following URLs:
- https://xena.treehouse.gi.ucsc.edu/download/CARE/TumorCompendium_v10_PolyA.tgz
- https://xena.treehouse.gi.ucsc.edu/download/CARE/TreehouseReferences-2020-03-30.tgz
- https://xena.treehouse.gi.ucsc.edu/download/CARE/TreehouseExampleFocusSample-TH03_0013_S02-2019-10-23.tgz
- https://xena.treehouse.gi.ucsc.edu/download/CARE/TreehouseExampleManifest-2019-10.24.tgz

MD5sums:
```
cfafeb5ff7b93591d0a80894061b5f3e  TumorCompendium_v10_PolyA.tgz
9fe4889ab1a69620ce725d47a3decba0  TreehouseReferences-2020-03-30.tgz
4bc8001b744d5ea6a8d9a94db06a7df9  TreehouseExampleFocusSample-TH03_0013_S02-2019-10-23.tgz
03bb977bb9610a19c84e98a788a96d41  TreehouseExampleManifest-2019-10.24.tgz
```

To use, place them in the ""resources"" folder as the original tgz archive without extracting.

## Documentation
Docs are available [here](/docs).

## Contact
You can open an issue with your question or get in touch via the [Treehouse contact form](https://treehousegenomics.soe.ucsc.edu/contact/).
",1,0.9,0.9,,,,,,0,2,,,
231274470,MDEwOlJlcG9zaXRvcnkyMzEyNzQ0NzA=,NSBE-Application-Day,adamjeanlaurent/NSBE-Application-Day,0,adamjeanlaurent,https://github.com/adamjeanlaurent/NSBE-Application-Day,,0,2020-01-01 23:16:54+00:00,2020-02-01 18:04:12+00:00,2020-02-01 18:04:10+00:00,,22,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['adamjeanlaurent'],,"# URI NSBE Application Day February 2nd 2020

## Scholarships
- Find More At The Following: 
 [College Scholarships.org](http://www.collegescholarships.org/scholarships/engineering-students.htm)
 ,[Finaid.org](https://finaid.org/scholarships/)
 ,[Scholarships.org](https://www.scholarships.com/financial-aid/college-scholarships/scholarships-by-major/engineering-scholarships/)
 ,[54 Best Engineering Scholarships](https://blog.prepscholar.com/engineering-scholarships)
 ,[ASHRAE](https://www.ashrae.org/communities/student-zone/scholarships-and-grants/undergraduate-engineering-scholarships)
- [MarvelOptics Scholarship Essay Contest](https://www.fastweb.com/college-scholarships/scholarships/165891)
- [Express Medical Supply Scholarship](https://www.fastweb.com/college-scholarships/scholarships/166137)
- [Mensa Foundation U.S Scholarship](https://www.fastweb.com/college-scholarships/scholarships/150224)
- [Girls Impact The World Film Festival Scholarship](https://www.fastweb.com/college-scholarships/scholarships/159658)
- [Live Mas Scholarship](https://www.fastweb.com/college-scholarships/scholarships/170486)
- [Admiral Grace Murray Hopper Scholarship](https://swe.org/scholarships/admiral-grace-murray-hopper-scholarship-est-1992/)
- [Anne Maureen Whitney Barrow Memorial Scholarship](https://swe.org/scholarships/anne-maureen-whitney-barrow-memorial-scholarship-est-1991/)
- [Ada I. Pressman Memorial Scholarship](https://swe.org/scholarships/ada-i-pressman-memorial-scholarship-est-2004/)
- [More SWE Scholarships](https://swe.org/scholarships/)

## Internships
- Find More At The Following 
[LinkedIn](http://linkedin.com/)
,[csinterns for my Comp Sci People](https://www.csinterns.com/)
,[Indeed](https://www.indeed.com/)
,[internships.com](https://www.internships.com/engineering)
###### In-State
- [PrepareRI College Internship Program](https://skillsforri.com/prepare-ri-college-internship-program)
- [2020 Summer Supply Chain/Engineering Internship At Johnson & Johnson](https://jobs.jnj.com/jobs/1968191216?lang=en-us&src=JB-10280&src=JB-10280)
- [Intern - Application Engineering At Infineon](https://www.infineon.com/cms/en/careers/jobsearch/jobsearch/300779-Intern-Application-Engineering/#!source=400)
- [Engineering Intern (Data Engineering Team)](https://careers-virginpulse.icims.com/jobs/1041/engineering-intern-%2528data-engineering-team%2529/job?s=LinkedIn&mode=job&iis=Job+Board&iisn=LinkedIn)
- [2020 Design & Engineering Summer Intern](https://www.manufacturingworkers.com/jobs/search?id=1121036039&aff=16AE119E-722C-4962-8098-083243C4FF3F&rgv=3&mlp=1)
- [Systems Engineering Intern At Open Learning Exchange](https://www.linkedin.com/jobs/view/systems-engineering-intern-at-open-learning-exchange-1611318183/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic)
- [Engineering Intern Summer 2020 At Navatek LLC](https://www.linkedin.com/jobs/view/engineering-intern-summer-2020-at-navatek-llc-1646755322/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic)
- [Systems Engineering Intern At The Job Network](https://www.linkedin.com/jobs/view/systems-engineering-intern-at-the-job-network-1665751011/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic)
- [Manufacturing Engineering Intern At Hexagon Manufacturing Intelligence](https://www.linkedin.com/jobs/view/manufacturing-engineering-intern-at-hexagon-manufacturing-intelligence-1582193688/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic)
- [Geotechnical Engineer Intern At GZA GeoEnvironmental](https://www.glassdoor.com/job-listing/geotechnical-engineer-intern-gza-geoenvironmental-JV_IC1151289_KO0,28_KE29,49.htm?jl=3402466098&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic)
- [Transportation Engineering Intern At CDM Smith](https://www.internships.com/posting/sam_3409128843?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic)
- [Process Engineering Co-op At Tremco](https://www.glassdoor.com/job-listing/process-engineering-co-op-tremco-JV_IC1151305_KO0,25_KE26,32.htm?jl=3442782783&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic)
- [Electrical Engineering Intern At Raytheon](http://jobs.rayjobs.com/TGnewUI/Search/home/HomeWithPreLoad?PageType=JobDetails&partnerid=25258&siteid=5366&jobId=1497331&codes=LIAS&utm_source=linkedin.com&utm_campaign=enterprise&utm_medium=social_media&utm_content=job_posting&ss=paid&dclid=CKSgp6rd4-YCFc6hswodU1MA4w#jobDetails=1497331_5366)
- [Applications Engineering Intern At Hexagon Manufacturing Intelligence](https://www.hexagonmi.com/en-US/about-us/careers/job-openings/usa/applications-engineering-intern-north-kingstown)
- [Facilities & Engineering Intern At Amgen](http://careers.amgen.com/ShowJob/Id/392382/Undergraduate%20Intern%20%e2%80%93%20F%20E%20Infrastructure%20Operations%20%20%20Utilities%20Winter%202020)
- [Software Engineering Intern - Angular At Open Learning Exchange](https://www.wayup.com/i-Education-j-Software-Engineering-Intern-Angular-Open-Learning-Exchange-938532938894352/?cid=23311615732&clickcastid=2579707-1077&refer=cpmxml-Cranston-RI-2725127-2579707&utm_campaign=cpmjobsXML-sscjobs-APPS-Cranston-RI-2725127-2579707&utm_medium=jobxml&utm_source=cpmjobscpa)
- [Software Engineering Intern At Virgin Pulse](https://jobs.jobvite.com/virginpulse/job/oA8rafw3?s=LinkedIn)
- [Software Engineering Intern At MojoTech](https://www.linkedin.com/jobs/search/?currentJobId=1654690598&geoId=104877241&keywords=software%20engineering%20intern&location=Rhode%20Island%2C%20United%20States)
- [Software Engineering Intern At Hexagon Manufacturing Intelligence](https://www.hexagonmi.com/en-US/about-us/careers/job-openings/usa/software-engineering-intern-north-kingstown)
- [Software Engineer Intern At The Glossary](https://www.linkedin.com/jobs/search/?currentJobId=1556084517&geoId=104877241&keywords=software%20engineering%20intern&location=Rhode%20Island%2C%20United%20States)
- [Software Development Engineer In Test Intern At CVS](https://jobs.cvshealth.com/ShowJob/Id/737768/Software%20Development%20Engineer%20in%20Test%20Intern%20(Undergrad)?utm_campaign=alljobs&utm_medium=recruitics_organic&utm_source=linkedinll&rx_c=alljobs&rx_medium=post&rx_paid=0&rx_source=linkedinll&rx_ts=20200101T002501Z&prefilters=none&CloudSearchLocation=none)
- [Automated Information Systems Intern At Amgen](https://www.applytracking.com/x.aspx?method=direct&type=apply&board=D92FCD45-1292-4AE2-B555-D7A273418B42&Job=R-90566&ClientCode=17267)

###### Out Of State
- [Operations Engineering Intern](https://thermofisher.contacthr.com/73784861?refId=34jd24)
- [R&D Leadership Development Internship Summer 2020 At Johnson & Johnson, Raytham, MA](https://www.linkedin.com/job-apply/1665227856?refId=44859e1f-aad9-4bec-aace-7ce66f01f114&trk=flagship3_search_srp_jobs)
- [Engineering Intern At SoFI, San Francisco, CA](http://jobs.jobvite.com/sofi/job/ouYfbfwC?%26s=LinkedIn&__jvst=Job+Board&__jvsd=LinkedIn)
- [Engineering Co-op](https://ashland.wd1.myworkdayjobs.com/en-US/AshlandCareers1/job/US-MA-ASSONET/Engineering-Co-op_2019-685)
- [Engineering Intern at Mercedes-Benz, Sunnyvale, CA](https://daimler.taleo.net/careersection/ex/jobdetail.ftl?job=405584&lang=en&portal=101430233)
- [Mobile Client Engineering Intern At Warner Bros. Entertainment, Needham, MA](https://careers.warnermediagroup.com/TGnewUI/Search/home/HomeWithPreLoad?PageType=JobDetails&partnerid=391&siteid=36&jobId=830441)
- [Engineering Co-Op At Environmental Partners, Woburn MA](https://recruiting.paylocity.com/recruiting/jobs/Details/192046/Environmental-Partners-Group-Inc/Engineering-Co-Op?source=JobTarget%20via%20LinkedIn%20PGM%20-%20Variable%20Placement&utm_source=JobTarget&utm_medium=LinkedIn%20PGM%20-%20Variable%20Placement&utm_campaign=Engineering%20Co-Op%20(192046)&_jtochash=p7OX5XIDoNkjI2RKINEQp&_jtocprof=Vk8CnNetB-K33V4iPVxgRq1bAbskWkH_)
- [Engineering Intern At Hanover, Needham MA](https://www.linkedin.com/jobs/search/?currentJobId=1643635619&geoId=90000007&keywords=engineering%20intern&location=Greater%20Boston)
- [Engineering Intern  At Holologic Inc. , Westford, CT](https://hologic.referrals.selectminds.com/jobs/summer-2020-engineering-internship-opportunities-2871)
- [Manufacturing - Industrial Engineering Internship At Tesla, Fremont, CA](https://www.tesla.com/careers/job/id=57000)
- [Engineering Intern At Team Fishel, Richmond VA](https://neuvoo.com/view/?id=fb3f83255e48&oapply=org_v2019-12&source=joveo_bulk2&utm_source=partner&utm_medium=joveo_bulk2&puid=fada3aee3ded3aeabda8dd9d4daefdabadab3aeb3decfdd8ged3addffdd9fed38bdb9bdd9cdcbed3fd)
- [Engineering Process and External Engagements Summer Intern At Bose, Framingham MA](https://boseallaboutme.wd1.myworkdayjobs.com/en-US/Bose_Careers/job/US-MA---Framingham/Engineering-Process-and-External-Engagements-Summer-Intern_R17269?companyApplyUrl=%26Source%3DLinkedIn)
- [Engineering Intern At Experitc Inc. , Memphis TN](https://neuvoo.com/view/?id=da09675d55bb&oapply=org_v2020-01&source=joveo_bulk2&utm_source=partner&utm_medium=joveo_bulk2&puid=fadc3aef3deg3de7bdaadd994dabfaacadab3deb3aeefadeged3cddfedd9fed37bdb9bdd9cdcbed3fd&splitab=1&action=emailAlert)
- [Software Engineer Intern At Lime, San Francisco CA](https://jobs.lever.co/limebike/abbcff74-b22f-4a84-84d6-1bc3919eabb1?lever-origin=applied&lever-source%255B%255D=linkedin-job-wrapping)
- [Software Engineering Intern At Nerd Wallet, San Francisco, CA](https://www.nerdwallet.com/careers/job/1799768)
- [Software Engineering Intern Twitter, San Francisco, CA](https://jobs.smartrecruiters.com/oneclick-ui/company/122273559/job/1514470938/publication/743999696971770)
- [Experience Researcher Intern At Airbnb, Seattle, WA](https://careers.airbnb.com/positions/1849181/)
- [Software Engineering Intern At Two Sigma, NYC](https://careers.twosigma.com/careers/JobDetail/New-York-New-York-United-States-Software-Engineering-Internship-NYC/5982?source=LinkedIn)

## Research 

###### In-State
- [List Of Opportunities From NSBE Drive (If You Don't Have Access Ask Me)](https://docs.google.com/document/d/1diRGUwe-jlDUiBldQKR9ZeVupuAkOL98ug21Go1noU0/edit)
- [URI MARC U*STAR Program ](https://docs.google.com/forms/d/e/1FAIpQLScHQ1SynaxIS1toT5LERZ2RtKS71BnfL-TSfpjkUGe-L0xinw/viewform)
- [Coastal And Evironmental Fellowship](https://web.uri.edu/coastalfellows/)
- [Cultivating Change Multicultural Scholars Program](https://web.uri.edu/cels/msp-app/)
- [Science And Engineering Fellows Program](https://web.uri.edu/cels/sef-application/)
- [URI CELS Research Opportunities](https://web.uri.edu/cels/academics/experiential-learning-opportunities-and-resources/)

###### Out Of State
- [Research Intern At Facebook, Boston, MA](https://www.facebook.com/careers/v2/jobs/520165665499001/?ref=a8lA00000004CFAIA2)
- [Machine Learning Research Intern At Apple, Seattle, WA](https://jobs.apple.com/en-us/details/200074277/ml-research-intern?team=MLAI&board_id=17682)
- [Research Intern At Microsoft, NYC](https://careers.microsoft.com/us/en/job/736979/Undergrad-Research-Intern-NYC-Labs?jobsource=linkedin&utm_source=linkedin&utm_medium=linkedin&utm_campaign=linkedin-feed)
- [Research Intern At Google, Princeton, NJ](https://careers.google.com/jobs/results/94168920430125766-research-intern/?src=Online%2FLinkedIn%2Flinkedin_us&utm_campaign=contract&utm_medium=jobposting&utm_source=linkedin)
- [Autopilot Research Intern At Tesla, Palo Alto, CA](https://www.tesla.com/careers/job/id=53757)
- [AMGEN Scholars Program](https://amgenscholars.com/)
- [Doris Duke Conservation Scholars Program At UC Santa Cruz](https://conservationscholars.ucsc.edu/)
",0,0.62,0.62,,,,,,0,1,,,
861619655,R_kgDOM1tFxw,Tile-Map-Creation-on-Processing-Python,mennatullahhendawy/Tile-Map-Creation-on-Processing-Python,0,mennatullahhendawy,https://github.com/mennatullahhendawy/Tile-Map-Creation-on-Processing-Python,,0,2024-09-23 08:30:10+00:00,2024-09-23 08:47:24+00:00,2024-09-23 08:30:13+00:00,,40,0,0,Python,1,1,1,0,0,0,0,0,0,1,,1,0,0,public,0,1,0,main,1,['mennatullahhendawy'],,"# Tile Map Creation on Processing Python
 
Tile Map Creation Tool
This repository contains a Tile Map Creation Tool developed as part of the Game Development 1 course at the University of California, Santa Cruz. The tool allows users to create tile maps by selecting different tiles and placing them on a grid.

Acknowledgments
Acknowledgments: This project was developed as an assignment for the Game Development 1 course (GAME 235) at the University of California, Santa Cruz. The code was developed with the help of Mohamed Samy and Mohamed-Ali-77. Additionally, we used ChatGPT to assist in structuring and refining parts of the project.

Overview
The Tile Map Creation Tool lets you click on the screen to create tiles on a grid, select between four different tile colors, and display the currently selected tile. You can also save the created tile map as an image.

Some of the Game Features
•        Clicking on the screen places a tile on the grid.
•        Keyboard keys (1, 2, 3, 4) allow switching between at least four different tile types.
•        The currently selected tile is visible and follows the mouse cursor with transparency.
•         Pressing a given key on the keyboard will save a screenshot

2. Installation
1.        Download and install Processing.
2.        Enable Python Mode:
o        Open Processing.
o        Go to the Mode drop-down menu at the top right.
o        Select Python mode from the list.
3.        Clone or download this repository to your local machine.
4.        Open the tile_map_creator.pde file in Processing.

Usage
Controls
1.        Press 1, 2, 3, or 4 to select a tile color:
o        1: Red Tile
o        2: Green Tile
o        3: Blue Tile
o        4: Yellow Tile
2.        Click on the screen to place the selected tile on the grid.
3.        Press 's' to save the tile map as a screenshot (saves as tilemap.png).

Running the Program
1.        Launch Processing.
2.        Open the tile_map_creator.pde file from this repository.
3.        Click the Run button in Processing to start the tile map creation tool.
4.        Select a tile color using the keyboard and place tiles by clicking on the grid.

Example Code Snippet
```python
Copy code
# Initial fixed variables
grid_size = 40
tiles = []  # Grid creation list
selected_tile = 0  # First tile starts on the top left corner
tile_images = []  # List to store tile colors

def setup():
    size(600, 600)  # Background size
    noStroke()  # No stroke for the grid
    load_tiles()  # Load the tile colors

def load_tiles():
    global tile_images
    # Load 4 different tiles (colored squares for simplicity)
    tile_images = [
        color(255, 0, 0),  # Red Tile
        color(0, 255, 0),  # Green Tile
        color(0, 0, 255),  # Blue Tile
        color(255, 255, 0)  # Yellow Tile
    ]

def draw():
    draw_grid()  # Draw the grid
    draw_tiles()  # Draw placed tiles
    draw_selected_tile()  # Show selected tile following the mouse cursor

def draw_grid():
    for x in range(0, width, grid_size):
        for y in range(0, height, grid_size):
            fill(0)  # No color for grid background
            rect(x, y, grid_size, grid_size)  # Draw grid rectangles

def draw_tiles():
    for t in tiles:
        fill(t[2])  # Fill the placed tile with selected color
        rect(t[0] * grid_size, t[1] * grid_size, grid_size, grid_size)  # Draw tile at the grid location

def draw_selected_tile():
    # Draw the selected tile following the mouse cursor with transparency
    fill(tile_images[selected_tile], 128)  # Transparency applied
    rect(mouseX // grid_size * grid_size, mouseY // grid_size * grid_size, grid_size, grid_size)

def mousePressed():
    # Add a tile to the grid based on mouse position
    x = mouseX // grid_size
    y = mouseY // grid_size
    tiles.append((x, y, tile_images[selected_tile]))

def keyPressed():
    global selected_tile
    # Change tile color based on key pressed (1, 2, 3, 4 for tiles)
    if key == '1':
        selected_tile = 0
    elif key == '2':
        selected_tile = 1
    elif key == '3':
        selected_tile = 2
    elif key == '4':
        selected_tile = 3
    elif key == 's':
        save_frame(""tilemap.png"")  # Save screenshot


",1,0.72,0.72,,,,,,0,1,,,
350052506,MDEwOlJlcG9zaXRvcnkzNTAwNTI1MDY=,bezier-curve,danmadeira/bezier-curve,0,danmadeira,https://github.com/danmadeira/bezier-curve,Cálculo da curva de Bézier em PHP,0,2021-03-21 16:20:49+00:00,2023-10-10 02:21:22+00:00,2021-03-21 16:22:03+00:00,,29,1,1,PHP,1,1,1,1,0,0,0,0,0,0,gpl-3.0,1,0,0,public,0,0,1,main,1,['danmadeira'],,"## Cálculo da curva de Bézier em PHP

Este é um código em PHP que contém funções que implementam o cálculo da curva de Bézier (quadrática e cúbica). Os algoritmos estão escritos em uma estrutura simples e semelhante a aplicação pura das respectivas equações, para deixar o código o mais limpo possível, e assim, didático.

As curvas de Bézier, cúbica e quadrática:

![Bezier Curves](img/bezier.png?raw=true)

### Referências

- AHLBERG, J. H.; NILSON, E. N.; WALSH, J. L. *The Theory of Splines and Their Applications*. Academic Press, 1967.

- BARNHILL, R. E.; RIESENFELD, R. F. *Computer Aided Geometric Design*. Academic Press, 1974.

- BAYDAS, S.; KARAKAS, B. *Defining a curve as a Bezier curve*. Journal of Taibah University for Science, Vol. 13, No. 1, pp. 522-528, 2019.

- BERTKA, B. T. *An Introduction to Bezier Curves, B-Splines, and Tensor Product Surfaces with History and Applications*. University of California Santa Cruz. May 30th, 2008.

- BISWAS, S.; LOVELL, B. C. *Bézier and Splines in Image Processing and Machine Vision*. Springer-Verlag London Limited, 2008.

- BOOR, C. *A Practical Guide to Splines, Revised Edition*. Applied Mathematical Sciences, Volume 27. Springer-Verlag New York, 2001.

- CASSELMAN, B. *From Bézier to Bernstein*. Feature Column, Monthly essays on mathematical topics. American Mathematical Society, November 2008. Disponível em: <http://www.ams.org/publicoutreach/feature-column/fcarc-bezier>

- FARIN, G. *Curves and Surfaces for CAGD: A Practical Guide, 5th Edition*. The Morgan Kaufmann Series in Computer Graphics and Geometric Modeling. Morgan Kaufmann Publishers, 2002.

- FARIN, G.; HANSFORD, D. *Mathematical Principles for Scientific Computing and Visualization*. A K Peters, 2008.

- FARIN, G.; HANSFORD, D. *Practical Linear Algebra: A Geometry Toolbox, Third Edition*. CRC Press, 2014.

- FARIN, G.; HOSCHEK, J.; KIM, M.-S. *Handbook of Computer Aided Geometric Design*. Elsevier Science B.V., 2002.

- FAROUKI, R. T. *The Bernstein polynomial basis: a centennial retrospective*. Department of Mechanical and Aerospace Engineering, University of California. March 3, 2012.

- FAROUKI, R. T.; NEFF, C. A. *Analytic properties of plane offset curves*. Computer Aided Geometric Design, Vol. 7, pp. 83-99, 1990.

- GALLIER, J. *Curves and Surfaces in Geometric Modeling: Theory and Algorithms*. Department of Computer and Information Science, University of Pennsylvania, November 21, 2018. Disponível em: <https://www.cis.upenn.edu/~jean/gbooks/geom1.html>

- HUGHES, J. F.; VAN DAM, A.; MCGUIRE, M.; SKLAR, D. F.; FOLEY, J. D.; FEINER, S. K.; AKELEY, K. *Computer Graphics: Principles and Practice, Third Edition*. Pearson Education, 2014.

- KAMERMANS, M. *A Primer on Bézier Curves*. Pomax, 2020. Disponível em: <https://pomax.github.io/bezierinfo/>

- PRAUTZSCH, H.; BOEHM, W.; PALUSZNY, M. *Bezier and B-Spline Techniques*. Mathematics and Visualization Series. Springer-Verlag Berlin Heidelberg, 2002.

- SIMONI, R. *Teoria Local das Curvas*. Universidade Federal de Santa Catarina, Centro de Ciências Físicas e Matemáticas, Curso de Licenciatura em Matemática. Florianópolis, 2005.

- WEISSTEIN, E. W. *Bézier Curve*. MathWorld: A Wolfram Web Resource. Disponível em: <https://mathworld.wolfram.com/BezierCurve.html>
",1,0.61,0.61,,,,,,0,2,,,
824411768,R_kgDOMSOGeA,NavGPT-2,GengzeZhou/NavGPT-2,0,GengzeZhou,https://github.com/GengzeZhou/NavGPT-2,[ECCV 2024] Official implementation of NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models,0,2024-07-05 04:51:15+00:00,2025-03-04 18:38:12+00:00,2024-09-20 00:43:48+00:00,,4348,122,122,Python,1,1,1,1,0,0,9,0,0,5,mit,1,0,0,public,9,5,122,master,1,['GengzeZhou'],,"<div align=""center"">

<h1>🎇NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models</h1>

<div>
    <a href='https://gengzezhou.github.io' target='_blank'>Gengze Zhou<sup>🍕</sup></a>;
    <a href='http://www.yiconghong.me' target='_blank'>Yicong Hong<sup>🌭</sup></a>;
    <a href='https://zunwang1.github.io' target='_blank'>Zun Wang<sup>🍔</sup></a>;
    <a href='https://eric-xw.github.io' target='_blank'>Xin Eric Wang<sup>🌮</sup></a>;
    <a href='http://www.qi-wu.me' target='_blank'>Qi Wu<sup>🍕</sup></a>
</div>
<sup>🍕</sup>AIML, University of Adelaide 
<sup>🌭</sup>Adobe Research 
<sup>🍔</sup>UNC Chapel Hill
<sup>🌮</sup>University of California, Santa Cruz

<br>

<div>
    <a href='https://github.com/GengzeZhou/NavGPT-2' target='_blank'><img alt=""Static Badge"" src=""https://img.shields.io/badge/NavGPT-v0.2-blue""></a>
    <a href='https://arxiv.org/abs/2407.12366' target='_blank'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>
    <a href=""https://opensource.org/licenses/MIT""><img src=""https://img.shields.io/badge/License-MIT-yellow.svg"" alt=""License: MIT""></a>
    <a href=""https://github.com/salesforce/LAVIS""><img alt=""Static Badge"" src=""https://img.shields.io/badge/Salesforce-LAVIS-blue?logo=salesforce""></a>
</div>

</div>


## 🍹 Abstract
 Capitalizing on the remarkable advancements in Large Language Models (LLMs), there is a burgeoning initiative to harness LLMs for instruction following robotic navigation. Such a trend underscores the potential of LLMs to generalize navigational reasoning and diverse language understanding. However, a significant discrepancy in agent performance is observed when integrating LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous downstream specialist models. Furthermore, the inherent capacity of language to interpret and facilitate communication in agent interactions is often underutilized in these integrations. In this work, we strive to bridge the divide between VLN-specialized models and LLM-based navigation paradigms, while maintaining the interpretative prowess of LLMs in generating linguistic navigational reasoning. By aligning visual content in a frozen LLM, we encompass visual observation comprehension for LLMs and exploit a way to incorporate LLMs and navigation policy networks for effective action predictions and navigational reasoning. We demonstrate the data efficiency of the proposed methods and eliminate the gap between LM-based agents and state-of-the-art VLN specialists.

## 🍸 Method
![](assets/NavGPT-2.png)

## 🍻 TODOs

- [x] Release 🎇NavGPT-2 policy finetuning code.
- [x] Release visual instruction tuning code.
- [x] Release navigational reasoning data.
- [x] Release pretrained models weights.
- [ ] Release data preparation scripts.

## 🧋 Prerequisites

### 🍭 Installation

Two ways are provided to set up the environment: Conda and Docker, can choose either one according to your preference.

#### Conda Environment
1. Create a conda environment and install all dependencies:

```bash
conda create --name NavGPT2 python=3.8
conda activate NavGPT2
pip install -r requirements.txt
```

2. Install Matterport3D simulator follow instructions [here](https://github.com/peteanderson80/Matterport3DSimulator).

    You could find some hints from the provided [Dockerfile](dockerfile) of how to build the simulator in conda environment :) .

#### Docker Environment

> A Dockerfile is provided to build the environment with all dependencies installed. You can either pull the Docker image directly from Docker Hub or build it yourself using the provided Dockerfile.

1. OPTION 1: Pull the Docker image from Docker Hub:

```bash
docker pull gengzezhou/mattersim-torch2.2.0cu118:v2
docker run -it gengzezhou/mattersim-torch2.2.0cu118:v2 /bin/bash
```

Start a container and run the following lines to make sure you activate the environment:
```bash
source /root/miniconda3/etc/profile.d/conda.s
conda activate
export PYTHONPATH=/root/Matterport3DSimulator/build
```

2. OPTION 2: Build the Docker image from the provided Dockerfile:

```bash
docker build -t navgpt2:v1 .
docker run -it navgpt2:v1 /bin/bash
```

### 🍬 Data Preparation

Download the required data:

```bash
python download.py --data
```

This script will automatically download the following datasets:

1. R2R Data and Pre-computed Image Features ([EVA-CLIP-g](https://github.com/salesforce/LAVIS/blob/ac8fc98c93c02e2dfb727e24a361c4c309c8dbbc/lavis/models/eva_vit.py#L442)):

    Source: [Huggingface Datasets: ZGZzz/NavGPT-R2R](https://huggingface.co/datasets/ZGZzz/NavGPT-R2R)

    Destination: `datasets`

2. Instruction Tuning Data for NavGPT-2:

    Source: [Huggingface Datasets: ZGZzz/NavGPT-Instruct](https://huggingface.co/datasets/ZGZzz/NavGPT-Instruct)

    Destination: `datasets/NavGPT-Instruct`

Unzip the downloaded R2R data:

```bash
cd datasets
cat R2R.zip.* > R2R.zip
unzip R2R.zip
```

The data directory is structed as follows:

```
datasets
├── NavGPT-Instruct
│   ├── NavGPT_train_v1.json
│   └── NavGPT_val_v1.json
└── R2R
    ├── annotations
    ├── connectivities
    └── features
        └── MP3D_eva_clip_g_can.lmdb
```

Alternatively, you can specify the datasets to download by providing the `--dataset` argument to the script. For example, to download only the R2R data:

```bash
python download.py \
    --data \
    --dataset 'r2r'
```

### 🍫 Pretrained Models

Download the pretrained models:

```bash
python download.py --checkpoints
```

This script will automatically download the following pretrained models:

<table border=""1"" width=""100%"">
    <tr align=""center"">
        <th>Model</th><th>Log</th><th colspan=""5"">R2R unseen</th><th colspan=""5"">R2R test</th>
    </tr>
    <tr align=""center"">
        <td></td><td></td><td>TL</td><td>NE</td><td>OSR</td><td>SR</td><td>SPL</td><td>TL</td><td>NE</td><td>OSR</td><td>SR</td><td>SPL</td>
    </tr>
    <tr align=""center"">
        <td><a href=""https://huggingface.co/ZGZzz/NavGPT2-FlanT5-XL/tree/main"">NavGPT2-FlanT5-XL</a></td><td><a href=""assets/NavGPT2-FlanT5-XL.log"">here</a></td><td>12.81</td><td>3.33</td><td>78.50</td><td>69.89</td><td>58.86</td><td>13.51</td><td>3.39</td><td>77.38</td><td>70.76</td><td>59.60</td>
    </tr>
    <tr align=""center"">
        <td><a href=""https://huggingface.co/ZGZzz/NavGPT2-FlanT5-XXL/tree/main"">NavGPT2-FlanT5-XXL</a></td><td><a href=""assets/NavGPT2-FlanT5-XXL.log"">here</a></td><td>14.04</td><td>2.98</td><td>83.91</td><td>73.82</td><td>61.06</td><td>14.74</td><td>3.33</td><td>80.30</td><td>71.84</td><td>60.28</td>
    </tr>
</table>

The checkpoints include the following files:

1. Pretrained NavGPT-2 Q-former weights, will be put in the `map_nav_src/models/lavis/output` directory.

2. Finetuned NavGPT-2 policy weights, will be put in the `datasets/R2R/trained_models` directory.

Alternatively, you can specify the models to download by providing the `--model` argument to the script. For example, to download only the NavGPT2-FlanT5-XL weights:

```bash
python download.py \
    --checkpoints \
    --model 'xl'
```

## 🧃 Stage 1: Visual Instruction Tuning of NavGPT-2

> You could skip this stage and directly use the provided pretrained NavGPT-2 Q-former for policy finetuning.

Set the cache directory in [defaults.yaml](map_nav_src/models/lavis/configs/default.yaml) as the absolute path to NavGPT-2.

Perform visual instruction tuning of NavGPT-2 Q-former with FlanT5-xl:

```bash
cd map_nav_src/models
bash run_script/train_NavGPT_xl.sh
```

Alternatively, you can switch the LLM to `FlanT5-xxl`, `Vicuna-7B`, or `Vicuna-13B` by running the following scripts:

```bash
bash run_script/train_NavGPT_xxl.sh
bash run_script/train_NavGPT_7B.sh
bash run_script/train_NavGPT_13B.sh
```

The training logs and checkpoints will be saved in the `models/lavis/output` directory.

## 🍹 Stage 2: Policy Finetuning of NavGPT-2

Evaluate the trained NavGPT-2 policy with FlanT5-xl on the R2R dataset:

```bash
cd map_nav_src
bash scripts/val_r2r_xl.sh
```

Finetune and evaluate the NavGPT-2 policy with FlanT5-xl on the R2R dataset:

```bash
cd map_nav_src
bash scripts/run_r2r_xl.sh
```

This script will use the released instruction-tuned NavGPT-2 Q-former as initialization. The results will be saved in the `map_nav_src/datasets/R2R/exprs_map/finetune` directory.

Replace the `--qformer_ckpt_path` argument in the `run_r2r_xl.sh` script with the path to the desired NavGPT-2 Q-former checkpoint to finetune the policy with a different model.

Alternatively, you can switch the LLM to `FlanT5-xxl`, `Vicuna-7B`, or `Vicuna-13B` by running the following scripts:

```bash
bash scripts/run_r2r_xxl.sh
bash scripts/run_r2r_vicuna7b.sh
bash scripts/run_r2r_vicuna13b.sh
```

## 🥂 Acknowledgements
We extend our gratitude to MatterPort 3D for their valuable contributions to the open-source platform and community.

We also acknowledge the significant benefits of using [DUET](https://github.com/cshizhe/VLN-DUET) and [InstructBLIP](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) in this work. Our thanks go out to the creators of these outstanding projects.

## 🍺 Citation

If you find this work helpful, please consider citing:

```bibtex
@article{zhou2024navgpt2,
  title={NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models},
  author={Zhou, Gengze and Hong, Yicong and Wang, Zun and Wang, Xin Eric and Wu, Qi},
  journal={arXiv preprint arXiv:2407.12366},
  year={2024}
}
```
",1,0.76,0.76,,,,,,0,8,,,
76395761,MDEwOlJlcG9zaXRvcnk3NjM5NTc2MQ==,TrackSig,morrislab/TrackSig,0,morrislab,https://github.com/morrislab/TrackSig,A framework to infer mutational signatures in cancer over time,0,2016-12-13 20:28:34+00:00,2024-07-23 08:25:27+00:00,2019-07-09 14:58:40+00:00,,197,53,53,R,1,1,1,1,0,0,17,0,0,5,mit,1,0,0,public,17,5,53,master,1,"['YuliaRubanova', 'rlesurf', 'RyogaLi', 'IanShi1996']",1,"# TrackSig (Trackature)

A framework to infer mutational signatures over time.

## Background
Cell processes leave a unique signature of mutation types in cancer genome. Using the mutational signatures, it is possible to infer the fraction of mutations contributed by each cell process. Mutational signatures are represented as multinomial distributions over 96 mutation types. Using our framework Trackature, we can infer mutational signatures changing over time.

## Dependencies
- Python 2.7.9  
  Packages: pyvcf, csv, scipy, numpy  
  Packages can be installed using `pip2 install package_name` command.
  
- Perl v5.18.2  
  Packages: Bio::DB::Fasta. Please refer to http://www.cpan.org/modules/INSTALL.html on how to install packages on your machine. On Mac OS and Unix: sudo cpan Bio::DB::Fasta
  
- R 3.1.2  
  Packages: reshape2, ggplot2, NMF  
  R packages can be installed using install.packages(""package_name"") command.


## Input
- VCF file with point mutations  
  VCF file should be named as tumor_id.vcf, where tumor_id is an id of the tumor.  
  INFO column in VCF file should contain reference and alternate read counts in the following format: ""t_alt_count=5;t_ref_count=20"". 

Optional:  
- Sample purity  
  The format is the following: (tab-delimited)
```
samplename	purity
example	0.7
```

  The file should contain for purities for all your samples. The ""samplename"" column should match the name of the vcf file. Please refer to the example in `data/example_purity.txt`

- Copy number alteration calls

  The format is the following: (tab-delimited)
```
chromosome      start   end     total_cn
1       2888343        3263790        3
```

## Usage 
The commands below assume starting from the 'Trackature' directory. The example.vcf is provided in the repo. Running the code as written below will compute signature trajectories for the example.

### Generating variant allele frequency estimates

We use variant allele frequency (VAF) to sort mutations by the order of their occurrence.

To generate VAF values:
```
python src/make_corrected_vaf.py --vcf data/example.vcf --output data/example_vaf.txt
```

It is recommended to correct VAF by copy number and tumor purity if those are available. You can specify file CNA calls and a file containing sample purities the following way:
```
python src/make_corrected_vaf.py --vcf data/example.vcf --cnv your_cna_call_file.txt --purity purity_file.txt --output data/example_vaf.txt
```
Please refer to the example for the format of tumor purity file. The file should be tab-delimited in the following format:
```
samplename	purity
example	0.7
```
The ""samplename"" column should match the name of the vcf file. To make use of purities when correcting VAF, provide the name of the purity file to make_corrected_vaf.py with parameter ""--purity"".

### Making mutation counts

To make mutation counts over 96 mutation types:
```
src/make_counts.sh data/example.vcf data/example_vaf.txt
```
where the first parameter is the vcf file and the second parameter is file with VAF values generated at the previous step.

Requires hg19 reference which can be downloaded from here: http://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/

Using an `rsync` command to download all the hg19 reference files: 
```
rsync -avzP rsync://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/ ./annotation/hg19/
```




### Generating signature trajectories
To use cancer-type specific signatures, please provide `data/tumortypes.txt` file listing tumor IDs and their cancer types in the format (tab-delimited):
```
ID	tumortype
example	LAML
```
The names of the cancer types must match the ones in `annotation/active_signatures_transposed.txt` table which lists active signatures for TCGA cancer types.

Modify `src/header.R` to set up the path to your data
```
tumortype_file <- ""data/tumortypes.txt""
```

Compute signature trajectories for all samples:
```
Rscript src/compute_mutational_signatures.R
```

Results can be found in ""results_signature_trajectories"" folder (by default, specified in by DIR_RESULTS in `src/header.R`) in appropriate cancer type and tumor id folders. Signature trajectories are stored in `mixtures.csv`. Rows correspond to signatures. Columns correspond to time points. The columns are named by the average cellular prevalence that corresponds to the time point.

### Estimating uncertainty
If you wish to compute uncertainty for trajectories as well, set `compute_bootstrap` parameter in `src/header.R` to TRUE before running the script (slows down the computation).

## Generating simulations
```
python src/generate_simulations.py
```
Output: files with simulated mutation counts and true signature activities in simulated_data/ folder

Optional arguments:

`--timepoints` -- number of time points (50 by default)

`--sig-file` -- file with mutational siggnatures (""annotation/alexSignatures_w_header.csv"" by default)

The simulations use a different format of mutation counts. If you want to run TrackSig on simulations, be sure to set `simulated_data = TRUE` in `src/header.R`.

Finally, run TrackSig on simulations:

```
Rscript src/compute_mutational_signatures.R
```
    

## Other functionality

### Computing overall signature activities across all mutations

Mean signature activities across all mutations in the tumor can be computed the following way:
```
R
source(""src/header.R"")
compute_overall_exposures_for_all_examples()
```
It will compute the signature activities (aka ""mixtures"") for all samples in the data/counts directory. The results can be found in the results directory under the appropriate cancer type and tumor ID ( `overall_mixtures.csv`).

Alternatively, you can call a function to compute activities directly:
```
mixtures <- fit_mixture_of_multinomials_EM(mutation_counts, alex.t)
```
Input to fit_mixture_of_multinomials_EM:
- mutation_counts : 1x96 data frame specifying mutation counts over 96 types
- alex.t : specifies signatures to fit (96 by number of signatures). 

### Determining active signatures

**Using COSMIC per-cancer active signatures**  
Active signatures vary across cancer types. To ensure that we fit only most relevant signatures for the particular cancer type, we refer to the table of active signatures from [COSMIC](http://cancer.sanger.ac.uk/cosmic/signatures), reproduced in `annotation/active_signatures_transposed.txt` file.

To use cancer-type specific signatures, please provide `data/tumortypes.txt` file listing tumor IDs and their cancer types (see example in the repo). The names of the cancer types must match the onces in `active_signatures_transposed.txt` table. 

**Estimating active signatures from scratch**

If active signatures are unavailable, they can be estimated by computing mean activities across all mutations (see ""Computing overall signature activities"" section) and taking the signatures with highest activities (for example, with activity > 10%). Next, these signatures should be specified as active in `src/header.R` before running `src/compute_mutational_signatures.R`. 

We recommmend to estimate active signatures first (as described above) instead of fitting all signatures over time through Trackature, as it provides more stable results and speeds up the computation.

### Providing per-tumor signatures

To specify a separate set of active signatures for each sample:

1) in `src/header.R` set cancer_type_signatures = FALSE   
2) in `src/header.R` set active_signatures_file to the file with active signatures per sample. See example of such file in `annotation/active_in_samples.txt`.

### Providing new signatures

Signatures provided in the repo are from [COSMIC](http://cancer.sanger.ac.uk/cosmic/signatures) located in `annotation/alexSignatures.txt`. You can use your own signatures by providing path to another signature file through `signature_file` parameter in `src/header.R`.

## Important notes
1) **Is not applicable for samples with <600 mutations.** Please note that Trackature does not run on samples with less than 600 mutations. Less than 600 mutations will result in less than 3 time points, and there is no point to analize it as a time series. On tumors with less than 600 mutations, you can compute signature activities without dividing mutations into time points (see ""Computing overall signature activities"" section).

2) **Results are not re-computed when script is re-started.** Please note that at every step if you stop the script and re-start it, the computations will *continue* instead of re-writing the previous results. It is useful for launching large batches of samples: scripts can be paused when needed; if one sample fails, other samples don't need to be re-computed again. However, if you wish some results to be re-computed, please erase the corresponding directory.

3) **If the tumour names contain a dot, please replace it with another symbol, for example, with underscore.**




",0,0.52,0.52,,,,,,0,2,,,
113701487,MDEwOlJlcG9zaXRvcnkxMTM3MDE0ODc=,HyperGarageSale,joarreola/HyperGarageSale,0,joarreola,https://github.com/joarreola/HyperGarageSale,Form UCSC-Extention class: Tara Loza,0,2017-12-09 21:10:47+00:00,2017-12-09 21:23:16+00:00,2017-12-18 06:07:54+00:00,,243,0,0,Java,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['joarreola'],,"# HyperGarageSale
Form UCSC-Extention class by Taral Oza

Implemented Functionality:

- Post search by Title.

- Post Addition, with multiple image support.

- Post removal.

- Detailed Post view.

- Full-size image view from Detailed Post view.

- Detailed Post Edit, with image removal and addition.

- Memory and Disk caching of bitmaps.

- All BitmapFactory operations on AsyncTasks.


To Do:

- Create utilities class for common image capture and storage operations.

- Fix obscured-description area in Edit Detailed Post activity.",1,0.72,0.72,,,,,,0,0,,,
918879258,R_kgDONsT8Gg,ashwinprabou,ashwinprabou/ashwinprabou,0,ashwinprabou,https://github.com/ashwinprabou/ashwinprabou,,0,2025-01-19 05:21:23+00:00,2025-01-19 05:27:58+00:00,2025-01-19 05:27:56+00:00,,7,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['ashwinprabou'],,"## 🎓 About Me  
Hi! I'm **Ashwin Prabou**, a junior at the University of California, Santa Cruz, set to graduate in **June 2026**. I’m pursuing a **B.S. in Computer Science** with a proposed minor in **Statistics**. My academic and project experiences have honed my skills in:  

- **Programming Languages:** C, C++, Python  
- **Mathematical Foundations:** Linear Algebra, Calculus, Discrete Mathematics  
- **Front-End Development:** HTML, CSS, TypeScript, React, Next
- **Current Focus:** Backend Development  

## 💡 Interests  
I’m passionate about solving real-world problems using **software engineering** and **machine learning**. Growing up around technology, I became intrigued by the **Internet of Things (IoT)** and its ability to improve everyday life.  

### Career Goals:  
- **Internship Role:** Software Engineer or Machine Learning Intern  
- **Key Skills I Offer:**  
  - Proficiency in **C/C++** and **data structures** to design efficient systems.  
  - Expertise in **Python** and statistical methods for building machine learning models.  
  - Passion for exploring **AI** and its transformative potential in shaping the future of tech.  

## 🏆 Highlight Project: **GauchoCourse**  
At **SBHacks XI**, my team and I created a project that won the **Best Use of AI** award! GauchoCourse is designed to simplify access to course information for UC Santa Barbara students. Here's what I contributed:  

- Built a **generative AI chatbot** leveraging Aryn parsing technology and Python's Pandas library to handle student inquiries.  
- Developed the backend using **Flask** for seamless client-side event handling.  
- Enhanced the frontend with **React** and **Tailwind CSS** for fast, dynamic animations.  

👉 [Check it out here!](https://devpost.com/software/gauchocourse)  

## 🌍 Preferred Work Locations  
I am eager to secure **in-person internships** to gain practical, hands-on experience. My preferred locations include:  
- **Bay Area**  
- **Southern California**  
- Other regions across the U.S.  

On-site work provides an environment for direct collaboration with peers and mentors, essential for professional growth.  

## 📅 Availability  
- **Start Date:** June 15, 2025  
- **End Date:** September 1, 2025  

## 🙌 Let's Connect!  
Thank you for taking the time to learn about me! I look forward to collaborating and contributing to impactful projects.  

— **Ashwin Prabou**  
",1,0.55,0.55,,,,,,0,1,,,
223833958,MDEwOlJlcG9zaXRvcnkyMjM4MzM5NTg=,LeetCode2019,dundunmao/LeetCode2019,0,dundunmao,https://github.com/dundunmao/LeetCode2019,,0,2019-11-25 00:55:56+00:00,2020-06-07 08:01:26+00:00,2020-06-07 08:01:24+00:00,,2280,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['dundunmao'],,"【题目】 公司之间级别对应，如下哪个是错误的？
Google T5 = Facebook E5/6

Uber Sr II = Lyft T6

Amazon L7 = Facebook E6

✓ Facebook L6 = Facebook E6

【题目】一亩三分地发帖可以选择内容用 hide 进行隐藏。隐藏方式：[hide=200]隐藏的内容[/hide] 下面哪个选项 hide 正确。
✓ A. 两人第一次华山论剑，争的是荣名与[hide=188]《九阴真经》[/hide]；

B. 第二次在桃花岛过招，是为了郭靖与[hide=188]欧阳克[\hide]争婚。

C. 第三次海上相斗，生死只隔一线，但(hide=188)洪七公(\ hide)手下尚自容让；

D. 现下第四次恶战，[hide188]才是各出全力[/hide]，再无半点留情。

E. 洪七公伸指疾点欧阳锋背心[hide=188]“凤尾穴”(/hide)，要迫他松手。

【题目】为什么一亩三分地除了租房广告找室友、学有飞友工友、本地版聚等少数版块之外，绝大多数板块都不允许拉群？
A. 地里信息是公开的，全部回复每个人都可以看到。而群里的信息，作为新入群的同学是无法看到历史消息的，并且无法查找

B. 公开自己的微信，将来可能会被人人肉或者骚扰

C. 地里的信息永远存在，有些群很可能不活跃甚至不存在了，里面的讨论也就消失了

✓ D. 以上全都理解并接受

【题目】在下面哪些板块里留微信号等各种联系方式，事后可以要求版主删除？
A. 如果发在求职类板块里，可以删除，我的隐私你得尊重啊

B. 如果发在留学类板块里，可以删除，毕竟我年少不懂事嘛

C. 如果发在非正事、无聊帖子里，可以删除，毕竟没啥营养啊

✓ D. 绝大多数板块禁止留微信、拉群，如果你非要发出来，那就永远不删。另外，私下联系建议发站内信。

【题目】为什么我们不鼓励用谐音或者各种拐弯抹角的说法来指代公司或者学校名称？ 比如：“湾区某元音开头和结尾公司”，你能猜到是哪家吗？
A. 也许有什么顾虑吧

B. 这样不好玩

✓ C. 写成这样子，别人看不懂，也搜不到。如果别人也这样写，你也看不懂、搜不到。信息没法分享和交流。

D. 不知道

【题目】下面哪个说法是错误的？
A. 连续超过 14 天不登录，每天扣一个大米，直到大米数=100

✓ B. 抖包袱版看帖，不会消耗积分

C. 可以消耗积分更改用户名（网站右上角设置->个人资料->更改用户名）

D. 看到干货帖子和回复，给作者加分，不会消耗我的积分 E. 每日签到、每日答题都可以拿到积分奖励

F. 手机 app 里也可以每日签到，好方便！

G. 给地里官方 app 五星好评，可以拿加分奖励: www.1point3acres.com/bbs/thread-446981-1-1.html H. 在手工加分的帖子里，多次回复骗取积分，会被系统检测到，积分扣除+额外扣分！

I. 网站上方导航栏 -> 道具中心，可以兑换匿名卡，把自己的帖子匿名。

【题目】 下面哪个情况，不会消耗你的积分？
超过 14 天不登录

使用论坛搜索

下载附件

✓ 看到干货帖子和精华回复，给作者加分！

【题目】 下面哪个州，没有 income tax?
✓ Nevada

New York

Nebraska

Massachusetts

【题目】 下面哪个州，有 state income tax
Tennessee

Alaska

Washington

✓ Mississippi

【题目】 求内推如何作死？
一下子叫好多人给内推同一家公司

别人回复慢了就抱怨

简历上撒谎

✓ 这些都会作死

【题目】 下面哪种方法，可以妥妥拿到积分？
上传头像

每日签到（需绑定微信）

分享干货

✓ 这些全都可以

【题目】 回答别人的私信提问还需要消耗我 5 大米怎么办？
✓ 直接在版面回答，这样大家都能看见

（其他三个错误答案未知，更新后请删除此行）

【题目】 下面哪种行为，在地里会被扣光积分，甚至封号？
✓ 这些全都会

（其他三个错误答案未知，更新后请删除此行）

【题目】一亩三分地发帖可以用 hide 语法隐藏内容。下面哪个写法正确？
✓ 柯南的名字是[hide=200]工藤新一[/hide]

柯南的名字是[hide=200]工藤新一[\hide]

柯南的名字是[hide=200]工藤新一[hide]

柯南的名字是[hide=200]工藤新一[/hide=200]

【题目】 在 Linkedin 上求内推如何作死
看也不看对方情况，直接扔简历要求内推，国人必须帮助国人啊

写模板内容要求内推，不论男女都叫学姐

也不自我介绍，就要求对方介绍公司情况

✓ 这些都会作死

【题目】一亩三分地鼓励如何发面经？
遇到有人留邮箱，私下发面经的，点举报

积分隐藏[hide==188]内容[/hide]

✓ 以上都正确

遇到私下提问但是根本没有什么隐私问题的，直接公开在版面回答

【题目】 下面哪个大学在华盛顿州？
Washington University

✓ University of Washington

George Washington University

Washington College

【题目】下面哪个大学不在 Virginia/DC 附近
✓ Washington and Jefferson College

Trinity Washington University

George Washington University

Washington and Lee University

【题目】 下面哪个州，对公司友好，所以吸引了美国很多公司注册？
加利福尼亚

✓ 特拉华

佛罗里达

纽约

【题目】 下面哪个州，有 state income tax
South Dakota

Wyoming

✓ North Dakota

Tennessee

【题目】 下面哪个州，没有 state income tax
New York

New Jersey

✓ New Hampshire

New Mexico

【题目】 下面哪个州，没有 state income tax?
✓ Florida

Georgia

Hawaii

Idahoda

【题目】 下面哪个州，没有 state income tax?
Alabama

✓ Alaska

Arizona

Arkansas

【题目】下面哪个州冬天最暖和？
Minnesota

✓ Oklahoma

Michigan

Massachusetts

【题目】下面哪个大学实际上不存在？
University of California, San Francisco

University of Massachusetts, Dartmouth

✓ University of Michigan, Twin City

University of Nevada, Las Vegas

【题目】下面哪所大学所在城市不是波士顿？
✓ Boston College

Berklee College Of Music

Northeastern University

Boston University

【题目】下面哪个说法错误？
伊利诺伊大学在芝加哥有校区

✓ 芝加哥是美国著名的雨城

美国西北大学在芝加哥有校区

芝加哥 skydeck 上可以看到四个州

【题目】 Which company is the largest single✓site employer in the US?
Walmart

Ford

Costco

✓ Disney World

【题目】 下面哪种方法，可以妥妥拿到积分？
分享干货

上传头像

每日签到（需绑定微信）

✓ 这些全都可以

【题目】 下面哪家公司的总部不在西雅图
亚马逊

阿拉斯加航空公司

星巴克

✓ 波音

【题目】 给论坛 ios 或者安卓手机应用留评价如何获取 50 大米？
留 5 星评价

截屏作为证据

上传到第一个大区的”官方开发版“

✓ 以上步骤都需要

【题目】 地里发帖可以隐藏内容。假如要设置 200 积分以上才可以看到，下面哪个语法正确？
[hide]想要隐藏的内容[/hide]

[hide=200 ]想要隐藏的内容[/hide]

✓ [hide=200]想要隐藏的内容[/hide]

[hide=200]想要隐藏的内容[hide]

【题目】 地里面经数目最多的是哪家公司？
Facebook

Google

✓ Amazon

Uber

【题目】 Negotiate 工资的时候，哪种做法有利于得到更大的包裹？
拿地里抖包袱版的工资数字要对方 match

直接告诉对方自己目前薪酬，让对方看着良心办

开一个天价，谈不拢就散伙

✓ 精读地里谈工资宝典，知己知彼，百战不殆

【题目】 which state is University of Miami located?
California

Nevada

✓ Florida

Ohio

【题目】 下面哪个城市没有 SUNY（纽约州立大学）校区？
Albany

Buffalo

✓ Fulton

Stony Brook

【题目】 下面哪个州里有 Disney World？
✓ Florida

New York

North Carolina

Texas

【题目】 下面哪所大学所在城市不是波士顿？
✓ MIT

Boston University

Northeastern University

Emerson College

【题目】 关于旧金山市中心描述，下面哪个不正确？
走路得看着路，很多流浪汉，地上屎尿一不小心会踩上

车里一定不要放东西，但即使不放，也可能被砸车玻璃

Uber/Airbnb/Pinterest/Twitter 等著名科技公司都在 SOMA 区

✓ 旧金山创业公司很多，被称为“硅谷”

【题目】 一亩三分地是哪年创立的？
✓ 2009

2011

2013

2015

【题目】 下面哪个州在美国西海岸
VirginiaNorth
DakotaMaine
✓ Washington

【题目】 which state is University of Miami located?
Ohio
✓ Florida
Nevada
California

【题目】 加州大学有多个分校，下面哪个成立时间最短？
UC Davis
✓ UC Merced
UC Riverside
UC Santa Cruz

【题目】 下面哪个专业，不是 STEM，OPT 没法延期？
会计学以前不是，现在很多学校 stem 获批

数据科学

EECS

✓ 教育学

【题目】 哪种选校策略最合理？
按照排名高低选，谁高谁就好

交给中介选，反正不想操心

所有学校都申，蒙中哪个算哪个

✓ 根据自己下一步职业和学业目标，参考地里数据和成功率，认真斟酌

【题目】 一亩三分地是谁创立的？
✓ Warald

俞敏洪

李大辉

徐小平

【题目】 下面几个州，哪个离美国首都最远？
Maryland

Delaware

✓ North Carolina

Virginia

【题目】 地里数据科学类职位面经放在在什么版最合理？
数据科学版

美国面经版数据科学分类

✓ 数科面经版

找工求职版

【题目】 下面哪个公司总部在圣地亚哥？
✓ Qualcomm

AMD

Nvidia

Netflix

【题目】 下面哪种情况，管理员会按照你的要求，进行删帖？
问了问题，得到了答案，然后我过河拆桥，删帖让其他人看不到

发帖赚到了积分，看到了有权限设置的内容，然后反悔

尽管地里不允许，但是我到处留微信号，然后说隐私暴露要求删帖

✓ 这些情况全都不删帖！

【题目】 Miami University 在哪个城市
Miami, Florida

Las Vegas, Nevada

✓ Oxford, Ohio

Los Angeles, California

【题目】 想找室友或者当房东，帖子发在哪里？
✓ 租房广告|找室友版

房地产版

生活版

面经版

【题目】 在论坛发 slack 群，qq 群，微信群，任何站外讨论方式，会如何？
如果发在求职面经大区，申请大区，都会被删帖扣分

举报这些群，可能得到加分奖励

✓ 以上都正确

如果发在版聚，或者本地版块，是允许的

【题目】 下面哪类版块，可以拉群，而且不会被警告扣分？
录取结果汇报

求职、面经

内推

✓ 学友工友、找室友或者版聚本地

【题目】下面哪个说法错误？
雪城大学尽管在纽约州，但是离纽约城很远！

✓ 中国驻纽约领事馆位于法拉盛中国城，周围全是好吃的！

哥伦比亚大学离纽约中央公园很近

纽约州立大学石溪分校学费很便宜

【题目】 下面哪个学术会议不是机器学习领域的？
CVPR

ICML

SIGKDD

✓ ICSE

【题目】 下面哪个童话故事不是安徒生写的
✓ 尼尔斯骑鹅旅行记

冰雪女王

卖火柴的小女孩

国王的新装

【题目】 下面哪个作家是英国人？
✓Charles Dickens

Ernest Hemingway

Victor Hugo

Alexander Pushkin

【题目】 income tax on wages
✓North Dakota

South Dakota

Wyoming

Teness..

【题目】 下面哪个machine learning 的模型不是supervised
Logistic regression

✓Clustering

SVM

Decision Tree

【题目】 Apollo 11是哪一年登月的？
1969

【题目】 下面哪个公司的streaming service不是会员subscription付费模式运营的？
✓tubi

【题目】 著名篮球运动员姚明效力的NBA球队是休斯敦火箭队。取名“ 火箭队”是因为休斯敦是美国著名的?
钢城

汽车城

✓ 宇航工业城

电影城

【题目】 音乐家贝多芬出生于哪国？
✓ 德国

法国

意大利

英国

提交答案

【题目】 下面哪个Ivy League，离东海岸最远？
Brown

Dartmouth

Princeton

✓ Cornell

【题目】 美国哪个州没有夏令时？
南达科他州

爱荷华州

✓亚利桑那州

阿肯色州

【题目】 下面哪部作品是喜剧？
麦克白

李尔王

✓仲夏夜之梦

哈姆雷特

【题目】 下面哪个公司总部不在湾区？
google

✓snapchat

facebook

Apple

【题目】 下面哪所纽约高校坐落于中央公园附近？
Fordham University

New York University

New York Institute of Technology

✓Columbia University

",0,0.69,0.69,,,,,,0,1,,,
408243669,MDEwOlJlcG9zaXRvcnk0MDgyNDM2Njk=,theodoreho8033,theodoreho8033/theodoreho8033,0,theodoreho8033,https://github.com/theodoreho8033/theodoreho8033,Config files for my GitHub profile.,0,2021-09-19 21:45:24+00:00,2024-08-28 23:38:44+00:00,2024-08-28 23:38:41+00:00,https://github.com/theodoreho8033,22,0,0,,0,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['theodoreho8033'],,"Hello! I'm Theodore Ho!

I graduated highest-honors in the major from the University of California, Santa Cruz in June 2023 for a B.S. in Computer Science. In Fall of 2023, I began my Masters program at the University of Southern California for a M.S. in Computer Science with a specialization in artificial intelligence where I currently maintain a 3.6/4 GPA and expect to graduate in May 2025.

While in school, I work part-time as a Data Engineer at Taulia a global supply chain financing company parented by SAP; a position I was offered after completing an internship with Taulia in the Summer of 2022.

In my years of study and professional experience, I've seen first hand how data can drive business operations, provide new perspectives, and fuel artificial intelligence to tackle the world's most complicated problems. I plan to continue to develop my expertise in leveraging data and make a career out of it.

Below you will find some of the projects I've conducted along the way. 

For any questions please contact me at theodoreho8033@gmail.com, or https://www.linkedin.com/in/theodoreho8033/.


<!---
theodoreho8033/theodoreho8033 is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
",1,0.66,0.66,,,,,,0,1,,,
19195968,MDEwOlJlcG9zaXRvcnkxOTE5NTk2OA==,FStitch,azofeifa/FStitch,0,azofeifa,https://github.com/azofeifa/FStitch,An algorithm to detect nascent RNA transcription in GRO-seq and ChIP-seq,0,2014-04-27 04:42:15+00:00,2024-09-28 07:58:19+00:00,2017-09-22 03:06:28+00:00,,34479,7,7,C++,1,1,1,1,0,0,6,0,0,5,,1,0,0,public,6,5,7,master,1,['azofeifa'],,"# FStitch
Below is the README for compiling, running and understanding Fast Read Stitcher (FStitch). This software is written primarily for scientists looking to identify putative nascent transcripts _de novo_ in Global Run-On sequencing<sup>1</sup>. However, users may also find this package useful as a ChIP-seq peak caller.

![Alt text](https://github.com/azofeifa/FStitch/blob/master/images/IGV_SNAP.png)


Above: IGV snap shot displays the classifications given by FStitch. Color ‘green’ is read data considered inactive noise. Color ‘blue’ is a putative nascent transcript on the forward strand and ‘red’ on the reverse strand. We identify both genes undergoing nascent transcription and many regions that are unannotated, characteristic of enhancer elements. 

## Brief Overview of FStitch Commands
Here are the minimal commands needed to run FStitch from start to finish; for greater detail on usage and file types see below. 
```
$ FStitch train -i </path/to/BedGraphFile> -j </path/to/TrainingFile>  -o </path/to/Parameters.out>

$ FStitch segment -i </path/to/forward/BedGraphFile> -j </path/to/reverse/BedGraphFile> -k </path/to/Parameters.out> -o </path/to/Classifications.bed>
```

## System Requirements
FStitch is written in the C++ programming language, with C++11 support and uses OpenMP<sup>4</sup> to parallelize portions of the program.  With this in mind, users will need to have a GCC compilers later than version 4.7 to compile and run FStitch. For mac users, downloading the latest Xcode will update the GCC compiler need be. To check you compiler version, 
```
$ gcc —-version
$ g++ —-version
```
Note, for those running FStitch on a compute cluster, commonly you will need to perform a ‘module load gcc<version>’ to compile FStitch. Please ask your sys admins for questions on module load behavior. 
##Setup
Download the FastReadStitcher/ directory from this url or clone to your local machine. If your compiler is up to date, you can compile FStitch by moving into the FastReadStitcher/ directory and running 
```
$ sh setup.sh
=========================================
Sucessfully Compiled
```
In short, the setup.sh just runs “make” in the src/ directory. If everything compiles, you should see ""Sucessfully Compiled"" at the end.

Importantly, you will now see the executable “FStitch” in the src directory. This will be the first command used for all the following computations. 
##Input File
The fast read stitcher program attempts to classify and identify contiguous regions of read coverage that are showing strong signal over background mapping noise. With this in mind, FStitch requires a file where for each genomic position, the number of reads mapping to that position are provided. This file is commonly known as a BedGraph file<sup>2</sup>. Briefly a BedGraph file consists of four columns: chromosome, start genomic coordinate, stop genomic coordinate, coverage. Below is an example:
  
![Alt text](https://github.com/azofeifa/FStitch/blob/master/images/BedGraphScreenShot.png)

Note: FStitch does not accept bedgraph files where 0 coverage values are reported and if the data is stranded (like GRO-seq) then there should be one BedGraph file corresponding to the positive strand and the negative strand . In short, you can convert your bam files to a bed graph file format using the _bedtools_<sup>3</sup> command:
```
$ bedtools genomecov -ibam <bamfile> -g <genome_file> -bg -s “+/-“
```
We note that specifying five prime (-5) in the “genomecov” may allow for cleaner annotations however unspecified five prime bed works just fine as well. 

##Running FStitch
The Fast Read Stitcher program is divided into two main commands: “train” and “segment”. “train” estimates the necessary probabilistic model parameters and “segment” pulls the output from “train” and classifies the entire genome into _active_ and _inactive_ regions of regions of high density read coverage. 

Note that a quick reference to the below parameters and software usage can be supplied by 
```
$/src/FStitch -h 

$/src/FStitch --help
```


## FStitch train
FStitch uses two probabilistic models to classify regions of high read density that may be indicative of nascent transcription (GRO-seq) or a read coverage peak (ChIP-seq): Logistic Regression and a Hidden Markov Model. The logistic regression coefficients are estimated via a user defined label training file.  Sense we are classifying regions as signal or noise, FStitch requires regions of the genome that show characteristic transcription or high read dense profiles and regions of the genome that display noise or not a profile of nascent transcription or a read dense region. With this information, FStitch trains a logistic regression classifier and then couples it to a Markov model. The transition parameters for the Markov model are learned via the Baum Welch algorithm and thus do not require user label training data.  

In short, FStitch requires regions the user considers active transcription (or a peak) and regions considered inactive (simply noise). We note that the more regions provided to FStitch the more accurate the classifications however we have in Cross Validation<sup>1</sup> analysis that roughly 10-15 regions of active and inactive regions will yield accurate classifications. These regions are provided to FStitch using a specific file format with four columns separated by tabs: chromosome, genomic coordinate start, genomic coordinate stop, (0 if “noise” or 1 “signal”). An example is given below:

![Alt text](https://github.com/azofeifa/FStitch/blob/master/images/TrainingFileImage2.png)

The segments do not need to be in any order and can be from any chromosome, however each region must not overlap any other segment as this will cause confusion in the learning algorithms for the logistic regression classifier. 

Running FStitch train is simple once you have your data in the correct format and have created the training file above. A description of the parameters for FStitch train are given below

|Flag|Type|Desription|
|----|----|----------|
|-i         | \</path/to/BedGraphFile> | BedGraph File from above
| -j | \</path/to/TrainingFile> | Training File from above
| -o | \</path/to/anyName> | TrainingParameterOutFile
| -np| number | number of processors, default 8
| -al| number |learning rate for newtons method, default 1
| -cm| number | max number of iterations for Baum-Welch, default 100
| -ct| number | convergence threshold for Baum Welch, default 0.01
| -reg| number | regularization parameter for logistic regression classifier, default 1
Putting this together
```
$ /src/FStitch train -i \</path/to/BedGraphFile\> -j \</path/to/TrainingFile> -o \</path/to/anyName.out>
```
This will produce the a fie called anyName.out that will store the learned parameters for the logistic regression and HMM transition parameters need in “FStitch segment”. Below is one such output for anyName.out

![Alt text](https://github.com/azofeifa/FStitch/blob/master/images/ParameterOutFile.png)

Very important: If FStitch is being used on stranded data, the BedGraph file used in the “FStitch train” command must correspond to the strand indicated in the TrainingFile. For example, if the strand in the training file comes from the forward strand but the user supplies a BedGraph file that is on the reverse strand, then learned parameters will not be accurate. 


## FStitch segment
FStitch segment follows from FStitch train and takes as input the TrainingParameterOutFile (from above, \</path/to/anyName.out>) as input, along with the original BedGraph file. A description of the parameters for FStitch segment are given below

|Flag|Type|Desription|
|----|----|----------|
| -i        | \</path/to/BedGraphFile> |BedGraph File Format from above
| -k         | \</path/to/anyName.out> |Training Parameter Out File from FStitch train call
| -o        | \</path/to/anyName.bed> |A bed file that gives the regions considered active nascent transcription (or ChIP-seq peak) and noise
| -np         | number |number of processors, default 8

Putting this together
```
$ /src/FStitch segment -i \</path/to/BedGraphFile\> -j \</path/to/anyName.out> -o \</path/to/anyName.bed> 
```
This will produce a file called anyName.bed, and can be imported into any genome browser)

Note you can use your parameter out file from FStitch train (i.e. anyName.out) to segment other datasets. In fact, using the same parameter out file will gurantee consistency and comparibility across datasets, so this is encouraged.

## Cite
If you find the Fast Read Stitcher program useful for your research please cite:

Joseph Azofeifa, Mary A. Allen, Manuel Lladser, and Robin Dowell. 2014. __FStitch: a fast and simple algorithm for detecting nascent RNA transcripts.__ In Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (BCB '14). ACM, New York, NY, USA, 174-183. 

##References 
1. Joseph Azofeifa, Mary A. Allen, Manuel Lladser, and Robin Dowell. 2014. __FStitch: a fast and simple algorithm for detecting nascent RNA transcripts.__ In Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (BCB '14). ACM, New York, NY, USA, 174-183. DOI=10.1145/2649387.2649427 http://doi.acm.org/10.1145/2649387.2649427   
2. http://genome.ucsc.edu/goldenpath/help/bedgraph.html
3. http://bedtools.readthedocs.org/en/latest/
4. http://openmp.org/wp/

",0,0.6,0.6,,,,,,0,2,,,
196047531,MDEwOlJlcG9zaXRvcnkxOTYwNDc1MzE=,generative-design,lucasnfe/generative-design,0,lucasnfe,https://github.com/lucasnfe/generative-design,Examples (in p5.js) implemented during the UCSC CMPM147-Generative Design course.,0,2019-07-09 16:42:05+00:00,2024-10-12 22:38:50+00:00,2022-07-27 20:20:57+00:00,,21891,54,54,JavaScript,1,1,1,1,1,0,4,0,0,0,,1,0,0,public,4,0,54,master,1,['lucasnfe'],,"This repository contains examples (in p5.js) implemented during the University of California, Santa Cruz - CMPM147 - Generative Design course and base code for programming assignments.

# CMPM147 - Generative Design

This course introduces students to fundamental methods of Generative Arts and Design.
The first half of the course focus on both constructive and search-based approaches and the second half
covers data-driven methods, with a focus on Neural Networks (Deep Learning). Every class we discuss a method,
including how to apply it to generate artifacts in domains such as visual arts, music, narrative,
video games, etc. Methods are implemented in Javascript with P5.js (Processing).

## Webpage

The webpage of the Summer 19 version of this course can be found [here](https://canvas.ucsc.edu/courses/26749). In this webpage you can find link to slides, readings and assignment descriptions.

## Examples

This is a hands-on course, so we implement in class all the covered generative methods with simple
applications. In this repository, you can find all the examples implemented in the *Summer 19*  version of this course:

- L01 - Introduction [[Slides]](https://docs.google.com/presentation/d/1ZaPR8NiZbaCHuKccnK0puyMztqJQXDuntZ_AUweYi8A/edit?usp=sharing)
- L02 - Javascript, Processing and p5.js [[Slides]](https://docs.google.com/presentation/d/1rEjv4W9VojQpBpXwti5sD5g_TOfu8Ky4OkYTuEYQGbg/edit?usp=sharing)
- L03 - Randomnes and Noise [[Slides]](https://docs.google.com/presentation/d/1UB9-_QaYwlsMAf5FtBRTyP8fqFhvvLnjeXhQ-n-FgNM/edit?usp=sharing)
    - [Generative Art](https://lucasnfe.github.io/generative-design/Examples/L03%20-%20Noise/art/index.html)
    - [Circle](https://lucasnfe.github.io/generative-design/Examples/L03%20-%20Noise/circle/noise/index.html)
    - [Line](https://lucasnfe.github.io/generative-design/Examples/L03%20-%20Noise/line/noise/index.html)
    - [Starfield](https://lucasnfe.github.io/generative-design/Examples/L03%20-%20Noise/starfield/index.html)
    - [Terrain 1D](https://lucasnfe.github.io/generative-design/Examples/L03%20-%20Noise/terrain/1D/index.html)
    - [Terrain 2D](https://lucasnfe.github.io/generative-design/Examples/L03%20-%20Noise/terrain/2D/index.html)
    - [Texture](https://lucasnfe.github.io/generative-design/Examples/L03%20-%20Noise/texture/index.html)
    - [Walker](https://lucasnfe.github.io/generative-design/Examples/L03%20-%20Noise/walker/noise/index.html)
- L04 - Cellular Automata [[Slides]](https://docs.google.com/presentation/d/1Ir5ZIKVdtG53N3CRt0_b142GM3_aV7BdTKbGfX67nkw/edit?usp=sharing)
    - [Elementary](https://lucasnfe.github.io/generative-design/Examples/L04%20-%20Cellular%20Automata/elementary/index.html)
    - [Game of Life](https://lucasnfe.github.io/generative-design/Examples/L04%20-%20Cellular%20Automata/game/index.html)
- L05 - Generative Grammars [[Slides]](https://docs.google.com/presentation/d/1jTkTiQSKg3o-Gn_xizj0XDyb61ajlsc0XTVvYIKtb1A/edit?usp=sharing)
    - [L-Systems](https://lucasnfe.github.io/generative-design/Examples/L05%20-%20Gerative%20Grammars/lsystem/index.html)
- L06 - State Space Search [[Slides]](https://docs.google.com/presentation/d/1EbGP_zosYEqoliAH3VsO8i0lE3xNXS74zEr7GzALhco/edit?usp=sharing)
    - [Maze](https://lucasnfe.github.io/generative-design/Examples/L06%20-%20Search/maze/index.html)
    - [BFS](https://lucasnfe.github.io/generative-design/Examples/L06%20-%20Search/search/index.html)
- L07 - Evolutionary Algortihms [[Slides]](https://docs.google.com/presentation/d/1kbTYoCIQL_YzOq55XHWW6TfXEVF9A138m4ti5Dbo4-I/edit?usp=sharing)
    - [Simmetric Maps](https://lucasnfe.github.io/generative-design/Examples/L07%20-%20Genetic%20Algorithms/ga/index.html)
- L08 - Markov Chains [[Slides]](https://docs.google.com/presentation/d/1eUVGcaCoE1IsfAsCwWJkwP3Ri2eFnwov8VA1AIBRUPY/edit?usp=sharing)
    - [Name Generator](https://lucasnfe.github.io/generative-design/Examples/L08%20-%20Markov%20Models/index.html)
- L09 - Perceptron [[Slides]](https://docs.google.com/presentation/d/1hZlbzsrzjgthP7xf7_5RFPdcG9ANHdEYetvArvjjw0A/edit?usp=sharing)
    - [Linear Classification](https://lucasnfe.github.io/generative-design/Examples/L09%20-%20Perceptron/index.html)
- L10 - Multilayer Perceptron [[Slides]](https://docs.google.com/presentation/d/1BBU6vpT0ZQH8Oa9FK1X-jQdWIXtagx6cr73ehLccTAo/edit?usp=sharing)
    - [Nonlinear Classification](https://lucasnfe.github.io/generative-design/Examples/L10%20-%20Multilayer%20Perceptron/index.html)
- L11 - TensorFlow.js [[Slides]](https://docs.google.com/presentation/d/1Xz8b3FbIcrwT2ALoZ8VDefJ3aAHZtUz_eDevJBraZu0/edit?usp=sharing)
- L12 - Recurrent Neural Networks [[Slides]](https://docs.google.com/presentation/d/161XPEpDYBBIe5EzEWhWoH9m3Z_3ejrOZY5tq_Hu96PQ/edit?usp=sharing)
    - [Shakeaspeare Language Modeling](https://lucasnfe.github.io/generative-design/Examples/L12%20-%20Recurrent%20Neural%20Networks/index.html)
- L13 - Autoencoders [[Slides]](https://docs.google.com/presentation/d/19nrhcywYcJEP3SJeY2dnnDMGHgHmXtimJ2f1R5GVMjo/edit?usp=sharing)
    - [Denoising](https://lucasnfe.github.io/generative-design/Examples/L13%20-%20Autoencoders/undercomplete/index.html)
- L14 - GANs [[Slides]](https://docs.google.com/presentation/d/1MM5tC2Z_3ngmJzQ6Bu8_nPj1q59BgloRH1meYCoHoew/edit?usp=sharing)

## Assignments

- ASG1 - Music Visualization with Particle System [[Description]](https://canvas.ucsc.edu/courses/26749/assignments/83078)
- ASG2 - Terrain Generation with Perlin Noise [[Description]](https://canvas.ucsc.edu/courses/26749/assignments/83079)
- ASG3 - Interactive Artist NPC with Generative Grammars [[Description]](https://canvas.ucsc.edu/courses/26749/assignments/83080)
- ASG4 - Evolving Cars [[Description]](https://canvas.ucsc.edu/courses/26749/assignments/83081)
    - [Base Code](https://lucasnfe.github.io/generative-design/Assignments/ASG4%20-%20Evolving%20Cars/index.html)
- ASG5 - Music with Markov Chains [[Description]](https://canvas.ucsc.edu/courses/26749/assignments/83082)
    - [Base Code](https://lucasnfe.github.io/generative-design/Assignments/ASG5%20-%20Music%20with%20Markov%20Models/index.html)

## TODO

- L06 - Include Planning (for narrative) and ASP.
- L07 - Change GA example to generate SMB levels.
- L14 - Implement simple GAN for digit generation.
- ASG2 - Include base code with optimized renderer to support large worlds.
- ASG4 - Change description to require Tournament selection instead of Roulette Wheel.
- ASG5 - Change description to generate monophonic music instead of polyphonic.
",1,0.92,0.92,,,,,,0,1,,,
69934005,MDEwOlJlcG9zaXRvcnk2OTkzNDAwNQ==,RiftBets-Web,bradleybernard/RiftBets-Web,0,bradleybernard,https://github.com/bradleybernard/RiftBets-Web,UCSC - CMPS 115 Project - REST API,0,2016-10-04 04:21:07+00:00,2017-05-09 00:52:51+00:00,2017-05-09 01:18:03+00:00,,431,2,2,PHP,1,1,1,1,0,0,0,0,0,0,gpl-3.0,1,0,0,public,0,0,2,master,1,"['bradleybernard', 'chphuynh', 'travistakai']",,"# RiftBets-Web
A JSON RESTful API created to serve our iPhone client. We are using the Laravel framework v5.3 (PHP 7.0) with MySQL 5.7 for most of our data and Redis 3.0, an in-memory database, for our leaderboards. We have automated builds/continuous integration via [Travis CI](https://travis-ci.com) that runs our test suite, PHPUnit.

### Team Members:
- [Bradley Bernard](https://github.com/bradleybernard/), bmbernar@ucsc.edu
- [Travis Takai](https://github.com/travistakai/), ttakai@ucsc.edu
- [Christopher Huynh](https://github.com/chphuynh/), chphuynh@ucsc.edu
- [Sushil Patel](https://github.com/sp1395/), supatel@ucsc.edu
",1,0.86,0.86,,,,,,0,4,,,
93178265,MDEwOlJlcG9zaXRvcnk5MzE3ODI2NQ==,group-project-1-archived,pasindubawantha/group-project-1-archived,0,pasindubawantha,https://github.com/pasindubawantha/group-project-1-archived,UCSC Year 2 Group Project 1,0,2017-06-02 15:17:54+00:00,2019-09-07 08:44:58+00:00,2017-09-06 05:09:29+00:00,,12111,0,0,JavaScript,1,0,1,1,0,0,0,0,0,1,,1,0,0,public,0,1,0,master,1,"['pasindubawantha', 'Amila95']",,"# group-project-1
UCSC Year 2 Group Project 1 

Install [Nodejs 6 V++](https://nodejs.org/en/) version 6 or above

to run client go to ""client"" folder run `npm install` to install all dependancies 
and then `npm start` to run development server
visit localhost:8080 and inspect the page to get console to debug
all the code is in ""client/src/js""

to run the server got to ""server"" folder and run `npm install` to install all depenancies 
and then `npm start`

Guides:
* [WebPack kind of our IDE](https://www.youtube.com/watch?v=9kJVYpOqcVU)
* [React & Redux(client)](https://www.youtube.com/watch?v=MhkGQAoc7bc&list=PLoYCgNOIyGABj2GQSlDRjgvXtqfDxKm5b)
* [Nodejs(server)](https://www.youtube.com/watch?v=xT2AvjQ7q9E)
* [How to buidl a RESTful api](http://scottksmith.com/blog/2014/05/02/building-restful-apis-with-node/)
* [REST api standards](https://en.wikipedia.org/wiki/Representational_state_transfer#Relationship_between_URL_and_HTTP_methods)

Libraries:
* [react-notifications (client)](https://www.npmjs.com/package/react-notifications)
* [axios (client)](https://www.npmjs.com/package/axios)
* [Redux (client)](http://redux.js.org/)
* [nodejs-mysql (server)](https://www.npmjs.com/package/nodejs-mysql)
* [express (server)](https://www.npmjs.com/package/express)
* [bcrypt(server)](https://www.npmjs.com/package/bcrypt-nodejs)

on Language:
* [Babel compilers guide to es2015](https://babeljs.io/learn-es2015/)
* [JavaScrip for Beginers](https://codeburst.io/javascript-for-beginners-a-new-series-22762d8e5c42)
* [You Dont Know JS Books Series](https://github.com/getify/You-Dont-Know-JS)

on Deiffernt Versions of JS:
* [article](https://bytearcher.com/articles/es6-vs-es2015-name/)
* [another article](https://benmccormick.org/2015/09/14/es5-es6-es2016-es-next-whats-going-on-with-javascript-versioning/)

On WebDev:
* [Become-A-Full-Stack-Web-Developer](https://github.com/bmorelli25/Become-A-Full-Stack-Web-Developer)

On SE:
* [coding-interview-university](https://github.com/jwasham/coding-interview-university)
",0,0.87,0.87,,,,,,0,0,,,
908444335,R_kgDONiXCrw,Sign2Text-Demo,Maz-AX/Sign2Text-Demo,0,Maz-AX,https://github.com/Maz-AX/Sign2Text-Demo,"Sign2Text by YeterlyAI, Yeterly Software, through University of California, Santa Cruz",0,2024-12-26 05:06:45+00:00,2024-12-26 05:54:07+00:00,2024-12-26 05:54:04+00:00,,2710,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['Maz-AX'],,"# Sign2Text - ASL Translation App
![Sign2Text](https://raw.githubusercontent.com/Maz-AX/Sign2Text-Demo/refs/heads/main/docs/screenshots/ic_launcher.png ""Sign2Text"")

## Overview
Sign2Text is an open-source American Sign Language (ASL) translation application that uses machine learning to convert sign language gestures into text in real-time. Built with Flutter and powered by PyTorch, it provides an accessible way for users to translate ASL signs through their mobile device's camera.

Currently, Sign2Text can only translate ASL Alphabet gestures.

## Developers:
**Developed by the team below, sponsored by Yeterly Software, YeterlyAI division through the University of California, Santa Cruz.**

* Aman, Maz - Product Owner
* Amudhasagaran, Rahul - Machine Learning Engineer
* Nguy, Megan - Frontend Engineer
* Tiwari, Akshat - Machine Learning Engineer
* Tran, Dylan - Machine Learning Engineer

## Features
- Real-time ASL gesture recognition
- Image capture and processing
- User authentication and profile management
- Dark/Light theme support
- Responsive design for various screen sizes
- Data collection module for continuous improvement

## Tech Stack
- **Frontend**: Flutter/Dart
- **Backend**: Python (FastAPI, Flask)
- **ML Framework**: PyTorch
- **Authentication**: Firebase
- **Cloud Infrastructure**: Google Cloud Run
- **CI/CD**: GitHub Actions

## Tech Components:
1. **Flutter Mobile Application**
   - Cross-platform mobile and Web client (Android, Browser)
   - Real-time ASL translation
   - User authentication and data collection features
   - Responsive design with dark/light theme support

2. **Authentication & Data Collection Server (Flask)**
   - User management and authentication
   - Training data collection and storage
   - Firebase integration for data persistence

3. **ML Inference Server (FastAPI)**
   - Real-time ASL gesture recognition
   - Optimized for low-latency predictions
   - Scalable containerized deployment

## Architecture
```
[Flutter App]
     │
     ├──► [Authentication Service]
     │
     ├──► [ML Inference Service]
     │
     └──► [Data Management Service]
```

## Screenshots
[<img alt=""Sign2Text"" width=""548px"" height=""281px"" src=""https://raw.githubusercontent.com/Maz-AX/Sign2Text-Demo/refs/heads/main/docs/screenshots/screenshot.jpg"" />](https://raw.githubusercontent.com/Maz-AX/Sign2Text-Demo/refs/heads/main/docs/screenshots/screenshot.jpg)

## Demo

This repository contains documentation and demonstrations of the Sign2Text application. To request access to the APK for evaluation or research purposes, please contact:

**Email:** maaman@ucsc.edu

Please include:
- Your name and organization
- Intended use case
- Research/Development purpose

### Requirements (Upon APK Access)
- Android 6.0 (API level 23) or higher
- Camera permission
- Internet connection for full functionality
- 100MB free storage

## Installation
1. Download the APK from the releases page
2. Enable installation from unknown sources in your device settings
3. Install the APK
4. Grant required permissions when prompted

## Contributing
While this is a demo version of a larger project, we welcome feedback and suggestions through issues.

---

# LICENSE

Copyright (c) 2024 Yeterly Software

## Terms of Use

This repository (""Software"") and its documentation are protected by copyright law and international treaties. The Software is licensed, not sold, and is intended for demonstration and educational purposes only.

### 1. Permissions
Users are permitted to:
- View and study the public documentation
- Fork this repository for personal study
- Request access to the application binary (APK) through proper channels

### 2. Restrictions
Unless explicitly authorized in writing, users may not:
- Redistribute any part of the Software
- Create derivative works based on the Software
- Reverse engineer, decompile, or disassemble the Software
- Use the Software for commercial purposes
- Access or attempt to access the application binary without permission

### 3. APK Access
- The application binary (APK) is available only through direct request
- Access is granted on a case-by-case basis
- Usage terms will be provided with approved access
- Additional agreements may be required for APK access

### 4. Intellectual Property
- All intellectual property rights are retained by Yeterly Software
- No license to patents, trademarks, or other rights is granted by this document
- The Software's machine learning models and algorithms remain proprietary

### 5. Disclaimer
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.

### 6. Limitation of Liability
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

### 7. Contact
For permissions, access requests, or questions:
Email: maaman@ucsc.edu

### 8. Termination
Yeterly Software reserves the right to terminate any access or permission granted under this license at any time and for any reason.

This license is subject to change without notice. Continued use of the Software constitutes acceptance of such changes.

---
",1,0.67,0.67,,,,,,0,1,,,
488776747,R_kgDOHSIkKw,archives_app,aa-dank/archives_app,0,aa-dank,https://github.com/aa-dank/archives_app,Application layer for UCSC PPDO Archives server,0,2022-05-04 23:56:48+00:00,2025-02-10 19:38:29+00:00,2025-02-26 22:59:28+00:00,,5937,1,1,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,master,1,['aa-dank'],,"# Archives Application

An application that provides services related to a UCSC PPDO file server containing archival files, primarily campus construction project files. It provides services related to the file server, facilitating file management, archival processes, and data scraping.

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Maintenance](#maintenance)

## Features

- **File Management Interface**: Allows users to manage and organize files on the server and collects data about file server changes. Offers additional file server change controls based on quantities of files changed. Also maintains a complete PostgreSQL database of files which can be used for search or other SQL functions.
- **Archival Tools**: Provides functionalities for archiving files on the records server and a file archiving inbox for iteratively archiving several files on the file server.
- **Data Scraping**: Scrapes file metadata from the file server to keep the application database updated with the latest information. Scrapes project data from a FileMaker database to maintain updated project data.
- **User Authentication and Roles**: Supports user registration and authentication using Google authentication. Also has role-based access control for application services and file server services.
- **Web-Based Interface**: Offers a user-friendly web interface for interacting with the file server and accessing application features.
- **Maintenance Utilities**: Includes tools for routine maintenance tasks like database backup, file location confirmation, database management, and system logs management.
- **API Endpoints for Programmatic Interaction**: Offers a set of endpoints that allow external programs to interact with the archives application programmatically. This enables automation of tasks, integration with other systems, and access to archival data through RESTful APIs.

## Installation

### Prerequisites

- **Python 3.9 or higher**: Ensure Python is installed on your system.
- **PostgreSQL 15**: Install PostgreSQL for the application's database.
- **Redis**: Install Redis for handling background tasks.
- **Nginx**: Install Nginx as the web server.
- **Supervisord**: Install Supervisord to manage application processes.
- **Git**: For cloning the repository.
- **Samba**: For network storage access to the file server.

### Setup Steps


    

",0,0.82,0.82,,,,,,0,1,,,
1100934,MDEwOlJlcG9zaXRvcnkxMTAwOTM0,SSLButton,ahospodor/SSLButton,0,ahospodor,https://github.com/ahospodor/SSLButton,SSLButton Firefox Extension,0,2010-11-21 23:54:59+00:00,2016-10-10 09:19:49+00:00,2013-01-10 00:54:43+00:00,,244,2,2,JavaScript,1,1,1,1,0,0,0,0,0,1,other,1,0,0,public,0,1,2,master,1,['ahospodor'],,"Copyright 2011 Andrew Hospodor. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are
permitted provided that the following conditions are met:

   1. Redistributions of source code must retain the above copyright notice, this list of
      conditions and the following disclaimer.

   2. Redistributions in binary form must reproduce the above copyright notice, this list
      of conditions and the following disclaimer in the documentation and/or other materials
      provided with the distribution.

THIS SOFTWARE IS PROVIDED BY Andrew Hospodor ``AS IS'' AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL Andrew Hospodor OR
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

The views and conclusions contained in the software and documentation are those of the
authors and should not be interpreted as representing official policies, either expressed
or implied, of Andrew Hospodor.


andrew.hospodor@gmail.com
andrewh@soe.ucsc.edu



This is the README for the SSLButton project,
a project of the: 
Working-Group On Applied Security & Privacy
at the University of California Santa Cruz.
(UCSC WASP).

Purpose:
This button is intended to show users whether or not
SSL is available on any given webpage, and allow them
to easily switch. 


Installation:
To install the firefox extension, simply drag and drop
the ""sslbutton.xpi"" into Firefox. Follow instructions on screen.

After this, go to ""customize"" for toolbar:
View -> Toolbars -> Customize
OR
Right click on toolbar area and click customize. 
Select the icon with the Padlock icon (Called SSL Button) and drag
it onto your favorite spot in the toolbar.

Testing:
It has only been tested in Firefox 3.6 at this time. If you would
like to do more testing, please do.

Things that need doing:
get ""hover"" to work in chrome/content/overlay.css
(done) update description
(done)add a nice graphic with the description


Planned things that will ""eventually"" get done:
hash table for visited sites.

Version 1.1 Notes:
Updated description with better verbiage.
Added a nice graphic with the description.
Built in functionality to place the button in the toolbar automatically.
",1,0.72,0.72,,,,,,0,2,,,
6409929,MDEwOlJlcG9zaXRvcnk2NDA5OTI5,ularn,ularn/ularn,0,ularn,https://github.com/ularn/ularn,Ularn,0,2012-10-26 20:22:39+00:00,2025-02-09 02:18:01+00:00,2025-02-09 02:17:57+00:00,,215,36,36,C,1,1,1,1,0,0,12,0,0,4,gpl-2.0,1,0,0,public,12,4,36,master,1,"['joshbressers', 'ethandicks', 'huth']",1,"
This is Ularn 1.5ish, patchlevel 4, as of 19 March 2001.

So, I'm taking his code and making it happier. Currently, Ularn 1.5 will
compile out of the box without errors under linux, FreeBSD, OpenBSD, HPUX11,
and Tru64 4.0g. It should also work okay on Solaris 7 and IRIX. HPUX11 also
currently requires that you use gcc. Compiling with HP's K&R C compiler
doesn't work-- Ularn is ANSI C. And I get weird linking problems and termcap
doesn't work right when I use HP's ANSI C compiler. I'm not really impressed
with their compiler anyway. You're probably better off with gcc. I may work
on this in the future, if people ask, but I probably won't bother.

Anyway, mail me patches and things if you have them. Fix things in the TODO
and I'll be really happy.

I would appreciate it, if you happen to include Ularn on a compilation or
software distribution of some sort, if you let me know. A copy of the
package you're including it in would also be welcome. If you make any
changes, please let me know, so they can be rolled back into the original.
(This goes for you FreeBSD ports guys, especially. I'd rather you not have
to have custom patches for your port. If you come up with FreeBSD-specific
patches, send them to me! I like FreeBSD! I'll include them myself, so you
can just have them grab the source!)

Anyway, have fun.


 -- Josh Brandt (Ularn maintainer)
 -- mute@sidehack.gweep.net



---- original readme below

	This is the latest and greatest version of Ularn, as of 6/12/92.
	It should be much improved over the version that was posted to
	comp.sources.games.

-Phil Cordier (Ularn author)
satyr@ucscb.ucsc.edu
philc@netcom.com
",1,0.79,0.79,,,,,,0,5,,,
161990234,MDEwOlJlcG9zaXRvcnkxNjE5OTAyMzQ=,Ucsc-Selenium-Test001,Induni0228/Ucsc-Selenium-Test001,0,Induni0228,https://github.com/Induni0228/Ucsc-Selenium-Test001,Ucsc Selenium Test001,0,2018-12-16 10:30:14+00:00,2018-12-16 10:32:12+00:00,2018-12-16 10:32:11+00:00,,14,0,0,Java,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['Induni0228'],,,0,0.7,0.7,,,,,,0,0,,,
384442470,MDEwOlJlcG9zaXRvcnkzODQ0NDI0NzA=,HGNN-Node-Classification,BingYu94860/HGNN-Node-Classification,0,BingYu94860,https://github.com/BingYu94860/HGNN-Node-Classification,,0,2021-07-09 13:16:50+00:00,2024-03-07 14:19:29+00:00,2021-08-01 09:26:46+00:00,,146092,2,2,Jupyter Notebook,1,1,1,1,0,0,2,0,0,0,,1,0,0,public,2,0,2,main,1,['BingYu94860'],,"# HGNN-Node-Classification

環境：

1. 使用 Tensorflow 2.x
2. 在 Anaconda 的 Jupyter Notebook 上執行
3. 亦可以在 Colab上運行

```python
#### 掛接 Google 雲端硬碟 ####
from google.colab import drive
drive.mount('/content/drive')

####　切換工作資料夾的目錄 ####
# root 需要更改成 自己的 資料夾路徑，並資料夾內放 所有 *.py 與 *.ipynb 的檔案，以及 utils 的資料夾。
root = ""/content/drive/MyDrive/Colab Notebooks/HGNN-Node-Classification""
import os
os.chdir(root)
```



###### 資料集：(planetoid_dataset.py)  # 模型主要運行的檔案

1. cora
2. citeseer
3. pubmed

會自動下載 小行星planetoid： https://github.com/kimiyoung/planetoid/raw/master/data/  (對應 小行星planetoid 的 paper https://arxiv.org/abs/1603.08861 )

而這篇 [Semi-Supervised Classification with Graph Convolutional Networks](http://arxiv.org/abs/1609.02907) 的 Github 也適用使用相同內容的檔案  https://github.com/tkipf/gcn/tree/master/gcn/data

以 cora 的資料集為例，執行 gd = load_planetoid('cora') 後，就會**自動下載**以下8個檔案，就會下載至該目錄的 ./download/ 內。

1. ind.cora.x
2. ind.cora.tx
3. ind.cora.allx
4. ind.cora.y
5. ind.cora.ty
6. ind.cora.ally
7. ind.cora.graph
8. ind.cora.test.index



###### 資料集：(citation_dataset.py)  # 分析讀取 最原始的資料集 

1. cora ( https://linqs-data.soe.ucsc.edu/public/datasets/cora/cora.tar.gz )
2. citeseer ( https://linqs-data.soe.ucsc.edu/public/datasets/citeseer-doc-classification/citeseer-doc-classification.tar.gz )
3. pubmed ( https://linqs-data.soe.ucsc.edu/public/datasets/pubmed-diabetes/pubmed-diabetes.tar.gz )

以 cora 的資料集為例 就會**自動下載**以下8個檔案，執行 gd = load_dataset('cora') 後，就會下載並解壓縮至該目錄的 ./download/ 內。

- ./download/cora/cora.cites
- ./download/cora/cora.content

而 citeseer的資料集就會下載並解壓縮成

- ./download/citeseer-doc-classification/citeseer.cites 
- ./download/citeseer-doc-classification/citeseer.content

而 pubmed 的資料集就會下載並解壓縮成

- ./download/pubmed-diabetes/data/Pubmed-Diabetes.NODE.paper.tab
- ./download/pubmed-diabetes/data/Pubmed-Diabetes.DIRECTED.cites.tab

※ 備註: planetoid_dataset.py 與 citation_dataset.py 所得 節點的排序是不一樣的。

※ 備註: 而標籤的順序  citation_dataset.py 會排序與 planetoid_dataset.py 一樣的順序。



# Part1 展示 planetoid_dataset.py 內的使用方法

執行「Show planetoid_dataset.ipynb」的 Jupyter Notebook的檔案，需要有以下「./planetoid_dataset.py 」結構的檔案。



# Part2 展示 citation_dataset.py 內的使用方法，並顯示相關統計數據

執行「Show citation_dataset.ipynb」的 Jupyter Notebook的檔案，需要有以下「./citation_dataset.py 」結構的檔案。



# Part3 查看 GCN Model 的執行結果 

執行「Run GCN Model.ipynb」的 Jupyter Notebook的檔案，需要有以下「./planetoid_dataset.py 、./utils/tsne.py 、./utils/norm.py 、./utils/convert.py」結構的檔案。



# Part4 查看 Chebyshev GCN Model 的執行結果

執行「Run Chebyshev GCN Model.ipynb」的 Jupyter Notebook的檔案，需要有以下「./planetoid_dataset.py 、./utils/tsne.py 、./utils/norm.py 、./utils/convert.py」結構的檔案。



# Part5 查看 HGNN Model (使用A+I來建立H) 的執行結果

執行「Run HGNN Model (A+I).ipynb」的 Jupyter Notebook的檔案，需要有以下「./planetoid_dataset.py 、./utils/tsne.py 、./utils/norm.py 、./utils/convert.py」結構的檔案。



# Part6 查看 HGNN Model (使用無向邊來建立超邊) 的執行結果

執行「Run HGNN Model (Edge).ipynb」的 Jupyter Notebook的檔案，需要有以下「./planetoid_dataset.py 、./utils/tsne.py 、./utils/norm.py 、./utils/convert.py」結構的檔案。



# Part7 查看 HGNN Model (使用最近n的鄰居來建立超邊) 的執行結果

執行「Run HGNN Model (KNeighbors).ipynb」的 Jupyter Notebook的檔案，需要有以下「./planetoid_dataset.py 、./utils/tsne.py 、./utils/norm.py 、./utils/convert.py」結構的檔案。



",0,0.65,0.65,,,,,,0,1,,,
56737824,MDEwOlJlcG9zaXRvcnk1NjczNzgyNA==,MEAN_DN,danielniclas/MEAN_DN,0,danielniclas,https://github.com/danielniclas/MEAN_DN,UCSC MEAN Course Work,0,2016-04-21 02:56:23+00:00,2016-04-21 02:56:23+00:00,2016-04-21 04:59:15+00:00,,9,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,[],,,0,0.68,0.68,,,,,,0,1,,,
75889959,MDEwOlJlcG9zaXRvcnk3NTg4OTk1OQ==,PennCNV-Seq,WGLab/PennCNV-Seq,0,WGLab,https://github.com/WGLab/PennCNV-Seq,Adaption of the original PennCNV algorithm for whole-genome sequencing data,0,2016-12-08 01:12:21+00:00,2023-05-18 06:36:57+00:00,2020-08-10 23:54:27+00:00,,21,5,5,Perl,1,1,1,1,0,0,5,0,0,0,,1,0,0,public,5,0,5,master,1,"['kaichop', 'lelimat', 'joeglessner1']",1,"# PennCNV-Seq
Adaptation of the original PennCNV algorithm for whole-genome sequencing data.

## PennCNV-Seq installation

### Install PennCNV (at least version v1.0.4)

   Releases can be found [here](https://github.com/WGLab/PennCNV/releases).
   Installation instructions at [http://penncnv.openbioinformatics.org/en/latest/user-guide/install/](http://penncnv.openbioinformatics.org/en/latest/user-guide/install/)
   

### Install BEDtools

   Instructions at [http://bedtools.readthedocs.io/en/latest/content/installation.html](http://bedtools.readthedocs.io/en/latest/content/installation.html)
   
### Download PennCNV-Seq scripts

	git clone git@github.com:WGLab/PennCNV-Seq.git
	cd PennCNV-Seq
	
	# Define your download options (run the command below to see the parameters)
	./download_and_format_database.sh
	

## Run PennCNV-Seq example

	cd PennCNV-Seq
	./penncnv-seq_example.sh [penncnv_dir] [penncnv_ref_dir] [genome_version] [population] [reference.fasta] [bam_file]
	

### Visualize PennCNV results in genome browsers (IGV or UCSC website)

	python penncnv2bed.py results.rawcnv > results.bed
	
",0,0.58,0.58,,,,,,0,10,,,
773557928,R_kgDOLhuOqA,char26,char26/char26,0,char26,https://github.com/char26/char26,Profile README,0,2024-03-18 00:49:11+00:00,2024-06-21 04:49:16+00:00,2024-06-21 04:49:13+00:00,,9,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['char26'],,"<div align=""center"">
  <a href=""https://www.linkedin.com/in/charles-alders-842bbb226/"" target=""_blank"">
    <img src=""https://img.shields.io/static/v1?message=LinkedIn&logo=linkedin&label=&color=0077B5&logoColor=white&labelColor=&style=for-the-badge"" height=""25"" alt=""linkedin logo""  />
  </a>
</div>

###

<h1 align=""center"">Hello 👋 I'm Charlie.</h1>

###

<p align=""left"">🌴 Living in Santa Cruz, California <br><br>- 🖥️ Software Engineer<br>- 📚 CS&E Grad Student @ UC Santa Cruz<br>- ⚡ Technology Enthusiast (Nerd)</p>

###

<h3 align=""left"">🛠 Tools</h3>

###

<div align=""left"">
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/typescript/typescript-original.svg"" height=""40"" alt=""typescript logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/javascript/javascript-original.svg"" height=""40"" alt=""javascript logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/python/python-original.svg"" height=""40"" alt=""python logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/java/java-original.svg"" height=""40"" alt=""java logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/docker/docker-plain-wordmark.svg"" height=""40"" alt=""docker logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linux/linux-original.svg"" height=""40"" alt=""linux logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/raspberrypi/raspberrypi-original.svg"" height=""40"" alt=""raspberry pi logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/bash/bash-original.svg"" height=""40"" alt=""bash logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/googlecloud/googlecloud-original.svg"" height=""40"" alt=""googlecloud logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/git/git-original.svg"" height=""40"" alt=""git logo""  />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/postgresql/postgresql-original.svg"" height=""40"" alt=""postgresql logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/neo4j/neo4j-original.svg"" height=""40"" alt=""neo4j logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/jenkins/jenkins-original.svg"" height=""40"" alt=""jenkins logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/react/react-original.svg"" height=""40"" alt=""react logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/nextjs/nextjs-original.svg"" height=""40"" alt=""nextjs logo""  />
  <img width=""12"" />
  <img src=""https://cdn.jsdelivr.net/gh/devicons/devicon/icons/tailwindcss/tailwindcss-original.svg"" height=""40"" alt=""tailwindcss logo""  />
  <img width=""12"" />
</div>
  
  

###

<h3 align=""left"">🔥   My Stats</h3>

###

<div align=""center"">
  <img src=""https://streak-stats.demolab.com?user=char26&locale=en&mode=daily&theme=dark&hide_border=false&border_radius=5&order=3"" height=""220"" alt=""streak graph""  />
</div>

###
",1,0.75,0.75,,,,,,0,1,,,
624123070,R_kgDOJTNcvg,asg1,UCSC-CSE-13S-02/asg1,0,UCSC-CSE-13S-02,https://github.com/UCSC-CSE-13S-02/asg1,Assignment1,0,2023-04-05 19:46:14+00:00,2023-05-04 18:07:51+00:00,2023-05-09 05:27:52+00:00,,11287,0,0,JavaScript,1,1,1,0,0,0,2,0,0,0,,1,1,0,public,2,0,0,main,1,"['nisthaKumar', 'JosephWick']",1,"# asg1
Assignment1
",1,0.86,0.86,,,,,,0,0,,,
844230739,R_kgDOMlHwUw,bioinformatics-bridge-course_2024,gepolianochaves/bioinformatics-bridge-course_2024,0,gepolianochaves,https://github.com/gepolianochaves/bioinformatics-bridge-course_2024,UCSC Genomics Institute Bioinformatics Summer Bridge Course,0,2024-08-18 19:12:59+00:00,2024-10-24 04:40:54+00:00,2025-01-30 19:45:37+00:00,,111638,0,0,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,branch-do-jose,1,['gepolianochaves'],,"# bioinformatics-bridge-course_2024
UCSC Genomics Institute Bioinformatics Summer Bridge Course
",1,0.68,0.68,,,,,,0,1,,,
74914568,MDEwOlJlcG9zaXRvcnk3NDkxNDU2OA==,CS242-Machine-Learning,ChandrahasJR/CS242-Machine-Learning,0,ChandrahasJR,https://github.com/ChandrahasJR/CS242-Machine-Learning,,0,2016-11-27 21:12:45+00:00,2016-11-27 21:30:17+00:00,2016-12-26 06:40:00+00:00,,6,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['ChandrahasJR'],,"# CS242-Machine-Learning
Python code for Machine learning course at UC Santa Cruz
",1,0.74,0.74,,,,,,0,0,,,
178911062,MDEwOlJlcG9zaXRvcnkxNzg5MTEwNjI=,enzo-dev,enzo-project/enzo-dev,0,enzo-project,https://github.com/enzo-project/enzo-dev,The Enzo adaptive mesh-refinement simulation code.,0,2019-04-01 17:15:16+00:00,2025-01-28 12:39:44+00:00,2024-08-05 19:30:37+00:00,,148124,87,87,C,1,1,1,1,0,0,98,0,0,77,other,1,0,0,public,98,77,87,main,1,"['matthewturk', 'brittonsmith', 'jwise77', 'bwoshea', 'drreynolds', 'gregbryan', 'aemerick', 'jsoishi', 'clairekope', 'samskillman', 'cms21', 'ibutsky', 'unitarymatrix', 'kohdaegene', 'pgrete', 'gsiisg', 'chummels', 'stephenskory', 'yl2501', 'galtay', 'cbrummelsmith', 'pengwang234', 'drenniks', 'yipihey', 'fearmayo', 'peeples', 'rpwagner', 'jobordner', 'dcollins4096', 'suniverse', 'yusuke-fujimoto', 'emfisk', 'mqk', 'dachrist', 'forrestglines', 'mihirskulkarni', 'nsimakov', 'cindytsai', 'Stoverbrock1', 'alvarozamora', 'pterjan', 'michaelkuhn', 'laurennc', 'jcforbes', 'DeovratPrasad', 'danielskatz', 'devinsilvia', 'h3jia', 'jameslarrue', 'mabruzzo', 'limiao0611', 'zingale', 'nirmitsakre', 'setsuna0402']",1,"# ENZO

ENZO IS AN OPEN SOURCE CODE.  We encourage you to take it, inspect it, use it,
and contribute back any changes you have made.  We strive to make the the Enzo
community a community of *developers*.

## RESOURCES

Enzo's main webpage is:

 * http://enzo-project.org

Enzo is developed in the open on github.com:

 * https://github.com/enzo-project/enzo-dev/

Documentation, including instructions for compilation, can be found at:

 * https://enzo.readthedocs.io/en/latest/

Please subscribe to the Enzo Users' mailing list at:

 * https://groups.google.com/forum/#!forum/enzo-users

If you are interested in Enzo development, you may want to sign up for
the Enzo Developer's mailing list as well:

 * https://groups.google.com/forum/#!forum/enzo-dev

If you have received this source code through an archive, rather than the
git version control system, we highly encourage you to upgrade to the
version controlled source, as no support can be provided for archived
(""tarball"") sources.

## REQUIREMENTS

Mandatory:

- C/C++ and Fortan90 compiler
- MPI (such as OpenMPI, MPICH, or IntelMPI) for multi-processor parallel jobs
- [HDF5](https://www.hdfgroup.org/) (serial version) for data outputs

Optional:

- [yt](https://yt-project.org/) for data analysis and visualization (highly recommended)
- [Grackle](https://github.com/grackle-project/grackle), a chemistry and radiative 
cooling library with support for Enzo
- [KROME](http://www.kromepackage.org/), a chemistry and microphysics library with
support for Enzo

## DEVELOPERS

Many people have contributed to the development of Enzo -- here's just a short
list of the people who have recently contributed, in alphabetical order:
   
   * Tom Abel               tabel@stanford.edu
   * Gabriel Altay          gabriel.altay@gmail.com
   * James Bordner          jobordner@ucsd.edu
   * Greg Bryan             gbryan@astro.columbia.edu
   * Corey Brummel-Smith    cdbs3@gatech.edu
   * Iryna Butsky           ibutsky@uw.edu
   * Renyue Cen             cen@astro.princeton.edu
   * Duncan Christie        duncanchristie@gmail.com
   * Dave Collins           dcollins4096@gmail.com
   * Lauren Corlies         lcorlies@lsst.org
   * Brian Crosby           crosby.bd@gmail.com
   * Philipp Edelmann	    pedelmann@mpa-garching.mpg.de
   * Andrew Emerick         aemerick11@gmail.com
   * Ethan Fisk             emf@lanl.gov
   * Forrest Glines         forrestglines@gmail.com
   * Nathan Goldbaum        ngoldbau@ucsc.edu
   * Philipp Grete          grete@pa.msu.edu 
   * John Forbes            jcforbes@ucsc.edu
   * Yusuke Fujimoto        yusuke.fujimoto.jp@gmail.com
   * Oliver Hahn            hahn@phys.ethz.ch
   * Robert Harkness        harkness@sdsc.edu
   * Elizabeth Harper-Clark h-clark@astro.utoronto.ca
   * Cameron Hummels        chummels@gmail.com
   * Peter Iannucci         iannucii@mit.edu
   * Ji-hoon Kim            mornkr@tapir.caltech.edu
   * Daegene Koh            dkoh30@gatech.edu
   * Shuo Kong              skong.astro@gmail.com
   * Claire Kopenhafer      clairekope@gmail.com
   * Alexei Kritsuk         akritsuk@ucsd.edu
   * Michael Kuhlen         kuhlen@gmail.com
   * James Larrue           james.larrue@diopolis.com
   * Eve Lee                elee@cita.utoronto.ca
   * Miao Li                ml3322@columbia.edu
   * Xinyu Li		          xl2359@columbia.edu
   * Yuan Li                yuan@astro.columbia.edu
   * Greg Meece             meecegre@msu.edu
   * Yuu Niino              yuuniino@ioa.s.u-tokyo.ac.jp
   * Michael Norman         mlnorman@ucsd.edu
   * JS Oishi               jsoishi@gmail.com
   * Boon Kiat Oh           bkoh@roe.ac.uk
   * Brian O'Shea           oshea@msu.edu
   * Pascal Paschos         ppaschos@minbari.ucsd.edu
   * Molly Peeples          molly@stsci.edu
   * Carolyn Peruta         perutaca@msu.edu
   * John Regan             johnanthonyregan@gmail.com
   * Alex Razoumov          razoumov@gmail.com
   * Dan Reynolds           reynolds@smu.edu
   * Munier Salem           msalem@astro.columbia.edu
   * Devin Silvia           devin.silvia@gmail.com
   * Christine Simpson      csimpson@astro.columbia.edu
   * Samuel Skillman        samskillman@gmail.com 
   * Stephen Skory          s@skory.us
   * Britton Smith          brittonsmith@gmail.com
   * Geoffrey So            gsiisg@gmail.com
   * Elizabeth Tasker       tasker@astro1.sci.hokudai.ac.jp
   * Pascal Terjan          pterjan@gmail.com
   * Jason Tumlinson        tumlinson@stsci.edu
   * Matthew Turk           matthewturk@gmail.com
   * Rick Wagner            rwagner@physics.ucsd.edu
   * Peng Wang              penwang@nvidia.com
   * John Wise              jwise@physics.gatech.edu
   * Hao Xu                 haoxu.physics@gmail.com
   * Alvaro Zamora          alvarozamora@stanford.edu  
   * Fen Zhao               fenzhao@stanford.edu
",1,0.76,0.76,,,,Directory exists,,0,18,,,
53974860,MDEwOlJlcG9zaXRvcnk1Mzk3NDg2MA==,ucsc-email-templates,luckyluke007/ucsc-email-templates,0,luckyluke007,https://github.com/luckyluke007/ucsc-email-templates,UC Santa Cruz  Email Templates,0,2016-03-15 20:06:45+00:00,2016-04-14 00:16:51+00:00,2018-05-22 18:00:21+00:00,,559,0,0,HTML,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,"['LuckyLuke001', 'luckyluke007']",,"# ucsc-email-templates
UC Santa Cruz  Email Templates

## Jekyll Site
- [View master template](http://luckyluke007.github.io/ucsc-email-templates/build/)

## Grunt Tasks
- [Premailer](https://github.com/dwightjack/grunt-premailer) for inline css
- [Litmus](https://www.npmjs.com/package/grunt-litmus) push Litmus for test

",1,0.87,0.87,,,,,,0,1,,,
143753666,MDEwOlJlcG9zaXRvcnkxNDM3NTM2NjY=,ayan-b.github.io,ayan-b/ayan-b.github.io,0,ayan-b,https://github.com/ayan-b/ayan-b.github.io,Personal Website.,0,2018-08-06 16:16:09+00:00,2021-11-12 05:49:00+00:00,2022-11-12 16:25:10+00:00,https://git.io/ayanb,1893,0,0,HTML,1,1,1,1,1,0,0,0,0,0,mit,1,0,0,public,0,0,0,master,1,['ayan-b'],,"# Ayan Banerjee

Email: ayanbn7@gmail.com  
[Blog](./blog) | [LinkedIn](https://www.linkedin.com/in/ayanb/) | [GitHub](https://github.com/ayan-b) | [Google Scholar](https://scholar.google.com/citations?user=HQhMWIIAAAAJ) | [Resume](https://ayan-b.github.io/profile)

## Experience

### Member Technical
_Bengaluru, India_  
The D. E. Shaw Group  
Nov, 2021 - Present
- Full Stack Developer at Primary Research Group.

### Software Developer
_Bengaluru, India_  
Siemens Healthineers  
Aug, 2020 - Oct, 2021  
- Software Developer at the Customer Service team.

### Google Summer of Code student at The Virtual Brain
_Open Source_  
International Neuroinformatics Coordinating Facility  
May - Aug, 2020
- Upgrading and Fixing TVB-Gdist library ([GitHub](https://github.com/the-virtual-brain/tvb-gdist)).
- Technologies: C++, Cython, Git, Continuous Integration, Unit Testing

### Google Summer of Code student at UCSC Xena
_Open Source_  
University of California, Santa Cruz  
May - Aug, 2019
- Implemented support more data formats for Xena Browser (https://xenabrowser.net) in an ETL (Extract Transform Load) pipeline ([GitHub](https://github.com/ucscXena/xena-GDC-ETL/)).
- Data manipulation was done in Python with extensive usage of Pandas module.
- Technologies: Python, Pandas, Numpy, Git, Continuous Integration, Unit Testing, AWS

### National Institute of Technology Durgapur
_Durgapur, India_  
Undergraduate Research Assistant  
Feb - Jun, 2019
- Worked on a deep learning project under the Mechanical Engineering department.
- Technologies: Python, PyTorch, Numpy, Pandas 

### Google Code-In Mentor
_Open Source_  
Mentor under coala organization  
Oct - Dec, 2018  
- Mentored pre-university students to get them started with Open Source Contributions.

### National Institute of Technology Durgapur  
_Durgapur, India_  
Student Software Developer  
Aug, 2018  
- Developed a GUI written in Kivy for High Voltage Lab, NIT Durgapur

### Indian Institute of Technology Bombay
_Mumbai, India_  
Summer Intern at FOSSEE under Prof.  Kannan Moudgalya  
May - July, 2018  

- Top 60 among more than 22,500 applicants.
- Integrated a plagiarism detector with Yaksh, a course taking application, used inside IIT Bombay and in Spoken Tutorial courses by thousands of students.
- Technologies: Python, Django, Git, SQL, HTML, CSS, Bootstrap, Unit Testing

## Education

- **National Institute of Technology Durgapur**

    _Durgapur, India_  
    B.Tech, Electronics and Communication Engineering, GPA: 9.29/10  
    Jul 2016 - Jun 2020

- **Sargachi Ramakrishna Mission High School**

    _Sargachi, WB, India_  
    Higher Secondary, Science, 96%, Ranked 13th in State, Topped in District; Secondary, 92.43%  
    2008 - 2016

## Publications

- Mary J. Goldman, Brian Craft, Mim Hastie, Kristupas Repeka, Fran McDade, Akhil Kamath, **Ayan Banerjee**, Yunhai Luo, Dave Rogers, Angela N. Brooks, Jingchun Zhu & David Haussler, _“Visualizing and interpreting cancer genomics data via the Xena platform”_, May 22, 2020, Published at Nature Biotechnology.

- Aniq Ur Rahman, Sarnava Konar, **Ayan Banerjee**, _“Test-bench for Task Offloading Mechanisms: Modelling the Rewards of Non-stationary Nodes”_, Published at IEEE ANTS (2019), Goa, India.

## Projects

- **Dustbin Management System**:  Full stack web app integrated with a smart dustbin which can sense the percentage filled and transmit the data using NodeMCU. The web app is built using Django, MaterializeCSS; this project came first in the IoT hackathon in Aavishkar, the tech fest of NIT Durgapur; a team project.

- **Chat Web App**:  A full stack web app for real time chatting using sockets, built using NodeJS, Express, ReactJS, Socket.io, MongoDB.

- **GitHub Activity Checker**:  A web app to show a GitHub user's public activities, built using JavaScript, jQuery, Ajax, Material Design and GitHub API.

## Achievements

- Ranked 191st among 8251 participants in CodeChef February 2018 Long Challenge.
- Ranked 1881st among more than 150,000 participants in CodeChef (Best Ranking, Sept, ’18, Best Rating 2027 (5-star)).

## Skills

- **Proficient In**:  C, C++, Python, Data Structures and Algorithms, Version Control System (git), Object Oriented Programming, DBMS.

- **Familiar With**:  Linux, Go, Computer Networks, Operating System, Web Technologies (HTML, CSS, JavaScript), Backend Technologies (Django), Deep Learning, PyTorch, ReactJS, NodeJS, Docker, LATEX.

## Extracurricular Activities

- National Service Scheme (2017 - 2019): Organized annual survey and medical camps in villages.
- National Cadet Corps (2011 - 13, 2016 - 17):  Took part in parades during Sports Day, Independence Day and Republic Day.
",1,0.79,0.79,,,,,,0,1,,,
31684536,MDEwOlJlcG9zaXRvcnkzMTY4NDUzNg==,UCSC_ProgrammingProjects,msouppe/UCSC_ProgrammingProjects,0,msouppe,https://github.com/msouppe/UCSC_ProgrammingProjects,Programming Projects that I implemented during my academic career at University of California Santa Cruz,0,2015-03-04 22:59:32+00:00,2016-07-25 02:41:10+00:00,2016-10-28 15:54:52+00:00,,3504,0,0,C,1,1,1,1,0,0,0,0,0,1,,1,0,0,public,0,1,0,master,1,['msouppe'],,,1,0.73,0.73,,,,,,0,1,,,
76773932,MDEwOlJlcG9zaXRvcnk3Njc3MzkzMg==,C3D,tahmidmehdi/C3D,0,tahmidmehdi,https://github.com/tahmidmehdi/C3D,Cross Cell-type Correlation in DNaseI hypersensitivity,0,2016-12-18 09:23:24+00:00,2021-03-01 16:21:05+00:00,2021-03-01 16:20:59+00:00,,19619,1,1,Shell,1,1,1,1,0,0,6,0,0,0,gpl-3.0,1,0,0,public,6,0,1,master,1,['tahmidmehdi'],,"# C3D
Cross Cell-type Correlation in DNaseI hypersensitivity

INSTRUCTIONS

C3D - Cross Cell-type Correlation in DNase I hypersensitivity

calculates correlations between open regions of chromatin based on DNase I hypersensitivity signals. Regions with high correlations are candidates for 3D interactions. It also performs association tests on each candidate and adjusts p-values. At minimum, C3D requires a BED file of accessible genomic regions, a BED file of potential chromatin loop anchors (for example, promoters of genes you are interested in), BEDGRAPH files of DNase I hypersensitivity signals for each biological sample you want to calculate correlations across, and an output directory for results. The file paths and names of these files and directories must be provided in a configuration file. In addition, many optional parameters can also be set in the file. For each anchor, C3D will compute correlations between open regions, that overlie the anchor, and regions distal to it, based on DNase I hypersensitivity signals for the provided biological samples. There is also an option to calculate correlations between each anchor region and all other open regions genome-wide. Then, each correlation is tested for significance and multiple test corrections are performed; p-values and q-values are reported.

RUNNING C3D

Usage: sh c3d < config file >

Parameters for config file

	reference (Optional if matrix provided): A BED file of accessible genomic regions.

	anchor (Required): A BED file of anchor regions.

		The recommended format of the anchor file is as follows (Do not include the header):

		CHR	START	END	STRAND	ID

		chr12   103348963       103351963       +       ASCL1_ENST00000266744.3
		chr7    5567229 5570229 -       ACTB_ENST00000464611.1
		chr7    155604467       155607467       .       SHH_ENST00000297261.2

	db (Optional if matrix provided): A TEXT file containing a list of full paths and names of BEDGRAPH files for calculating correlations across.

	outDirectory (Required): Output directory for results.

	matrix (Optional if reference and db provided): A tab-delimited TEXT file of signal data with regions as rows and samples as columns. It must have row names in chr:start-end format. If not provided, a matrix will be generated using reference and db. If you want to run C3D multiple times on the same reference and db, pass reference and db on the first run. For any subsequent runs, pass matrix with the signalMatrix.txt file created during the first run. 

	window (Optional): The maximum number of bps from an anchor an open region can be to be considered distal. Correlations are only calculated between distal regions unless window is set to genome. If set to genome, it will compute correlations between each anchor region and every region in the reference file. Defaults to 500000.

	correlationThreshold (Optional): Only interaction candidates with correlations above or equal to this will be shown in figures. Defaults to 0.7.

	pValueThreshold (Optional): Only interaction candidates with p-values below or equal to this will be shown. Defaults to 0.05.

	qValueThreshold (Optional): Only interaction candidates with q-values below or equal to this will be shown. Defaults to 0.1.

	correlationMethod (Optional): Correlation coefficient (pearson, spearman, kendall). Defaults to pearson.

	figures (Optional): y if an arc diagram should be generated for each anchor. Figures will be shown in the same order as anchors from the anchor file. Leave this parameter out if you do not want figures or have many anchors.

	figureWidth (Optional): Number of flanking bps from anchor to display on figures. Use a comma-separated list to assign different widths to different figures. For example, if you have 3 figures and 500000,100000,400 is passed then the figures will show 500000, 100000, and 400 bps around the first, second, and third anchors respectively. Defaults to window.

	zoom (Optional): Number of flanking bps from anchor to show for zoomed figures. You can also pass a comma-separated list of numbers if you want different zoom regions for each figure. For example, if you have 3 figures and you pass 10000,50000,75000 then the zoom regions for figures 1,2, and 3 will be 10000,50000, and 75000 respectively. Use 0 if you do not wish to zoom. Defaults to no zoom.

	colours (Optional): 4 colours for the loops in the arc diagrams. Loops are colour-coded based on q-values. For example, if blue,red,green,purple is passed then loops of interactions with q-values>0.05 will be blue, between 0.05 and 0.01 will be red, between 0.01 and 0.001 will be green and below 0.001 will be purple. Hexadecimal digits accepted. Defaults to shades of blue.

	tracks (Optional): y if files with tracks should be generated. These files can be uploaded to the UCSC Genome Browser or the Integrative Genomics Viewer (IGV) to view interaction candidates. Leave this parameter out if you do not want figures or have many anchors.

	assembly (Optional): Reference genome for tracks. Defaults to hg19.
Multi-sample Parameters

If you want to run C3D on multiple samples, do not include reference or matrix. Instead, include one of the following:

	references (Optional if matrices provided): A file containing a list of reference BED files where each line is of the form: 

		<reference.bed> <sample name>

	matrices (Optional if references and db provided): A file containing a list of TEXT files with matrices where each line is of the form: 

		<signalMatrix.txt> <sample name>

All other parameters are still available.

OUTPUT

The results file is named results_< timestamp >.txt and the results are formatted as a table.

	COORD_1	Genomic coordinates of open region in an anchor	
	COORD_2	Genomic coordinates of open region that is distal to an anchor
	R_<correlationMethod>	Correlation score between COORD_1 and COORD_2
	ID	ID of anchor that COORD_1 overlaps
	p_Value	P-value from association test between COORD_1 and COORD_2
	q_Value	Adjusted p-value after performing multiple test correction

Arc diagrams can be viewed in a file called figures_< timestamp >.pdf (if figures=y is in config file).
Tracks can be found in files called < anchor ID >.tracks.txt, < anchor ID >.anchor.bedGraph and < anchor ID >.bedGraph for each anchor (if tracks=y is in config file).

TUTORIAL (CASE EXAMPLES)

Setting up the software environment

	Bash (version >= 4.0), R (version >= 3.3.1) and bedtools (version >= 2.19.0) must be installed on the system to run C3D properly. Instructions for downloading Bash, R and bedtools can be found from http://www.gnu.org/software/bash/manual/html_node/Installing-Bash.html, http://cran.r-project.org and http://bedtools.readthedocs.io/en/latest/content/installation.html, respectively.

	The following R packages must be installed: GenomicRanges, Sushi, data.table, preprocessCore and dynamicTreeCut. To install them, start R and enter the following commands:
		source(""https://bioconductor.org/biocLite.R"")
		biocLite(c(""GenomicRanges"", ""Sushi"", ""preprocessCore""))
		install.packages(c(""data.table"", ""dynamicTreeCut""))
		
Download C3D and example data

	C3D and its documentation are available from https://github.com/mlupien/C3D.
	For this example, we will predict genomic interactions involving the promoters of abelson tyrosine-protein kinase 1 (ABL1) and the breakpoint cluster region protein (BCR) in K562 (chronic myelogenous leukemia) cells by calculating correlations across DNase-seq signals for 79 ENCODE cell lines. All of the data for this example is located in https://github.com/mlupien/C3D/tree/master/example.
	
    First, download the BED file called test_anchors.bed. This contains anchors for ABL1 and BCR.
    
        Note: this file was generated by entering the following command in the terminal:
        
            Rscript make_anchors_from_genes.R genes.txt 1000 0 test_anchors.bed
    
    Now, download signalMatrix.txt. This file is formatted as a matrix whose rows correspond to DNase I Hypersensitive Sites (DHSs) and columns correspond to cell lines. Each entry is the maximum DNase-seq signal over the given DHS in the given cell line.
    
        Note: signalMatrix.txt was generated by following these steps:
    
        1.  download a BEDGRAPH file for each of the 79 cell lines. This can be done by running download_79_ENCODE_cell_lines.sh from https://github.com/mlupien/C3D/tree/master/example.

            sh download_79_ENCODE_cell_lines.sh
	
	    This script requires wget. If it is not installed, instructions for downloading wget can be found from https://www.gnu.org/software/wget/ (http://gnuwin32.sourceforge.net/packages/wget.htm for windows).
        
            Aside: You can also download 126 BEDGRAPH files from ENCODE and 53 BEDGRAPH files from Roadmap for the db.
            
            sh download_126_encode_bedgraphs.sh
            sh download_convert_53_roadmap_bigwigs.sh
            
        One of these scripts requires gzip and UCSC command line bioinformatic utilities. If they are not installed, instructions for downloading gzip and UCSC command line bioinformatic utilities can be found from http://www.gzip.org/ (http://gnuwin32.sourceforge.net/packages/gzip.htm for windows) and https://github.com/ENCODE-DCC/kentUtils, respectively. 

	    2.  Create a file that lists all of the full paths and file names of the BEDGRAPH files. Each file name should be on a separate line. For example, the first three lines may be formatted as follows:
       
            /path/to/wgEncodeUwDnaseA549Aln_2Reps.norm5.rawsignal.bedgraph
            /path/to/wgEncodeUwDnaseAg04449Aln_2Reps.norm5.rawsignal.bedgraph
            /path/to/wgEncodeUwDnaseAg04450Aln_2Reps.norm5.rawsignal.bedgraph

        3.  download a BED file of DHSs in K562.
    
            wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep1.narrowPeak.gz
            gunzip wgEncodeUwDnaseK562PkRep1.narrowPeak.gz
        
        This command requires gzip.
        
        4.  Edit the config.txt located in https://github.com/mlupien/C3D/tree/master/example by replacing the line starting with 'matrix=' with:
            
            reference=</path/to/>wgEncodeUwDnaseK562PkRep1.narrowPeak
            db=</path/to/list_of_bedgraphs.txt>
            
        Set reference, db, anchor and outDirectory with appropriate paths and file names.
        
Configure and run C3D

    Download and edit config.txt by setting anchor, outDirectory and matrix with appropriate paths and file names. Run C3D with the following command:
    
        sh c3d config.txt

C3D will output the results to results_< timestamp >.txt. Arc diagrams for each anchor can be found in figures_< timestamp >.pdf. Tracks will be generated in files ending in .tracks.txt and .bedGraph. Files ending with .tracks.txt can be uploaded to the UCSC Genome Browser (https://genome.ucsc.edu/cgi-bin/hgGateway) to visualize the correlations. Alternatively, tracks ending in .bedGraph can be uploaded to IGV. Instructions for downloading and using IGV can be found at https://software.broadinstitute.org/software/igv/download. All of these files will be located in the specified outDirectory.
",0,0.5,0.5,,,,,,0,2,,,
745116827,R_kgDOLGmUmw,2024-02-23-UCSC-online,stephr1000/2024-02-23-UCSC-online,0,stephr1000,https://github.com/stephr1000/2024-02-23-UCSC-online,UC Santa Cruz Spring Software Carpentry Series 2024,0,2024-01-18 17:13:21+00:00,2024-01-18 17:13:28+00:00,2024-01-29 18:17:29+00:00,,2245,0,0,HTML,1,1,1,1,1,0,0,0,0,0,other,1,0,0,public,0,0,0,gh-pages,1,[],,"[![Website](https://github.com/carpentries/workshop-template/actions/workflows/website.yml/badge.svg)](https://github.com/carpentries/workshop-template/actions/workflows/website.yml)

# The Carpentries Workshop Template

This repository is The Carpentries' ([Software Carpentry][swc-site], [Data Carpentry][dc-site], and
[Library Carpentry][lc-site]'s) template for creating websites for workshops.

1. **Please _do not fork this repository directly on GitHub._** Instead, please use GitHub's
   ""template"" function following [the instructions below](#creating-a-repository) to copy this
   `workshop-template` repository and customize it for your workshop.

2. Please *do your work in your repository's `gh-pages` branch*, since that is what is
   [automatically published as a website by GitHub][github-project-pages].

3. Once you are done, please also [let us know][email] the workshop URL. If this is a self-organised
   workshop, you should also [fill in the self-organized workshop
   form][self-organized-workshop-form] (if you have not already done so), so we can keep track of
   all workshops. We build the list of workshops on our websites from the data included in your
   `index.md` page. We can only do that if you [customize][customization] that page correctly *and*
   let us know the workshop URL.

If you run into problems,
or have ideas about how to make this process simpler,
please [get in touch](#getting-and-giving-help).
The pages on [customizing your website][customization],
the [FAQ][faq],
and the [design notes][design] have more detail on what we do and why.
And please note:
if you are teaching Git,
please [create a separate repository](#setting-up-a-separate-repository-for-learners)
for your learners to practice in.

## Video Tutorial

There is a [YouTube video](https://www.youtube.com/watch?v=_Ag1JiZzyUQ) that demonstrates how to
create a workshop website.

## Creating a Repository

1.  Log in to GitHub.
    (If you do not have an account, you can quickly create one for free.)
    You must be logged in for the remaining steps to work.

2.  On this page (<https://github.com/carpentries/workshop-template>),
    click on the green ""Use this template"" button (top right)

    ![screenshot of this repository's GitHub page with an arrow pointing to the the 'use this template' button on the top left](fig/select-github-use-template.png?raw=true)
    Alternatively, use this link: [Use this template](https://github.com/new?template_name=workshop-template&template_owner=carpentries).

3.  Select the owner for your new repository.
    (This will probably be you, but may instead be an organization you belong to.)

4.  Choose a name for your workshop website repository.
    This name should have the form `YYYY-MM-DD-site`,
    e.g., `2016-12-01-oomza`,
    where `YYYY-MM-DD` is the start date of the workshop.
    If your workshop is held online, then the respository name should have `-online` in the end.
    e.g., `2016-12-01-oomza-online`

5.  Make sure the repository is public, leave ""Include all branches"" unchecked, and click
on ""Create repository from template"".
You will be redirected to your new copy of the workshop template respository.

6. Your new website will be rendered at `https://your_username.github.io/YYYY-MM-DD-site`.
For example, if your username is `gvwilson`, the website's URL will be
`https://gvwilson.github.io/2016-12-01-oomza`.

If you experience a problem, please [get in touch](#getting-and-giving-help).

## Customizing Your Website (Required Steps)

There are two ways of customizing your website. You can either:

- edit the files directly in GitHub using your web browser
- clone the repository on your computer and update the files locally

### Updating the files on GitHub in your web browser

1.  Go into your newly-created repository,
    which will be at `https://github.com/your_username/YYYY-MM-DD-site`.
    For example,
    if your username is `gvwilson`,
    the repository's URL will be `https://github.com/gvwilson/2016-12-01-oomza`.

3.  Ensure you are on the gh-pages branch by clicking on the branch under the drop
    down in the menu bar (see the note below):

    ![screenshot of this repository's GitHub page showing the ""Branch"" dropdown menu expanded with the ""gh-pages"" branch selected](fig/select-gh-pages-branch.png?raw=true)

3.  Edit the header of `index.md` to customize the list of instructors,
    workshop venue, etc.
    You can do this in the browser by clicking on it in the file view on GitHub
    and then selecting the pencil icon in the menu bar:

    ![screenshot of top menu bar for GitHub's file interface with the edit icon highlighted in the top right](fig/edit-index-file-menu-bar.png?raw=true)

    Editing hints are embedded in `index.md`,
    and full instructions are in [the customization instructions][customization].

4.  Remove the notice about using the workshop template in the `index.md` file. You can safely
    delete everything between the `{% comment %}` and `{% endcomment %}` (included) as indicated
    below (from line 35 to line 58):

    ```jekyll
    {% comment %} <------------ remove from this line
    8< ============= For a workshop delete from here =============
    For a workshop please delete the following block until the next dashed-line
    {% endcomment %}

    <div class=""alert alert-danger"">
      ....
    </div>

    {% comment %}
     8< ============================= until here ==================
    {% endcomment %} <--------- until this line
    ```

4.  Edit `_config.yml` to customize certain site-wide variables, such as: `carpentry` (to tell your
    participants the lesson program for your workshop), `curriculum` and `flavor` for the
    curriculum  taught in your workshop, and `title` (overall title for all pages).

    Editing hints are embedded in `_config.yml`,
    and full instructions are in [the customization instructions][customization].

5. Edit the `schedule.html` file to edit the schedule for your upcoming workshop. This file is
   located in the `_includes` directory, make sure to choose the one from the appropriate `dc` (Data
   Carpentry workshop), `lc` (Library Carpentry), or `swc` (Software Carpentry) subdirectory.

### Working locally

> Note: you don't have to do this, if you have already updated your site using the web interface.


If you are already familiar with Git, you can clone the repository to your desktop, edit `index.md`,
`_config.yml`, and `schedule.html` following the instruction above there, and push your changes back to the repository.

```shell
git clone https://github.com/your_username/YYYY-MM-DD-site
```

In order to view your changes once you are done editing, if you have bundler installed (see the
[installation instructions below](#installing-software)), you can preview your site locally with:

```shell
make serve
```
and go to <http://0.0.0.0:4000> to preview your site.

Before pushing your changes to your repository, we recommend that you also check for any potential
issues with your site by running:

```shell
make workshop-check
```

Once you are satisfied with the edits to your site, commit and push the changes to your repository.
A few minutes later, you can go to the GitHub Pages URL for your workshop site and preview it. In the example above, this is `https://gvwilson.github.io/2016-12-01-oomza`. [The finished
page should look something like this](fig/completed-page.png?raw=true).


## Optional but Recommended Steps


### Update your repository description and link your website

At the top of your repository on GitHub you'll see

~~~
No description, website, or topics provided. — Edit
~~~

Click 'Edit' and add:

1.  A very brief description of your workshop in the ""Description"" box (e.g., ""Oomza University workshop, Dec. 2016"")

2.  The URL for your workshop in the ""Website"" box (e.g., `https://gvwilson.github.io/2016-12-01-oomza`)

This will help people find your website if they come to your repository's home page.

### Update the content of the README file

You can change the `README.md` file in your website's repository, which contains these instructions,
so that it contains a short description of your workshop and a link to the workshop website.


## Additional Notes

**Note:**
please do all of your work in your repository's `gh-pages` branch,
since [GitHub automatically publishes that as a website][github-project-pages].

**Note:**
this template includes some files and directories that most workshops do not need,
but which provide a standard place to put extra content if desired.
See the [design notes][design] for more information about these.

Further instructions are available in [the customization instructions][customization].
This [FAQ][faq] includes a few extra tips (additions are always welcome)
and these notes on [the background and design][design] of this template may help as well.


## Creating Extra Pages

In rare cases,
you may want to add extra pages to your workshop website.
You can do this by putting either Markdown or HTML pages in the website's root directory
and styling them according to the instructions give in
[the lesson template][lesson-example].


## Installing Software

If you want to set up Jekyll so that you can preview changes on your own machine before pushing them
to GitHub, you must install the software described in the lesson example [setup
instructions](https://carpentries.github.io/lesson-example/setup.html#jekyll-setup-for-lesson-development).

## Setting Up a Separate Repository for Learners

If you are teaching Git,
you should create a separate repository for learners to use in that lesson.
You should not have them use the workshop website repository because:

* your workshop website repository contains many files that most learners don't need to see during
  the lesson, and

* you probably don't want to accidentally merge a damaging pull request from a novice Git user into
  your workshop's website while you are using it to teach.

You can call this repository whatever you like, and add whatever content you need to it.

## Getting and Giving Help

We are committed to offering a pleasant setup experience for our learners and organizers.
If you find bugs in our instructions,
or would like to suggest improvements,
please [file an issue][issues]
or [mail us][email].

[email]: mailto:team@carpentries.org
[customization]: https://carpentries.github.io/workshop-template/customization/index.html
[dc-site]: https://datacarpentry.org
[design]: https://carpentries.github.io/workshop-template/design/index.html
[faq]: https://carpentries.github.io/workshop-template/faq/index.html
[github-project-pages]: https://help.github.com/en/github/working-with-github-pages/creating-a-github-pages-site
[issues]: https://github.com/carpentries/workshop-template/issues
[lesson-example]: https://carpentries.github.io/lesson-example/
[self-organized-workshop-form]: https://amy.carpentries.org/forms/self-organised/
[swc-site]: https://software-carpentry.org
[lc-site]: https://librarycarpentry.org
",1,0.81,0.81,"---
layout: page
title: ""Contributor Code of Conduct""
---
As contributors and maintainers of this project,
we pledge to follow the [Carpentry Code of Conduct][coc].

Instances of abusive, harassing, or otherwise unacceptable behavior
may be reported by following our [reporting guidelines][coc-reporting].

[coc]: https://docs.carpentries.org/topic_folders/policies/code-of-conduct.html
[coc-reporting]: https://docs.carpentries.org/topic_folders/policies/incident-reporting.html
","# Contributing

[Software Carpentry][swc-site] and [Data Carpentry][dc-site] are open source projects,
and we welcome contributions of all kinds:
new lessons,
fixes to existing material,
bug reports,
and reviews of proposed changes are all welcome.

## Contributor Agreement

By contributing,
you agree that we may redistribute your work under [our license](LICENSE.md).
In exchange,
we will address your issues and/or assess your change proposal as promptly as we can,
and help you become a member of our community.
Everyone involved in [Software Carpentry][swc-site] and [Data Carpentry][dc-site]
agrees to abide by our [code of conduct](CONDUCT.md).

## How to Contribute

The easiest way to get started is to file an issue
to tell us about a spelling mistake,
some awkward wording,
or a factual error.
This is a good way to introduce yourself
and to meet some of our community members.

1.  If you do not have a [GitHub][github] account,
    you can [send us comments by email][contact].
    However,
    we will be able to respond more quickly if you use one of the other methods described below.

2.  If you have a [GitHub][github] account,
    or are willing to [create one][github-join],
    but do not know how to use Git,
    you can report problems or suggest improvements by [creating an issue][issues].
    This allows us to assign the item to someone
    and to respond to it in a threaded discussion.

3.  If you are comfortable with Git,
    and would like to add or change material,
    you can submit a pull request (PR).
    Instructions for doing this are [included below](#using-github).

## Where to Contribute

1.  If you wish to change the template used for workshop websites,
    please work in <https://github.com/swcarpentry/workshop-template>.
    The home page of that repository explains how to set up workshop websites,
    while the extra pages in <https://swcarpentry.github.io/workshop-template>
    provide more background on our design choices.

2.  If you wish to change CSS style files, tools,
    or HTML boilerplate for lessons or workshops stored in `_includes` or `_layouts`,
    please work in <https://github.com/swcarpentry/styles>.

## What to Contribute

There are many ways to contribute,
from writing new exercises and improving existing ones
to updating or filling in the documentation
and submitting [bug reports][issues]
about things that don't work, aren't clear, or are missing.
If you are looking for ideas,
please see [the list of issues for this repository][issues],
or the issues for [Data Carpentry][dc-issues]
and [Software Carpentry][swc-issues] projects.

Comments on issues and reviews of pull requests are just as welcome:
we are smarter together than we are on our own.
Reviews from novices and newcomers are particularly valuable:
it's easy for people who have been using these lessons for a while
to forget how impenetrable some of this material can be,
so fresh eyes are always welcome.

## What *Not* to Contribute

Our lessons already contain more material than we can cover in a typical workshop,
so we are usually *not* looking for more concepts or tools to add to them.
As a rule,
if you want to introduce a new idea,
you must (a) estimate how long it will take to teach
and (b) explain what you would take out to make room for it.
The first encourages contributors to be honest about requirements;
the second, to think hard about priorities.

We are also not looking for exercises or other material that only run on one platform.
Our workshops typically contain a mixture of Windows, macOS, and Linux users;
in order to be usable,
our lessons must run equally well on all three.

## Using GitHub

If you choose to contribute via GitHub,
you may want to look at
[How to Contribute to an Open Source Project on GitHub][how-contribute].
In brief:

1.  The published copy of the lesson is in the `gh-pages` branch of the repository
    (so that GitHub will regenerate it automatically).
    Please create all branches from that,
    and merge the [master repository][repo]'s `gh-pages` branch into your `gh-pages` branch
    before starting work.
    Please do *not* work directly in your `gh-pages` branch,
    since that will make it difficult for you to work on other contributions.

2.  We use [GitHub flow][github-flow] to manage changes:
    1.  Create a new branch in your desktop copy of this repository for each significant change.
    2.  Commit the change in that branch.
    3.  Push that branch to your fork of this repository on GitHub.
    4.  Submit a pull request from that branch to the [master repository][repo].
    5.  If you receive feedback,
        make changes on your desktop and push to your branch on GitHub:
        the pull request will update automatically.

Each lesson has two maintainers who review issues and pull requests
or encourage others to do so.
The maintainers are community volunteers,
and have final say over what gets merged into the lesson.

## Other Resources

General discussion of [Software Carpentry][swc-site] and [Data Carpentry][dc-site]
happens on the [discussion mailing list][discuss-list],
which everyone is welcome to join.
You can also [reach us by email][contact].

[contact]: mailto:admin@software-carpentry.org
[dc-issues]: https://github.com/issues?q=user%3Adatacarpentry
[dc-lessons]: http://datacarpentry.org/lessons/
[dc-site]: http://datacarpentry.org/
[discuss-list]: http://lists.software-carpentry.org/listinfo/discuss
[github]: http://github.com
[github-flow]: https://guides.github.com/introduction/flow/
[github-join]: https://github.com/join
[how-contribute]: https://egghead.io/series/how-to-contribute-to-an-open-source-project-on-github
[issues]: https://github.com/swcarpentry/workshop-template/issues/
[repo]: https://github.com/swcarpentry/workshop-template/
[swc-issues]: https://github.com/issues?q=user%3Aswcarpentry
[swc-lessons]: http://software-carpentry.org/lessons/
[swc-site]: http://software-carpentry.org/
",,,"<details>
<summary><strong>Instructions</strong></summary>

Thanks for contributing! :heart:

If this contribution is for instructor training, please email the link to this contribution to
instructor.training@carpentries.org so we can record your progress. You've completed your contribution
step for instructor checkout by submitting this contribution!

Keep in mind that **lesson maintainers are volunteers** and it may take them some time to
respond to your contribution. Although not all contributions can be incorporated into the lesson
materials, we appreciate your time and effort to improve the curriculum. If you have any questions
about the lesson maintenance process or would like to volunteer your time as a contribution
reviewer, please contact The Carpentries Team at team@carpentries.org.

You may delete these instructions from your comment.

\- The Carpentries
</details>
",0,2,,,
47953738,MDEwOlJlcG9zaXRvcnk0Nzk1MzczOA==,ucsc-class-info-bot,pfroud/ucsc-class-info-bot,0,pfroud,https://github.com/pfroud/ucsc-class-info-bot,Posts comments on /r/UCSC with info about classes mentioned.,0,2015-12-14 05:23:20+00:00,2025-01-23 23:19:27+00:00,2022-07-21 07:26:06+00:00,,1743,10,10,Python,0,0,1,0,0,0,2,1,0,0,gpl-3.0,1,0,0,public,2,0,10,main,1,['pfroud'],,"# /r/UCSC class info bot

Searches for class mentions on the [/r/UCSC](https://www.reddit.com/r/ucsc) subreddit, then looks up and posts class information. 

This bot lives on the reddit account [/u/ucsc-class-info-bot](https://www.reddit.com/user/ucsc-class-info-bot).

## Origin

In online discussions about UCSC classes, people often post a four-letter department code followed by a two- or three-digit class number.

Experienced students in a department will know what a class is just from the number. But new students would need to look up a class number to participate in the discussion. Additionally, students outside the department probably have no idea what class they're talking about.

I always liked reading the descriptions of classes people refer to, so I wrote this bot to automatically post them.

## Terminology

From here on I use 'course' instead of 'class' because `class` is a reserved Python keyword.

A *course mention* occurs when a redditor names one or more courses in a post or comment. See section [mention types](#mention-types).

A *course object* is an instance of the [`Course`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/db_core.py#L87-L99) class from [`db_core.py`](https://github.com/pfroud/ucsc-class-info-bot/blob/master/db_core.py). A course object contains a course's department, number, name, and description.

A *department code* is a string of between two and four (inclusive) letters that is an abbreviation of a department's name. For example, `CMPS` is the department code for Computer Science.

A *course number* is a string, not an integer, because a course number might have a letter at the end. For example, `112`  and  `12A` are both course numbers. 

## The course database 
The course database uses a course's department and number to look up that course's name and description. In other words, we input a course *mention* and get a Course *object*. The files [`db_core.py`](https://github.com/pfroud/ucsc-class-info-bot/blob/master/db_core.py)  and [`db_extra.py`](https://github.com/pfroud/ucsc-class-info-bot/blob/master/db_extra.py) create the database.

### Database structure
The database stores a [Pickled](https://docs.python.org/3/library/pickle.html)  instance of `CourseDatabase`, which has a dict mapping a department code string to a `Department` instance. A `Department` instance has a dict mapping a course number to a `Course` instance. A `Course` instance has department, number, name, description. The relationship between these structures is illustrated below.

![Database structure diagram](database-structure-diagram.svg)

You can see the log from building the database at [misc/db build log.txt](misc/db build log.txt). You can see the database's contents at [misc/db print.txt](misc/db print.txt).

### Implementation attempts

I had to try a few ways to make the database work. HTML parsing in each attempt is done by [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/).

#### First attempt - class search page

My original idea for scraping course info was through the [class search](https://pisa.ucsc.edu/class_search/) page. This works but is a pain in the ass because I need to send a POST request *and* parse the returned HTML page. Also, it was not suitable for building the database because the class search page only lists courses offered in the current quarter.

The implementation is preserved in [misc/get_course_info.py](misc/get_course_info.py) for your viewing pleasure.

#### Second attempt - department websites

My second idea was to scrape course info from the website of each academic department. There were multiple problems.

First, different departments put their course catalogs on different URLs. Each of these departments use a slightly different (but tantalizingly similar) URL pattern: [Chemistry](http://chemistry.ucsc.edu/academics/courses/course-catalog.php), [History](http://history.ucsc.edu/courses/catalog-view.php), [Mathematics](http://www.math.ucsc.edu/courses/course-catalog.php), [Linguistics](http://linguistics.ucsc.edu/courses/course-catalog-view.php), [Anthropology](http://anthro.ucsc.edu/courses/course_catalog.php).

Second, some courses appear in a department that doesn't match their department code. For example, classes in Chinese (CHIN), French (FREN), and German (GERM) are all listed on the [Language department's](http://language.ucsc.edu/courses/course-catalog.php) page.

Third, some departments use a custom layout to list course info. For example, compare the standard layout used by the [History department](http://history.ucsc.edu/courses/catalog-view.php) to the custom layouts used by the [Art department](http://art.ucsc.edu/courses/2015-16) and the [School of Engineering](https://courses.soe.ucsc.edu/).

All of these aspects would've made scraping extremely difficult.

#### Third, successful, attempt - Registrar website

The third version works.  The UCSC Registrar lists every course in every department with a beautifully consistent URL: `http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/<DEPARTMENT_CODE>.html`. Users can go to [`index.html`](http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/index.html) and choose a department on the left (scroll down).

This option is clearly the best. I didn't use it from the beginning because it was hard to find: from the [Registrar homepage](http://registrar.ucsc.edu/), you need to click on Quick Start Guide > Catalog > Programs and Courses > Course Descriptions.

Even through the Registrar's website is mostly well-organized, some things are broken. Read more in the next section.

### Special cases for building the database

The file [`db_core.py`](https://github.com/pfroud/ucsc-class-info-bot/blob/master/db_core.py) handles almost every department when scraping the Registrar's site, but [`db_extra.py`](https://github.com/pfroud/ucsc-class-info-bot/blob/master/db_extra.py) is needed to handle the following four special cases.

Some of these special cases have since been fixed on the Registrar's website.

#### Indentation

First, some courses are indented in their own paragraph. For example, [Psychology](http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/psyc.html) 118A-D are all indented under the header for 118.

The functions [`is_next_p_indented()`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/db_extra.py#L37-L56) and [`in_indented_paragraph()`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/db_extra.py#L59-L67) check for this case and additional logic compensates.

####  Literature department

&rarr; The Registrar website has fixed  this. It seems all sub-departments have been combined into the Literature department. You can see what the Literature page used to look like [here](http://web.archive.org/web/20160521192216/http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/lit.html).

~~Second, the [Literature](http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/lit.html) department contains courses from multiple department codes. For example, Creative Writing (LTCR) and and Latin Literature (LTIN) classes are both under [`lit.html`](http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/lit.html).~~

~~The page uses subdepartment *names* but we care about subdepartment *codes*, so the dict [`lit_department_codes`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/db_extra.py#L6-L17) maps names to codes. For example, ""Modern Literary Studies"" maps to ""LTMO"" and ""Greek Literature"" maps to ""LTGR"".~~

~~Consequently the lit page is scraped by its own function, [`get_lit_depts()`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/db_extra.py#L135-L164), with help from the function [`get_real_lit_dept()`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/db_extra.py#L118-L132).~~


#### Inconsistent HTML layout

&rarr; The Registrar website has fixed this.

~~Third, some departments deviate from the standard HTML layout.~~

~~For almost every department, key information about a course is contained in three `<strong>` tags. Here's an example from [Biomolecular Engineering (BME)](http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/bme.html):~~

```html
<strong>110.</strong>
<strong>Computational Biology Tools.</strong>
<strong>F,W</strong>
```
~~To build the database, I being by looking for `<strong>` tags containing a course number followed by a period.  (The ""F,W"" indicates which general education requirements are satisfied by that course.)~~

 ~~However, *one single department does this differently*. [College Eight (CLEI)](http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/clei.html) puts the entire header in one `<strong>` tag:~~
```html
<strong>81C. Designing a Sustainable Future. S</strong>
```
&rarr; You can see what the College Eight page used to look like [here](http://web.archive.org/web/20160429201042/http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/clei.html).

~~So, there's one [stupid special case](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/db_core.py#L219-L220).~~

~~Furthermore, two departments miss the first `<strong>` tag. The first courses on the [German](http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/germ.html) and [Economics](http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/econ.html) pages look like this:~~
```html
1. <strong>First-Year German.</strong>
<strong>F</strong>
```
~~I only look for course numbers inside of `<strong>` tags, so course 1 gets left out. There's another [stupid special case](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/db_core.py#L224-L225).~~

&rarr; You can see what the German page used to look like [here](http://web.archive.org/web/20160429201246/http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/germ.html). You can see what the Economics page used to look like [here](http://web.archive.org/web/20160429201150/http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/econ.html). 

#### Inconsistent department naming

Fourth, the latest special cases arise from inconsistent department naming.

The Registrar's page for the [Ecology and Evolutionary Biology department ](http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/eeb.html) is on `eeb.html`, but the [class search](https://pisa.ucsc.edu/class_search/) reveals that those courses use the dapertment code `BIOE`.

Similarly, the  Registrar listing for  the [Molecular, Cell, and Developmental Biology department ](http://registrar.ucsc.edu/catalog/programs-courses/course-descriptions/mcdb.html) is on `mcdb.html`, but the courses use the department code `BIOL`.

[Two more conditionals](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/db_core.py#L198-L206) address this issue.

## Finding course mentions

A course mention occurs when a redditor names one or more courses in a Reddit post or comment.

I pulled the list of department codes from the source of the [class search](https://pisa.ucsc.edu/class_search/) page, in the element`<select id=""subject"">`.  Unfortunately this list includes defuct and renamed departments. For example, the Arabic department (ARAB) is gone and Environmental Toxicology (ETOX) is now [Microbiology and Environmental Toxicology (METX)](http://www.metx.ucsc.edu/). All the presently avaliable departments appear in the regular expression [`_pattern_depts`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/mention_parse.py#L10-L16).


### Mention types

This bot can see these three types of mention, all case-insensitive. Recall that a *course number* is actually a string because it may contain one optional letter at the end.

1. **Normal mention:** department code, optional space, and course number.
For example, ""CMPS 12B"" and ""econ105"" are normal mentions.
	* Specified by regex [`_pattern_mention_normal`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/mention_parse.py#L21-L22).
1. **Multi-mention:** shorthand for multiple courses in the same department with different course numbers.
For example, ""Math 21, 23b, and 100"" is a multi-mention containing Math 21, Math 23, and Math 100.
	* Not specified by a single regex. The function [`_parse_multi_mention()`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/mention_parse.py#L58-L92) splits a multi-mention into normal mentions.
1. **Letter-list mention:** shorthand for multiple courses in the same department, where the course number has the same numeric part but different letters.  
For example, ""CE 129A/B/C"" is a letter-list mention containing CE 129A, CE 129B, and CE 129C.
	* Specified by regex [`_pattern_mention_letter_list`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/mention_parse.py#L18-L19).
	* Function [`_parse_letter_list()`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/mention_parse.py#L36-L55) splits a letter-list mention into normal mentions.
	* You can have a letter-list mention **inside** a multi-mention! aFor example, the string ""CS 8a, 15, and 163x/y/z"" CS 8A, CS 15, CS 163X, CS 163Y, and CS 163Z.

Five regular expressions are combined to form the gigantic regular expression [`_pattern_final`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/mention_parse.py#L30-L33), which is used to search strings.

### Result

In the file `mention_search_posts.py`, the function [`find_mentions()`](https://github.com/pfroud/ucsc-class-info-bot/blob/4dae0bb220513ce29fb889410570b1397c3efbde/mention_search_posts.py#L111-L150) gets new posts from /r/UCSC then parses everything using [`mention_parse.py`](https://github.com/pfroud/ucsc-class-info-bot/blob/master/mention_parse.py).

If `find_mentions()` is called from [`reddit_bot.py`](https://github.com/pfroud/ucsc-class-info-bot/blob/master/reddit_bot.py), it returns a [`PostWithMentions`](https://github.com/pfroud/ucsc-class-info-bot/blob/master/mention_search_posts.py#L12-L20) instance to be immediately processed; if `mention_search_posts.py` is ran on its own from the Python console, the result is Pickled (serialized) and saved to disk. The `PostWithMentions`class is a container which holds the ID of a submission and a list of course mentions found in that submission.


## Posting comments

If [`post_comments.py`](https://github.com/pfroud/ucsc-class-info-bot/blob/master/post_comments.py) is ran on its own from the Python console, it loads mentions found from the last run of `mention_search_posts.py`. If the function `post_comments()` is called from `reddit_bot.py`, data about found mentions is passed directly as a parameter to the function. [Those function names are outdated]

If a post doesn't already have a a comment by /u/ucsc-class-info-bot, add one. If it does already have a comment, compare the mentions most recently found with the mentions that are already in the comment. If there are new ones, update the comment.


## Known bugs & future work

* After the Registrar fixed some HTML special cases, my scraping script is broken.
* In the comments posted by this bot, classes are sorted by department name (I think) instead of by order mentioned.
* I might make the bot see mentions of some department names instead of department codes, e.g. ""chemistry 103"" instead of ""chem 103"".
",1,0.85,0.85,,,,,,0,1,,,
76764432,MDEwOlJlcG9zaXRvcnk3Njc2NDQzMg==,UCSC-Grades,bradleybernard/UCSC-Grades,0,bradleybernard,https://github.com/bradleybernard/UCSC-Grades,UCSC grade scraper/notifier via text message,0,2016-12-18 05:24:29+00:00,2017-05-08 01:17:28+00:00,2016-12-18 05:48:19+00:00,,13,1,1,Python,1,1,1,1,0,0,0,0,0,0,apache-2.0,1,0,0,public,0,0,1,master,1,['bradleybernard'],,"### UCSC Grade Notifier
A simple Python 3 script to login to MyUCSC student portal and check if new grades are posted or old grades are updated. SMS notifications delievered through the Twilio SMS API. Input your own Twilio API keys in the twilio.yaml file. 

# Usage:
```$ python3 grades.py CruzID GoldPass PhoneNum TermID [--no-texts]```

## Required:
CruzID    = UCSC Cruz ID for Ecommons <br/>
GoldPass  = UCSC Gold Password for Ecommons <br/>
PhoneNum  = Mobile phone number to receive texts <br/>
TermID    = Term ID to check grades for (2152, 2154, 2156, ...) <br/>

## Optional: 
--no-texts: Turn off text message notifications 

## Run checker on an interval
- Create linux virtual machine (such as Ubuntu 16.04LTS on DigitalOcean)
- Install requirements for the Python script
- Create bash script (in same directory) to run script and then wait an interval (30 mins)
- Install supervisord
- Create new supervisor watcher

## Bash script (commands.sh):
```
#!/bin/bash
python3.5 grades.py 'CruzId' 'Password' '+15555555555' 2170;
sleep 30m;
```

## Supervisor config (/etc/supervisor/conf.d/utility.conf):
```
[program:utility]
directory = /root/Python
command = bash commands.sh
stdout_logfile = /var/log/grades-stdout.log
stdout_logfile_maxbytes = 1GB
stdout_logfile_backups = 1
stderr_logfile = /var/log/grades-stderr.log
stderr_logfile_maxbytes = 1GB
stderr_logfile_backups = 1
autostart = true
autorestart = true
```
",1,0.75,0.75,,,,,,0,1,,,
32006570,MDEwOlJlcG9zaXRvcnkzMjAwNjU3MA==,PerceptionCheck,Dnguye1393/PerceptionCheck,0,Dnguye1393,https://github.com/Dnguye1393/PerceptionCheck,Final Project for CMPS 183 at UCSC,0,2015-03-11 08:30:38+00:00,2015-03-17 21:23:36+00:00,2015-03-17 21:23:36+00:00,,1428,0,0,Python,1,1,1,1,0,0,0,0,0,0,other,1,0,0,public,0,0,0,master,1,['Dnguye1393'],,"# README #

### About  ###

This project is to allow People who want to play Table top Role playing games to find each other. I have provided multiple ways for GM's and players to find each other and talk. I have made a Games page. This is to allow Gm's to post their games and allow players to request to join. When the players request to join, It is up to the GM to add them. They can check the player's profile, which every user can edit their own, accept the request, or ignore the request. Any person can Edit the wikia for the game, as world building should be done by all players. Possible additional functionality is to restrict the wiki to only people with permission.

The user, from the main page, can also access the forum. Which has topics from them to choose from. Overall it is for discussion or for players or Gm to throw out ideas.

The Inbox and message system is a more private form of the forum. It is to allow the GM and Players to message each other, privately. This is for Players/Gm's to bring up concerns, as personal questions , or discuss the campaign.

 Website: https://dnguye1393.pythonanywhere.com/perceptioncheck/default/user/login?_next=/perceptioncheck/default/index

### Setup ###

Clone this repository as one of your web2py applications.

### For more information ###

Talk to luca@ucsc.edu
",1,0.77,0.77,,,,,,0,1,,,
249840881,MDEwOlJlcG9zaXRvcnkyNDk4NDA4ODE=,UCSC,SamWeisiger/UCSC,0,SamWeisiger,https://github.com/SamWeisiger/UCSC,,0,2020-03-24 23:29:42+00:00,2020-03-24 23:29:42+00:00,2020-03-24 23:29:43+00:00,,0,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,[],,,-1,0.81,0.81,,,,,,0,0,,,
631457210,R_kgDOJaNFug,CSE30,zhiqi-chen/CSE30,0,zhiqi-chen,https://github.com/zhiqi-chen/CSE30,,0,2023-04-23 04:37:23+00:00,2023-04-23 04:40:09+00:00,2023-04-23 04:40:05+00:00,,813,0,0,Jupyter Notebook,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['zhiqi-chen'],,"# CSE30
University of California - Santa Cruz\
Fall 2020
",1,0.8,0.8,,,,,,0,1,,,
177983287,MDEwOlJlcG9zaXRvcnkxNzc5ODMyODc=,academic-research,wsn333/academic-research,0,wsn333,https://github.com/wsn333/academic-research,Collection related to academic research,0,2019-03-27 11:50:07+00:00,2023-04-03 11:40:27+00:00,2020-05-16 05:11:00+00:00,,135,7,7,,1,1,1,1,0,0,6,0,0,0,,1,0,0,public,6,0,7,master,1,"['MQQM', 'ztygalaxy', 'songzblink', 'claudiania', 'limingLiu-708']",1,"<p align=""center"">
	<img width=""150"" height=""150"" src=""icon.png"" alt=""logo"">
</p>

# Academic Research

Academic research is an important lesson for postgraduates. In addition to the paper reading, understanding what are these productive researchers doing is a matter of great importance. To learn these academic hot spots and research trends, browsing homepages of these researchers can yet be regarded as a straightforward way. Here are some homepages that we have ever collected, mostly about ""Sensor"" and ""Communication"" since it is exactly what we are concerning about. Surely, the fields that productive researchers are working on is much more than these two aspects. They all achieve excellent jobs and are my role models. This repository is only used for recording, collecting and sharing. We are very gratified if it can help you learn more academic information. One things shall be noted is that *classification is not completely appropriate, only for reference when searching*. If you accept the practical significance of this repository, we hope to build it with you. 

By the way, a suitable format is required when you push a PR.

## Homepage 
- Yunhao Liu, Tsinghua, http://www.cse.ust.hk/~liu/, https://www.egr.msu.edu/news/2018/08/15/yunhao-liu,  http://www.tsinghua.edu.cn/publish/soft/3641/2011/20110608151912005129309/20110608151912005129309_.html      
-> *Network*; *RFID*; *Localization*; *Distributed Systems*

- Yuan He, Tsinghua, http://www.greenorbs.org/people/heyuan/index.html  
-> *Network*; *IoT*; *Mobile Computing*

- Jiliang Wang, Tsinghua, http://tns.thss.tsinghua.edu.cn/~jiliang/  
-> *Network*; *IoT*; *Visible Light*

- Zheng Yang, Tsinghua, http://tns.thss.tsinghua.edu.cn/~yangzheng/   
->*Sensing*; *Localization*; *Smart City*

- Kun Qian, Tsinghua, http://www.kunqian.info/  
->*Sensing*; *Tracking*; *WiFi*

- Xufei Mao, Tsinghua, http://mypages.iit.edu/~xmao3/, https://www.tsinghua.edu.cn/publish/soft/3641/2014/20140115110331781785449/20140115110331781785449_.html    
-> *Span Wireless Ad Hoc Networks*; *Wireless Sensor Networks*; *Pervasive Computing*; *Mobile Cloud Computing*; *Game Theory*; *Security*  

- Lingkun Li, MSU, http://www.cse.msu.edu/~lilingk1/      
->*Internet of Things (IoTs)*； *Mobile Computing*; *Network Systems* 





- Mo Li, NTU, https://www.ntu.edu.sg/home/limo/   
-> *Networked and Distributed Sensing*, *IoT*, *Urban Computing*   

- Jun Luo, NTU, http://www.ntu.edu.sg/home/junluo/index.htm  
->*Visible Light*; *Crowdsourcing*; *Network*

- Hsin-Mu (Michael) Tsai, NTU, https://www.csie.ntu.edu.tw/~hsinmu/wiki/   
->*Visible Light*; *Vehicle*



- Kate Lin, NCTU, https://people.cs.nctu.edu.tw/~katelin/   
->*Visible Light*; *MIMO Network*



- DartNets Lab, Dartmouth, http://dartnets.cs.dartmouth.edu/

- Tianxing Li, Dartmouth, https://www.cs.dartmouth.edu/~ltx/   
->*Wireless Communication*; *Sensing*

- Zhao Tian, Dartmouth, https://www.cs.dartmouth.edu/~tianzhao/    
->*Visible Light*; *WiFi Sensing*



- Chen Qian, UCSC, https://users.soe.ucsc.edu/~qian/  
->*Computer Networking*; *IoT*; *Edge Computing*



- Tian He, UMN, https://www-users.cs.umn.edu/~tianhe/index.html, http://tianhe.cs.umn.edu/index.html   
->*Cross-technology Communication*; *Visible Light*; *IoT*; *Distributed Systems*



- Yanchao Zhang, ASU, https://web.asu.edu/cnsg/zhang   
->*Security and Privacy*; *Wireless Network*; *Crowdsourcing*; *IoT*;




- Shan Lin, Stony Brook, http://www.ece.sunysb.edu/~slin/   
->*Wireless Network Protocols*; *Intelligent Sensing*; *Distributed Computing*; *Robotics*



- Cong Wang, ODU, http://www.lions.odu.edu/~c1wang/index.html   
->*Mobile Computing*; *Security and Privacy*; *Combinatorial Optimization*; *Energy-efficiency*





- Xinbing Wang, SJTU, http://www.cs.sjtu.edu.cn/~wang-xb/index.html  
->*Sensor Network*; *Crowdsensing*; *Cognitive Radio Networks*

- Fan Wu, SJTU, http://www.cs.sjtu.edu.cn/~fwu/    
->*Wireless Networking*, *Economic Incentives for Cooperation*; *Peer-to-Peer Computing*

- Yi-Chao Chen, SJTU, http://www.cs.sjtu.edu.cn/~yichao/pmwiki/pmwiki.php?n=Main.HomePage    
-> *Wireless Networking*, *Network Measurement and Analytics*, *Mobile Computing*



- Wei Wang, HUST, http://ei.hust.edu.cn/professor/wangwei/  
->*Wireless Network*; *Mobile Computing*; *IoT*; *UAV*

- Xiaoqiang Ma, HUST, http://ei.hust.edu.cn/teacher/xqma/     
->*Wireless Networks*; *Mobile Computing*; *Edge Computing*  



- Cong Liu, UTDALLAS, http://www.utdallas.edu/~cong/
- Zheng Dong, UTDALLAS, https://www.utdallas.edu/~zheng/



- Abdallah Khreishah, https://web.njit.edu/~abdallah/index.html   
->*Visible light*; *Network*; *Security* 



- Lin Yang, HKUST, https://yanglin-jason.github.io/about/



- Yu Luo, https://sites.google.com/site/yuluosite/home



- Xiangyang Li, USTC, http://staff.ustc.edu.cn/~xiangyangli/  
->*Wireless Networking/Mobile Computing/RFID*; *Privacy and Security*; *Cyber-Physical Systems and IoT*; *Social Computing*; *Interdisciplinary Research*

- Weiming Zhang, USTC, http://staff.ustc.edu.cn/~zhangwm/index.html    
->*Information Hiding*; *Multimedia Security*; *Privacy-preserving Data Searching and Analysis* 



- Mohammad Alizadeh, MIT, http://people.csail.mit.edu/alizadeh/index.html   

- Dinesh Bharadia, UCSD, http://web.eng.ucsd.edu/~dineshb/   

- Xinyu Zhang, UCSD, http://xyzhang.ucsd.edu/   
->*Millimeter Wave*, *Distributed MIMO*, *Visible Light*

- Chi Zhang, UCSD, http://dword1511.info/me/   
->*Visible Light*, *IoT*, *Low-power System*, *Mobile Sensing*



- Zhangyu Guan, Buffalo, http://www.acsu.buffalo.edu/~guan/   
->*Wireless Network*



- Tommaso Melodia, NEU, http://www.ece.neu.edu/wineslab/tmelodia.php  
->*Wireless Network*, *Ultrasonic Intra-body Networks*, *Underwater Networks*

- Nan Cen, NEU, http://www.ece.neu.edu/wineslab/Nan_Cen.php  
->*Visible Light*, *IoT*, *WSN*


- Zhice Yang, ShanghaiTech, https://www.yangzhice.com/  
->*Mobile Security*, *Wireless Network*, *Ubiquitous Computing* 





- Shiwen Mao, AUBURN, http://www.eng.auburn.edu/~szm0001/   
->*Wireless Networks and Multimedia Communications*; *5G*; *Indoor Localization and RF Sensing*; *Smart Grid*

- Bernhard Rinner, http://www.bernhardrinner.com/  

- Gerhard Petrus HANCKE, CityU, https://scholars.cityu.edu.hk/en/persons/gerhard-petrus-hancke(9e59c8eb-ba32-4075-97f7-e44e82367742).html   
->*Information and System Security*; *RFID, Smart Cards and IoT*; *Sensing and Control Systems*; *Embedded and Mobile Platforms* 

- Anna Maria Vegni, http://www.comlab.uniroma3.it/vegni.htm, https://scholar.google.com/citations?user=VwczFYsAAAAJ&hl=zh-CN    
->*Visible Light*; *IoT*; *Network Protocol*

- Nathalie Mitton, http://researchers.lille.inria.fr/~mitton/home.html, https://scholar.google.fr/citations?user=3U2F9TsAAAAJ&hl=fr     
->*IoT*; *WSN, RFID*; *Communication*; *RFID Middleware*



- Qian Wang, WHU, http://nisplab.whu.edu.cn/people.html   
->*AI Security*; *Data Storage*; *Search and Computation Outsourcing Security and Privacy*; *Wireless Systems Security*; *Big Data Security and Privacy*; *Applied Cryptography*


- MIT Computer Science & Artificial Intelligence Lab, https://www.csail.mit.edu  

- Ness B. Shroff, http://newslab.ece.ohio-state.edu/home/index.html  
-> *Span Communication (both wireless and wireline) Network*, *Social Network*, *Cyberphysical Networks*


- Yu Wang, UNC Charlotte, https://sites.google.com/view/wang-yu/home    
->*Wireless Networking*; *Smart Sensing*; *Mobile Computing*



- Stanford HAI, 以人为本实验室, https://hai.stanford.edu/    
-> *AI*, *Medicine*, *Education*, *Human Impact*



- Yongxin Tong, http://sites.nlsde.buaa.edu.cn/~yxtong/      
->*Crowdsourcing*; *Spatio-temporal Data Processing and Analysis*; *Uncertain Data Mining and Management* 


- Disney Research, https://la.disneyresearch.com/  

- Stefan Schmid, Disney Research, https://la.disneyresearch.com/people/stefan-schmid/    
->*Visible Light*; *Human Computer Interaction*


- Carla-Fabiana Chiasserini, Polytechnic of Turin Department of Electronics and Telecommunications, https://www.telematica.polito.it/member/carla-fabiana-chiasserini/     


- Falko Dressler, Paderborn University, https://www.ccs-labs.org/~dressler/    


- Xiaofeng Gao, Shanghai JiaoTong University, http://www.cs.sjtu.edu.cn/~gao-xf/    
-> *Data Engineering*; *Database Management*; *Wireless Network*; *Optimization Algorithms* 








## Download
- Video Grabber, https://www.videograbber.net/
- Free Online YouTube Downloader, https://en.savefrom.net/
- IEEE Xplore Digital Library, https://ieeexplore.ieee.org/Xplore/home.jsp
- ACM Digital Library, https://dl.acm.org/
- SCI-Hub, http://tool.yovisun.com/scihub/
- LabXing, https://www.labxing.com/paper
- Jiumo Search, https://www.jiumodiary.com/
- 猫咪论文, https://lunwen.im/
- Iconfont-阿里巴巴矢量图标库, https://www.iconfont.cn/
- 大木虫学术导航-谷歌学术、SCI-HUB镜像, http://www.4243.net/
- 龙猫学术, http://www.6453.net/
- Mockaroo-数据集生成, https://mockaroo.com/
- SemanticScholar-论文、源代码查询, https://www.semanticscholar.org/
- 论文向后查找, https://www.researchgate.net/
- 论文PPT下载, [https://slideplayer.com](https://slideplayer.com/)




## Tips
- How to read a paper, https://www.zhihu.com/question/304334959/answer/553782865?utm_source=wechat_session&utm_medium=social&utm_oi=1002998754877722624
- How to develop a good research topic, https://www.zhihu.com/question/23647187/answer/581714116?utm_source=wechat_session&utm_medium=social&utm_oi=1002998754877722624
- How to implement an algorithm from a scientific paper, http://codecapsule.com/2012/01/18/how-to-implement-a-paper/
- How to do the research with nobody's taught, https://www.zhihu.com/question/318640569/answer/647983796?utm_source=wechat_session&utm_medium=social&utm_oi=1002998754877722624



## Tools
- Convert table in Excel to LaTeX code, https://ctan.org/pkg/excel2latex, https://blog.csdn.net/piaopiaopiaopiaopiao/article/details/17427199  
- Convert equation screenshot to LaTeX code, https://zhuanlan.zhihu.com/p/61485193?utm_source=wechat_session&utm_medium=social&utm_oi=1002998754877722624 
- Convert image of equations into text, https://zhuanlan.zhihu.com/p/48077774?utm_source=wechat_session&utm_medium=social&utm_oi=1002998754877722624 



## Tutorials
- Digital photograph tutorial, https://www.cambridgeincolour.com/tutorials.htm
- Basic working principle of LCD panel, http://qxwujoey.tripod.com/lcd.htm



## Contributors
[![0](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/images/0)](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/links/0)
[![1](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/images/1)](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/links/1)
[![2](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/images/2)](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/links/2)
[![3](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/images/3)](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/links/3)
[![4](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/images/4)](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/links/4)
[![5](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/images/5)](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/links/5)
[![6](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/images/6)](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/links/6)
[![7](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/images/7)](https://sourcerer.io/fame/ztygalaxy/wsn333/academic-research/links/7)
",0,0.66,0.66,,,,,,0,1,,,
262178116,MDEwOlJlcG9zaXRvcnkyNjIxNzgxMTY=,IDL,kabir-kapur/IDL,0,kabir-kapur,https://github.com/kabir-kapur/IDL,UCSC Income Dynamics Lab,0,2020-05-07 23:15:24+00:00,2021-02-12 01:31:28+00:00,2021-02-12 01:31:25+00:00,,377966,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['kabir-kapur'],,"DESCRIPTION
-------------
This program makes web requests to the Federal Election Commission's campaign donor database and saves them in .csv format. Currently, it is configured to Schedule A contributions, but feel free to contact me if you'd like to discuss modifying it to make different requests. 

SETUP
-------------
Ensure that you have the following requirements installed:
 - Python3 and all of its native libraries
 - Requests for Python3

Get started:
 - Receive an API key from the FEC's API Key Signup Page. (https://api.data.gov/signup/)
 - Set the environment variable, 'FECAPIKEY'=your key in the desired directory using the following Bash command:
	
	export FECAPIKEY=""myapikey""

 - Ensure that you input your key as a string, as the program requires keys inputted as strings to parse correctly.
 - Modify min_date and committee_id variables according to your desired search terms. 
 - Run!  ",1,0.63,0.63,,,,,,0,1,,,
631451259,R_kgDOJaMuew,CSE13E,zhiqi-chen/CSE13E,0,zhiqi-chen,https://github.com/zhiqi-chen/CSE13E,,0,2023-04-23 04:02:49+00:00,2023-05-29 05:54:50+00:00,2023-04-23 04:28:05+00:00,,6833,1,1,C,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,main,1,['zhiqi-chen'],,"# CSE13E
University of California - Santa Cruz\
Spring 2021
",1,0.76,0.76,,,,,,0,1,,,
59755866,MDEwOlJlcG9zaXRvcnk1OTc1NTg2Ng==,RED-ML,BGIRED/RED-ML,0,BGIRED,https://github.com/BGIRED/RED-ML,RNA editing detection based on machine learning,0,2016-05-26 14:14:47+00:00,2025-02-18 08:05:45+00:00,2018-02-07 05:38:56+00:00,,14586,9,9,C,0,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,9,master,1,"['BGIHENG', 'leojlee']",1,"# RED-ML: RNA editing detection based on machine learning

## Description

RED-ML is a software tool to do genome-wide RNA editing dectection (RED) based on RNA-seq data. All source codes and executables are located in the ""bin"" directory. The tool can be run on a Linux platform and the main program is red_ML.pl.

## Parameters

    --rnabam       [STR] the sorted BAM file obtained from RNA-seq to detect RNA editing sites.
    --reference    [STR] the fasta file containing the reference genome, e.g., hg19.fa.
    --dbsnp        [STR] the SNP database file, e.g., dbSNP138.
    --simpleRepeat [STR] genome-wide simple repeat annotation, should be in BED format.
    --alu          [STR] genome-wide Alu repeat annotation, should be in BED format.
    --snplist      [STR] a tab-delimited file listing known SNPs, with the first two columns being chromosome and position of each SNP [optional].
    --outdir       [STR] the directory of output.
    --p            [NUM] the detection threshold, a number between 0 and 1 [default 0.5];
    --help         [STR] show this help information!

## Examples

We have provided a simple example to test the installation of RED-ML. Under the ""example"" directory, run:

   	perl ../bin/red_ML.pl --rnabam example.rna.bam --reference /usr/hg19.fa --dbsnp example.dbsnp.vcf --simpleRepeat example.simpleRepeat.bed --alu example.alu.bed --outdir ./test/

It should finish running in ~2 minutes with three output files (RNA_editing.sites.txt, variation.sites.feature.txt and mut.txt.gz). Here is another example of using RED-ML:

   	perl red_ML.pl --rnabam in.bam --reference hg19.fa --dbsnp dbsnp138.vcf --simpleRepeat hg19_simpleRepeat.reg.bed --alu hg19.alu.bed --snplist snp.list --outdir outdir

## Requirements

RED-ML requires the following data files at the time of public release:
- The reference genome (hg19), downloaded from: http://hgdownload.soe.ucsc.edu/goldenPath/hg19/chromosomes.
- dbSNP138, downloaded from: http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database.
- simpleRepeat, downloaded from: http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database, and then do:
```
	awk '{print $2""\t""$3""\t""$4}' simpleRepeat.txt > simpleRepeat.bed
	bedtools merge -i simpleRepeat.bed > simpleRepeat.merge.bed
```   
- Alu, downloaded from: http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database, and do:
```
	grep Alu rmsk.txt | awk '{print $6""\t""$7""\t""$8}' > hg19.alu.bed
```
We have also provided the simpleRepeat and Alu files under the ""database"" directory for the user's convenience.

## Optional Steps

### SNP calling

If you have matching DNA-seq data or aligned DNA BAM files, we strongly recommend to take advantage of them. You could call SNPs by GATK (haplotypecaller) or SOAPsnp and modify the format of the resulting file (such as vcf) to fit the format required by --snplist.
    
### Alignment

Although RED-ML can accept BAM files produced by different alignment tools, the current version has been optimized for BWA and TopHat2 during the construction of our ML model, and we find that the choice of alignment tools and the associated parameters could have a large impact on RED. To help users with proper alignment strategies, we recommend the following steps:

1. When reads are aligned by BWA (preferred), one should first build a new reference which combines reference genome (hg19) and exonic sequences surrounding all known splice junctions, and the detail method is the same as in Ramaswami et al. (Nature Methods 2012) and Wang et al. (GigaScience 2016). SAMtools can be used to sort the alignment file and remove the PCR duplicate reads.

2. When TopHat2 is chosen, the cleaned reads can be mapped to the reference genome (hg19) directly with default parameters. Picard should be used to sort the alignment and to remove duplicate reads induced by PCR, and base quality score recalibration can be carried out by GATK.
    
## Outputs

When the program finishes running, three files will be created in the output directory. RNA_editing.sites.txt lists all detected RNA editing sites that pass the detection threshold p; variation.sites.feature.txt lists all variant sites with associated feature values; mut.txt.gz contains all variant sites with pileup information.

## Notice

The input bam should be sorted (indexed), you could use samtools to create index.

    samtools index in.bam

## License

GNU General Public License version 3.0 (GPLv3), with no restrictions for commercial usage.

## Citation

The paper has been accepted by journal of GigaScience.
",0,0.48,0.48,,,,,,0,4,,,
778421149,R_kgDOLmXDnQ,asst_prof_salaries,mongiardino/asst_prof_salaries,0,mongiardino,https://github.com/mongiardino/asst_prof_salaries,Dataset of average assistant professor salaries across US public R1 institutions,0,2024-03-27 17:32:55+00:00,2024-03-31 14:32:54+00:00,2024-04-09 21:05:17+00:00,,1321,7,7,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,7,main,1,['mongiardino'],,"# asst_prof_salaries
Dataset of average assistant professor salaries across US public R1 institutions

## Data
* Average assistant professor salaries for year 2022: [AAUP Faculty Compensation Survey](https://data.aaup.org/ft-faculty-salaries/)
* County-level cost of living for family of 2 adults and 2 children for year 2023: [Economic Policy Institute Family Budget Map](https://www.epi.org/resources/budget/budget-map/). It should be noted that these estimates are enought to ""attain a modest yet adequate standard of living"", which translates to no savings and no mortgage (costs incorporate rent).
* List of public R1 institution in the US: [Wikipedia](https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States)


Assignment of county (or unincorporated city) to universities was done by hand and based on the city where the main campus is located according to Wikipedia. For large cities spanning multiple counties, cost of living is the average between all of them.

Expected salary represents the value predicted by the linear regression.

The major caveat of this analysis is that the composition of assistant professors across universities can vary depending on the presence/absence/strength of different departments. For example, places with strong departments of engineering, business, economy, or other discipline with a high market value, would be biased upwards, while institutions without some/all of those departments would be biased downwards. Such large differences in composition could mean that the average salary is a poor predictor of the salary that would be offered to any incoming faculty.

## Table
A csv file with the data can be downloaded from this repository.

The visualization of it below was generated using https://csvtomd.com/#/

| University                                  | Average assistant professor salary | County/city                                      | Cost of living | Expected salary | Salary difference (actual - expected) | Relative difference (actual / expected) * 100 | Above average | Above 1:1 |
| ------------------------------------------- | ---------------------------------- | ------------------------------------------------ | -------------- | --------------- | ------------------------------------- | --------------------------------------- | ------------- | --------- |
| University of Texas at Dallas               | 125.6                              | Dallas County-Collin County                      | 103.6          | 93.4            | 32.2                                  | 134.5                                   | yes           | yes       |
| University of Texas at Austin               | 119.6                              | Travis County                                    | 108.6          | 95.3            | 24.3                                  | 125.5                                   | yes           | yes       |
| Georgia Institute of Technology             | 119.7                              | Fulton County                                    | 112.8          | 96.9            | 22.8                                  | 123.5                                   | yes           | yes       |
| University of California, Los Angeles       | 123.6                              | Los Angeles County                               | 130.1          | 103.5           | 20.1                                  | 119.4                                   | yes           | no        |
| Texas A&M University                        | 104.1                              | Brazos County                                    | 88.8           | 87.8            | 16.3                                  | 118.6                                   | yes           | yes       |
| University of California, Berkeley          | 129.5                              | Alameda County                                   | 155.9          | 113.4           | 16.1                                  | 114.2                                   | yes           | no        |
| University of California, San Diego         | 123.5                              | San Diego County                                 | 140.6          | 107.5           | 16.0                                  | 114.8                                   | yes           | no        |
| University of Maryland, College Park        | 111.2                              | Prince George's County                           | 110            | 95.9            | 15.3                                  | 116.0                                   | yes           | yes       |
| University of California, Davis             | 119.2                              | Yolo County                                      | 131.1          | 103.9           | 15.3                                  | 114.7                                   | yes           | no        |
| New Jersey Institute of Technology          | 109.2                              | Essex County                                     | 106.1          | 94.4            | 14.8                                  | 115.7                                   | yes           | yes       |
| University of Illinois Urbana-Champaign     | 107.1                              | Champaign County                                 | 102.4          | 93.0            | 14.1                                  | 115.2                                   | yes           | yes       |
| University of Houston                       | 103.5                              | Harris County                                    | 93.3           | 89.5            | 14.0                                  | 115.7                                   | yes           | yes       |
| University of Georgia                       | 102.3                              | Clarke County                                    | 91.3           | 88.7            | 13.6                                  | 115.3                                   | yes           | yes       |
| Indiana University Bloomington              | 104.7                              | Monroe County                                    | 99.5           | 91.8            | 12.9                                  | 114.0                                   | yes           | yes       |
| University of California, Riverside         | 111                                | Riverside County                                 | 117            | 98.5            | 12.5                                  | 112.7                                   | yes           | no        |
| University of Maryland, Baltimore County    | 103.5                              | Baltimore County                                 | 102.4          | 93.0            | 10.5                                  | 111.3                                   | yes           | yes       |
| Purdue University                           | 100.5                              | Tippecanoe County                                | 94.9           | 90.1            | 10.4                                  | 111.6                                   | yes           | yes       |
| Pennsylvania State University               | 106.9                              | Centre County                                    | 112.4          | 96.8            | 10.1                                  | 110.5                                   | yes           | no        |
| University of Michigan                      | 106.9                              | Washtenaw County                                 | 113.9          | 97.3            | 9.6                                   | 109.8                                   | yes           | no        |
| Clemson University                          | 97.4                               | Pickens County-Anderson County                   | 90             | 88.2            | 9.2                                   | 110.4                                   | yes           | yes       |
| Ohio State University                       | 100.5                              | Franklin County-Delaware County-Fairfield County | 99.7           | 91.9            | 8.6                                   | 109.3                                   | yes           | yes       |
| Virginia Tech                               | 100.9                              | Montgomery County                                | 100.9          | 92.4            | 8.5                                   | 109.2                                   | yes           | yes       |
| University of Florida                       | 97.2                               | Alachua County                                   | 94.1           | 89.8            | 7.4                                   | 108.3                                   | yes           | yes       |
| University of Delaware                      | 103.2                              | New Castle County                                | 111            | 96.2            | 7.0                                   | 107.2                                   | yes           | no        |
| University of Virginia                      | 104.4                              | Charlottesville city                             | 114.9          | 97.7            | 6.7                                   | 106.8                                   | yes           | no        |
| University of Texas at El Paso              | 91.1                               | El Paso County                                   | 80.7           | 84.7            | 6.4                                   | 107.6                                   | yes           | yes       |
| University of Wisconsin-Madison             | 105.8                              | Dane County                                      | 119.6          | 99.5            | 6.3                                   | 106.3                                   | yes           | no        |
| University of Arkansas                      | 93.6                               | Washington County                                | 87.7           | 87.3            | 6.3                                   | 107.2                                   | yes           | yes       |
| Mississippi State University                | 90.7                               | Oktibbeha County                                 | 80.3           | 84.5            | 6.2                                   | 107.3                                   | yes           | yes       |
| University of California, Santa Barbara     | 119.1                              | Santa Barbara County                             | 154.8          | 113.0           | 6.1                                   | 105.4                                   | yes           | no        |
| University of Illinois Chicago              | 101.6                              | Cook County-DuPage County                        | 112.4          | 96.8            | 4.8                                   | 105.0                                   | yes           | no        |
| Florida State University                    | 94                                 | Leon County                                      | 92.8           | 89.3            | 4.7                                   | 105.3                                   | yes           | yes       |
| University of Tennessee                     | 93.3                               | Knox County                                      | 91.1           | 88.6            | 4.7                                   | 105.3                                   | yes           | yes       |
| University of California, Irvine            | 114.3                              | Orange County                                    | 147            | 110.0           | 4.3                                   | 103.9                                   | yes           | no        |
| University of Kansas                        | 91.9                               | Douglas County                                   | 88.5           | 87.6            | 4.3                                   | 104.9                                   | yes           | yes       |
| University of Utah                          | 97.1                               | Salt Lake County                                 | 102.9          | 93.1            | 4.0                                   | 104.2                                   | yes           | no        |
| University of Kentucky                      | 93                                 | Fayette County                                   | 92.5           | 89.2            | 3.8                                   | 104.3                                   | yes           | yes       |
| University of Colorado Boulder              | 109                                | Boulder County                                   | 134.8          | 105.3           | 3.7                                   | 103.5                                   | yes           | no        |
| University of Colorado Denver               | 104.4                              | Denver County                                    | 125.2          | 101.7           | 2.7                                   | 102.7                                   | yes           | no        |
| Texas Tech University                       | 90.4                               | Lubbock County                                   | 88.6           | 87.7            | 2.7                                   | 103.1                                   | yes           | yes       |
| Florida International University            | 96.5                               | Miami-Dade County                                | 106            | 94.3            | 2.2                                   | 102.3                                   | yes           | no        |
| Temple University                           | 94.5                               | Philadelphia County                              | 100.9          | 92.4            | 2.1                                   | 102.3                                   | yes           | no        |
| University of Nebraska-Lincoln              | 95.4                               | Lancaster County                                 | 104.1          | 93.6            | 1.8                                   | 101.9                                   | yes           | no        |
| University of Oregon                        | 98.1                               | Lane County                                      | 113.5          | 97.2            | 0.9                                   | 100.9                                   | yes           | no        |
| University of South Carolina                | 90.2                               | Richland County                                  | 94.1           | 89.8            | 0.4                                   | 100.5                                   | yes           | no        |
| University of North Carolina at Chapel Hill | 95.3                               | Orange County-Durham County                      | 108.6          | 95.3            | 0.0                                   | 100.0                                   | yes           | no        |
| Auburn University                           | 91.4                               | Lee County                                       | 98.6           | 91.5            | -0.1                                  | 99.9                                    | no            | no        |
| University of Massachusetts Amherst         | 98.9                               | Hampshire County                                 | 118.4          | 99.1            | -0.2                                  | 99.8                                    | no            | no        |
| Louisiana State University                  | 91                                 | East Baton Rouge Parish                          | 97.9           | 91.2            | -0.2                                  | 99.7                                    | no            | no        |
| University of Missouri                      | 89.7                               | Boone County                                     | 94.5           | 89.9            | -0.2                                  | 99.7                                    | no            | no        |
| Michigan State University                   | 90.7                               | Ingham County-Clinton County                     | 98             | 91.3            | -0.6                                  | 99.4                                    | no            | no        |
| Iowa State University                       | 92.1                               | Story County                                     | 102.4          | 93.0            | -0.9                                  | 99.1                                    | no            | no        |
| University of Texas at San Antonio          | 89                                 | Bexar County-Comal County-Medina County          | 94.5           | 89.9            | -0.9                                  | 99.0                                    | no            | no        |
| University of Washington                    | 104.4                              | King County                                      | 135.1          | 105.4           | -1.0                                  | 99.0                                    | no            | no        |
| University of South Florida                 | 92.1                               | Hillsborough County                              | 102.9          | 93.1            | -1.0                                  | 98.9                                    | no            | no        |
| University of Iowa                          | 93.9                               | Johnson County                                   | 108            | 95.1            | -1.2                                  | 98.7                                    | no            | no        |
| University of Oklahoma                      | 89.5                               | Cleveland County                                 | 97             | 90.9            | -1.4                                  | 98.5                                    | no            | no        |
| Oregon State University                     | 97.9                               | Benton County                                    | 120            | 99.7            | -1.8                                  | 98.2                                    | no            | no        |
| Binghamton University                       | 91                                 | Broome County                                    | 102.2          | 92.9            | -1.9                                  | 98.0                                    | no            | no        |
| Old Dominion University                     | 87.9                               | Norfolk city                                     | 94.6           | 90.0            | -2.1                                  | 97.7                                    | no            | no        |
| University of Minnesota                     | 97.4                               | Hennepin County                                  | 119.9          | 99.6            | -2.2                                  | 97.8                                    | no            | no        |
| University of New Hampshire                 | 93.2                               | Strafford County                                 | 109            | 95.5            | -2.3                                  | 97.6                                    | no            | no        |
| University of Arizona                       | 86.8                               | Pima County                                      | 93             | 89.4            | -2.6                                  | 97.1                                    | no            | no        |
| Ohio University                             | 88.2                               | Athens County                                    | 97.9           | 91.2            | -3.0                                  | 96.7                                    | no            | no        |
| Arizona State University                    | 92.1                               | Maricopa County                                  | 108.2          | 95.2            | -3.1                                  | 96.8                                    | no            | no        |
| Colorado School of Mines                    | 99.2                               | Jefferson County                                 | 128            | 102.7           | -3.5                                  | 96.6                                    | no            | no        |
| University of Pittsburgh                    | 88.1                               | Allegheny County                                 | 99.6           | 91.9            | -3.8                                  | 95.9                                    | no            | no        |
| University of Connecticut                   | 97.8                               | Tolland County                                   | 125.1          | 101.6           | -3.8                                  | 96.2                                    | no            | no        |
| University of Hawaii at Manoa               | 98.7                               | Honolulu County                                  | 128.3          | 102.8           | -4.1                                  | 96.0                                    | no            | no        |
| University of California, Santa Cruz        | 115.8                              | Santa Cruz County                                | 174.1          | 120.3           | -4.5                                  | 96.2                                    | no            | no        |
| University of Alabama at Birmingham         | 87.3                               | Jefferson County                                 | 100.7          | 92.3            | -5.0                                  | 94.6                                    | no            | no        |
| Oklahoma State University-Stillwater        | 83.1                               | Payne County                                     | 89.9           | 88.2            | -5.1                                  | 94.2                                    | no            | no        |
| Wayne State University                      | 84.2                               | Wayne County                                     | 93.1           | 89.4            | -5.2                                  | 94.2                                    | no            | no        |
| Kansas State University                     | 81.7                               | Riley County                                     | 90.1           | 88.3            | -6.6                                  | 92.6                                    | no            | no        |
| University of Central Florida               | 86.8                               | Orange County                                    | 103.5          | 93.4            | -6.6                                  | 93.0                                    | no            | no        |
| University of Alabama in Huntsville         | 84.8                               | Madison County                                   | 98.6           | 91.5            | -6.7                                  | 92.7                                    | no            | no        |
| University of Alabama                       | 82.9                               | Tuscaloosa County                                | 94             | 89.7            | -6.8                                  | 92.4                                    | no            | no        |
| Washington State University                 | 81.1                               | Whitman County                                   | 91.6           | 88.8            | -7.7                                  | 91.3                                    | no            | no        |
| University of New Mexico                    | 80.6                               | Bernalillo County                                | 92             | 89.0            | -8.4                                  | 90.6                                    | no            | no        |
| North Carolina State University             | 87.2                               | Wake County                                      | 109.8          | 95.8            | -8.6                                  | 91.0                                    | no            | no        |
| University of Memphis                       | 77.9                               | Shelby County                                    | 86             | 86.7            | -8.8                                  | 89.9                                    | no            | no        |
| Virginia Commonwealth University            | 82.7                               | Richmond city                                    | 99.7           | 91.9            | -9.2                                  | 90.0                                    | no            | no        |
| Rutgers University-New Brunswick            | 93.4                               | Middlesex County                                 | 127.8          | 102.7           | -9.3                                  | 91.0                                    | no            | no        |
| North Dakota State University               | 75.9                               | Cass County                                      | 82.1           | 85.2            | -9.3                                  | 89.1                                    | no            | no        |
| University of Cincinnati                    | 80.9                               | Hamilton County                                  | 95.3           | 90.2            | -9.3                                  | 89.6                                    | no            | no        |
| University of Louisville                    | 79.5                               | Jefferson County                                 | 92.5           | 89.2            | -9.7                                  | 89.1                                    | no            | no        |
| Utah State University                       | 79.6                               | Cache County                                     | 95.4           | 90.3            | -10.7                                 | 88.2                                    | no            | no        |
| University at Albany, SUNY                  | 88.5                               | Albany County                                    | 118.8          | 99.2            | -10.7                                 | 89.2                                    | no            | no        |
| University at Buffalo                       | 84.2                               | Erie County                                      | 107.7          | 95.0            | -10.8                                 | 88.7                                    | no            | no        |
| George Mason University                     | 97.6                               | Fairfax County                                   | 146.3          | 109.7           | -12.1                                 | 89.0                                    | no            | no        |
| Colorado State University                   | 87.3                               | Larimer County                                   | 124.1          | 101.2           | -13.9                                 | 86.2                                    | no            | no        |
| University of Wisconsin-Milwaukee           | 81.6                               | Milwaukee County                                 | 109.4          | 95.6            | -14.0                                 | 85.3                                    | no            | no        |
| Kent State University                       | 76.1                               | Portage County                                   | 97.7           | 91.2            | -15.1                                 | 83.5                                    | no            | no        |
| Stony Brook University                      | 100                                | Suffolk County                                   | 162.9          | 116.1           | -16.1                                 | 86.2                                    | no            | no        |
| Montana State University                    | 77.6                               | Gallatin County                                  | 104.3          | 93.7            | -16.1                                 | 82.8                                    | no            | no        |
| Graduate Center, CUNY                       | 98                                 | New York County                                  | 160.2          | 115.0           | -17.0                                 | 85.2                                    | no            | no        |
| University of Southern Mississippi          | 67.9                               | Forrest County                                   | 83.8           | 85.9            | -18.0                                 | 79.1                                    | no            | no        |
| University of Nevada, Reno                  | 77.8                               | Washoe County                                    | 112.3          | 96.7            | -18.9                                 | 80.4                                    | no            | no        |
| University of Maine                         | 71.7                               | Penobscot County                                 | 97.9           | 91.2            | -19.5                                 | 78.6                                    | no            | no        |
| University of Nevada, Las Vegas             | 74                                 | Clark County                                     | 104.8          | 93.9            | -19.9                                 | 78.8                                    | no            | no        |
| Georgia State University                    | 75.8                               | Fulton County                                    | 112.8          | 96.9            | -21.1                                 | 78.2                                    | no            | no        |
| University of Montana                       | 69.2                               | Missoula County                                  | 102.4          | 93.0            | -23.8                                 | 74.4                                    | no            | no        |
|                                             |

## Plot
Plotting was done in R using *ggplot* and *ggrepel*. Labels were added to 1/5th of dots, selected at random.

![plot](https://github.com/mongiardino/asst_prof_salaries/blob/main/salary_plot.jpg)
**Fig. 1:** Average assistant professor salary against county-level cost of living for a family of four. All values are annual and expressed in thousands of dollars. Both a 1:1 line and a linear regression (with confidence interval) are plotted and used to classify universities into three categories.


",0,0.64,0.64,,,,,,0,1,,,
842842785,R_kgDOMjzCoQ,UCSC_SE,DISHMI2003/UCSC_SE,0,DISHMI2003,https://github.com/DISHMI2003/UCSC_SE,,0,2024-08-15 07:59:24+00:00,2024-08-15 09:00:25+00:00,2024-08-15 09:00:22+00:00,,1,0,0,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,"['DISHMI2003', 'sara-4510']",,# UCSC_SE,0,0.73,0.73,,,,,,0,1,,,
779335550,R_kgDOLnO3fg,UCSC_hosting,benoitballester/UCSC_hosting,0,benoitballester,https://github.com/benoitballester/UCSC_hosting,,0,2024-03-29 15:41:37+00:00,2024-11-20 15:44:38+00:00,2024-11-20 15:44:34+00:00,,39480,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['benoitballester'],,"## Overview

The UCSC Genome Browser is a widely used tool for visualizing and sharing genomic data. Hosting data on the UCSC Genome Browser allows researchers to explore and analyze their data in the context of the reference genome, making it a valuable resource for the scientific community.

## Raw access

[https://github.com/benoitballester/UCSC_hosting/raw/main/snpFabian_allEE_JasparRemapFilter.bb](https://github.com/benoitballester/UCSC_hosting/raw/main/snpFabian_allEE_JasparRemapFilter.bb)

[https://github.com/benoitballester/UCSC_hosting/raw/main/tcga_snpFabian_allEE_JasparRemapFilter_hg38.bb](https://github.com/benoitballester/UCSC_hosting/raw/main/tcga_snpFabian_allEE_JasparRemapFilter_hg38.bb)

## Resources

- [UCSC Genome Browser User Guide](https://genome.ucsc.edu/goldenPath/help/hgTracksHelp.html): Comprehensive documentation on using the UCSC Genome Browser.
- [UCSC Genome Browser FAQ](https://genome.ucsc.edu/FAQ/FAQformat.html): Frequently asked questions about data formatting and hosting on the Genome Browser.

## Contributing

If you have suggestions for improvements or additional resources, please open an issue or submit a pull request.

## License

This repository and its contents are licensed under the [MIT License](LICENSE).
",0,0.73,0.73,,,,,,0,1,,,
802645714,R_kgDOL9dm0g,Calculated-College-Counselor,hellomynameisnotyourinfo/Calculated-College-Counselor,0,hellomynameisnotyourinfo,https://github.com/hellomynameisnotyourinfo/Calculated-College-Counselor,HackMHS,0,2024-05-18 21:39:18+00:00,2024-05-18 21:39:39+00:00,2024-05-18 21:39:37+00:00,,9,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['hellomynameisnotyourinfo'],,"# Calculated-College-Counselor
HackMHS
Want_Close_to_home = False
major = input(""What major would you like to pursue? "")
WarmStates = [""AZ"", ""FL"",""TX"",""CA"",""SC"",""HA"",""GE"",""Al"",""LA""]
Weather = input(""What weather would you like it to have? (Warm, Cold or Neither) "")
ColdStates = [""AK"",""ND"",""MN"",""ME"",""WY"",""MT"",""ID"",""VE"",""MI""]

maximum_tuition = input(""What is the maximum amount of tuition you are willing to pay? "")
maximum_tuition = float(maximum_tuition)
LiveCloseToHome = input(""Do only want to go to Univerisities close to New Jersey? "")
ClosetoNJ = [""NJ"",""NY"",""MA"",""PE"",""DE"",""MD"",""NH"",""RI"",""VA""]
LiveClosetoNJ = False
if LiveCloseToHome == ""Yes"" or LiveCloseToHome ==""yes"":
    LiveClosetoNJ = True

perfect = 0


if Weather == ""Warm"":
   Want_Warm = True
   Want_Cold = False
elif Weather == ""Cold"":
   Want_Cold = True
   Want_Warm = False
else:
   Want_Cold = False
   Want_Warm = False
colleges = []




class college:
   def __init__(self,name, tuition, acceptance_rate,state):
       global colleges
       self.name = name
       self.tuition = tuition
       self.acr = acceptance_rate
       self.st = state






   def get_name(self):
       return self.name
   def get_acr(self):
       return (100*self.acr)
   def get_tuition(self):
       return (self.tuition)
   def get_state(self):
       return self.st
   def too_expensive(self):
       global maximum_tuition
       if self.tuition > maximum_tuition:
           return True
def sell_college(name):
   global Justmajor, colleges, Want_Warm, Want_Cold, perfect


   for x in range (len(colleges)):
  
       if name == colleges[x]:
          
           if Want_Warm == True or Want_Cold == True:
               print ("""")
               if perfect == 0:
                   perfect += 1
                  
                   return name.get_name() + "" would be a great college for you, it fits your temperature conditions. It is in your price range, and it is one of the top universities in your respective field, I would recommend this for you ""
               else:
                   return name.get_name() + "" would also be a great college for you, it fits your temperature conditions. It is in your tuition range, and it is one of the top universities in your respective field, I would also reccomend this one for you ""
           else:
               if perfect == 0:
                   perfect += 1
                  
                   return name.get_name() + "" would be a great college for you. It is in your price range, and it is one of the top universities in your respective field, I would recommend this for you ""
               else:
                   return name.get_name() + "" would also be a great college for you, It is in your tuition range, and it is one of the top universities in your respective field, I would also reccomend this one for you ""
def sell_expensive():
   global Justmajor
   return ""If price and toughness is not a problem, I would recomend "" + (Justmajor[0].get_name()) + "", because it is the highest ranked college for this field. It costs  ""  
Princeton_University = college(""Princeton University"", 59710, 0.06, ""NJ"")
Massachusetts_Institute_of_Technology = college(""Massachusetts Institute of Technology"",60156,0.04,""MA"")
Harvard_University = college(""Harvard University"",59076,0.03, ""MA"")
Standford_University = college(""Standford University"",62484,0.04,""CA"")
Yale_University = college(""Yale University"", 64700,0.05, ""CT"")
University_of_Pennsylvania = college(""University of Pennsylvania"", 66104,0.07,""PA"")
California_Institute_of_Technology = college(""California Institute of Technology"",63255,0.03,""CA"")
Duke_University = college(""Duke University"",66172,0.06,""NC"")
Brown_University = college(""Brown University"",68230,0.05,""RI"")
Johns_Hopkins_University = college(""Johns Hopkins University"",63340,0.07,""MD"")
Northwestern_University = college(""Northwestern University"",65997,0.07,""IL"")
Columbia_University = college(""Columbia University"",65524,0.04,""NY"")
Cornell_University = college(""Cornell University"",66014,0.07,""NY"")
University_of_Chicago = college(""University of Chicago"",65619,0.05,""IL"")
University_of_California_Berkeley = college(""University of California, Berkeley"",48465,0.11,""CA"")
University_of_California_Los_Angeles = college(""University of California, Los Angeles"",46326,0.09,""CA"")
Rice_University = college(""Rice_University"",58128,0.09,""TX"")
Dartmouth_College = college(""Dartmouth College"",65511,0.06,""NH"")
Vanderbilt_University = college(""Vanderbilt University"",63946,0.07,""TN"")
University_of_Notre_Dame = college(""University of Notre Dame"",62693,0.13,""IN"")
University_of_Michigan_Ann_Arbor = college(""University of Michigan Ann Arbor"",57273,0.18,""MI"")
Georgetown_University = college(""Georgetown University"",65082,0.12,""DC"")
University_of_North_Carolina_Chapel_Hill = college(""University of North Carolina Chapel Hill"",39338,0.17,""NC"")
Carnegie_Mellon_University = college(""Carnegie Mellon University"",63829,0.11,""PA"")
Emory_University = college(""Emory University"",60774,0.11,""GA"")
University_of_Virginia = college(""University of Virginia"",58950,0.19,""VA"")
Washington_University_in_St_Louis = college(""Washington University in St Louis"",62982,0.11,""MO"")
University_of_California_San_Diego = college(""University of California San Diego"",48630,0.24,""CA"")
University_of_California_Davis = college(""University of California"",48360,0.24,""CA"")
University_of_Florida = college(""University of Florida"",28658,0.23,""FL"")
University_of_Southern_California = college(""University of Southern California"",68237,0.12,""CA"")
University_of_Texas_at_Austin = college(""University of Texas at Austin"",41070,0.31,""TX"")
Georgia_Institute_of_Technology = college(""Georgia Institute of Technology"",32876,0.17,""GA"")
University_of_California_Irvine = college(""University of California Irvine"",47759,0.21,""CA"")
New_York_University = college(""New York University"",60438,0.12,""NY"")
University_of_California_Santa_Barbara = college(""University of California Santa Barabara"",45658,0.26,""CA"")
University_of_Illinois_Urbana_Champagne = college(""University of Illinois Urbana Champange"",36068,0.45,""IL"")
University_of_Wisconsin_Madison = college(""University of Wisconsin Madison"",40603,0.49,""WI"")
Boston_College = college(""Boston College"",67680,0.17,""MA"")
Rutgers_University = college(""Rutgers University"",36001,0.66,""NJ"")
Tufts_University = college(""Tufts University"",67844,0.10,""MA"")
University_of_Washington = college(""University of Washington"",41997,0.48,""WA"")
Boston_University = college(""Boston University"",65168,0.14,""MA"")
Ohio_State_University = college(""Ohio State University"",36722,0.53,""OH"")
Purdue_University = college(""Purdue University"",28794,0.53,""IN"")
University_of_Maryland_College_Park = college(""University of Maryland College Park"",40306,0.44,""MD"")
Lehigh_University = college(""Lehigh University"",62180,0.37,""PA"")
Texas_AandM_University = college(""Texas A&M University"",40607,0.63,""TX"")
University_of_Georgia = college(""University of Georgia"",30220,0.43,""GA"")
University_of_Rochester = college(""University of Rochester"",64384,0.39,""NY"")
Virginia_Tech = college(""Virginia Tech"",36090,0.57,""VA"")
Wake_Forest_University = college(""Wake Forest University"",64758,0.21,""NC"")
Case_Western_Reserve_University = college(""Case Western Reserve University"",62234,0.27,""OH"")
Florida_State_University = college(""Florida State University"",21683,0.25,""FL"")
Northeastern_University = college(""Northeastern University"",63141,0.07,""MA"")
University_of_Minnesota_Twin_Cities = college(""University of Minnesota Twin Cities"",36402,0.75,""MN"")
William_and_Mary = college(""William & Mary"",48841,0.33,""VA"")
Stony_Brook_University = college(""Stony Brook University"",30350,0.49,""NY"")
University_of_Connecticut = college(""University of Connecticut"",43034,0.55,""CT"")
Brandeis_University = college(""Brandeis University"",62722,0.39,""MA"")
Michigan_State_University = college(""Michigan State University"",41958,0.88,""MI"")
North_Carolina_State_University = college(""North Carolina State University"", 31976,0.47,""NC"")
Pennsylvania_State_University = college(""Pennsylvania State University"",38651,0.55,""PA"")
Rensselear_Polytechnic_Institute = college(""Rensselear Polytechnic Institute"",61884,0.65,""NY"")
Santa_Clara_University = college(""Santa Clara University"",59241,0.52,""CA"")
University_of_California_Merced = college(""University of California Merced"",43777,0.89,""CA"")
George_Washington_University = college(""George Washington University"",64990,0.49,""DC"")
Syracuse_University = college(""Syracuse University"",63061,0.52,""NY"")
University_of_Massachusettes_Amherst = college(""University of Massachusettes Amherst"",39293,0.64,""MA"")
University_of_Miami = college(""University of Miami"",59926,0.19,""FL"")
University_of_Pittsburgh = college(""University of Pittsburgh"",39890,0.49,""PA"")
Villanova_University = college(""Villanova_University"",64906,0.23,""PA"")
Binghamton_University_SUNY = college(""Binghamton University SUNY"",28203,0.42,""NY"")
Indiana_University_Bloomington = college(""Indiana University Bloomington"",40482,0.82,""IN"")
Tulane_University = college(""Tulane University"",65538,0.11,""LA"")
Colorado_School_of_Mines = college(""Colorado School of Mines"",42120,0.58,""CO"")
Pepperdine_University = college(""Pepperdine University"",66742,0.49,""CA"")
Stevens_Institute_of_Technology = college(""Stevens Institute of Technology"",600952,0.46,""NJ"")
University_at_Buffalo_SUNY = college(""University_at_Buffalo_SUNY"",30571,0.68,""NY"")
University_of_California_Riverside = college(""University of California Riverside"",46266,0.69,""CA"")
University_of_Delaware = college(""University of Delaware"",39720,0.72,""DE"")
Rutgers_University_Newark = college(""Rutgers_University_Newark"",35348,0.74,""NJ"")
University_of_California_Santa_Cruz = college(""University of California Santa Cruz"",47862,0.47,""CA"")
University_of_Illinois_Chicago = college(""University of Illinois Chicago"",32833,0.79,""IL"")
Worcester_Polytechnic_Institute = college(""Worcester Polytechnic Institute"",58870,0.57,""MA"")
Clemson_University = college(""Clemson University"",39502,0.43,""SC"")
Marquette_University = college(""Marquette University"",48700,0.87,""WI"")
New_Jersey_Institute_of_Technology = college(""New Jersey Institute of Technology"",61567,0.54,""NY"")
Fordham_University = college(""Fordham University"",61567,0.54,""NY"")
Southern_Methodist_University = college(""Southern Methodist University"",64460,0.52,""TX"")
Temple_University = college(""Temple University"",35956,0.80,""PA"")
University_of_South_Florida = college(""University of South Florida"",17324,0.44,""FL"")
Auburn_University = college(""Auburn University"",33944,0.44,""AL"")
Baylor_University = college(""Baylor University"",54844,0.46,""TX"")
Gonzaga_University = college(""Gonzaga University"",53500,0.70,""WA"")
Loyola_Marymount_University = college(""Loyola Marymount University"",58489,0.41,""CA"")
University_of_Iowa = college(""University of Iowa"",32927,0.86,""IA"")
Drexel_University =  college(""Drexel University"",60663,0.80,""PA"")
Illinois_Institute_of_Technology = college(""Illinois Institute of Technology"",51763,0.61,""IL"")
Rochester_Institute_of_Technology = college(""Rochester Institute of Technology"",57016,0.67,""NY"")




#The following would be the majors
Business_and_Management = [University_of_Chicago,Standford_University,Northwestern_University,Harvard_University,University_of_Pennsylvania,Dartmouth_College,Columbia_University, Massachusetts_Institute_of_Technology,Cornell_University,University_of_Michigan_Ann_Arbor,University_of_California_Berkeley,Yale_University,University_of_Virginia,Duke_University,University_of_North_Carolina_Chapel_Hill,University_of_California_Los_Angeles, Carnegie_Mellon_University, University_of_Texas_at_Austin,Indiana_University_Bloomington,New_York_University]
Nursing = [Johns_Hopkins_University,Duke_University,University_of_Washington,Rutgers_University_Newark,Emory_University,University_of_Michigan_Ann_Arbor,University_of_Minnesota_Twin_Cities,University_of_Pennsylvania,University_of_Pittsburgh,University_of_Illinois_Chicago,New_York_University,Vanderbilt_University,Case_Western_Reserve_University,University_of_North_Carolina_Chapel_Hill,Ohio_State_University,University_of_Southern_California,University_of_Iowa,Purdue_University,Boston_College]
Psychology = [Standford_University,University_of_California_Berkeley,Harvard_University,University_of_California_Los_Angeles,University_of_Michigan_Ann_Arbor,Princeton_University,University_of_Illinois_Urbana_Champagne,Yale_University,Cornell_University,Northwestern_University,University_of_Wisconsin_Madison,Columbia_University,Duke_University,Johns_Hopkins_University,University_of_California_Davis,University_of_Chicago,University_of_California_San_Diego,University_of_Minnesota_Twin_Cities,University_of_North_Carolina_Chapel_Hill,University_of_Pennsylvania,Vanderbilt_University,Brown_University]
Biology = [Harvard_University,Massachusetts_Institute_of_Technology,Standford_University,University_of_California_Berkeley,University_of_California_San_Diego,University_of_Washington,Johns_Hopkins_University,University_of_California_Los_Angeles,Cornell_University,Columbia_University,University_of_Pennsylvania,Yale_University,University_of_Michigan_Ann_Arbor, New_York_University,University_of_Washington,Duke_University,California_Institute_of_Technology,University_of_North_Carolina_Chapel_Hill,University_of_California_Davis]
Engineering = [Massachusetts_Institute_of_Technology, Standford_University,University_of_California_Berkeley,Purdue_University,Carnegie_Mellon_University,Georgia_Institute_of_Technology,California_Institute_of_Technology,University_of_Michigan_Ann_Arbor,University_of_Texas_at_Austin,Texas_AandM_University,University_of_Illinois_Urbana_Champagne,University_of_California_San_Diego,Cornell_University,Johns_Hopkins_University,University_of_Southern_California,University_of_California_Los_Angeles,Columbia_University,Northwestern_University,University_of_Maryland_College_Park,University_of_Pennsylvania,Duke_University]
Education = [Columbia_University,University_of_Wisconsin_Madison,University_of_California_Los_Angeles,University_of_Michigan_Ann_Arbor,Northwestern_University,Vanderbilt_University,University_of_Pennsylvania,Harvard_University,Johns_Hopkins_University,New_York_University,Standford_University,University_of_Texas_at_Austin,University_of_Virginia,Florida_State_University,University_of_California_Berkeley,University_of_Florida,University_of_California_Irvine,Michigan_State_University,Ohio_State_University]
Communications = [Massachusetts_Institute_of_Technology,Standford_University,University_of_Pennsylvania,Brown_University,Northwestern_University,Cornell_University,University_of_California_Berkeley,University_of_California_Los_Angeles,Vanderbilt_University,University_of_Michigan_Ann_Arbor,University_of_North_Carolina_Chapel_Hill,Emory_University,University_of_Virginia,University_of_California_San_Diego,University_of_Southern_California,University_of_Texas_at_Austin,New_York_University,University_of_California_Santa_Barbara,University_of_Illinois_Urbana_Champagne,University_of_Wisconsin_Madison]
Finance_and_Accounting = [Harvard_University,Massachusetts_Institute_of_Technology,Standford_University,University_of_Chicago,University_of_Pennsylvania,University_of_California_Berkeley,New_York_University,Columbia_University,Yale_University,University_of_California_Los_Angeles,Princeton_University,Northwestern_University,Cornell_University,University_of_Michigan_Ann_Arbor,University_of_Texas_at_Austin,Duke_University,University_of_Illinois_Urbana_Champagne,University_of_Southern_California,University_of_Washington,Ohio_State_University]
Criminal_Justice = [Rutgers_University,Boston_University,Georgia_Institute_of_Technology,Florida_State_University,Northeastern_University,Michigan_State_University,Pennsylvania_State_University,George_Washington_University,Villanova_University,Indiana_University_Bloomington,Rutgers_University_Newark,University_of_Illinois_Chicago,Temple_University,Drexel_University,Rochester_Institute_of_Technology]
Anthropology_and_Sociology = [Princeton_University,Massachusetts_Institute_of_Technology,Harvard_University,Standford_University,University_of_Pennsylvania,Duke_University,Brown_University,Johns_Hopkins_University,Columbia_University,Cornell_University,University_of_Chicago,University_of_California_Berkeley,University_of_California_Los_Angeles,Rice_University,Dartmouth_College,Vanderbilt_University,University_of_Notre_Dame,University_of_Michigan_Ann_Arbor]
Computer_Science = [Carnegie_Mellon_University,Massachusetts_Institute_of_Technology,Standford_University,University_of_California_Berkeley,University_of_Illinois_Urbana_Champagne,Cornell_University,Georgia_Institute_of_Technology,University_of_Texas_at_Austin,University_of_Washington,Princeton_University,University_of_Michigan_Ann_Arbor,Columbia_University,California_Institute_of_Technology,University_of_California_Los_Angeles,University_of_California_San_Diego,University_of_Wisconsin_Madison,Harvard_University,University_of_Maryland_College_Park,Purdue_University,Pennsylvania_State_University]
English = [University_of_California_Berkeley,Yale_University,Harvard_University,Princeton_University,Standford_University,University_of_Chicago,University_of_Pennsylvania,Columbia_University,Cornell_University,University_of_Michigan_Ann_Arbor,University_of_California_Los_Angeles,University_of_Virginia,Brown_University,Duke_University,Johns_Hopkins_University,Northwestern_University,Rutgers_University,University_of_Texas_at_Austin,New_York_University,University_of_California_Irvine]
Economics = [Harvard_University,Massachusetts_Institute_of_Technology,Standford_University,Princeton_University,University_of_California_Berkeley,University_of_Chicago,Yale_University,Northwestern_University,Columbia_University,University_of_Pennsylvania,New_York_University,University_of_California_Los_Angeles,University_of_Michigan_Ann_Arbor,California_Institute_of_Technology,Cornell_University,University_of_California_San_Diego,University_of_Wisconsin_Madison,Duke_University,University_of_Minnesota_Twin_Cities,Brown_University]
Political_Science = [Standford_University,Harvard_University,Princeton_University,University_of_California_Berkeley,University_of_Michigan_Ann_Arbor,Yale_University,Massachusetts_Institute_of_Technology,Columbia_University,University_of_California_San_Diego,Duke_University,University_of_Chicago,University_of_California_Los_Angeles,University_of_North_Carolina_Chapel_Hill,Washington_University_in_St_Louis,Cornell_University,New_York_University,Ohio_State_University,University_of_Wisconsin_Madison,Emory_University,Northwestern_University,University_of_Pennsylvania]
History = [Princeton_University,Massachusetts_Institute_of_Technology,Harvard_University,Standford_University,Yale_University,University_of_Pennsylvania,California_Institute_of_Technology,Duke_University,Brown_University,Johns_Hopkins_University,Northwestern_University,Columbia_University,Cornell_University,University_of_Chicago,University_of_California_Berkeley,University_of_California_Los_Angeles,Rice_University,Dartmouth_College,Vanderbilt_University,University_of_Notre_Dame]
Kinesiology_and_Physical_Therapy = [Washington_University_in_St_Louis,University_of_Delaware,University_of_Iowa,Emory_University,Northwestern_University,Duke_University,University_of_Southern_California,Ohio_State_University,University_of_Pittsburgh,Boston_University,University_of_Florida,University_of_Miami,University_of_North_Carolina_Chapel_Hill,Marquette_University,Columbia_University,University_of_Wisconsin_Madison,George_Washington_University,University_of_Washington,Northeastern_University,Rutgers_University_Newark,University_of_Minnesota_Twin_Cities]
Health_Professions = [Rice_University,University_of_Michigan_Ann_Arbor,University_of_North_Carolina_Chapel_Hill,Emory_University,University_of_Virginia,University_of_Florida,University_of_Illinois_Urbana_Champagne,University_of_Wisconsin_Madison,Rutgers_University,Ohio_State_University,Purdue_University,University_of_Maryland_College_Park,Texas_AandM_University,University_of_Georgia,Wake_Forest_University,Florida_State_University,University_of_Minnesota_Twin_Cities,William_and_Mary,University_of_Connecticut]
Art = [University_of_California_Los_Angeles,Yale_University,Carnegie_Mellon_University,University_of_Michigan_Ann_Arbor,Columbia_University,University_of_California_San_Diego,University_of_California_Berkeley,University_of_California_Davis,University_of_Wisconsin_Madison,Rochester_Institute_of_Technology,Rutgers_University,University_of_Iowa,University_of_Texas_at_Austin,Washington_University_in_St_Louis,Boston_University,Indiana_University_Bloomington,Ohio_State_University,Standford_University,Temple_University,University_of_Georgia,University_of_Washington]
Math = [Standford_University,Massachusetts_Institute_of_Technology,Princeton_University,Harvard_University,University_of_California_Berkeley,Columbia_University,University_of_California_Los_Angeles,University_of_Washington,New_York_University,Texas_AandM_University,University_of_Chicago,University_of_Michigan_Ann_Arbor,University_of_Wisconsin_Madison,University_of_Texas_at_Austin,University_of_Minnesota_Twin_Cities,Purdue_University,Brown_University,University_of_Pennsylvania,Pennsylvania_State_University,Duke_University]
Environmental_Science = [Harvard_University,Standford_University,University_of_California_Berkeley,Yale_University,University_of_Minnesota_Twin_Cities,University_of_Washington,University_of_California_Davis,University_of_California_Santa_Barbara,University_of_Florida,Princeton_University,Duke_University,Cornell_University,University_of_California_Los_Angeles,University_of_Wisconsin_Madison,Massachusetts_Institute_of_Technology,Columbia_University,University_of_Michigan_Ann_Arbor,Michigan_State_University,University_of_Maryland_College_Park,University_of_California_Irvine]
Foreign_Languages = [University_of_Notre_Dame,Columbia_University,Vanderbilt_University,University_of_Virginia,University_of_California_Los_Angeles,Duke_University,Georgetown_University,University_of_Southern_California,University_of_California_Berkeley,University_of_Illinois_Urbana_Champagne,University_of_Wisconsin_Madison,Brown_University,Tufts_University,Southern_Methodist_University,University_of_Florida,New_York_University,University_of_Chicago,University_of_California_Davis,University_of_Pittsburgh,University_of_Washington]
Design = [Duke_University,University_of_California_Los_Angeles,University_of_Notre_Dame,University_of_Michigan_Ann_Arbor,Carnegie_Mellon_University,Washington_University_in_St_Louis,University_of_California_Davis,University_of_Florida,University_of_Southern_California,University_of_Texas_at_Austin,University_of_California_Irvine,New_York_University,University_of_Illinois_Urbana_Champagne,University_of_Wisconsin_Madison,Rutgers_University,University_of_Washington,Boston_University,Ohio_State_University,Purdue_University,Lehigh_University,Virginia_Tech]
Trades_and_Personal_Services = [New_York_University,Johns_Hopkins_University,Cornell_University,University_of_Texas_at_Austin,University_of_Wisconsin_Madison,Florida_State_University,Purdue_University,Ohio_State_University,Loyola_Marymount_University,University_of_Minnesota_Twin_Cities,Pennsylvania_State_University,North_Carolina_State_University,Worcester_Polytechnic_Institute,Syracuse_University,Baylor_University]
International_Relations = [Standford_University,Yale_University,University_of_Pennsylvania,Brown_University,Johns_Hopkins_University,Northwestern_University,University_of_Chicago,Georgetown_University,Carnegie_Mellon_University,Emory_University,University_of_Virginia,Washington_University_in_St_Louis,University_of_California_Davis,University_of_California_San_Diego,University_of_Southern_California,Georgia_Institute_of_Technology,Northwestern_University,Tufts_University,Boston_University,Ohio_State_University,Lehigh_University]
Chemistry = [California_Institute_of_Technology,Massachusetts_Institute_of_Technology, University_of_California_Berkeley,Harvard_University,Standford_University,Northwestern_University,Princeton_University,University_of_Chicago,University_of_Illinois_Urbana_Champagne,Columbia_University,Cornell_University, Yale_University,University_of_Michigan_Ann_Arbor,University_of_Wisconsin_Madison,University_of_California_Los_Angeles,University_of_North_Carolina_Chapel_Hill,Pennsylvania_State_University,University_of_Texas_at_Austin,Georgia_Institute_of_Technology,Georgia_Institute_of_Technology,Johns_Hopkins_University]
Agricultural_Sciences = [University_of_Massachusettes_Amherst,Cornell_University,University_of_California_Davis,University_of_Florida,Harvard_University,University_of_Illinois_Urbana_Champagne,Michigan_State_University,Purdue_University,University_of_Wisconsin_Madison,Ohio_State_University,Texas_AandM_University,University_of_Minnesota_Twin_Cities,Rutgers_University,North_Carolina_State_University,Pennsylvania_State_University,University_of_Georgia,University_of_Maryland_College_Park,Tufts_University,Johns_Hopkins_University,University_of_California_Berkeley]
Technology = [Carnegie_Mellon_University,New_York_University,Georgetown_University,Georgia_Institute_of_Technology,Washington_University_in_St_Louis,George_Washington_University,Johns_Hopkins_University,University_of_Virginia,Southern_Methodist_University,Cornell_University,Columbia_University,Northeastern_University,University_of_Pittsburgh,Drexel_University,Stevens_Institute_of_Technology,University_of_Miami,Brandeis_University,University_of_Southern_California,Rochester_Institute_of_Technology, University_of_Texas_at_Austin]
Performing_Arts = [New_York_University,Rochester_Institute_of_Technology,Harvard_University,Yale_University,Columbia_University,Standford_University,Northwestern_University,University_of_Southern_California,University_of_Michigan_Ann_Arbor,University_of_California_Los_Angeles,University_of_California_Berkeley,Boston_University,Indiana_University_Bloomington,Massachusetts_Institute_of_Technology,University_of_California_San_Diego,University_of_Chicago,University_of_Texas_at_Austin,Cornell_University,Princeton_University, University_of_Illinois_Urbana_Champagne]
Food_and_Nutrition = [University_of_North_Carolina_Chapel_Hill,University_of_California_Davis,New_York_University,University_of_Illinois_Urbana_Champagne,Ohio_State_University,Virginia_Tech,Case_Western_Reserve_University,University_of_Minnesota_Twin_Cities,Syracuse_University,University_of_Delaware,Baylor_University,Rochester_Institute_of_Technology, Rutgers_University, Princeton_University, Stevens_Institute_of_Technology,New_Jersey_Institute_of_Technology]
Religious_Studies = [Princeton_University,Harvard_University,Standford_University,Yale_University,University_of_Pennsylvania,Duke_University,Brown_University,Northwestern_University,Columbia_University,Cornell_University,University_of_Chicago,University_of_California_Los_Angeles,Rice_University,Dartmouth_College,Vanderbilt_University,University_of_Michigan_Ann_Arbor,Georgetown_University,University_of_North_Carolina_Chapel_Hill,Emory_University,University_of_Virginia]
Film_and_Photography = [Yale_University,New_York_University,University_of_Southern_California,University_of_Pennsylvania,Dartmouth_College,University_of_Chicago,Washington_University_in_St_Louis,Tufts_University,University_of_California_Los_Angeles,University_of_Michigan_Ann_Arbor,Emory_University,Vanderbilt_University,University_of_Miami,University_of_California_Irvine,University_of_California_Berkeley,Loyola_Marymount_University,Boston_University,Boston_College,University_of_California_Santa_Barbara,Tulane_University]
Music = [Princeton_University,Massachusetts_Institute_of_Technology,Harvard_University,Standford_University,Yale_University,University_of_Pennsylvania,Duke_University,Brown_University,Johns_Hopkins_University,Northwestern_University,Columbia_University,Cornell_University,University_of_Chicago,University_of_California_Berkeley,University_of_California_Los_Angeles,Rice_University,Dartmouth_College,Vanderbilt_University]
Physics = [Massachusetts_Institute_of_Technology,Standford_University,California_Institute_of_Technology,Harvard_University,Princeton_University,University_of_California_Berkeley,Cornell_University,University_of_Chicago,Columbia_University,University_of_California_Santa_Barbara,University_of_Illinois_Urbana_Champagne,Yale_University,Johns_Hopkins_University,University_of_Michigan_Ann_Arbor,University_of_Pennsylvania,University_of_Texas_at_Austin,University_of_California_Los_Angeles,University_of_Maryland_College_Park,University_of_Washington]
Philosophy = [Columbia_University,Standford_University,Yale_University,Harvard_University,University_of_California_Los_Angeles,University_of_Chicago,Princeton_University,University_of_Pennsylvania,Rice_University,Duke_University,University_of_Southern_California,Dartmouth_College,Georgetown_University,Northwestern_University,University_of_Michigan_Ann_Arbor,Vanderbilt_University,New_York_University,Carnegie_Mellon_University,Brown_University]
Architecture = [Cornell_University,Rice_University,Washington_University_in_St_Louis,University_of_California_Berkeley,Yale_University,University_of_Southern_California,Virginia_Tech,Carnegie_Mellon_University,University_of_Virginia,Massachusetts_Institute_of_Technology,Texas_AandM_University,University_of_Florida,University_of_Notre_Dame,Georgia_Institute_of_Technology,Clemson_University,University_of_Miami,University_of_Michigan_Ann_Arbor,Columbia_University,University_of_Illinois_Urbana_Champagne]
Legal_Studies = [Vanderbilt_University,Northwestern_University,University_of_Miami,Tulane_University,University_of_Massachusettes_Amherst,University_of_California_Berkeley,University_of_Washington,University_of_Wisconsin_Madison,University_of_Pittsburgh,Temple_University,Purdue_University,Michigan_State_University,Pennsylvania_State_University,University_of_California_Santa_Cruz,Drexel_University,University_of_Massachusettes_Amherst,Auburn_University,New_Jersey_Institute_of_Technology,Texas_AandM_University,University_of_Georgia]
Pharmacy = [University_of_Michigan_Ann_Arbor,Northeastern_University,Ohio_State_University,University_of_California_Irvine,University_of_Pittsburgh,University_of_Georgia,Purdue_University,University_of_Connecticut,Clemson_University,University_of_Washington,University_of_Maryland_College_Park,Temple_University]




if major == ""Business"" or major == ""Managment"" or major == ""Business and Management"":
   listlenth = len(Business_and_Management)
   for x in range (listlenth):
       colleges.append(Business_and_Management[x])
elif major == ""Nursing"":
   listlenth = len(Nursing)
   for x in range (listlenth):
       colleges.append(Nursing[x])
elif major == ""Psychology"":
   listlenth = len(Psychology)
   for x in range (listlenth):
       colleges.append(Psychology[x])   
elif major == ""Biology"":
   listlenth = len(Biology)
   for x in range (listlenth):
       colleges.append(Biology[x])
elif major == ""Engineering"":
   listlenth = len(Engineering)
   for x in range (listlenth):
       colleges.append(Engineering[x])
elif major == ""Education"":
   listlenth = len(Education)
   for x in range (listlenth):
       colleges.append(Education[x])
elif major == ""Communications"":
   listlenth = len(Communications)
   for x in range (listlenth):
       colleges.append(Communications[x])
elif major == ""Finace"" or major == ""Accounting"" or major == ""Finance and Accounting"":
   listlenth = len(Finance_and_Accounting)
   for x in range (listlenth):
       colleges.append(Finance_and_Accounting[x])
elif major == ""Criminal Justice"":
   listlenth = len(Criminal_Justice)
   for x in range (listlenth):
       colleges.append(Criminal_Justice[x])
elif major == ""Anthropology"" or major == ""Sociology"" or major == ""Anthropology and Sociology"":
   listlenth = len(Anthropology_and_Sociology)
   for x in range (listlenth):
       colleges.append(Anthropology_and_Sociology[x])
elif major == ""Computer Science"":
   listlenth = len(Computer_Science)
   for x in range (listlenth):
       colleges.append(Computer_Science[x])
elif major == ""English"":
   listlenth = len(English)
   for x in range (listlenth):
       colleges.append(English[x])
elif major == ""Economics"":
   listlenth = len(Economics)
   for x in range (listlenth):
       colleges.append(Economics[x])
elif major == ""Political Science"":
   listlenth = len(Political_Science)
   for x in range (listlenth):
       colleges.append(Political_Science[x])
elif major == ""History"":
   listlenth = len(History)
   for x in range (listlenth):
       colleges.append(History[x])
elif major == ""Kinesiology"" or major == ""Physical Therapy"" or major == ""Kinesiology and Physical Therapy"":
   listlenth = len(Kinesiology_and_Physical_Therapy)
   for x in range (listlenth):
       colleges.append(Kinesiology_and_Physical_Therapy[x])
elif major == ""Health Professions"":
   listlenth = len(Health_Professions)
   for x in range (listlenth):
       colleges.append(Health_Professions[x])
elif major == ""Art"":
   listlenth = len(Art)
   for x in range (listlenth):
       colleges.append(Art[x])
elif major == ""Math"":
   listlenth = len(Math)
   for x in range (listlenth):
       colleges.append(Math[x])
elif major == ""Enviormental Science"":
   listlenth = len(Environmental_Science)
   for x in range (listlenth):
       colleges.append(Environmental_Science[x])
elif major == ""Foreign Languages"":
   listlenth = len(Foreign_Languages)
   for x in range (listlenth):
       colleges.append(Foreign_Languages[x])
elif major == ""Design"":
   listlenth = len(Design)
   for x in range (listlenth):
       colleges.append(Design[x])
elif major == ""Trades"" or major == ""Personal Services"" or major == ""Trades and Personal Services"":
   listlenth = len(Trades_and_Personal_Services)
   for x in range (listlenth):
       colleges.append(Trades_and_Personal_Services[x])
elif major == ""International Relations"":
   listlenth = len(International_Relations)
   for x in range (listlenth):
       colleges.append(International_Relations[x])
elif major == ""Chemistry"":
   listlenth = len(Chemistry)
   for x in range (listlenth):
       colleges.append(Chemistry[x])
elif major == ""Agricultural Sciences"":
   listlenth = len(Agricultural_Sciences)
   for x in range (listlenth):
       colleges.append(Agricultural_Sciences[x])
elif major == ""Technology"":
   listlenth = len(Technology)
   for x in range (listlenth):
       colleges.append(Technology[x])
elif major == ""Performing Arts"":
   listlenth = len(Performing_Arts)
   for x in range (listlenth):
       colleges.append(Performing_Arts[x])
elif major == ""Food"" or major == ""Nutrition"" or major == ""Food and Nutrition"":
   listlenth = len(Food_and_Nutrition)
   for x in range (listlenth):
       colleges.append(Food_and_Nutrition[x])
elif major == ""Religious Studies"":
   listlenth = len(Religious_Studies)
   for x in range (listlenth):
       colleges.append(Religious_Studies[x])
elif major == ""Film"" or major == ""Photography"" or major == ""Film and Photography"":
   listlenth = len(Film_and_Photography)
   for x in range (listlenth):
       colleges.append(Film_and_Photography[x])
elif major == ""Music"":
   listlenth = len(Music)
   for x in range (listlenth):
       colleges.append(Music[x])
elif major == ""Physics"":
   listlenth = len(Physics)
   for x in range (listlenth):
       colleges.append(Physics[x])
elif major == ""Philosophy"":
   listlenth = len(Philosophy)
   for x in range (listlenth):
       colleges.append(Philosophy[x])
elif major == ""Architecture"":
   listlenth = len(Architecture)
   for x in range (listlenth):
       colleges.append(Architecture[x])
elif major == ""Legal Studies"":
   listlenth = len(Legal_Studies)
   for x in range (listlenth):
       colleges.append(Legal_Studies[x])
elif major == ""Pharmacy"":
   listlenth = len(Pharmacy)
   for x in range (listlenth):
       colleges.append(Pharmacy[x])
for x in range (len(colleges)):
   print (colleges[x].get_name())
Justmajor = colleges
print ("""")
removed = []
for x in range (len(colleges)):
   if (colleges[x]).too_expensive():
       removed.append(colleges[x])
for x in range (len(removed)):
   colleges.remove(removed[x])
for x in range (len(colleges)):
   print (colleges[x].get_name())
print("""")
priceremoved = removed
removed = []
safe = []
for x in range (len(colleges)):
   if Want_Cold == True:
       for y in range (9):
           if colleges[x].get_state() == WarmStates[y]:
               removed.append(colleges[x])
   elif Want_Warm == True:
       for y in range (9):
           if colleges[x].get_state() == ColdStates[y]:
               removed.append(colleges[x])
for x in range (len(removed)):
   colleges.remove(removed[x])
if LiveClosetoNJ:
    for x in range (len(colleges)):
        for y in range (9):
            if colleges[x] == ClosetoNJ[y]:
                safe.append(colleges[x])
    colleges = []
    colleges = safe



for x in range (len(colleges)):
   print (sell_college(colleges[x]))
print( sell_expensive() )
print(Justmajor[0].get_tuition() + "" dollars a year, and it also has"")
print(""dollars a year, and it also has a "")
print(Justmajor[0].get_acr())
print(""percent acceptance rate so it might be hard to get into it"")
",0,0.65,0.65,,,,,,0,1,,,
940783241,R_kgDOOBM2iQ,NLP-220,Shriya613/NLP-220,0,Shriya613,https://github.com/Shriya613/NLP-220,,0,2025-02-28 19:32:58+00:00,2025-03-03 22:19:38+00:00,2025-03-03 22:19:35+00:00,,30815,0,0,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['Shriya613'],,"# NLP-220 Data Science and Machine Learning Fundamentals

The course is taught by **Dr. Jalal Mahmud** , Adjunct Professor (NLP MS Program) at __The University of California - Santa Cruz__.

The teaching assistant for this course is **Nilay Patel**, III yr Ph.D student.

I have taken this course in the Fall Quarter-2024.

This course covers a broad set of tools and core skills required for working with Natural Language Data. It covers core traditional machine learning methods such as classification methods using Naive Bayes, SVMs, Linear regression and Support Vector Regression, as well as the use of Pytorch and other programming frameworks commonly used in the field. It also includes methods used for collecting, merging, cleaning, structuring and analyzing the properties of large and heterogeneous datasets of natural language, in order to address questions and support applications relying on those data. It covers working with existing corpora as well as the challenges in collecting new corpora. 

",1,0.73,0.73,,,,,,0,1,,,
138581030,MDEwOlJlcG9zaXRvcnkxMzg1ODEwMzA=,hospital_management,DimuthuKasunWP/hospital_management,0,DimuthuKasunWP,https://github.com/DimuthuKasunWP/hospital_management,ucsc-dbms,0,2018-06-25 10:41:31+00:00,2019-11-21 16:21:51+00:00,2018-06-25 10:41:32+00:00,,0,1,1,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,master,1,['DimuthuKasunWP'],,"# hospital_management
ucsc-dbms
",0,0.76,0.76,,,,,,0,0,,,
419600287,R_kgDOGQKXnw,OneButton-mnm,Jnolen15/OneButton-mnm,0,Jnolen15,https://github.com/Jnolen15/OneButton-mnm,Made for CMPM170 UCSC Fall 2021,0,2021-10-21 06:02:01+00:00,2021-10-21 06:06:44+00:00,2021-10-21 06:06:41+00:00,,8834,0,0,TypeScript,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,0,main,1,['Jnolen15'],,"# OneButton-mnm
Made for CMPM170 UCSC Fall 2021

made with Crisp Game Lib https://github.com/abagames/crisp-game-lib
",1,0.87,0.87,,,,,,0,1,,,
39597038,MDEwOlJlcG9zaXRvcnkzOTU5NzAzOA==,hexmap-view,Stuartlab-UCSC/hexmap-view,0,Stuartlab-UCSC,https://github.com/Stuartlab-UCSC/hexmap-view,,0,2015-07-23 22:32:00+00:00,2021-11-05 01:12:22+00:00,2023-04-16 16:20:56+00:00,,69065,3,3,JavaScript,1,1,1,1,0,0,3,0,0,53,mit,1,0,0,public,3,53,3,master,1,"['terraswat', 'mrJeppard', 'ynewton']",1,"# UCSC hexmap-viewer

### Retrieve from the code repository.
```
git clone https://github.com/stuartlab-UCSC/hexmap-view
```
### Install a development environment
View the developer documentation by pointing your web browser to
```
file://HEXMAP/docs/dev/index.html
```
where HEXMAP is the full path to the above install.
",1,0.8,0.8,,,,,,0,7,,,
54582918,MDEwOlJlcG9zaXRvcnk1NDU4MjkxOA==,ExTraMapper,ay-lab/ExTraMapper,0,ay-lab,https://github.com/ay-lab/ExTraMapper,ExTraMapper is a tool to find Exon and Transcript-level Mappings of a given pair of orthologous genes between two organisms using sequence conservation.,0,2016-03-23 18:16:34+00:00,2024-12-18 09:38:48+00:00,2022-04-20 18:38:06+00:00,,84491,10,10,Python,1,1,1,1,0,0,6,0,0,0,mit,1,0,0,public,6,0,10,master,1,['ay-lab'],,"# ExTraMapper
ExTraMapper is a tool to find Exon and Transcript-level Mappings of a given pair of orthologous genes between two organisms using sequence conservation. The figure below shows the overall schematic description of ExTraMapper mapping the homologous transcript and exon-pairs between human and mouse genome. 


![ExTraMapper_Figure](https://user-images.githubusercontent.com/18036388/90572310-8b693e00-e168-11ea-9fbc-8188c2834de9.jpg)

# Steps to run ExtraMapper (For python version 3 or later usage)

### Step 1: Prepare the input files
ExTraMapper requires a set of preprocessed files to find the conservation scores. Examples to create these files are provided within the following folders
 
1. [__Human-Mouse-Preprocessed-Data__](https://github.com/ay-lab/ExTraMapper/tree/master/Human-Mouse-Preprocess-Data) 

    and 
    
2. [__Human-Rhesus_macaque-Preprocessed-Data__](https://github.com/ay-lab/ExTraMapper/tree/master/Human-Monkey-Processed-Data) 

### Steps to generate the input files
The users should run the _extMpreprocess_ to generate the inputfiles within the above Preprocessed-Data folders. All the input files will be generated under _preprocess/data_ folder. All the required executables and scripts are provided here. The _extMpreprocess_ has 7 individual steps and should be run in the following manner 
 
 - ![#f03c15](https://via.placeholder.com/15/f03c15/000000?text=+) For help, type <br>
   
    ```bash
    ./extMpreprocess help
    
    This script will download and preprocess the dataset required for exon-pair and transcript pair finding by ExTraMapper.
    Type ./extMpreprocess <config.conf> <step> to execute the script.
    Type ./extMpreprocess example to print a example config.conf file.

    This script will run seven (7) sequential steps to create the inputs for ExTraMapper program.
    Users can provide step numbers (1-7) or all in the <step> arugemt of this script.
    Short description of the individual scripts:
    Step 1: Download per organism specific files e.g. reference genomes, gene annotation files.
    Step 2: Will create genomedata archives with the genomes of org1 and org2 (Make sure to install genomedata package).
    Step 3: Pickle files for each homologous gene pair will be created.
    Step 4: Perform coordinate liftOver of exons with multiple mappings (This step requires bedtools and liftOver executables).
    Step 5-7: postprocessing the liftOver files.
    
    example: 
    
    ./extMpreprocess config.human-mouse.conf all
    ```
   <br>
<br>

### Step 2: Set the following path
```bash export EXTRAMAPPER_DIR=/path/to/this/folder```

<br>

### Step 3: Run ExTraMapper individually
```bash
$ python ExTraMapper.py -h
usage: ExTraMapper.py [-h] -m MAPPING -o1 ORG1 -o2 ORG2 -p ORTHOLOG

Check the help flag

optional arguments:
  -h, --help   show this help message and exit
  -m MAPPING   ExTraMapper Exon threshold value [e.g. 1]
  -o1 ORG1     First organism name [e.g. human]
  -o2 ORG2     Second organism name [e.g. mouse]
  -p ORTHOLOG  Orthologous gene pair [e.g. ENSG00000141510-ENSMUSG00000059552 OR all]
```

#### Example run of ExTraMapper.py using orthologous gene pair ENSG00000141510-ENSMUSG00000059552 
```bash
$ python ExTraMapper.py -m 1 -o1 human -o2 mouse -p ENSG00000141510-ENSMUSG00000059552

Finding exon mappings for gene pair number 0    ENSG00000141510-ENSMUSG00000059552
*****************************************************************
Gene pair ID: ENSG00000141510-ENSMUSG00000059552

Information about each gene. Last two numbers are no of transcripts and exons
ENSG00000141510 chr17   7661779 7687538 -       ENSG00000141510 TP53    protein_coding  27      49      gene
ENSMUSG00000059552      chr11   69580359        69591873        +       ENSMUSG00000059552      Trp53   protein_coding  6       24      gene

Number of exons before and after duplicate removal according to coordinates
Org1    49      40
Org2    24      20

*****************************************************************

*****************************************************************
GCGCTGGGGACCTGTCCCTAGGGGGCAGATGAGACACTGATGGGCGTACTTAGAGATTTGCCATGAAGTGGGTTTGAAGAATGGAGCTGTGTGTGAAAT
Exon file type summaries for the first gene from: ENSG00000141510-ENSMUSG00000059552
        0 exons with: No file exists
        22 exons with: Only Mapped
        0 exons with: Only nonintersecting
        11 exons with: Only unmapped
        15 exons with: Mapped and unmapped
        0 exons with: Mapped and nonintersecting
        1 exons with: Nonintersecting and unmapped
        0 exons with: All three files
Exon file type summaries for the second gene from: ENSG00000141510-ENSMUSG00000059552
        0 exons with: No file exists
        14 exons with: Only Mapped
        0 exons with: Only nonintersecting
        3 exons with: Only unmapped
        7 exons with: Mapped and unmapped
        0 exons with: Mapped and nonintersecting
        0 exons with: Nonintersecting and unmapped
        0 exons with: All three files
Writing exon-level similarity scores into file:
 /path/output/human-mouse/ENSG00000141510-ENSMUSG00000059552/exonLevelSimilarities-1.0.txt

Writing exon classes into file:
 /path/output/human-mouse/ENSG00000141510-ENSMUSG00000059552/exonClasses-1.0.txt
        For org1: Mapped exons= 17, Unmapped exons= 21, Nonintersecting exons= 1, OTHER= 10
        For org2: Mapped exons= 13, Unmapped exons= 7, Nonintersecting exons= 0, OTHER= 4
*****************************************************************

*****************************************************************
Writing exon-level mappings into file:
 /path/output/human-mouse/ENSG00000141510-ENSMUSG00000059552/exonLevelMappings-1.0.txt
Writing trascript-level similarity scores into file:
 /path/output/human-mouse/ENSG00000141510-ENSMUSG00000059552/transcriptLevelSimilarities-1.0.txt
Writing transcript-level mappings into file:
 /path/output/human-mouse/ENSG00000141510-ENSMUSG00000059552/transcriptLevelMappings-1.0.txt

Condition counter from the greedy transcript mapping stage:
        5 pairs with Condition1: Unique winner pair
        0 pairs with Condition2: Tie in one score, not in the other
        0 pairs with Condition3: Tie in both scores but coding exon length diff breaks the tie
        0 pairs with Condition4: Tie in both scores and coding exon length diff but overall exon length breaks the tie
        1 pairs with Condition5: Tie in all the above but coding length (bp) diff breaks the tie
        0 pairs with Condition6: Tie in all the above, just give up and report all

Writing UCSC browser bed output for org1 into file:
 /path/output/human-mouse/ENSG00000141510-ENSMUSG00000059552/org1-ucsc-1.0.bed
Writing UCSC browser bed output for org2 into file:
 /path/output/human-mouse/ENSG00000141510-ENSMUSG00000059552/org2-ucsc-1.0.bed

........
ExTraMapper ran successfully for 1 gene pairs between: human and mouse


*****************************************************************
$ tree ./output

./output
`-- human-mouse
    `-- ENSG00000141510-ENSMUSG00000059552
        |-- exonClasses-1.0.txt
        |-- exonLevelMappings-1.0.txt
        |-- exonLevelSimilarities-1.0.txt
        |-- org1-ucsc-1.0.bed
        |-- org2-ucsc-1.0.bed
        |-- transcriptLevelMappings-1.0.txt
        `-- transcriptLevelSimilarities-1.0.txt
```

Note: The __exonLevelMappings-1.0.txt__ & __transcriptLevelMappings-1.0.txt__ file contains the mapped exon and transcript pairs from __ENSG00000141510-ENSMUSG00000059552__ orthologous gene-pair. 

<br>

# OR

### Step 3: Run ExTraMapper for all the gene pairs
```bash
$ python ExTraMapper.py -h
usage: ExTraMapper.py [-h] -m MAPPING -o1 ORG1 -o2 ORG2 -p all
```

<br>

### Summarise the ExTraMapper results ###
Run _extMsummarise_ script to generate a concatenated file will all the results. Run the script in the follwoing manner 
```bash
$ ./extMsummarise help
Type ./extMsummarise <preprocess_folder> <extramapper_folder> <orthologous_genepair_list> <org1name> <org2name> <outputprefix>
preprocess_folder  : Path to the preprocess folder generated by the extMpreproces script
extramapper_folder : Path to the output folder generated by ExTraMapper program
orthologous_genepair_list : A list of orthologous gene-pairs
org1name : org1 name e.g. human
org2name : org2 name e.g. mouse
outputprefix : output file prefix

example : 
./extMsummarise ./preprocess ./output gene-pair.list human mouse extramapper-result
```
<br>


# Prepocessed Results

Check the [Result/Exon-Pairs](https://github.com/ay-lab/ExTraMapper/tree/master/Result/Exon-Pairs) and [Result/Transcript-Pairs](https://github.com/ay-lab/ExTraMapper/tree/master/Result/Transcript-Pairs) to download the precomputed ExTraMapper result for human-mouse and human-rhesus orthologous exon and transcript pairs.

<br>

### Refer the work
[_ExTraMapper: Exon- and Transcript-level mappings for orthologous gene pairs._](https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btab393/6278896?redirectedFrom=fulltext)

__Chakraborty A, Ay F, Davuluri RV. ExTraMapper: Exon- and Transcript-level mappings for orthologous gene pairs. Bioinformatics. 2021 May 20:btab393. doi: 10.1093/bioinformatics/btab393. Epub ahead of print. PMID: 34014317.__

The data shown in the above paper was performed using Human & Mouse ENSMBL release 81 with python 2.7 code. 
The current update is with ENSMBL release 102 and python 3 or later version. To see the older code and data please
change the __Branch__ to [__ExTraMapper-python2v__](https://github.com/ay-lab/ExTraMapper/tree/ExTraMapper-python2v) from __master__

### Check the webserver for a nice vizualization 
https://ay-lab-tools.lji.org/extramapper/index.html
",0,0.5,0.5,,,,,,0,3,,,
42518970,MDEwOlJlcG9zaXRvcnk0MjUxODk3MA==,find_circ,marvin-jens/find_circ,0,marvin-jens,https://github.com/marvin-jens/find_circ,tools to find circRNAs in RNA-seq data,0,2015-09-15 12:50:00+00:00,2025-02-04 15:02:22+00:00,2017-11-16 18:16:49+00:00,,17663,42,42,Python,1,1,1,1,0,0,28,0,0,6,gpl-3.0,1,0,0,public,28,6,42,master,1,"['marvin-jens', 'mschilli87']",,"## circRNA detection from RNA-seq reads ##
This repository holds python code to detect head-to-tail spliced (back-spliced)
sequencing reads, indicative of circular RNA (circRNA) in RNA-seq data. It is 
also used extensively by [circbase](http://circbase.org).

### Author notes and preface ###

The scripts in this package were designed by Nikolaus Rajewsky, 
Antigoni Elefsinioti and Marvin Jens. 
The current implementation was written by Marvin Jens.

This software is provided to you ""as is"", without any warranty. We used 
this code (v1) to produce the results presented in 

```
   Nature. 2013 Mar 21;495(7441):333-8. 
   doi: 10.1038/nature11928. Epub 2013 Feb 27.
  
   Circular RNAs are a large class of animal RNAs with regulatory potency.
  
   Memczak S, Jens M, Elefsinioti A, Torti F, Krueger J, Rybak A, Maier L,
   Mackowiak SD, Gregersen LH, Munschauer M, Loewer A, Ziebold U, 
   Landthaler M, Kocks C, le Noble F, Rajewsky N.
```
  
Have a look at [Memczak *et al.* 2013](http://www.ncbi.nlm.nih.gov/pubmed/23446348) 
and the supplementary material for additional
information. Please don't contact the authors, who are very busy, regarding 
this software unless you have found a bug, wish to provide improvements 
to the code, propose a collaboration, or use our code for commercial 
purposes. Thank you very much for your understanding!

### License ###

The code is released under the GNU General Public License (version 3).
See the file `LICENSE` for more detail.

### Brief version history ###

The current code (v1.2) has some small bug fixes and improvements. 
Each version deemed stable is tagged and a brief summary of the improvements 
is given here:
    
  - **v1** : as used in [Memczak *et al.* 2013](http://www.ncbi.nlm.nih.gov/pubmed/23446348)
  - **v1.2** : 
    - fix the uniqueness handling. Occasionally reads would have 
      either anchor align uniquely, but never both. These rare cases now get 
      flagged as ""NO_UNIQ_BRIDGES"". 
    - support all chromosomes in one FASTA file
    - store the size of each chromosome upon first access, pad violations of chromosome bounds by padding with 'N'
    - category labels have been extended for clarity
      (""UNAMBIGUOUS_BP"" instead of ""UNAMBIGUOUS"", etc.), in a manner which preserves 
      functionality of `grep` commands.
    - by default no longer report junctions with only one uniquely aligned anchor. Original behaviour can be restored using the 
      `--halfuniq` switch.
    - by default no longer report junctions that lack a read where both anchors align uniquely (NO_UNIQ_BRIDGES keyword).
      Original behaviour can be restored using the `--report_nobridge` switch
      
  - **v2** : (*under development*): produce multiple anchor lengths to potentially 
yield more junctions with unique anchor mappings.

### Prerequisites ###

The scripts run on Ubuntu 12.04.2 on a 64Bit machine with python 2.7.
We do not know if it runs in different environments but other 64Bit unix
versions should run if you can get the required 3rd party software 
installed. 

You need to install the python packages numpy and pysam. If there are no 
packages in your linux distro's repositories, try the very useful python 
installer (building numpy requires many dependencies, so obtaining pre-
compiled packages from a repository is a better option).

```
    pip install --user pysam
```
Next you need the short read mapper bowtie2 and samtools up and running. 
samtools now is in the repositories of many distros, but here you can 
get the most fresh versions:
    
```
    http://samtools.sourceforge.net/
    http://bowtie-bio.sourceforge.net/bowtie2/index.shtml
```
At this point you should have everything to run a built-in test data set
```
    cd test_data
    make
```
If you get error messages here, first make sure the dependencies are
really installed correctly and run on their own.
The authors of this code can not give support bowtie2, samtools, python, 
or any other third-party packages! Sorry, but not enough time for this.
If you are sure that the problem is with our code, just zip the test_data
folder and e-mail it to us. MAYBE, we can help.

In case you are working with human data and have the hg19 genome and a bowtie2 
index around, there is an additional test/sanity-check you can run:
    
```
    cd test_data
    make hek_test2 \
        GENOME_HG19=/path_to/hg19.fa \
        INDEX_HG19=/path_to/bowtie2_hg19_index
```
(obviously, the paths to genome and index will have to be changed for this to work)
This will push known spliced reads, belonging to previously identified junctions, 
through `find_circ.py`, then take the found spliced reads and run them 
through `find_circ.py` a second time. Ultimately, it compares the detected splice 
sites and ensures the two sets are identical.

If everything goes well you can get started with your real data! :)   
You need to have the reference genome and a bowtie2 index for it.
As an example, let's assume you use C.elegans genome ce6 (WS190):

```
    wget -c http://hgdownload.cse.ucsc.edu/goldenPath/ce6/bigZips/chromFa.tar.gz \
        -O - | gzip -dc | tar -xO > ce6.fa
```
This will retrieve the genome from the UCSC website, unpack it into a single 
fasta file with all chromosomes to build the index:
```
    bowtie2-build ce6.fa bt2_ce6 
```

### How to use the unmapped2anchors.py script ###

It is recommended to map your RNA-seq reads against the genome first and 
keep the part that can not be mapped contiguously to look for splice-
junctions afterwards. The genome alignments can be used for gene 
expression analysis and the unmapped reads will represent a fraction of
the input, thus downstream analysis will be faster.
```
    bowtie2 -p16 --very-sensitive --score-min=C,-15,0 --mm \
    -x bt2_ce6 -q -U <your_reads.fastq.gz> 2> bowtie2.log  \
    | samtools view -hbuS - | samtools sort - test_vs_ce6
```
single out the unaligned reads and split those with good quality into anchors
for independent mapping (used to identify splice junctions)
```
    # get the unmapped and pipe through unmapped2anchors.py
    samtools view -hf 4 test_vs_ce6.bam | samtools view -Sb - | \
        ./unmapped2anchors.py unmapped_ce6.bam | gzip \
            > ce6_anchors.fastq.gz
```

### How to use find_circ.py ###

Now we have everything to screen for spliced reads, from either linear or
head-to-tail (circular) splicing:

```
    mkdir -p <run_folder>
    bowtie2 -p 16 --score-min=C,-15,0 --reorder --mm \
        -q -U ce6_anchors.fastq.gz -x bt2_ce6 |\
            ./find_circ.py \
                --genome=ce6.fa \
                --prefix=ce6_test_ \
                --name=my_test_sample \
                --stats=<run_folder>/stats.txt \
                --reads=<run_folder>/spliced_reads.fa \
                    > <run_folder>/splice_sites.bed
```
The prefix `ce6_test_` is arbitrary, and pre-pended to every identified splice 
junction. You may consider setting it to `tmp` or similar for single samples out of a 
larger set. Note that `find_circ.py` outputs both, circRNA splice junctions (containing the keyword `CIRCULAR`) linear splice junctions (containing the keyword `LINEAR`). 
You may want to `grep CIRCULAR <run_folder>/splice_sites.bed > circs_sample1.bed` or similar, to sort out the circRNAs.


   
### Output format ###

The detected linear and circular candidate splice sites are printed to stdout. The first 6 columns are standard BED. The rest hold various 
quality metrics about the junction. Here is an overview:

| column | name | description |
|--------|------|-------------|
| 1  | chrom | chromosome/contig name |
| 2  | start | left splice site (zero-based) |
| 3  | end | right splice site (zero-based). |
|    |     | (Always: end > start. 5' 3' depends on strand) |
| 4  | name | (provisional) running number/name assigned to junction |
| 5  | n_reads | number of reads supporting the junction (BED 'score') |
| 6  | strand | genomic strand (+ or -) |
| 7  | n_uniq | number of *distinct* read sequences supporting the junction |
| 8  | uniq_bridges | number of reads with both anchors aligning uniquely |
| 9  | best_qual_left | alignment score margin of the best anchor alignment |
|    |                | supporting the left splice junction (`max=2 * anchor_length`) |
| 10 | best_qual_right | same for the right splice site |
| 11 | tissues | comma-separated, alphabetically sorted list of |
|    |           | tissues/samples with this junction |
| 12 |tiss_counts |  comma-separated list of corresponding read-counts |
| 13 | edits | number of mismatches in the anchor extension process |
| 14 | anchor_overlap | number of nucleotides the breakpoint resides within one anchor |
| 15 | breakpoints | number of alternative ways to break the read with flanking GT/AG | 
| 16 | signal | flanking dinucleotide splice signal (normally GT/AG) |
| 17 | strandmatch | 'MATCH', 'MISMATCH' or 'NA' for non-stranded analysis |
| 18 | category | list of keywords describing the junction. Useful for quick `grep` filtering |

The following list of keywords is assigned to splice sites by `find_circ.py` for easy filtering:
    
| keyword           | description |
|-------------------|-------------|
| LINEAR            | linear (mRNA) splice site, joining consecutive exons |
| CIRCULAR          | potential circRNA splice site. Exons are joint in reverse order. |
| UNAMBIGUOUS_BP    | demanding flanking GT/GA, only one way of splitting the spliced |
|                   | read was found (only one possible breakpoint within the read) |
| PERFECT_EXT       | The read sequence between the anchors aligned perfectly during the |
|                   | extension process. |
| GOOD_EXT          | The extension (see above) required not more than one mismatch or one |
|                   | nucleotide overlap with an anchor |
| OK_EXT            | The extension (see above) required not more than two mismatches or two |
|                   | nucleotides overlap with an anchor |
| ANCHOR_UNIQUE     | Unique anchor alignments have been found, supporting both sides |
|                   | of the junction. Unless `--halfunique` is used, this should be  |
|                   | true for all reported results. |
| CANONICAL         | splice sites are flanked by GT/AG. Unless `--noncanonical` is |
|                   | used, this should be true for all reported results. |
| NO_UNIQ_BRIDGES   | While both sides of the junction are individually supported by |
|                   | unique anchor alignments, there is not a single read, where both |
|                   | anchors align uniquely at the same time (bridging the junction). |
|                   | Unless `--report_nobridge` is used, this should never appear. |
| STRANDMATCH       | Only appears when `--stranded` is used and GT/AG were found in the |
|                   | correct orientation |


### How to filter the output ###

It is usually a good idea to demand at least 2 reads supporting 
the junction, unambiguous breakpoint detection and some sane mapping 
quality:
    
To get a reasonable set of circRNA candidates try:
```
    grep CIRCULAR <run_folder>/splice_sites.bed | \
        grep -v chrM | \        
        awk '$5>=2' | \
        grep UNAMBIGUOUS_BP | grep ANCHOR_UNIQUE | \
        ./maxlength.py 100000 \
        > <run_folder>/circ_candidates.bed
```
This selects the circular splice sites supported by at least 2 reads with unambiguous detection of the breakpoint (*i.e.* the exact nucleotides at which splicing occurred), and unique anchor alignments on both sides of the junction. The last part subtracts start from end coordinates to compute the genomic length, and removes splice sites that are more than 100 kb apart. These are perhaps trans-splicing events, but for sure they are so huge they can seriously slow down any downstream scripts you may want to run on this output.

### Analyzing multiple samples ###

If you intend to analyze multiple samples, it is now strongly advised to
run them individually through `find_circ.py`, and merge the separate outputs
later! Use the `find_circ.py --name <sample_name>` flag to assign sample IDs, tissue names, *etc.* to 
each sample.

Merging should then be done with `merge_bed.py`:
    
```
    ./merge_bed.py sample1.bed sample2.bed [...] > combined.bed
```

This will deal properly with the various columns: quality scores will be assigned the maximum value of all samples, total read counts will be summed up, `tissues` column will contain a comma-separated list, *etc.*.

### Command line reference `unmapped2anchors.py` ###
```
    ./unmapped2anchors.py -h
    Usage: 

    unmapped2anchors.py <alignments.bam> > unmapped_anchors.qfa

    Extract anchor sequences from unmapped reads. Optionally permute.


    Options:
    -h, --help            show this help message and exit
    -a ASIZE, --anchor=ASIZE
                          anchor size
    -q MINQUAL, --minqual=MINQUAL
                          min avg. qual along both anchors (default=5)
    -r REV, --rev=REV     permute read parts or reverse A,B,R,C,N for control
    -R, --reads           instead of unmapped reads from BAM, input is
                          sites.reads from find_circ.py
    -F, --fasta           instead of unmapped reads from BAM, input is FASTA
                          file
    -Q, --fastq           instead of unmapped reads from BAM, input is FASTQ
                          file
```

### Command line reference `find_circ.py` ###

```
    Usage: 

    bowtie2 [mapping options] anchors.fastq.gz | find_circ.py [options] > candidates.bed


    Options:
    -h, --help            show this help message and exit
    -v, --version         get version information
    -S SYSTEM, --system=SYSTEM
                          model system database (optional! Requires byo
                          library.)
    -G GENOME, --genome=GENOME
                          path to genome (either a folder with chr*.fa or one
                          multichromosome FASTA file)
    -n NAME, --name=NAME  tissue/sample name to use (default='unknown')
    -p PREFIX, --prefix=PREFIX
                          prefix to prepend to each junction name (default='')
    -q MIN_UNIQ_QUAL, --min_uniq_qual=MIN_UNIQ_QUAL
                          minimal uniqness for anchor alignments (default=2)
    -a ASIZE, --anchor=ASIZE
                          anchor size (default=20)
    -m MARGIN, --margin=MARGIN
                          maximum nts the BP is allowed to reside inside an
                          anchor (default=2)
    -d MAXDIST, --maxdist=MAXDIST
                          maximum mismatches (no indels) allowed in anchor
                          extensions (default=2)
    --noncanonical        relax the GU/AG constraint (will produce many more
                          ambiguous counts)
    --randomize           select randomly from tied, best, ambiguous hits
    --allhits             in case of ambiguities, report each hit
    --stranded            use if the reads are stranded. By default it will be
                          used as control only, use with --strandpref for
                          breakpoint disambiguation.
    --strandpref          prefer splice sites that match annotated direction of
                          transcription
    --halfunique          also report junctions where only one anchor aligns
                          uniquely (less likely to be true)
    --report_nobridges    also report junctions lacking at least a single read
                          where both anchors, jointly align uniquely (not
                          recommended. Much less likely to be true.)
    -R READS, --reads=READS
                          write spliced reads to this file instead of stderr
                          (RECOMMENDED!)
    -B BAM, --bam=BAM     filename to store anchor alignments that were recorded
                          as linear or circular junction candidates
    -r READS2SAMPLES, --reads2samples=READS2SAMPLES
                          path to tab-separated two-column file with read-name
                          prefix -> sample ID mapping
    -s STATS, --stats=STATS
                          write numeric statistics on the run to this file
```

### Command line reference `merge_bed.py` ###

```
    Usage:

    merge_bed.py 1.bed 2.bed [3.bed] [4.bed] [...] > merged.bed

    Merge BED or BED-like files on the genomic coordinates. Deals properly
    with find_circ.py output and adds a few extra columns.


    Options:
      -h, --help            show this help message and exit
      -f FLANK, --flank=FLANK
                            add flanking nucleotides to define more fuzzy overlap
                            (default=0)
      -s STATS, --stats=STATS
                            write statistics to this file (instead of stderr)
      -c, --old-input       switch on compatibility mode for old input format
```
",0,0.6,0.6,,,,,,0,6,,,
172437143,MDEwOlJlcG9zaXRvcnkxNzI0MzcxNDM=,origami,jacksonvanover/origami,0,jacksonvanover,https://github.com/jacksonvanover/origami," ""The Shortcomings of the Field of Constructible Numbers: Generating Field Extensions with Origami"". Senior thesis, researched and written in the winter of 2016, to complete a BA in computational mathematics at the University of California, Santa Cruz.",0,2019-02-25 04:59:15+00:00,2020-05-10 00:02:49+00:00,2020-05-10 00:00:01+00:00,,270,0,0,TeX,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['jacksonvanover'],,"## The Shortcomings of the Field of Constructible Numbers: Generating Field Extensions with Origami

This paper details an exploration of fields and field extensions motivated by the solvability of the classic geometric problem of doubling the cube. The reader will compare and contrast the field of numbers generated by a compass and straightedge combination, call it E_s, and the field of numbers generated by paper folding, call it E_o. This project involves building up these fields using the axioms for each that determine what are and are not _constructible_ points and lines. In doing so, we will show that E_o is actually a field extension of E_s; the reader will see why the solution to the doubling of the cube is not constructible with a compass and straightedge and then how simply folding a piece of paper can solve the problem.

Researched and written in the winter of 2016 as a senior thesis.
",1,0.79,0.79,,,,,,0,0,,,
355035588,MDEwOlJlcG9zaXRvcnkzNTUwMzU1ODg=,Raymond-YZ,Raymond-YZ/Raymond-YZ,0,Raymond-YZ,https://github.com/Raymond-YZ/Raymond-YZ,Config files for my GitHub profile.,0,2021-04-06 02:43:53+00:00,2024-07-24 07:28:07+00:00,2024-07-24 07:28:03+00:00,https://github.com/Raymond-YZ,6,0,0,,0,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['Raymond-YZ'],,"- 👋 Hi, I’m @Raymond-YZ and I graduated from the University of California, Santa Cruz where I received a B.S. in Computer Science. During my time in university, I had the pleasure of working with different development teams on web and software applications while implementing Scrum practices. These experiences have led me to an interest mostly in front-end development and user experience.
- Aside from projects, I am very customer-driven and love to interact with customers at a personal level. My time in student organizations and previous jobs have taught me how to lead by example, compromise with civility, and be willing to try something new even if it does not always lead to immediate success. One of my biggest accomplishments as a student leader comes from my time as a Banquet Coordinator for the Chinese Student Association, where we used social media to create an online banquet and raise donations for the San Francisco Chinatown community during the height of the pandemic.
- 👀 I’m interested in gaming/esports and swimming. I used to swim competitively for 9 years and have been competiting in esports (CS:GO and VALORANT) for the last 4 years! During my time in esports, my CS:GO team won the inaugural season of North American Collegiate Counter-Strike league in May 2020 and even founded our university's VALORANT roster. With my experience in both esports, my main goal is to create an application dedicated to help players analyze playstyles depending on maps in these games.
- Currently, I am seeking a full-time job in the front-end and software engineering fields where I can thrive in a team-oriented environment. Feel free to contact me at raylee@ucsc.edu for further inquiries.
- 📫 How to reach me: raylee@ucsc.edu

<!---
Raymond-YZ/Raymond-YZ is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
",1,0.74,0.74,,,,,,0,1,,,
473680142,R_kgDOHDvJDg,hse_hw3_chromhmm,AlinaPokryshchenko/hse_hw3_chromhmm,0,AlinaPokryshchenko,https://github.com/AlinaPokryshchenko/hse_hw3_chromhmm,,0,2022-03-24 16:09:51+00:00,2022-03-25 06:16:32+00:00,2022-03-25 17:21:31+00:00,,20723,0,0,HTML,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['AlinaPokryshchenko'],,"# Домашнее задание 3
Клеточная линия, рассмотренная в ДЗ2 (DOHH2), не содержит ChIP-seq эксперименты в рассматриваемых гистоновых метках, 
поэтому я буду работать с клеточной линией Monocytes-CD14+_RO01746.

#### [Colab](https://colab.research.google.com/drive/1H4611BxHOLed8mnEz5DOr2tYKI9pyV93?usp=sharing)
### Список гистоновых меток
Метка|Ссылка на файл
-----|--------------
H2az|http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneMonocd14ro1746H2azAlnRep1.bam
H3k79me2|http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneMonocd14ro1746H3k79me2AlnRep1.bam
H4k20me1|http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneMonocd14ro1746H4k20me1AlnRep1.bam  
H3k04me2|http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneMonocd14ro1746H3k04me2AlnRep1.bam  
H3k04me3|http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneMonocd14ro1746H3k04me3AlnRep1.bam  
H3k09ac|http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneMonocd14ro1746H3k09acAlnRep1.bam
H3k09me3|http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneMonocd14ro1746H3k09me3AlnRep1.bam 
H3k27ac|http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneMonocd14ro1746H3k27acAlnRep1.bam 
H3k27me3|http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneMonocd14ro1746H3k27me3AlnRep1.bam  
H3k36me3|http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneMonocd14ro1746H3k36me3AlnRep1.bam 

Ссылка на контроль: http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/wgEncodeBroadHistoneMonocd14ro1746ControlAlnRep1.bam

### Выдача ChromHMM

![transitions](/results_10/transitions_10.png)|![emissions](/results_10/emissions_10.png)|![fold1](/results_10/Monocytes-CD14+_RO01746_10_overlap.png)|![fold2](/results_10/Monocytes-CD14+_RO01746_10_RefSeqTES_neighborhood.png)|![fold3](/results_10/Monocytes-CD14+_RO01746_10_RefSeqTSS_neighborhood.png)
 -|-|-|-|-

### Из геномного браузера
![1_genome](/images/1_genome.png)
![2_genome](/images/2_genome.png)
![3_genome](/images/3_genome.png)

Для более удобного определения какому состоянию соответствует тот или иной элемент генома, была создана таблица, в которой указаны места, где чаще всего располагаются гистоновые метки. Данные были взяты из статьи Ernst J, et al. Mapping and analysis of chromatin state dynamics in nine human cell types. Nature. 2011 и других статей, которые будут указаны далее

Источники:

H2az: (https://pubmed.ncbi.nlm.nih.gov/34643712/)

H3K79me2: (https://pubmed.ncbi.nlm.nih.gov/29665865/)

H3k09me3:(https://pubmed.ncbi.nlm.nih.gov/26675384/)

Гистоновая метка|H2az|H3k79me2|H4k20me1|H3k04me2|H3k04me3|H3k09ac|H3k09me3|H3k27ac|H3k27me3|H3k36me3	
-|-|-|-|-|-|-|-|-|-|-
Где чаще всего расположена|в транскрибируемых областях рядом с TSS усиляет экспрессию гена|участвует в альтернативном сплайсинге, поэтому располагается в активно транскрибируемых районах| в повторяющихся последовательностях и транскрибируемых районах|в сильных энхансерах и промоторах|в сильных промоторах и энхансерах|в сильных промоторах и энхансерах|супрессор в гетерохроматине|в сильных энхансерах и промоторах|в неактивных промоторах, повторяющихся областях, иногда в факультативном хроматине|в повторяющихся областях, в транскрибируемых областях

### Итог
Состояние|1|2|3|4|5|6|7|8|9|10
-|-|-|-|-|-|-|-|-|-|-
Наиболее часто встречающиеся гистоновые метки|H3K4me3, H3K4me2|H3K27ac|H3K79me2,H4K20me1,H3K27ac|H3K79me2, H4K20me1|H3K36me3|H3K9ac|H3K36me3|H3K9me3|-|H3K27me3 
Области генома, в которых чаще встречаются|CpG-островки, экзоны, старт транскрипции|ядерная ламина, TES|гены, расположение в 2kb от старта транскрипции|гены, ядерная ламина, расположение в 2kb от старта транскрипции, рядом с TES|экзоны, гены, рядом с TES|экзоны, гены, TES, ламина|экзоны, гены, TES|экзоны, ламина, TES|ламина (+занимает основную часть генома)|распределено равномерно
Как расположены относительно старта транскрипции|больше сконцентрированы перед, на и после TSS|располагаются до TSS|1000 нуклеотидов после TSS|1400 нуклеотидов после TSS|редко встречаются|редко встречаются|-|-|-|распределены равномерно
Как расположены относительно конца транскрипции|в основном до TES|распределены равномерно|-|встречаются редко после TES|сосредоточены до TES|чаще встречаются после TES|сосредоточены до TES|распределены равномерно|-|распределены равномерно
Состояния, которые часто расположены рядом|2,10|1,6,9|1,4|3,5|4,7|2,7|6|9|-|9
Итоговое название|сильный промотор|сильный энхансер|повторяющиеся последовательности в генах (возможно, какой-то регуляторный элемент, можно заметить на 1 скриншоте из геномного браузера, что данные последовательности расположены перед областями, с которых транскрибируются малые ядрышковые РНК (обозначены SNORD). Есть предположение, что данные области состояния 3 нужны для связывания РНК-полимеразы II типа, поскольку именно она транскрибирует мякРНК с интронов генов. Но, возможно, это просто регуляторные элементы)|регуляторные последовательности|транскрибируемая область (transcriptional transition)|возможно, слабый промотор|транскрибируемая область (transcriptional elongation)|гетерохроматин|конститутивный гетерохроматин|факультативный гетерохроматин (Polycomb repressed)

### Бонус

```
import pandas as pd
df = pd.read_csv('Monocytes-CD14+_RO01746_10_dense.bed', encoding='utf-8', sep='\t', comment='t', header=None)
header = ['chrom', 'chromStart', 'chromEnd', 'state', 'zero', 'dot', 'chromStart', 'chromEnd', 'rgb']
df.columns = header[:len(df.columns)]
df.loc[df.state == 1, 'state'] = '1_promoter'
df.loc[df.state == 2, 'state'] = '2_enhancer'
df.loc[df.state == 3, 'state'] = '3_regulatory_element_1'
df.loc[df.state == 4, 'state'] = '4_regulatory_element_2'
df.loc[df.state == 5, 'state'] = '5_trx_transition'
df.loc[df.state == 6, 'state'] = '6_weak_promoter'
df.loc[df.state == 7, 'state'] = '7_trx_elongation'
df.loc[df.state == 8, 'state'] = '8_heterochromatin'
df.loc[df.state == 9, 'state'] = '9_constitutive heterochromatin'
df.loc[df.state == 10, 'state'] = '10_facultative heterochromatin'
df.to_csv('Monocytes-CD14+_RO01746_10_new_dense.bed', sep='\t', index=False, header=None)
```
![bonus](/images/bonus.png)
",0,0.41,0.41,,,,,,0,1,,,
579148779,R_kgDOIoUb6w,diwaav,diwaav/diwaav,0,diwaav,https://github.com/diwaav/diwaav,,0,2022-12-16 19:36:03+00:00,2022-12-16 19:36:03+00:00,2023-06-22 02:41:06+00:00,,3,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['diwaav'],,"### Diwa Ashwini Vittala
Hello! My name is Diwa and I've graduated from UC Santa Cruz with a B.S. in Technology and Information Management and a minor in Computer Science.    

You can reach me at diwa.av02@gmail.com 
",1,0.7,0.7,,,,,,0,1,,,
445051461,R_kgDOGobyRQ,up206a-lupe,velezlupe/up206a-lupe,0,velezlupe,https://github.com/velezlupe/up206a-lupe,,0,2022-01-06 05:27:12+00:00,2022-03-14 04:31:54+00:00,2022-03-14 04:31:50+00:00,,22128,0,0,HTML,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['velezlupe'],,"# up206a-lupe
## About Me: 
My name is Guadalupe Velez, I grew up in Minneapolis, Minnesota after my parents migrated from Chihuahua, Mexico. I also lived with my mom and sister in Denver, Colorado before I did my undergrad at the University of California Santa Cruz, studying Latin American and Latino Studies and Education. I am currently a first year MURP with a concentration in Community Economic Development and Housing. 

I have a pet tortoise named Chickpea: 
![This is a photo of Chickpea.](chickpea.jpg ""This is Chickpea."")


## Intro to GIS Course Description: 
Course begins with an introduction to various data science tools, and review the basics of programming with Python. Once a foundation of Python programming and data wrangling is achieved, spatial analysis through Python Libraries, and subsequently, through advanced geoprocessing will be introduced. All lessons will be based on “real” data with analytical methods addressing relevant and contemporary urban problems. At the conclusion of this course, students will be able to critically describe, analyze, and visualize spatial data for planning practices and research.

## Learning Objectives: 
I have no previous experience with GIS and mapmaking, coding or Python; however I want to put myself out of my comfort zone in order to learn a new valuable skill. By the end of the class I want to be able to: 

* Analyze and visualize data in a new way.
* Obtain a basic understanding of Python programming and data wrangling.
* Learn research methods and organizing data.
* Understand how to utilize data to create tables, charts, graphics. 
* Think spatially in order to better understand racial/social inequities 
* Raise awareness/find ways to raise awareness to racial and social inequities. 

## Project Ideas: 
I am interested in the development of affordable housing, displacement and homelessness. 

**Project Ideas could include**: 
* Mapping sites that are eligible to become affordable housing, juxtaposed with ongoing private development. 
* Identifying areas of high eviction rates and identifying communities of high risk.
* Visualizing the history and change of neighborhoods, trends, zoning, racial makeup, demographics, development. 
* How climate change impacts low income communities and people of color.
",1,0.73,0.73,,,,,,0,1,,,
183266802,MDEwOlJlcG9zaXRvcnkxODMyNjY4MDI=,DesignUCSCWebsite,scduchin/DesignUCSCWebsite,0,scduchin,https://github.com/scduchin/DesignUCSCWebsite,Created a webpage from html css and python that displays how a specific website is meant to look.,0,2019-04-24 16:27:05+00:00,2019-04-24 16:35:32+00:00,2019-04-24 16:35:32+00:00,,401,0,0,CSS,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['scduchin'],,,1,0.89,0.89,,,,,,0,0,,,
127357667,MDEwOlJlcG9zaXRvcnkxMjczNTc2Njc=,xenahub,UCSC-Treehouse/xenahub,0,UCSC-Treehouse,https://github.com/UCSC-Treehouse/xenahub,Stand-alone Dockerized Xena Hub,0,2018-03-29 23:27:13+00:00,2021-02-01 20:32:33+00:00,2021-02-01 20:32:30+00:00,https://treehouse.xenahubs.net,10,0,0,Dockerfile,1,1,1,1,0,0,3,0,0,0,,1,0,0,public,3,0,0,master,1,['e-t-k'],1,"# Looking for Treehouse data on Xena?<br>➜ [treehouse.xenahubs.net](https://treehouse.xenahubs.net)

## xenahub
Stand-alone Dockerized Xena Hub

## Requirements:
- docker
- docker-compose
- make (optional)


## Notes
You will need to provide a certificate file and keyfile.
They should be placed in the xena/certs subdirectory. (When running in standalone mode.
See below for running behind a reverse proxy.)

The hub must be accessible at the URL that the certificate is for.
Otherwise you will get an ""ERR_CERT_COMMON_NAME_INVALID"" error.

## Hosted Files
To load a data file into xenahub, you will need two files:
- the TSV data file itself, eg `example.tsv`. This must not be gzipped.
- a json-formatted metadata file, eg `example.tsv.json`. [Xena's FAQ provides details on the format](https://ucsc-xena.gitbook.io/project/local-xena-hub/loading-data-from-the-command-line).

Place both of these in the`xenahub/xena/files` directory.
Then, from the `xenahub` directory, run:

`make load file=example.tsv`

The request will complete within a few seconds and the file will then load asynchronously into the xena db.
Leave the TSV file within the `files` subdirectory, or the download link on its xena page will not work.

For backups of the metadata for Treehouse's hosted files, see [the reference-file-info repo](https://github.com/UCSC-Treehouse/reference-file-info).

## Root-Squash
If your xena files are stored on an NFS mount that has been ""root-squashed"", it is a little
bit complicated to make the permissions work.

The way I am doing it is to make everything group-readable, then run the docker with user
root (the default) but the same group id as the group on the host.
You will have to edit your docker-compose.yml file to update the group ID to the correct one.
To find the desired group id:

`getent group GROUPNAME`

If you get the following error, set the `xena` dir to be group-writable:

`Error opening database: ""Could not save properties /root/xena/database.lock.db""`

## Running behind Apache reverse proxy
This is how to set it up on a shared server running on a high-level port, with Apache proxying to it.
Instructions may be approximate.

### DNS
Set up a CNAME to your shared server with your hub URL:

xenahub.example.com 28800 CNAME sharedserver.example.com

### Apache
Apache is responsible for holding the certificates for the `xenahub.example.com` site in this mode.
For example, store in `/etc/httpd/xena_certs`:
`chain.crt  xena.crt  xena.csr  xena.key`

Then, in `/etc/httpd/conf.d/ssl.conf`, include the following VirtualHost.
This will send https requests to Xena's 7223 (https) port. ProxyPreserveHost ensures that xena knows
that its hostname is xenahub.example.com instead of 127.0.0.1.

```
<VirtualHost *:443>
  ServerName xenahub.example.com
  SSLEngine on
  SSLProtocol all -SSLv2 -SSLv3
  SSLCipherSuite HIGH:3DES:!aNULL:!MD5:!SEED:!IDEA
  SSLCertificateFile /etc/httpd/xena_certs/xena.crt
  SSLCertificateKeyFile /etc/httpd/xena_certs/xena.key
  SSLCertificateChainFile /etc/httpd/xena_certs/chain.crt

  ProxyRequests Off
  ProxyPreserveHost On
  SSLProxyEngine On
  SSLProxyVerify none
  SSLProxyCheckPeerCN off
  SSLProxyCheckPeerName off
  SSLProxyCheckPeerExpire off
  ProxyPass ""/""  ""https://127.0.0.1:7223/""
  ProxyPassReverse ""/""  ""https://127.0.0.1:7223/""
</VirtualHost>
```

### Xena Docker
For behind reverse proxy, you will want to use the no-certs configuration & docker build.
This is currently the default.
",1,0.87,0.87,,,,,,0,0,,,
33350036,MDEwOlJlcG9zaXRvcnkzMzM1MDAzNg==,ucsc,paldhous/ucsc,0,paldhous,https://github.com/paldhous/ucsc,,0,2015-04-03 06:27:08+00:00,2024-02-17 14:33:05+00:00,2021-10-23 18:39:15+00:00,,526605,1,1,HTML,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,1,master,1,['paldhous'],,"# ucsc
",1,0.8,0.8,,,,,,0,2,,,
104622297,MDEwOlJlcG9zaXRvcnkxMDQ2MjIyOTc=,Bayesian-Statistics,SNaveenMathew/Bayesian-Statistics,0,SNaveenMathew,https://github.com/SNaveenMathew/Bayesian-Statistics,Coursera's Bayesian Statistics module - University of California Santa Cruz,0,2017-09-24 05:53:52+00:00,2017-09-24 06:00:54+00:00,2017-10-09 05:01:21+00:00,,682,0,0,HTML,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,master,1,[],,,1,0.65,0.65,,,,,,0,2,,,
155790088,MDEwOlJlcG9zaXRvcnkxNTU3OTAwODg=,woset-workshop.github.io,woset-workshop/woset-workshop.github.io,0,woset-workshop,https://github.com/woset-workshop/woset-workshop.github.io,Workshop on Open-Source EDA Technology (WOSET),0,2018-11-01 23:54:11+00:00,2025-02-23 13:17:25+00:00,2024-11-18 15:45:27+00:00,,2461225,49,49,,1,1,1,1,1,0,8,0,0,1,,1,0,0,public,8,1,49,master,1,"['mguthaus', 'sheriefreda', 'jcirimel', 'dpetrisko', 'ArkaneMoose', 'dobios', 'schoeberl', 'Eyantra698Sumanto']",1,"
# Workshop on Open-Source EDA Technology


The WOSET workshop aims to galvanize the open-source EDA movement. The
workshop will (virtually) bring together EDA researchers who are committed to
open-source principles to share their experiences and coordinate
efforts towards developing a reliable, fully open-source EDA flow. The
workshop will feature presentations that overview existing
or under-development open-source tools, designs and technology
libraries. Break-out rooms will be utilized for discussion of works-in-progress. 
The workshop will feature a panel on the present status and
future challenges in open-source EDA, and how to coordinate efforts
and ensure quality and interoperability across open-source tools.


# Event

WOSET will take place via Zoom on November 18, 2024 from 8:00am to 1:00pm Pacific Time.
Please see the [WOSET 2024 Program](WOSET2024.md) for more details on the papers and
the [WOSET 2024 Schedule](WOSET2024-schedule.md) for the schedule of events.

Please [register here to receive instructions to join the meeting](https://forms.gle/Z8uGR7MTGvAzQMc6A).

# Organization

### Co-Chairs
* Jose Renau, UC Santa Cruz (Co-Chair)
* Matthew Guthaus, UC Santa Cruz (Co-Chair)

### Program Chair
* Dustin Richmond, UC Santa Cruz

### Proceedings Chair
* Dan Petrisko, University of Washington

### Zoom Czar
* Haoyuan Wang, UC Santa Cruz

### Program Committee
* Jonathan Balkind, UC Santa Barbara
* Tim Edwards, efabless
* Steve Hoover, Redwood EDA
* Lucas Klemmer, JKU Linz
* Dirk Koch, University of Manchester
* Christian  Krieg, TU Wien
* Rajit Manohar, Yale University
* Guillem Lopez Paradis, Barcelona Supercomputing Center
* Frans Skarman, Linköping University
* Matt Venn, YosysHQ, TinyTapeout



# Previous WOSETs

[WOSET 2018](WOSET2018.md)

[WOSET 2019](WOSET2019.md)

[WOSET 2020](WOSET2020.md)

[WOSET 2021](WOSET2021.md)

[WOSET 2022](WOSET2022.md)

WOSET 2023 - No workshop
",0,0.6,0.6,,,,,,0,15,,,
12764860,MDEwOlJlcG9zaXRvcnkxMjc2NDg2MA==,MuSiC2,ding-lab/MuSiC2,0,ding-lab,https://github.com/ding-lab/MuSiC2,identifying mutational significance in cancer genomes,0,2013-09-11 18:59:28+00:00,2024-10-06 15:19:16+00:00,2022-11-16 23:21:49+00:00,,46418,60,60,Perl,1,1,1,1,0,0,20,0,0,8,mit,1,0,0,public,20,8,60,master,1,"['Beifang', 'mwyczalkowski', 'AdamDS', 'ckandoth']",1,"MuSiC2
===========
Mutational Significance in Cancer (Cancer Mutation Analysis) version 2.

Usage
-----

    Program:     music2 - Mutational Significance in Cancer (Cancer Mutation Analysis) version 2.
    Version:     V0.2
    Author:      Beifang Niu && Matthew Wyczalkowski

    Usage:  music2 <command> [options]

Key commands:

    bmr                    ...  Calculate gene coverages and background mutation rates.
    smg                         Identify significantly mutated genes.
    long-gene-filter            Find conditions for which significance status is no longer related to gene size. 
    survival                    Create survival plots and P-values for clinical and mutational phenotypes.  
    clinical-correlation        Correlate phenotypic traits against mutated genes, or against individual variants.
    cosmic                      Match a list of variants to those in COSMIC, and highlight druggable targets.
    cosmic-omim                 Compare the amino acid changes of supplied mutations to COSMIC and OMIM databases.
    dendrix                     Discovery of mutated driver pathways in cancer using only mutation data. 
    dendri-permutation     ...  Run the permutation test for Dendrix. 
    mutation-relation           Identify relationships of mutation concurrency or mutual exclusivity in genes across cases.
    path-scan                   Find signifcantly mutated pathways in a cohort given a list of somatic mutations.
    pfam                        Add Pfam annotation to a MAF file.
    proximity                   Perform a proximity analysis on a list of mutations.
    proximity-window            Perform a sliding window proximity analysis on a list of mutations.
    
    help      this message


Install (Ubuntu & CentOS)
-------
Note: We provided binaries for joinx, samtools, calcRoiCovg and bedtools in /bin dir, and which were compiled on CentOS, and tested on CentOS/Ubuntu.

Prerequisites for Ubuntu:

        sudo apt-get install build-essential \
        git \
        cmake \
        curl \
        cpanminus
        libbz2-dev \
        libgtest-dev \
        libbam-dev \
        zlib1g-dev 

Prerequisites for CentOS:

        sudo yum install yum-utils
        sudo yum install curl
        sudo yum install git
        sudo yum install cmake
        sudo yum groupinstall ""Development Tools""
        sudo yum update -y nss curl libcurl
        sudo yum install perl-devel
        sudo yum install perl-CPAN
        sudo yum install bzip2-libs
        sudo yum install zlib-devel
        sudo curl -L http://cpanmin.us | perl - --sudo App::cpanminus


Change C++11 compiler for CentOS (required for joinx installation)

   Reference 
> https://www.softwarecollections.org/en/scls/rhscl/devtoolset-3/ 

    1. Install a package with repository for your system:
    On CentOS, install package centos-release-scl available in CentOS repository:
        $ sudo yum install centos-release-scl
    On RHEL, enable RHSCL repository for you system:
        $ sudo yum-config-manager --enable rhel-server-rhscl-7-rpms
    2. Install the collection:
        $ sudo yum install devtoolset-3
    3. Start using software collections:
        $ scl enable devtoolset-3 bash
    Set env variables --optional
        CC=gcc CXX=g++ 

Install samtools ( Download the samtools-0.1.19 from SOURCEFORGE (http://sourceforge.net/projects/samtools/files/samtools/0.1.19) )

        tar jxf samtools-0.1.19.tar.bz2
        cd samtools-0.1.19
        make
        export SAMTOOLS_DIR=$PWD
        sudo mv samtools /usr/local/bin/

Install calcRoiCovg 

        git clone https://github.com/Beifang/calcRoiCovg.git
        cd calc-roi-covg
        make
        sudo mv calcRoiCovg /usr/local/bin/

Install bedtools 

        wget https://github.com/arq5x/bedtools2/archive/v2.27.1.tar.gz
        tar -zxvf v2.27.1.tar.gz
        cd bedtools2-2.27.1/
        make
        sudo mv ./bin /usr/local/bin/

Install joinx 

        git clone --recursive https://github.com/genome/joinx.git
        cd joinx
        mkdir build
        cd build
        cmake ..
        make deps
        make
        sudo make install

Fix joinx bugs

        StreamLineSource.cpp
        bool StreamLineSource::getline(std::string& line) {
            std::getline(_in, line);
            return true;
        }

Intall Perl modules

        sudo cpanm Test::Most 
        sudo cpanm Statistics::Descriptive
        sudo cpanm Statistics::Distributions
        sudo cpanm Bit::Vector

Install MuSiC2 package
        
        git clone https://github.com/ding-lab/MuSiC2
        cd MuSiC2
        sudo cpanm MuSiC2-#.#.tar.gz

Notes: Python is needed to be installed if you run music2 dendrix & dendrix-permutation 


example
-------

1. smg test example:

Make a dir for MuSiC2 smg running

        mkdir music2_smg_running
        cd music2_smg_running

Make subdirs where all the runtime logs can be written

        mkdir logs
        mkdir logs/calc_covg
 
Get calculate coverage command list

        music2 bmr calc-covg --roi-file ./example/smg/example.roi_file --reference-sequence /reference_dir/ucsc.hg19.fa --bam-list ./example/smg/example.bam_list --output-dir . --cmd-list-file example.run-coverage-command

Run roi coverage for each sample

        bash example.run-coverage-command

Run bmr calc-covg again to get gene coverage

        music2 bmr calc-covg --roi-file ./example/smg/example.roi_file --reference-sequence /reference_dir/ucsc.hg19.fa --bam-list ./example/smg/example.bam_list --output-dir .

Run calc-bmr to measure overall and per-gene mutation rates. Give it extra memory, because it may need it

        music2 bmr calc-bmr --roi-file ./example/smg/example.roi_file --reference-sequence /reference_dir/ucsc.hg19.fa --bam-list ./example/smg/example.bam_list --maf-file ./example/smg/example.input.maf --output-dir . --show-skipped

Run SMG test using an FDR threshold appropriate for these mutation rates

        music2 smg --gene-mr-file gene_mrs --output-file smgs --max-fdr 0.05 --processors 1

2. dendrix example:

Runs the MCMC for 1000000 iterations, sampling sets of size 3 every 1000
iterations. Produces two files  (since 1 experiment is run):
        
        music2 dendrix --mutations-file example/dendrix/mutation_matrix --set-size 3 --minimum-freq 1 \
            --number-interations 1000000 --analyzed-genes-file example/dendrix/analyzed_genes \
            --number-experiments 1 --step-length 1000

If you want to compute the p-value for the second set having weight 47, you can run:
    
        music2 dendrix-permutation --mutations-file example/dendrix/mutation_matrix --set-size 3 --minimum-freq 1 \
            --number-interations 1000000 --analyzed-genes-file example/dendrix/analyzed_genes \
            --number-permutations 100 --value-tested 47 --rank 2

SUPPORT
-------

If you have any questions, please contact one or more of the following folks:

Beifang Niu <bniu@sccas.cn>
Li Ding <lding@wustl.edu>
",0,0.56,0.56,,,,,,0,22,,,
558725035,R_kgDOIU13qw,gf-complete,stone-rs/gf-complete,0,stone-rs,https://github.com/stone-rs/gf-complete,,0,2022-10-28 06:41:53+00:00,2022-10-28 06:42:51+00:00,2022-10-28 06:42:26+00:00,,2907,0,0,C,1,1,1,1,0,0,0,0,0,0,other,1,0,0,public,0,0,0,master,1,"['kmgreen2', 'dalgaaf', 'jannau', 'osfrickler', 'ldachary', 'animetosho', 'adisney1', 'Lack30', 'badone', 'ethanlmiller', 'gregsfortytwo', 'dx9', 'tchaikov', 'beol', 'bassam']",1,"This is GF-Complete, Revision 1.03.   January 1, 2015.

Authors: James S. Plank (University of Tennessee)
         Ethan L. Miller (UC Santa Cruz)
         Kevin M. Greenan (Box)
         Benjamin A. Arnold (University of Tennessee)
         John A. Burnum (University of Tennessee)
         Adam W. Disney (University of Tennessee,
         Allen C. McBride (University of Tennessee)

The user's manual is in the file Manual.pdf.  

The online home for GF-Complete is:

  - https://jerasure.org/jerasure/gf-complete

To compile, do:

   ./configure
   make
   sudo make install
",1,0.75,0.75,,,,,,0,1,,,
371372482,MDEwOlJlcG9zaXRvcnkzNzEzNzI0ODI=,UCSC.github.io,Dushanee/UCSC.github.io,0,Dushanee,https://github.com/Dushanee/UCSC.github.io,website for ucsc,0,2021-05-27 12:52:44+00:00,2021-05-27 13:16:22+00:00,2021-05-27 13:16:20+00:00,https://dushanee.github.io/UCSC.github.io/,16,0,0,HTML,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['Dushanee'],,"# UCSC.github.io
website for ucsc
",0,0.89,0.89,,,,,,0,1,,,
112232905,MDEwOlJlcG9zaXRvcnkxMTIyMzI5MDU=,ucsc-library-digital-collections,UCSCLibrary/ucsc-library-digital-collections,0,UCSCLibrary,https://github.com/UCSCLibrary/ucsc-library-digital-collections,A rails app based on Hyrax to be used as a repository for UCSC library digital collections.,0,2017-11-27 18:30:59+00:00,2023-04-11 15:21:49+00:00,2024-01-10 22:20:52+00:00,,12332,1,1,JavaScript,1,1,1,1,0,0,2,0,0,117,,1,0,0,public,2,117,1,master,1,"['NedHenry', 'snehagunduraoUL', 'bkiahstroud', 'rschwab', 'summer-cook', 'JzGo', 'dependabot[bot]', 'orangewolf', 'rmjaffe']",1,"# UCSC Library Digital Collections

This Rails application is based on the Hyrax gem as part of the Samvera project. With Solr and Fedora it is meant to be used as a repository and access point for some of UCSC's digital collections. 
A basic Hyrax installation has been customized with our own metadata schema, styling, and some features that are specific to our instutition. We have developed our own batch ingest widget, and we integrate our samvera-hls gem for audiovisual transcoding and streaming. 

This project is under heavy development.

## Stack Car Development Setup

### Prerequisites
- Docker

## Setting up your development environment

### Clone repositories and set up directory structure

This project relies on a specific directory structure in order for it to reliably spin up.

```bash
# Make a directory for UCSC project
> mkdir digital-collections && cd digital-collections
# Clone the digital collections repo and put it in a `hyrax` directory
> git clone git@github.com:UCSCLibrary/ucsc-library-digital-collections.git hyrax

# Clone the rest of the dependencies and place into respective folders
> git clone git@github.com:UCSCLibrary/scoobysnacks.git scooby_snacks
> git clone git@github.com:UCSCLibrary/samvera_hls.git samvera_hls

# Install and unzip fits library (Samvera requirement)
> wget https://github.com/harvard-lts/fits/releases/download/1.5.0/fits-1.5.0.zip # You may need to install wget (`brew update && brew install wget`)
> mkdir fits
> unzip fits-1.5.0.zip -d fits
> rm fits-1.5.0.zip

# Create additional asset folders
> mkdir dams_ingest
> mkdir dams_derivatives

# Start the application using Stack Car
> cd hyrax; sc up
# or
> cd hyrax/stack_car; docker-compose up
```

It may take a few minutes for the app to start up. When the **hycruz** logs '`Listening on tcp://0.0.0.0:3000`', navigate to `http://localhost:3000` in your browser to view the site.

### Edit private configuration files
All configuration is done in .env.development.  Currently defaults can be found in .env, but items in .env.development can be used to override these values.

## Development
Once your project directories and remotes are set up, you are set to develop. The `hyrax` directory contains project code that you will edit and commit

**Workflows**

Here is the general workflow you'll start with. (Please edit this when/if requirements change or a better workflow is determined)

- Make sure sandbox is up-to-date: `git checkout sandbox; git fetch; git pull`
- Make a new branch from sandbox using the ticket number, ex: `git checkout -b 123-bug-fix`
- Make your changes and commit.
- `git push origin <branch-name> ` to push your new branch to github. 
- Create a pull request detailing the changes and link back to the ticket it resolves.
- Assign someone for code review.

## Log in to a repl on the dev site
If you need a repl on the dev site, first log in to the webapp container: `docker exec -it hycruz bash`. Then you can just enter `repl` to activate a shortcut I created to set the bundle parameters correctly and start the repl.
",1,0.92,0.92,,,,Directory exists,,0,5,,,
50697712,MDEwOlJlcG9zaXRvcnk1MDY5NzcxMg==,MyGeoCruz,kercoffm/MyGeoCruz,0,kercoffm,https://github.com/kercoffm/MyGeoCruz,An interactive website that lets UCSC students know what's going on on campus and where cool places are at UCSC.,0,2016-01-29 23:12:33+00:00,2016-01-31 00:47:46+00:00,2016-01-31 19:59:40+00:00,,286,2,2,HTML,1,1,1,1,1,0,1,0,0,0,,1,0,0,public,1,0,2,gh-pages,1,"['ksuhr1', 'Emonemo']",,,1,0.8,0.8,,,,,,0,3,,,
134649685,MDEwOlJlcG9zaXRvcnkxMzQ2NDk2ODU=,racets,fordsec/racets,0,fordsec,https://github.com/fordsec/racets,Faceted execution in Racket,0,2018-05-24 02:22:41+00:00,2024-09-18 10:26:11+00:00,2018-09-11 14:28:15+00:00,,384,12,12,TeX,1,1,1,1,0,0,3,0,0,0,,1,0,0,public,3,0,12,master,1,"['kmicinski', 'zhanpengwang888', 'thomasgilray']",1,"# Racets -- Faceted Execution in Racket

An implementation of faceted execution in the Racket programming
language. Faceted execution is a technique that dynamically tracks
information flow to ensure privileged data does not leak to
unprivileged outputs. A rough analogy might be a more powerful version
of taint tracking.

For more information about Racket, you can read the following papers:

- Multiple Facets for Dynamic Information Flow. Austin et al. (POPL
  '12) https://users.soe.ucsc.edu/~cormac/papers/popl12b.pdf
  
- Faceted Execution of Policy-Agnostic Programs. Austin et al. (PLAS
  '13) https://users.soe.ucsc.edu/~cormac/papers/plas13.pdf

You may also want to look at the
[Jeeves](https://projects.csail.mit.edu/jeeves/about.php) programming
language, which uses similar concepts to Racets.

Racets extends the Racket programming language to allow writing
programs that compute with secret values. Racets does this by
providing a `#lang` language built on top of Racket.

# Codebase

The main implemenation of the language lives in
`racets-mlang.rkt`. Many macros are provided that implement faceted
execution for Racet. `facets.rkt` contains a library of operations on
facets that are used to construct and manage facets.

# Paper

Lives in the `/paper` directory, and may be built using the Makefile
provided.

# Case study: Battleship webapp

As a case study in how Racets can be used to perform policy-agnostic
programming, we have implemented a web-based game of Battleship in
Racket. The source for this game lives in `case-study.rkt`, and is
written using Racets macros through Racket's `#lang reader`
syntax. The game uses Racket's `web-server` package, which may need to
be installed through `raco pkg install web-server`.

To run the webapp, use:

    racket case-study.rkt

The webapp has four routes:
- `/player1/<id>`: reveal's player 1's game board when `id` is
  `player1`, otherwise shows the empty board.

- `/player2strike/x,y`: makes a strike on player 2's board game at the
  coordinate `(x,y)`.

- `/player2/<id>`, `/player1strike/x,y`: do the same things for player
  2.
",0,0.71,0.71,,,,,,0,4,,,
99209799,MDEwOlJlcG9zaXRvcnk5OTIwOTc5OQ==,BITsystem,Ayliandc/BITsystem,0,Ayliandc,https://github.com/Ayliandc/BITsystem,BIT System for UCSC,0,2017-08-03 08:30:51+00:00,2017-08-03 08:30:51+00:00,2017-08-03 08:30:52+00:00,,14,0,0,,1,1,1,1,0,0,0,0,0,0,gpl-3.0,1,0,0,public,0,0,0,master,1,['Ayliandc'],,,0,0.62,0.62,,,,,,0,0,,,
280043799,MDEwOlJlcG9zaXRvcnkyODAwNDM3OTk=,Tidal-Stripping-Of-Two-Component-Systems,peter-santana/Tidal-Stripping-Of-Two-Component-Systems,0,peter-santana,https://github.com/peter-santana/Tidal-Stripping-Of-Two-Component-Systems,Backup for all the processes I did as a computational astrophysics fellow at the University of California - Santa Cruz,0,2020-07-16 03:32:22+00:00,2022-01-02 04:36:17+00:00,2022-01-02 04:36:14+00:00,,10200,0,0,Jupyter Notebook,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,['peter-santana'],,"# Tidal-Stripping-Of-Two-Component-Systems
Backup for all the processes I did as a computational astrophysics fellow at the University of California - Santa Cruz
I will eventually document and comment on all the processes I did, as well as add new notebooks for future simulations :)
",1,0.7,0.7,,,,,,0,1,,,
45485342,MDEwOlJlcG9zaXRvcnk0NTQ4NTM0Mg==,news-archives,ucsc/news-archives,0,ucsc,https://github.com/ucsc/news-archives,Intermediate home of static UCSC news articles from 1993-2001.,0,2015-11-03 18:03:17+00:00,2023-06-07 21:04:58+00:00,2016-02-09 21:11:48+00:00,,4560,1,1,Python,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,1,master,1,"['shleewhite', 'pguther', 'knice']",1,"# News archive: 1993-2001

Contains UC Santa Cruz press releases from 1993-2001. These news articles will eventually end up in the UCSC Newscenter website.

## Tips for development

- [Keeping your branch up-to-date with master][1]
- [Modifying Sublime Text keybindings][2] so you can convert headlines to title case from the keyboard.
- [Creating Sublime Text snippets][3] to quickly create metadata blocks at the top of new files.

[1]: http://stackoverflow.com/questions/19758915/keeping-a-branch-up-to-date-with-master#19759728
[2]: http://docs.sublimetext.info/en/latest/customization/key_bindings.html
[3]: http://www.sublimetext.info/docs/en/extensibility/snippets.html
",1,0.84,0.84,,,,,,0,9,,,
319433198,MDEwOlJlcG9zaXRvcnkzMTk0MzMxOTg=,CMPM35-Final,jaedon-lee/CMPM35-Final,0,jaedon-lee,https://github.com/jaedon-lee/CMPM35-Final,,0,2020-12-07 20:14:01+00:00,2020-12-12 03:13:48+00:00,2020-12-12 03:13:46+00:00,,105834,0,0,JavaScript,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,"['Michelle-Lytle', 'awilbert00']",,"Michelle Lytle mnlytle@ucsc.edu\
Alexa Wilbert awilbert@ucsc.edu\
Jaedon Lee jlee563@ucsc.edu

"" The Neko Stop-off "" - Hand-painted Diorama https://creativecommons.org/licenses/by/4.0/

![project3Screenshot](https://github.com/jaedon-lee/CMPM35-Final/blob/master/project3Screenshot.png?raw=true)
",1,0.97,0.97,,,,,,0,1,,,
768220031,R_kgDOLcobfw,file-extension-testing,meri-e/file-extension-testing,0,meri-e,https://github.com/meri-e/file-extension-testing,,0,2024-03-06 17:27:24+00:00,2024-03-06 17:27:24+00:00,2024-03-06 17:59:48+00:00,,58287,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['meri-e'],,"# file-extension-testing


**Most Used file formats**
Explore a wide range of common file formats and learn how to work with them effectively.

Text: This type of file contains only text without any formatting and can be opened with any text editor.
Different types of text formats include: **.doc, .docx, .rtf, .pdf, .wpd**

Image: This file type includes binary information about images and defines how the image will be stored and compressed.
Different types of Image File Format include: **.JPEG, .PNG, .GIF, .HEIF**

Audio: This type of file format stores audio data. It stores raw data in an encoded format and uses codec to perform compression and decompression.
Different types of Audio file formats include **.aac, .mp3, .wav**

Video: This type of file format contains digital video data. It performs lossy compression to store video data where audio and video are separately encoded and stored.
Different types of Video File Formats include: **.amv, .mpeg, .flv, .avi**

Program: These file formats store codes that can be run on the computer through compiling or interpreting.
Different types of Programming File Formats include: **.c, .java, .py, .js**

Compressed/Archive: These files store data in a compressed format on the computer and can be used to easily transport data between computers. These files need to be decompressed before use.
Different types of Compressed File Formats include: **.iso, .rar, .tar, .7z**

Web page: These files include information related to the website, web pages, and server. these generally include programming scripts for static or dynamic web pages. 
Different types of Web Page File Format include: **.html, .asp, .css, .xps.**



---------------------
**Text File Formats**
.txt        Plain Text        The most basic text file format, containing only ASCII characters and carriage returns to separate lines.
.rtf        Rich Text Format        A more advanced text file format that allows basic formatting like bold, italics, and font styles.
.docx        Word Open XML Document        Commonly used by Microsoft Word for storing and saving documents
.csv        Comma-Separated Values        A simple format for storing tabular data, with each row representing a data record and commas separating fields.
.doc        Word Document        Used for word processing documents stored in Microsoft Word Binary File Format
.wps        WPS Office Word Document        A proprietary document file format developed by Kingsoft Office.
.wpd        WordPerfect Document        A document file format associated with WordPerfect, a word processing software.
.msg  Message  Microsoft Outlook message format; contains email messages with formatting, attachments, and other information.
.xlsx, .pptx, etc.: Office documents

**Image File Formats**
.jpg        Joint Photographic Experts Group        A lossy compression format that is commonly used for photographs and other images with a lot of detail.
.png        Portable Network Graphics        A lossless compression format that is commonly used for images with sharp edges or text.
.webp        Web Picture Format        It Supports both lossy and lossless image compression with support of 24-bit RGB color.
.gif        Graphics Interchange Format        The limited-color format is commonly used for animations and small images.
.tif        Tagged Image File Format        High-quality format that is commonly used for professional photography and printing.
.bmp        Bitmap        An uncompressed format that is commonly used by Microsoft Windows.
.eps        Encapsulated PostScript file        A vector format that is commonly used for print graphics.

**Audio File Formats**
.mp3        MP3 Audio File        Commonly used for storing and distributing music.
.wma  Windows Media Audio Developed by Microsoft for audio compression, often used for streaming and downloading music.
.snd        Sound        A generic file extension for sound files, often associated with audio data.
.wav        WAVE Audio File        Commonly used for storing and recording audio.
.ra          RealAudio        It’s a playlist file format that is commonly used for storing and distributing playlists.
.au   Audio Used for storing audio data, commonly associated with Sun Microsystems.
.aac  Advanced Audio Coding

**Video File Formats**
.mp4        MPEG-4 Video File        Multimedia container format that commonly stores video and audio data.
.3gp        3GPP Multimedia File        Multimedia container format that is commonly used for mobile phones.
.avi        Audio Video Interleave File        An older multimedia container format that is still supported by many devices.
.mpg        MPEG Video File        Older video compression format that is still supported by some devices.
.mov        Apple QuickTime Movie        The format that is commonly used by Apple devices.
.wmv        Windows Media Video File         The format that is commonly used by Microsoft devices.

**Program File Formats**
.c        C/C++ Source Code File        General-purpose programming language developed by Dennis Ritchie at Bell Labs between 1969 and 1972.
.cpp        C++ source Code File        A general-purpose programming language developed by Bjarne Stroustrup as an extension to the C programming language.
.java        Java Source Code File        Programming language created by Sun Microsystems that is now owned by Oracle Corporation.
.py        Python script         The programming language was developed by Guido van Rossum and first released in 1991.
.js        Javascript        A scripting language that is primarily used to add interactivity to web pages.
.ts        TypeScript        A superset of JavaScript that adds optional static typing.
.cs        C# Ssource Code File        A programming language developed by Microsoft as part of the .NET framework.
.swift        Swift Source Code File        Programming language developed by Apple for developing iOS, macOS, watchOS, tvOS, and Linux applications
.dta        Document Type Definition File        A data storage format commonly used by Stata, a statistical software program.
.pl        Perl Script        A programming language developed by Larry Wall at the University of California, Santa Cruz in the early 1980s.
.sh        Bash Shell Script        A shell scripting language commonly used to automate tasks on Unix-like operating systems
.bat Batch file Batch file format used to automate tasks on Windows systems; contains a series of commands to be executed by the command interpreter.
.com Command file A COM file is an executable file format used for programs on older Windows systems. COM files have limited functionality compared to modern formats.
.exe Executable file An executable file is a type of computer file that contains compiled code that can be run directly by the operating system. Executable files are commonly used to run programs.

**Compressed/Archive File Formats**
.rar        WinRAR Compressed Archive        A proprietary file archiver developed by Eugene Roshal.
.zip        Zipped File        A lossless data compression format that packages multiple files into a single archive file.
.hqx        BinHex        A Macintosh binary-to-text encoding format, often used to transfer binary files through email.
.arj        Archived by Robert Jung        A file compression format, similar to ZIP and RAR, used to compress and archive files.
.tar        Compressed Tarball File        This file archiving format groups multiple files into a single archive file.
.arc  ARC archive file An ARC file is an archive file format used for compressing and storing files. ARC is an outdated format and has been replaced by ZIP and other newer options.
.sit  StuffIt archive file A SIT file is an archive file format used on Macintosh systems. SIT is similar to ARC but is specific to Macs.
.gz   GZIP compressed file A GZ file is a file format created with gzip compression. Gzip shrinks the size of files for storage and transmission.
.z    Compressed file A Z file is a compressed file format associated with the “compress” compression program on Unix systems.
.bak, .old and other extensions indicative of backup files (for example: ~ for Emacs backup files)

**Web page File Formats**
.html        HyperText Markup Language File        HTML is the standard markup language for creating web pages.
.htm HyperText Markup Language File Hypertext Markup Language (HTML) document format with the less common file extension; identical to .html files. 
.xhtml        Extensible Hypertext markup language File        This is a markup language that combines HTML with XML.
.asp        Active Server page        A web development technology that allows developers to create dynamic web pages using server-side scripting.
.css        Cascading Style Sheet        This is a style sheet language used to describe the presentation of a web page.
.aspx        Active Server Page Extended File        This allows developers to create dynamic web pages using server-side scripting in ASP.NET.
.rss        Rich Site Summary        This is a web feed format that allows users to subscribe to updates from websites.
",0,0.48,0.48,,,,,,0,1,,,
517844726,R_kgDOHt2u9g,patcdaniel,patcdaniel/patcdaniel,0,patcdaniel,https://github.com/patcdaniel/patcdaniel,,0,2022-07-25 23:02:00+00:00,2022-07-25 23:02:00+00:00,2022-07-25 23:07:54+00:00,,2,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['patcdaniel'],,"### Howdy 👋,

My name is Patrick Daniel (I use he/him pronouns). I am currently a PhD student in the Ocean Science department at the University of California Santa Cruz (🐌➖🐚). My focus is on how phytoplankton communities vary at different scales in both time and space. To do this, our research group deploys instruments that take thousands of pictures of phytoplankton every hour and we use machine learning methods to classify the images. 

<!--
**patcdaniel/patcdaniel** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- 🔭 I’m currently working on ...
- 🌱 I’m currently learning ...
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...
-->
",1,0.83,0.83,,,,,,0,1,,,
567343649,R_kgDOIdD6IQ,FedConv,UCSC-VLAA/FedConv,0,UCSC-VLAA,https://github.com/UCSC-VLAA/FedConv,"[TMLR'24] This repository includes the official implementation our paper ""FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning""",0,2022-11-17 15:35:34+00:00,2024-09-17 14:22:40+00:00,2024-04-30 15:50:17+00:00,,110,25,25,Python,1,1,1,1,0,0,0,0,0,0,mit,1,0,0,public,0,0,25,master,1,"['cihangxie', 'PerAn-X']",1,"# FedConv
* **Pytorch implementation for paper:** [FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2310.04412)
* Note that we simulate Federated Learning in one local machine for research usage, and do not involve real communication between different clients.

<div align=""center"">
  <img src=""fedconv_teaser.png""/>
</div>

## Usage
### 0. Installation

- Run `cd FedConv`
- Install the libraries listed in requirements.txt 

### 1. Prepare Dataset 

We provide the data partitions for CIFAR-10, COVID-FL, and iNaturalist datasets. 

- CIFAR-10 dataset 
    * Download the three sets of simulated data partitions from [CIFAR-10](https://drive.google.com/file/d/17Dz0u1wRqWfN9yXptTsmTe3mL6fGgIQX/view?usp=sharing)
    * Put the downloaded cifar10.npy at sub-folder ```data ```
    
- COVID-FL dataset
    * Download the data and partitions file from [COVID-FL](https://drive.google.com/file/d/1BiG30JJ7U2BT0x92DjwfPeLb-uwTHdUV/view?usp=sharing)

- iNaturalist dataset
    * Download the partition following instructions from [FedScale](https://github.com/SymbioticLab/FedScale/tree/master/benchmark/dataset/inaturalist)

### 2. Set (download) the Pretrained Models
- We provide our models pretrained from Imagenet-1k
    - [FedConv-Normal](https://drive.google.com/file/d/16sI242zjpM2grd_gmeeo4QkOEAfcRhDW/view?usp=sharing)
    - [FedConv-Invert](https://drive.google.com/file/d/1mj53LsN2_a5dRW0hNEBaKt0kaGnfg0tT/view?usp=sharing)
    - [FedConv-InvertUp](https://drive.google.com/file/d/1JIImj1r2wkgSj-a_y41ovkuh8SmkrBNf/view?usp=sharing)
- Then put the pretrained model under the sub-folder ```checkpoint```

### 3. Train Model
- Use the commands below to train models in different datasets
    - CIFAR-10: ```bash cifar_fedconv.sh```
    - COVID-FL: ```bash covid_fedconv.sh```
    - iNatualist: ```bash inat_fedconv.sh```

- All the checkpoints, results, and log files will be saved to the ```--output_dir``` folder, with the final performance saved at log_file.txt 

### 4. Trained Models Checkpoint
- We provide our models trained and validated in the COVID-FL dataset
    - [FedConv-Normal](https://drive.google.com/file/d/1p8BdYK9n8UlC8Cw6oShHr5CJElvLgPhU/view?usp=sharing)
    - [FedConv-Invert](https://drive.google.com/file/d/1AaYKJB25Bfb_-ETARR3qTZlJI9a0rOZv/view?usp=sharing)
    - [FedConv-InvertUp](https://drive.google.com/file/d/1A85XIQSTYikIhWU-J1k1ANKWR52KmblD/view?usp=sharing)

## Additional Notes
- Some important tags for training setting:  
    - ```--net_name```: name of models to run. In our works, you can choose models directly from resnet50, vit_small_patch16_224, swin_tiny_patch4_window7_224, convnext_tiny, fedconv_base, fedconv_invert, and fedconv_invertup. 
    - ```--dataset```: we provide implement of CIFAR-10 and COVID-FL in  ```main.py```, iNatualist in  ```main_select.py```
    - ```--save_model_flag```: set to True if need to save the checkpoints 
    - ```--output_dir```: the output directory where checkpoints/results/logs will be written 
    - ```--E_epoch```: local training epoch E in FL train
    - ```--max_communication_rounds```: total communication rounds, set 100 in default.
    - ```--split_type```: type of data partitions, supports [""split_1"", ""split_2"", ""split_3""] for CIFAR-10, [""real_test""] for COVID-FL and iNatualist.
    - ```--num_local_clients```: Num of local clients joined in each FL train. -1 (usage of all local clients) for CIFAR-10 and COVID-FL, 25 for iNaturalist.  

- Also refer to the ```main.py``` and ```main_select.py``` for more tags

## Acknowledgments
- This work is supported by a gift from Open Philanthropy, TPU Research Cloud Program, and Google Cloud Research Credits program.
- ResNet50, ViT, Swin-Transformer, and ConvNext implementations are based on https://github.com/rwightman/pytorch-image-models
- Our code is based on https://github.com/Liangqiong/ViT-FL-main

## Citation

```
@article{xu2024fedconv,
   title   = {FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning},
   author  = {Xu, Peiran and Wang, Zeyu and Mei, Jieru and Qu, Liangqiong and Yuille, Alan and Xie, Cihang and Zhou, Yuyin},
   journal = {TMLR},
   year    = {2024}
}
```

",1,0.7,0.7,,,,,,0,1,,,
7569490,MDEwOlJlcG9zaXRvcnk3NTY5NDkw,werescrewed,asajbel/werescrewed,0,asajbel,https://github.com/asajbel/werescrewed,Were Screwed by Blind Tiger Games for Game Design Workshop at UCSC,0,2013-01-12 00:16:34+00:00,2015-06-23 19:37:38+00:00,2013-06-14 15:27:57+00:00,,398693,2,2,Java,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,2,master,1,"['legendxmyth', 'abigpotostew', 'mrunderhill89', 'danmalear', 'FiniteZombie', 'jmakaiwi', 'wpacini', 'vitnguye']",,"werescrewed
===========

Were Screwed by Blind Tiger Games for Game Design Workshop at UCSC",1,0.7,0.7,,,,,,0,2,,,
362547020,MDEwOlJlcG9zaXRvcnkzNjI1NDcwMjA=,TE-Aid,clemgoub/TE-Aid,0,clemgoub,https://github.com/clemgoub/TE-Aid,Annotation helper tool for the manual curation of transposable element consensus sequences,0,2021-04-28 17:03:20+00:00,2025-03-03 21:27:13+00:00,2025-03-03 21:31:10+00:00,https://doi.org/10.1186/s13100-021-00259-7,2235,47,47,R,1,1,1,1,0,0,7,0,0,1,other,1,0,0,public,7,1,47,main,1,"['clemgoub', 'foriin', 'bricoletc', 'simonorozcoarias']",,"# TE+Aid [![support](https://img.shields.io/badge/support:-yes-green)]() [![publication](https://img.shields.io/badge/publication:-Mobile_DNA-blue)](https://doi.org/10.1186/s13100-021-00259-7)
<img src=https://i.imgur.com/pxxR3Ec.png width=""500"">

**TE-Aid** is a `shell`+`R` program aimed to help the manual curation of transposable elements (TE). It inputs a TE consensus sequence (fasta format) and requires a reference genome (in fasta as well). Using `R` and the `NCBI blast+ suite`, TE-Aid produces 4 figures reporting:
 1. (top left) the genomic hits with divergence to consensus
 2. (top right) the genomic coverage of the consensus
 3. (bottom left) a self dot-plot 
 4. (bottom right) a structure analysis including: TIR and LTR suggestions, open reading frames (ORFs) and TE protein hit annotation.

🗞️ TE-Aid is presented in [""A beginner’s guide to manual curation of transposable elements""](https://doi.org/10.1186/s13100-021-00259-7) by Clement Goubert, Rory J. Craig, Agustin F. Bilat, Valentina Peona, Aaron A. Vogan & Anna V. Protasio, published in Mobile DNA (2022)

<img src=https://github.com/clemgoub/TE-Aid/blob/main/Example/TE1.jpeg width=""900"">

**Pipeline overview:**

- The TE (ideally, candidate consensus sequence) is searched against the provided reference genome with `blastn` 
        - Fig 1: genomic hits (horizontal lines) are represented relative to the query (TE consensus), the y axis represent the `blastn` divergence
        - Fig 2: pileup of the genomic hits relative to position along the query (TE consensus)
- The query is then blasted against itself in order to detect micro repeats and inversions (putative TIRs, LTRs)
        - Fig 3: self dot-plot and Fig 4 (top): TIR and LTR are suggested (colored arrows)
        - Bonus: a self dot-plot with `emboss dotmatcher` is also produced in an extra file
- Putative ORFs are searched with `emboss getorf` and the peptides queried against a TE protein database (distributed with [`RepeatMasker`](https://github.com/rmhubley/RepeatMasker))
        - Fig 4: ORFs (black rectangles: + orientation; red rectangles: - orientation), TE protein hits 

The consensus size, number of fragments (hits) and full length copies (according to user-defined threshold) are automatically printed on the graph.
If any ORFs and protein hits are found, their locations relative to the consensus are printed in the `stdout`


TE-Aid has been tested on MacOSX (shell, sh, zsh) and Linux (shell, sh)
support: click the ""issues"" tab on github or [email me](mailto:goubert.clement@gmail.com)

**TE-Aid** comes from `consensus2genome` that is now deprecated

## Version and branches

TE+Aid is a fully open software and is being integrated in a growing number of projects (thank you! ❤️). In order to track project-specific modifications of the base code, I have created specific branches based on the pull requests of developpers. Do not hesitate to check them out!

The main branch may not includes all these modifications, but I am happy to consider any request to modify the main branch. If you think your changes should make it to the main branch but are only available in a parallel branch, please let me know, and when time allows, I'll be happy to review and merge!

## Install

### Dependencies

- [R (Rscript)](https://cran.r-project.org/mirrors.html)
  - Biostrings
  - Rcpp (when using -r option)
- [NCBI Blast+ suite](https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/)
- [EMBOSS `getorf`](http://emboss.sourceforge.net/download/)

TE-Aid calls **NCBI blast** and **R** from the command line with `blastn`, `blastp`, `makeblastdb` and `Rscript` commands. All these executables must be accessible in the user path (usually the case following the default install). You can also set up a conda environment specifically for TE-Aid (see below).
If not, you need to locate the executables' location and add them to your local path before using TE-Aid.
For instance: 
```
export PATH=""/path/to/blast/bins/folder/:$PATH""` 
export PATH=""/path/to/R/bins/folder/:$PATH""` 
```
These lines can be added to the user `~/.bashrc` (Linux) or `~/.zshrc` (macOS) to add these programs permanently to `$PATH`.

### Install **TE-Aid** from github
```
git clone https://github.com/clemgoub/TE-Aid.git
```

### Setting a conda environment with all dependencies

You can set a conda environment for running TE-Aid after you cloned the repository with this command (use [mamba](https://anaconda.org/conda-forge/mamba) instead of conda because it's way faster):
```
cd TE-Aid
mamba env create -f TE_AID.yml
```
After that, you'll have all the dependencies ready once you activate the environment:
```
mamba activate TE_AID
```

## Usage and options

### Minimal command line

```
<user-path>/TE-Aid [-q|--query <query.TE.fa>] [-g|--genome <genome.fa>] [options]
```
>**Note.** replace `<user-path>` with the path of the downloaded `TE-Aid` folder.

### Mandatory arguments:
```
    -q, --query                   TE consensus (fasta file)
    -g, --genome                  Reference genome (fasta file)
```
### Optional arguments:

```
    -h, --help                    show this help message and exit
    
    -o, --output                  output folder (default ""./"")
    -t, --tables                  write features coordinates in tables (self dot-plot, ORFs and protein hits coordinates)
    -T, --all-Tables              same as -t plus write the genomic blastn table. 
                                  Warning: can be very large if your TE is highly repetitive!
    -r, --remove-redundant        remove redundant hits from genomic blastn table and a title of the first plot
    
    -e, --e-value                 genome blastn: e-value threshold to keep hit (default: 10e-8)
    -f, --full-length-threshold   genome blastn: min. proportion (hit_size)/(consensus_size) to be considered ""full length"" (0-1; default: 0.9)

    -m, --min-orf                 getorf: minimum ORF size (in bp)
    -R, --no-reverse-orfs         getorf: don't use ORFs in ther reverse complement of your sequence

    -a, --alpha                   graphical: transparency value for blastn hit (0-1; default 0.3)
    -F, --full-length-alpha       graphical: transparency value for full-length blastn hits (0-1; default 1)
    -y, --auto-y                  graphical: manual override for y lims (default: TRUE; otherwise: -y NUM)

    -D | --emboss-dotmatcher      Produce a dotplot with EMBOSS dotmatcher
```

## Tutorial

In this example we are going to analyze some transposable elements of *Drosophila melanogaster*. The consensus sequences for this tutorial are located in the `Example/` folder, and you will need to download the *D. melanogaster* reference genome (dm6). Let's go!

#### 1. Download the *D. melanogaster* genome

```shell
curl -o Example/dm6.fa.gz https://hgdownload.soe.ucsc.edu/goldenPath/dm6/bigZips/dm6.fa.gz
gunzip Example/dm6.fa.gz
```
A couple of *D. melanogaster* TE consensus sequences are present in the folder `Examples`

#### 2. Analyze the TE consensus

Let's start with Jockey, a recent **LINE** element in the *D. melanogaster* genome

```shell
./TE-Aid -q Example/Jockey_DM.fasta -g Example/dm6.fa -o ../dm6example
```
<img src=https://github.com/clemgoub/TE-Aid/blob/main/Example/Jockey.TEaid.png width=""1024"">

Next is Gypsy-2, from the **LTR** lineage

```shell
./TE-Aid -q Example/Gypsy2_DM.fasta -g Example/dm6.fa -o ../dm6example
```
<img src=https://github.com/clemgoub/TE-Aid/blob/main/Example/Gypsy2.TEaid.png width=""1024"">


",0,0.65,0.65,,,,,,0,2,,,
761249706,R_kgDOLV-_qg,nifh_amplicons_analysis,mo-morando/nifh_amplicons_analysis,0,mo-morando,https://github.com/mo-morando/nifh_amplicons_analysis,"nifh_amplicons_analysis contains the bioinformatic workflow of the analysis of the nifH database, most of which is described in (Morando, Magasin) et al. 2025. ",0,2024-02-21 14:17:47+00:00,2025-02-25 20:14:54+00:00,2025-02-13 00:12:54+00:00,,153844,0,0,R,1,1,1,1,0,0,0,0,0,0,other,1,0,0,public,0,0,0,main,1,['mo-morando'],,"# nifh_amplicons_analysis

____

***nifh_amplicons_analysis*** contains the bioinformatic workflow of the analysis of the [*nifH* database](https://figshare.com/articles/dataset/Global_biogeography_of_N_sub_2_sub_-fixing_microbes_i_i_i_nifH_i_amplicon_database_and_analytics_workflow/23795943/1?file=46033371), most of which is described in [(Morando, Magasin) et al. 2025](https://essd.copernicus.org/articles/17/393/2025/essd-17-393-2025.html). This workflow is managed by a Snakefile that creates a conda environment and then executes multiple R scripts that load and process the data before preforming the ecological and biogeochemical analysis of nifH ASVs and their metadata.

> **Note**: This project is actively maintained and will expand over time.

## Table of Contents

- [nifh\_amplicons\_analysis](#nifh_amplicons_analysis)
  - [Table of Contents](#table-of-contents)
  - [Data](#data)
  - [Workflows that generated database used in analysis](#workflows-that-generated-database-used-in-analysis)
  - [Installation](#installation)
  - [Running the analysis](#running-the-analysis)
  - [Output](#output)
  - [Troubleshooting and error handling](#troubleshooting-and-error-handling)
  - [License](#license)

Outputs are figures, tables, and data (csv) and log (txt) files that represent both work found in [(Morando, Magasin) et al. 2025](https://essd.copernicus.org/articles/17/393/2025/essd-17-393-2025.html) as well as other information providing a broader view of the [*nifH* database](https://figshare.com/articles/dataset/Global_biogeography_of_N_sub_2_sub_-fixing_microbes_i_i_i_nifH_i_amplicon_database_and_analytics_workflow/23795943/1?file=46033371).

## Data

____

The [*nifH* database](https://figshare.com/articles/dataset/Global_biogeography_of_N_sub_2_sub_-fixing_microbes_i_i_i_nifH_i_amplicon_database_and_analytics_workflow/23795943/1?file=46033371) comprises:

- Nearly all published _nifH_ amplicon MiSeq data sets that existed at the time of publication
- Two new data sets produced by the [Zehr Lab](https://www.jzehrlab.com/) at [UC Santa Cruz](https://www.ucsc.edu/). 

Click on the map for an interactive Google Map with detailed information (study names, sample IDs, collection information)

[![Map of studies used in Morando, Magasin et al. 2024](images_for_readme/Morando_Magasin_et_al_2024_studies_used.png)](https://www.google.com/maps/d/u/0/edit?mid=1OlWftvxU_o7Fy3nFsSJDcUlbEWSX_U0&usp=sharing)

## Workflows that generated database used in analysis

____

Two separate bioinformatic workflows were used to generate the data (the *nifH* database) used in this analysis, i.e., the ASV database and associated metadata.

1. [*nifH-amplicons-DADA2*](https://github.com/jdmagasin/nifH_amplicons_DADA2): nifH DADA2 pipeline that aggregated and produced the intial ASV table
2. [*nifH-ASV-workflow*](https://github.com/jdmagasin/nifH-ASV-workflow): Post-processing pipeline for quality filtering, data validation, and metadata acquisition via CMAP. **Ultimately generated the *nifH* database**

**Workflow overview**:
![Overview of DADA2 niH workflow](images_for_readme/workflow_overview.png)

**Workflow Steps**:
1. [DADA2](https://benjjneb.github.io/dada2/) ASVs were created by our [DADA2 _nifH_ pipeline](https://github.com/jdmagasin/nifH_amplicons_DADA2) (green).
2. Post-pipeline stages (lavender) executed by Makefile or Snakefile:
   - Gather ASVs from all studies
   - Filter ASVs for quality
   - Annotate ASVs
   - Download sample-colocated environmental data from [Simons Collaborative Marine Atlas Project (CMAP)](https://simonscmap.com)

The resulting _nifH_ ASV database supports future research into N<sub>2</sub>-fixing marine microbes.

> **Access the Database**:
> - [WorkspaceStartup directory](https://github.com/jdmagasin/nifH-ASV-workflow/tree/master/WorkspaceStartup):
>   - `nifH_ASV_database.tgz`
>   - R image `workspace.RData`
> - [Figshare](https://doi.org/10.6084/m9.figshare.23795943.v1)

## Installation

____

1. Clone the *nifh_amplicons_analysis* workflow repository:

```bash
git clone https://github.com/mo-morando/nifh_amplicons_analysis
cd nifh_amplicons_analysis
```

2. Install Snakemake (version 8.27.1 recommended):

The analysis can then be carried out by executing a Snakefile located in the scripts directory. This requires the installation of Snakemake. We recommend using a package manager, e.g., conda/mamba, and creating a contained environment. This ensures that the analysis runs smoothly and avoids potential conflicts with other programs and packages. 
> **Note**: This analysis workflow has been tested with Snakemake version *8.27.1* and so we suggest using this, however, newer versions may work.

To create a conda environment named *snakemake*:

```bash
conda create --name snakemake snakemake=8.27.1
conda activate snakemake
```

However, if you are already using the [*nifH-ASV-workflow*](https://github.com/jdmagasin/nifH-ASV-workflow), it contains a conda environment with Snakemake already, so the above step is not necessary. You can simply active this existing environment.

```bash
conda activate nifH_ASV_workflow
```

## Running the analysis

____

Once this environment is created and activated, the entire analysis is managed by a Snakemake that:
1. Creates a conda environment (`nifh_amplicons_analysis`)
2. Executes 8 R scripts for data analysis and the production of figures and tables
3. Closes the conda environment upon completion

Enter into the `scripts` directory and execute the Snakefile:

```bash
cd scripts
snakemake -c1 --use-conda
```

## Output

____

The workflow generates:
- Figures and tables from [(Morando, Magasin) et al. 2025](https://essd.copernicus.org/articles/17/393/2025/essd-17-393-2025.html)
- Additional figures and tables
- Log files for each process executed with detailed information

> **Note:** These log files contain potential error messages as well as detailed information regarding the processing of the data and results of interest. 
>
> In particular, *basic_sample_stats.log* and *samp_n_tax_breakdown.log* have a lot of useful information regarding stats on the samples in general, e.g., number of DNA or replicate samples, as well as breakdowns of their taxonomy supplied within its log file.

Output locations:

```bash

analysis/out_files/ # The majority of the tables (csv)
analysis/out_files/logs # All log files
analysis/out_files/plots # All figures
analysis/out_files/tables # Important tables specific to Morando, Magasin et al. 2025
```

## Troubleshooting and error handling

____

The workflow is designed to be self-contained with comprehensive error handling. For custom analyses or data changes, refer to the documentation and error messages in the log files for debugging assistance.

## License

____

[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by/4.0/)
",0,0.55,0.55,,,,,,0,1,,,
728920191,R_kgDOK3Jwfw,classification-fairness,richardho200/classification-fairness,0,richardho200,https://github.com/richardho200/classification-fairness,Using Python  to develop algorithms for improved classification fairness in Google Collab for Fall 2023 quarter,0,2023-12-08 01:44:35+00:00,2023-12-11 18:22:55+00:00,2023-12-14 03:12:24+00:00,,1304,0,0,Jupyter Notebook,1,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['richardho200'],,"# Classification Fairness on improving the prediction of Criminals’ recidivate
UC Santa Cruz-Fall 2023 Quarter Independent Study

Richard Ho UCSC CSE 2nd year Masters student

Advisor: Dr. Luca de Alfaro Professor & Department Vice Chair

Using Python libraries to develop algorithms for improved classification fairness. 
Conducted literature reviews on analysis of past research works on decision tree divergence issues. 
Implemented a decision tree algorithm on predicting recidivity to improve classification fairness. Implementated false positive prediction task accuracy and false positive divergence calculations via Google Collab.
The implementation of a decision tree algorithm is under divexplorer/Notebooks/Fall2023ResearchDivExplorer.ipynb.

Wrote a final report discussing about my research and results of my classification tree's false positive divergence

Google Collab Link for all of my implementations mentioned above:
https://colab.research.google.com/drive/1edMl3T_hwnR847_DtQAI5SyAqfCIeIIr
",1,0.69,0.69,,,,,,0,1,,,
221561900,MDEwOlJlcG9zaXRvcnkyMjE1NjE5MDA=,aaai-bowl,linqs/aaai-bowl,0,linqs,https://github.com/linqs/aaai-bowl,This contains all code and data necessary to reproduce the results of AAAI 20 paper BOWL. https://linqs.soe.ucsc.edu/node/355,0,2019-11-13 22:15:39+00:00,2019-11-13 22:35:48+00:00,2020-10-13 17:31:23+00:00,,377,0,0,Java,1,1,1,1,0,0,1,0,0,0,,1,0,0,public,1,0,0,master,1,['sriramsriniv'],1,"This folder contains all the code, model and data required to run all experiments.

File explanation:
*.psl: PSL model files. e.g. Citeseer.psl, Cora.psl, ...
*.data: data file required to run PSL. e.g. Citeseer-learn-0.data, Citeseer-eval-0.data,...
data/: folder containing all data. 
    Compilation of datasets from :
        https://github.com/stephenbach/bach-uai13-code/tree/0079c8f02d3aca58521946699afcfd6bfc1d00c6/data, and 
        https://github.com/pkouki/recsys2015
psl_code.zip: Contains PSL code found in http://psl.linqs.org/ .
psl-cli-2.2.0-SNAPSHOT.jar: Jar file compiled using the code in psl_code.zip


Running experiments:
BOWLSS learning:
To perform learning using BOWLSS one may run the following command:
./bowlSS_learn.sh Citeseer.psl Citeseer-learn-0.data Discrete

The above command will run BOWLSS and learn weights for rules in Citeseer.psl using Citeseer-learn-0.data and optimize over F1 score
Metric values that are feasible.
    Discrete : F1
    Cointinuous: MSE
    Categorical: Accuracy (Applicable only for Citeseer and Cora)
    Ranking: AUROC

Note that the fold number has to be changed to run learning on different fold data.

OUTPUT: the above mentioned command will generate inferred-predicates, bowlSS_learn.log, and Citeseer-learned.psl

TO learn using other approaches similarly replace the .sh file:
    BOWLOS: ./bowlOS_learn.sh
    MLE: ./MLE.sh
    MPLE: ./MPLE.sh

Evaluation :
./inference.sh Citeseer-learned.psl Citeseer-eval-0.data Discrete

Will run evaluation on Citeseer-eval-0.data and produce run_eval.out which will contain the F1 metric. 
",1,0.82,0.82,,,,,,0,5,,,
573217027,R_kgDOIiqZAw,riscv-easy-as-pi,psherman42/riscv-easy-as-pi,0,psherman42,https://github.com/psherman42/riscv-easy-as-pi,Getting started with RISC-V,0,2022-12-02 00:21:59+00:00,2025-02-18 02:42:09+00:00,2023-06-04 22:14:17+00:00,,2179,11,11,C,1,1,1,1,0,0,0,0,0,1,other,1,0,0,public,0,1,11,main,1,['psherman42'],,"# RISC Five As Easy As PI

RISC-V is the new thing on the block. Here we show how to build up the lemonade stand, using only those everyday things you can find around home. We go through the basic process of compiling, assembling, linking, and loading; describe the basics how JTAG works (and how it fails); and do it all from the comfort of one's Pi that happens to be lying around collecting dust. Reduced Instruction Set Computing is a simple thing, deserving of the reduced development platform. RISC-V (RV32I) has only 47 instructions, 32 registers, and about 236 pages of reading material. By comparison, ARM-32 has 50 instructions with over 500 distinct opcodes, only 16 registers, and about 2,736 pages of reading material. Lastly, x86-32 has 81 instructions, only 8 registers, and about 2,198 pages of reading material. The basic knowledge learned from this presentation will serve both the data storage professional and the computer enthusiast  well for many years - and many variants - to come.

The novice and budding hardware enthusiast, who wishes to get their hands dirty and their feet wet, yet has only a few pennies in their pocket with which to spend, will enjoy this beginner-level presentation. Fundamental basics are covered in a fun and simple way using vernacular and practices of modern technology. By the end of this talk, everyone will be able to blink a light and display [store and retrieve] ""Hello, World"".

This presentation will show and remind all of us just what are the basic blocks and steps necessary for any development endeavor, in a simple and easy to follow manner. All knowledge and tips are completely self-contained, without reliance on any fancy tool or third party product. Thus, the reader gains fundamental knowledge which will be transferrable in timeless fashion for many years to come.

Paul Sherman is a computer engineer in the Silicon Valley with concerns for the problems of the people of the world. He has played an active role in the evolution of the data storage industry for the past 25 years, working with companies such as Western Digital and Seagate. He has published many articles with the SAS Institute user group community on statistical problems, co-authored numerous articles and books on progressive economics, and received four U.S. patents for inventions on testing and manufacture of data storage devices. He is an avid evangelist for MRAM. He earned an MS in Physics from University of California Irvine and a BA in Physics from University of California Santa Cruz.

Presented at the *Flash Memory Summit, 2022 Aug* https://flashmemorysummit.com

## Overview

- [First Things, First](#first-things-first)
- [Installing the O/S on a Raspberry PI](#installing-the-os-on-a-raspberry-pi)
  * [*Assembler, Compiler, Linker*](#assembler-compiler-linker)
  * [*Loader*](#loader)
- [Building the ""Tool Chain"" for RISC-V](#building-the-tool-chain-for-risc-v)
- [Wiring the Hardware](#wiring-the-hardware)
- [Assembling, Compiling, Linking, and Loading](#assembling-compiling-linking-and-loading)
  * [*The Makefile*](#assembling-compiling-linking---the-makefile)
  * [*Linker Script*](#linker-script)
  * [*Loader Script*](#loader-script)
  * [*Loading & Running*](#loading--running)
- [What Can Go Wrong](#what-can-go-wrong)
- [Sample Program](#sample-program)
- [Simple Terminal](#simple-terminal)
- [Linux Logic Analyzer](#linux-logic-analyzer)
- [Further Reading](#further-reading)
- [Is RISC Five as easy as Mac or PC?](#is-risc-five-as-easy-as-mac-or-pc)
- [Can I do it all with one click (or key press)?](#can-i-do-it-all-with-one-click-or-key-press)
- [Are all RISC Five Boards as easy as PI?](#are-all-risc-five-boards-as-easy-as-pi)
- [Any Other Questions or Comments?](#any-other-questions-or-comments)

## First Things, First

Low voltage supply can and/or will quickly kill an SD card, especially when it’s used in a development system (assembler, compiler, linker, loader)

Use ~**5.25 V**, 2.5A supply with good, thick 20 AWG cables, such as www.adafruit.com/product/1995

Prevents this

<img src=""https://user-images.githubusercontent.com/36460742/205295723-e8557d09-5874-4f97-ae45-d052d7470b2f.png"" width=""50"">

and this

<img src=""https://user-images.githubusercontent.com/36460742/205295642-12ecacb4-4d19-42ff-8eac-16d11a8aae36.png"" width=""300"">


## Installing the O/S on a Raspberry PI

To Clean an older SD card, if needed, using the Windows system:
> START &rarr; Run &rarr; diskpart &rarr; List Disk &rarr; Select disk x  
> &rarr; List Partition &rarr; Select partition x &rarr; Delete partition  
> &rarr; Create Partition Primary &rarr; Format fs=fat32  

Get the *Raspberry PI Imager* program from www.raspberrypi.com/software
> Choose OS &rarr; Raspberry PI OS (Other) &rarr; Raspberry PI OS Lite (32-bit)  
> &rarr; Choose STORAGE &rarr; Generic STORAGE DEVICE USB DEVICE  
> &rarr; (gear) set hostname, uid, pwd, wifi, locale as desired  
> &rarr; WRITE  

Put the newly imaged SD card into the PI, plug in the PI, and follow commands below.

`sudo rasp-config` &rarr; Localization [\*] en_US UTF-8

Edit two files using the `sudo vi` editor. Disable `bt` and `wifi` to save power, and only do so if you are using a direct connection to keyboard and ethernet.

```
/boot/cmdline.txt : console=tty1 root=... rootfstype=ext4 fsck.repair=yes
                    quiet loglevel=3 logo.nologo rootwait
```

```
/boot/config.txt : disable_splash=1
                   dtparam=audio=off
                   camera-auto-detect=0
                   enable_uart=1
                   dtoverlay=[pi3-]disable-bt
                   dtoverlay=[pi3-]disable-wifi
```

The following line prevents the start-up warning message *WiFi is currently blocked by rfkill*. Use this command if you decide to disable `bt` and `wifi`.

`sudo sed –i ‘2i\ \ \ \ \ \ \ \ exit 0’ /etc/profile.d/wifi-check.sh`

```
sudo apt-get update
sudo apt-get install autoconf automake autotools-dev curl python3 git
             libmpc-dev libmpfr-dev libgmp-dev
             gawk build-essential bison flex texinfo gperf
             libtool patchutils bc zlib1g-dev libexpat-dev
             libfdt-dev libisl-dev
sudo apt clean
sudo apt autoremove
```

For best Linux filesystem and SD flash memory card health: **DON’T** pull the plug before you `sudo shutdown now`

## Building the “Tool Chain” for RISC-V

### ***Assembler, Compiler, Linker***

Before fetching and building a fresh copy of the tool chain, it's prudent to clear out old existing files. You may want to save any locally made changes before issuing the `rm` commands.

The toolchain builds smoothly when the working directory is *not* at the same place as the source files. For this reason, do the `configure` and `make` steps *one level below*, in a separate folder.

**DO NOT** use the *many thread* `-j` option of `make`, it is too hard on the SD flash memory card.
```
sudo rm –fr /opt/riscv32
sudo rm –fr ./riscv-gnu-toolchain

git clone https://github.com/riscv/riscv-gnu-toolchain
cd riscv-gnu-toolchain
mkdir x-rv32imac-ilp32
cd x-rv32imac-ilp32
../configure –prefix=/opt/riscv32 --enable-languages=c,c++
                                  --with-arch=rv32imac
                                  --with-abi=ilp32
sudo make
export RISCV=/opt/riscv32
export PATH=$PATH:$RISCV/bin
```

The toolchain builds in the following sequence: `binutils` &rarr; `gcc` &rarr; `newlib` &rarr; `gdb`

### ***Loader***

As above, clear out old existing files from previous loader builds.  Remember to save any locally made changes before issuing the `rm` commands.

Since the OpenOCD make file internally manages the installation process, it is not necessary to explicitly *export* any environment variables when building the loader portion of the tool chain.

```
sudo apt-get install libusb-1.0-0 libusb-1.0-0-dev
sudo rm –fr ./openocd

git clone git://git.code.sf.net/p/openocd/code openocd
cd openocd
./bootstrap
./configure –prefix=/opt/openocd --enable-bcm2835gpio --enable-sysfsgpio
make
sudo make install
```

If all goes well, you can test your shiny new tool chain versions like so:

```
riscv32-unknown-elf-gcc --version   <== should show something like 11.1.0
riscv32-unknown-as --version                                       2.38
riscv32-unknown-ld --version                                       2.38
riscv32-unknown-gdb --version                                      10.1
openocd --version                                                  0.11.0
```

## Wiring the Hardware

It takes only a few wires to connect a PI to a RISC-V chip. Throw in a few more if you wish to use serial or parallel I/O pins like the UART, SPI, I2C, or GPIO ports.
                                            
Oh, and please don't forget one wire for signal ground.

|  RPi    |               | LoFive-R1          |
|    ---: |     :---:     | :---               |
| GPIO  6 | 31 -------  5 | TCK                <td rowspan=""6"">JTAG connections</td> |
| GPIO 13 | 33 -------  7 | TMS                |
| GPIO 26 | 37 -------  8 | TDI                |
| GPIO  5 | 29 -------  4 | TDO                |
| GPIO 12 | 32 -------  6 | SRST               |
|     GND |  39 ===== 28  | GND                |
| UART TX | 8  ------- 20 | UART0.RX (GPIO 17) <td rowspan=""2"">UART serial port</td> |
| UART RX | 10 ------- 21 | UART0.TX (GPIO 16) |
| GPIO 17 | 11 ------- 15 | SPI1.SS2 (GPIO 9)  <td rowspan=""3"">GPIO parallel port</td> |
| GPIO 27 | 13 ------- 16 | SPI1.SS3 (GPIO 10) |
| GPIO 22 | 15 ------- 17 | PWM2.1 (GPIO X)    |

***Physical pinout***

The wiring pictorial described here is specific to the LoFive-R1 board, all discussion applies equally well to any board. See the later section [*Are all RISC Five Boards as easy as PI?*](#are-all-risc-five-boards-as-easy-as-pi) for guidance on using other evaluation boards.

```
    RPi (3B+)                     LoFive-R1
+---------------+              +----------------+
|    Display    |              |  1          28 | <== note square
|               |              |  2          27 |     pads on both
|          1  2 |              |  3          26 |     pins 1 and 28
| USB      3  4 |              |  4          25 |
|          5  6 |              |  5          24 |
|          7  8 |              |  6          23 |
|          9 10 |              |  7          22 |
|         11 12 |              |  8          21 |
| HDMI    13 14 |              |  9          20 |
|         15 16 |              | 10          19 |
|         17 18 |              | 11          18 |
|         29 20 |              | 12          17 |
|         21 22 |              | 13          16 |
|         23 24 |              | 14          15 |
|         25 26 |              +----------------+
|         27 28 |
|         39 30 |
|         31 32 |
|         33 34 |
|         35 36 |
|         37 38 |
|         39 40 |
|               |
| LAN     USB   |
+---------------+
```

## Assembling, Compiling, Linking, and Loading

### ***Assembling, Compiling, Linking - the Makefile***

The MAKEFILE script `foo.mk` does not need to be changed when switching between Flash and RAM 
boot or code execution.

Notice the two places where the linker script file `foo.lds` gets used in the build process.

`foo.mk`:
```
RISCVGNU ?= riscv32-unknown-elf
AOPS = -march=riscv32imac –mabi=ilp32
COPS = -march=riscv32imac –mabi=ilp32 –Wall –O2 –nostdlib –nostartfiles –ffreestanding

start.o : start.s
       $(RISCVGNU)-as $(AOPS) start.s –o start.o

... all other ASM and C source files go here ...

main.o : main.c
       $(RISCVGNU)-gcc $(COPS) –c main.c –o main.o

foo.bin : foo.lds start.o ... main.o
       $(RISCVGNU)-ld start.o ... main.o –T foo.lds –o foo.elf –Map foo.map
       $(RISCVGNU)-objdump –D foo.elf > foo.lst
       $(RISCVGNU)-objcopy foo.elf –O ihex foo.hex
       $(RISCVGNU)-objcopy foo.elf –O binary foo.bin

clean:
       rm –f *.o
       rm –f *.elf
       rm –f *.bin
       rm –f *.lst
       rm –f *.hex
       rm –f *.map
```

Note that indented lines are with a __single tab character__, not many spaces, as standard practice for any MAKEFILE.

### ***Linker Script***

This is how to selectively load and/or boot from Flash (**ROM**) or **RAM**. It is a bit bare but should be easy to see all of the moving parts.

There are only two places to change when making the choice between Flash (**ROM**) or **RAM**: The linker script file “foo.lds” shown here, and the Loading & Running command lines, shown next.

`foo.lds`
```
OUTPUT_ARCH(“riscv”)
ENTRY( _start_ )
MEMORY`
{
    rom : ORIGIN = 0x20000000, LENGTH = 0x2000
    ram (rxa!ri) : ORIGIN = 0x80000000, LENGTH = 0x4000
}
SECTIONS
{
    .text : { *(.text*) } > ram ... or ... rom
    .rodata : { *(.rodata*) } > ram ... or ... rom
    .bss : { *(.bss*) } > ram
}
```

It is easiest to make a pair of linker script files, suffixed with `-ram` and `-rom`. That way, you don't need to keep re-editing the linker script file and risk accidentally breaking something.

### ***Loader Script***

There are two main parts here, the physical wiring connections and the logical target device definition. They are mutually exclusive, and you can keep each in its own configuration file as shown.

__Interface specification__ – How to tell OpenOCD which pins and wires of the *host system* to use.

`jtag_nums # # # #` is where you define the four connection signals: `TCK TMS TDI TDO` in that order! Note that these are gpio port numbers, *not* physical connector pin numbers. Similarly for `swd_nums # #` which defines the two connection signals `SWCLK SWDIO` in that order. The order of these signal number arguments is defined by the implementation of `jtag_nums` and `swd_nums` in the OpenOCD program. Although other choices of RPi port numbers are possible, the numbers shown here give greatest flexibility for using the other ports and their multiple functions.

`rpi-3b.cfg`:
```
adapter driver bcm2835gpio
bcm2835gpio peripheral_base 0x3f000000
bcm2835gpio speed_coeffs 97469 24
bcm2835gpio jtag_nums 6 13 26 5
bcm2835gpio swd_nums 6 13
bcm2835gpio srst_nums 12
reset_config srst_only separate srst_nogate
```

__Target specification__ – How to tell OpenOCD what kind of chip to talk to.

`fe310-g002.cfg`:
```
transport select jtag
jtag newtap riscv cpu –irlen 5 –expected-id 0x20000913
target create riscv.cpu.0 riscv –chain-position riscv.cpu
riscv.cpu.0 configure –work-area-phys 0x80000000
                      -work-area-size 0x100000
                      -work-area-backup 0
```

### ***Loading & Running***

The Load command line needs to change in two places when switching between **RAM** or Flash (**ROM**) boot, as shown by the use of the `load_image` and `verify_image` statements, and the `flash bank` and `flash write_image` commands.

The Run command line needs to change in one place when switching between **RAM** or Flash (**ROM**) boot, as shown by the target address `0x80000000` and `0x20000000`.

An encapsulation of all of the necessary steps, including physical wiring connections, logical target device definition, target device memory loading, and target device running, as well as great improvement in speed and efficiency of flashing code into ROM, is available at https://github.com/psherman42/demystifying-openocd. See the section below, *Can I do it all with one click (or key press)?* for further explanation.

__***Load to***__ **RAM**

```
sudo openocd –f rpi-3b.cfg –f fe310-g002.cfg –c “adapter speed 1000”
             –c init
             –c “reset init”
             –c “sleep 25”
             –c “riscv set_reset_timeout_sec 25”
             –c “adapter speed 2500”
             –c “load_image foo.bin 0x80000000 bin”
             –c “verify_image foo.bin 0x80000000 bin”
             –c shutdown –c exit
```

__***Load to***__ **ROM**

```
sudo openocd –f rpi-3b.cfg –f fe310-g002.cfg
             –c “flash bank spi0 fespi 0x20000000 0 0 0 riscv.cpu.0 0x10014000”
             –c “adapter speed 1000”
             –c init
             –c “reset init”
             –c “sleep 25”
             –c “riscv set_reset_timeout_sec 25”
             –c “adapter speed 2500”
             –c “flash write_image erase unlock foo.bin 0x20000000 bin”
             –c shutdown –c exit
```

__***Run from***__ **RAM**

```
sudo openocd –f rpi-3b.cfg –f fe310-g002.cfg –c “adapter speed 1000”
             –c init
             –c “reset init”
             –c “sleep 25”
             –c “adapter speed 2500”
             –c “resume 0x80000000” 
             –c shutdown –c exit
```

__***Run from***__ **ROM**

```
sudo openocd –f rpi-3b.cfg –f fe310-g002.cfg –c “adapter speed 1000”
             –c init
             –c “reset init”
             –c “sleep 25”
             –c “adapter speed 2500”
             –c “resume 0x20000000”
             –c shutdown –c exit
```

## What Can Go Wrong

**Load & Run Successful**

A message like this is usually accompanied by a beer or other celebration.

```
Info : Examined RISC-V core: found 1 harts
Info :  hart 0: XLEN=32, misa=0x40101105
```

**Load & Run Unsuccessful**

Anguish and melancholy arise when you see these. Don't despair. Both indicate the possibility of JTAG not reset, possibly due to insufficient reset pulse timing, low voltage, or noise supply lines such as from bad ground connections, and are easily remedied.

In rare cases a hard power reset of the target might be needed; see discussion of [Understanding the PRCI Clock Path](https://forums.sifive.com/t/understanding-the-prci-clock-path/5827/2) in the SiFive forums.

`Error: Fatal: Hart 0 failed to halt during examine()`

or

```
Error executing event examine-start on target riscv.cpu.0
Error: DMI operation didn't complete in 2 seconds. The target is either really slow or broken. You could increase the timeout with riscv set_command_timeout_sec.
```

## Sample Program

Demonstration for *Simple Terminal* and *Linux Logic Analyzer* following below. Send characters `F`, `M`, `S`, `f`, `m`, `s`, `?`, or `enter` in any order and watch the output in the Terminal and the Analyzer. All of the source files for an FE310 SoC are included in this repository.

`main.c`
```
#include <stdint.h> // for uint32_t
#include <stddef.h> // for size_t
#include “clock.h""  // for clock_init()
#include “uart0.h”  // for uart0_...()
#include “gpio.h”  // for gpio_...()

unsigned int x;
uint32_t clk_hz;

void main() {
    clk_hz = clock_init( PRCI_EXT_DIR );
    gpio_init();
    uart0_init( clk_hz, 115200 );
    uart0_write_string( “welcome to uart test\r\n”);
    gpio_dir( 9, GPIO_OUT );  // LoFive-R1 pin 15, signal GPIO9/SPI1.SS2, 48-QFN pin 33
    gpio_dir( 10, GPIO_OUT );  // LoFive-R1 pin 16, signal GPIO10/PWM2.0, 48-QFN pin 34
    gpio_dir( 11, GPIO_OUT );  // LoFive-R1 pin 17, signal GPIO11/PWM2.1, 48-QFN pin 35
    while(1) {
        x = uart_read();
        switch( x ) {
            case ‘F’: uart0_write_string(“Flash “); gpio_high( 9 ); break;
            case ‘f’: uart0_write_string(“flash “); gpio_low( 9 ); break;
            case ‘M’: uart0_write_string(“Memory “); gpio_high( 10 ); break;
            case ‘m’: uart0_write_string(“memory “); gpio_low( 10 ); break;
            case ‘S’: uart0_write_string(“Summit “); gpio_high( 11 ); break;
            case ‘s’: uart0_write_string(“summit “); gpio_low( 11 ); break;
            case ‘\r’: uart0_write_string(“\r\n”); break;
            default: uart0_write( (uint8_t *) &x, 1); break;
        }
    }
}
```

Build the *Sample Program* with the command `make -f uart.mk`. It will create the binary file `uart.bin` along with the object files (`*.o`), memory map file (`*.map`), assembled listing file (`*.lst`), and Intel `.hex` and `.elf` formats of the binary file.

Load the binary file into the target device with one of the *Load* commands shown above in the *Loading & Running* part of the *Assembling, Compiling, Linking, Loading* section. Replace the occurances of `foo.bin` with `uart.bin`, in the *load_image*, *verify_image*, and/or *flash write_image* portions, of course.

Instruct the target device to start running its code with one of the *Run* commands above.

## Simple Terminal

Probably one of the world's smallest terminal emulators. Run in a separate session (Alt-F2, etc) for best results.

`sudo ~/prj/boot/term.sh /dev/serial0 115200`

<img src=""https://user-images.githubusercontent.com/36460742/184531061-d63deebf-061f-41b8-8b69-95e41ea14af5.jpg"" width=""700"" alt=""Simple Terminal"">

Available at https://github.com/psherman42/simple-term

## Linux Logic Analyzer

It's neither fast nor fancy but it shows what happens and when it happens. Run in a separate session (Alt-F3, etc) for best results.

```
sudo ~/prj/boot/sense.sh --c1 17 --c2 27 --c3 22
                         --tc 17 --tp + --tm norm
                         --cl1 GPIO17 --cl2 GPIO-27 --cl3 GPIO-22
```

Where
> `c1`, `c2`, `c2` – channel GPIO pin(s)  
> `tc` – trigger channel GPIO pin  
> `tp` – trigger polarity (+ or -)  
> `tm` – trigger mode (auto or norm)  
> `cl1`, `cl2`, `cl3` – channel label(s)  

<img src=""https://user-images.githubusercontent.com/36460742/184530503-dff819aa-8683-4606-90f7-7425a1cf5a06.jpg"" width=""700"" alt=""Linux Logic Analyzer"">

Available at https://github.com/psherman42/linux-logic-analyzer

## Further Reading

**SiFive Docs** – https://www.sifive.com/documentation
> [E310 Manual](https://sifive.cdn.prismic.io/sifive/034760b5-ac6a-4b1c-911c-f4148bb2c4a5_fe310-g002-v1p5.pdf) - *programmer's reference material*  
> [E310 Datasheet](https://sifive.cdn.prismic.io/sifive/4999db8a-432f-45e4-bab2-57007eed0a43_fe310-g002-datasheet-v1p2.pdf) - *electrical and physical specifications*  
> [E31 Core Complex Manual](https://sifive.cdn.prismic.io/sifive/c29f9c69-5254-4f9a-9e18-24ea73f34e81_e31_core_complex_manual_21G2.pdf) - *complete general information*  

**SiFive Technical Discussion** - https://forums.sifive.com/u/pds  

**SiFive Hardware Design** - https://github.com/sifive/sifive-blocks  (complete set of rtl and scala files)  

**LoFive R1** – https://github.com/mwelling/lofive

**RPi** – [Connector pinout and signal descriptions (pinout.xyz)](https://pinout.xyz), [Official software (raspberrypi.com)](https://www.raspberrypi.com/software) (use Raspberry Pi OS Lite, without desktop, for best results)

**USB Adapters**: [Olimex](https://www.olimex.com/Products/ARM/JTAG/), [FTDI FT-2232](https://ftdichip.com/product-category/products/modules/?series_products=66), etc.

**Availability**: [digikey](https://www.digikey.com/en/products/filter/programmers-emulators-and-debuggers/799?s=N4IgTCBcDaIFYBcCGBzEBdAvkA), [mouser](https://www.mouser.com/c/embedded-solutions/engineering-tools/embedded-tools-accessories/programmers-processor-based/?q=jtag), etc.

## Is RISC Five as easy as Mac or PC?

**It sure is!** Use the FT(2)232 chip with any USB port.
> Mac - drivers already supported  
> PC - may need to disable the UEFI driver security check  

JTAG Reset line glitches at startup, so revise a little bit as shown below.

The setting `layout_init 0x0808 0x0a1b` shows the bug, which is a tiny 10uS glitch on nTRST at startup. Instead, the setting `layout_init 0x0b08 0x0b1b` fixes the bug, by specifying the rst lines as *outputs* with *push-pull* drive.

`ftdi.cfg`:
```
adapter driver ftdi
ftdi device_desc ""Olimex OpenOCD JTAG ARM-USB-TINY-H“
ftdi vid_pid 0x15ba 0x002a
         
#----------------- P/U –-- DIR --
#ftdi layout_init 0x0808 0x0a1b
 ftdi layout_init 0x0b08 0x0b1b
           
 ftdi layout_signal nSRST -oe 0x0200
 ftdi layout_signal nTRST -data 0x0100 -oe 0x0100
 ftdi layout_signal LED -data 0x0800
```

The `layout_init` setting words are defined by MPSSE below:

```
Sig  MPSSE  PIN    BIT P/U DIR
---  -----  ---    --- --- ---
TCK  TCK/SK ADBUS0  0   0   1
TDI  TDI/DO ADBUS1  1   0   1
TDO  TDO/DI ADBUS2  2   0   0
TMS  TMS/CS ADBUS3  3   1   1
???  GPIOL0 ADBUS4  4   0   1
.    GPIOL1 ADBUS5  5   0   0
.    GPIOL2 ADBUS6  6   0   0
.    GPIOL3 ADBUS7  7   0   0
TRST GPIOH0 ACBUS0  8   1   1  <== P/U and DIR fix the bug noted above
SRST GPIOH1 ACBUS1  9   1   1  <== P/U fixes the bug noted above
.    GPIOH2 ACBUS2  a   0   0
LED  GPIOH3 ACBUS3  b   1   1
.    GPIOH4 ACBUS4  c   0   0
.    GPIOH5 ACBUS5  d   0   0
.    GPIOH6 ACBUS6  e   0   0
.    GPIOH7 ACBUS7  f   0   0
```

## Can I do it all with one click (or key press)?

**Yes!**

`make –f foo.mk ram –tgt=LOAD`

The link step is invoked by the `-ld` command, and the load step is invoked by the (optional) `openocd` command, shown in the *ram* target below. Assembling and Compiling steps are not shown, for clarity.

`foo.mk`:
```
ram : foo.lds start.o ... main.o
       $(RISCVGNU)-ld start.o ... main.o -T foo.lds -o foo.elf -Map foo.map
       $(RISCVGNU)-objdump -D foo.elf > foo.lst
       $(RISCVGNU)-objcopy foo.elf -O ihex foo.hex
       $(RISCVGNU)-objcopy foo.elf -O binary foo.bin
ifeq ($(tgt), LOAD)
       @openocd -f interface/ftdi/olimex-arm-usb-tiny-h.cfg -f foo.cfg
                                                            -c init -c ""asic_ram_load foo“
                                                            -c shutdown -c exit
else
       @echo ""target not changed“
endif
```

Indented lines are with a __single tab character__, not many spaces.

Note the `@` symbol to run a shell command from within a makefile.

See [Demystifying OpenOCD](https://github.com/psherman42/Demystifying-OpenOCD) for more information and a full working example.

## Are all RISC Five Boards as easy as PI?

**Almost ...**

In fact, all of the tool chain make files, linker scripts, and loader scripts are the same, regardless of evaluation board of the target device. Not even the number of wires needs to change, either. Only the physical pins to where the wires connect.

What determines whether a board is *as easy as PI* is if it has externally accessible JTAG pins. Many boards do, although not all of them populate the connectors for those pins. Some boards have their own embedded USB-to-JTAG peripherals, which further complicate the process of learning RISC-V - you have to learn the specific differences of the JTAG peripheral and its required USB drivers first.

The following table can help you [Wiring the Hardware](#wiring-the-hardware) to a few of the other popular evaluation boards.

All of these evaluation boards contain the FE310 SoC and cost less than $100, while the RED-V Thing is *not* as easy as PI because it does not expose its JTAG signals.

|   _   | RPi Pin<br />(Port) |  RISC-V<br />Signal | LoFive-R1 |  RED-V Thing | HiFive 1<br />Rev B | RED-V RedBoard |
| :---: |        :---:        |       :---:         |   :---:   |    :---:     |        :---:        |     :---:      |
| JTAG  |   31<br />(GPIO6)   |        TCK          |     5     |     n/a      |         J1-4        |      J5-4      |
| JTAG  |   33<br />(GPIO13)  |        TMS          |     7     |     n/a      |         J1-2        |      J5-2      |
| JTAG  |   37<br />(GPIO26)  |        TDI          |     8     |     n/a      |         J1-8        |      J5-8      |
| JTAG  |   29<br />(GPIO5)   |        TDO          |     4     |     n/a      |         J1-6        |      J5-6      |
| JTAG  |   32<br />(GPIO12)  |        SRST         |     6     |     n/a      |         J1-10       |      J5-10     |
| GND   |   39                |        GND          |    28     |   J7-1,13    |      J1-3,5,7,9     |    J5-3,5,9    |
| UART  |    8<br />(GPIO14)  |  UART0.RX / GPIO17  |    20     |     J7-3     |         J2-2        |      JP11-1    |
| UART  |   10<br />(GPIO15)  |  UART0.TX / GPIO16  |    21     |     J7-2     |         J2-1        |      JP11-2    |
| GPIO  |   11<br />(GPIO17)  |  SPI1.SS2 / GPIO9   |    15     |     J6-6     |         J4-2        |      JP13-2    |
| GPIO  |   13<br />(GPIO27)  |  SPI1.SS3 / GPIO10  |    16     |     J6-7     |         J4-3        |      JP13-3    |
| GPIO  |   15<br />(GPIO22)  |  PWM2.1 / GPIO11    |    17     |     J6-8     |         J4-4        |      JP13-4    |
| PWR     <td colspan=""2"" align=""center"">+5V</td>   |     1     |     J6-10    |         J6-8        |      JP10-1    |
| PWR     <td colspan=""2"" align=""center"">GND</td>   |     2     |   J6-9 jmpr  |        J6-6,7       |     JP10-2,3   |
| Info    <td colspan=""2"" align=""center"">Docs</td>  | [Schem.](https://github.com/mwelling/lofive/blob/master/lofive.pdf) | [Schem.](https://cdn.sparkfun.com/assets/a/c/3/e/4/RedVThingPlus.pdf) | [Schem.](https://sifive.cdn.prismic.io/sifive/c34f4c7f-0d3a-493e-8a19-a0b18f8a4555_hifive1-b01-schematics.pdf) | [Schem.](https://cdn.sparkfun.com/assets/d/d/1/e/7/RedFive.pdf) |
| Info    <td colspan=""2"" align=""center"">Descr</td> | [Avail.](https://groupgets.com/manufacturers/qwerty-embedded-design/products/lofive-risc-v) | [Avail.](https://www.sparkfun.com/products/15799) | [Avail.](https://www.sifive.com/boards/hifive1-rev-b) | [Avail.](https://www.sparkfun.com/products/15594) |
| Info    <td colspan=""2"" align=""center"">Price</td> |   $25.00  |    $32.50    |      $65       |     $42.95     |

Of course, you might need to change your application or Sample Program, and re-`make` it, if you select a different UART or GPIO pin for your specific program function.

## Any Other Questions or Comments?

Post them to the [Issues](https://github.com/psherman42/riscv-easy-as-pi/issues) of this repo!
",1,0.57,0.57,,,,,,0,1,,,
564524015,R_kgDOIaXz7w,Group13,MintyComet/Group13,0,MintyComet,https://github.com/MintyComet/Group13,"""Tips for Surviving UCSC""",0,2022-11-10 22:39:58+00:00,2022-11-10 22:40:04+00:00,2022-12-05 20:20:56+00:00,,6169,0,0,HTML,1,1,1,1,1,0,0,0,0,0,,1,0,0,public,0,0,0,master,1,"['FuzzyShaque', 'MatthewOropeza', 'kmwashin', 'Lmorganw', 'MintyComet']",,"# Lab Template
File tempate for basic HTML, CSS, and JavaScript classes

## Basic Structure
Here is the basic structure with stub or empty files in place.

```
.
├── css
│   └── site.css
├── experiments
├── img
│   └── my_photo.jpg
├── index.html
├── js
│   └── site.js
├── lab1
│   ├── css
│   │   └── lab.css
│   ├── img
│   │   └── thing1.jpg
│   ├── index.html
│   └── js
│       └── lab.js
├── lab2
│   ├── css
│   │   └── lab.css
│   ├── img
│   │   └── thing1.jpg
│   ├── index.html
│   └── js
│       └── lab.js
├── lab3
│   ├── css
│   │   └── lab.css
│   ├── img
│   │   └── thing1.jpg
│   ├── index.html
│   └── js
│       └── lab.js
├── lab4
│   ├── css
│   │   └── lab.css
│   ├── img
│   │   └── thing1.jpg
│   ├── index.html
│   └── js
│       └── lab.js
├── lab5
│   ├── css
│   │   └── lab.css
│   ├── img
│   │   └── thing1.jpg
│   ├── index.html
│   └── js
│       └── lab.js
└── lab6
    ├── css
    │   └── lab.css
    ├── img
    │   └── thing1.jpg
    ├── index.html
    └── js
        └── lab.js
```
",1,0.69,0.69,,,,,,0,2,,,
783987189,R_kgDOLrqx9Q,UCSC-BIT-Notes,saw1993/UCSC-BIT-Notes,0,saw1993,https://github.com/saw1993/UCSC-BIT-Notes,"This repository contains UCSC BIT Notes ,Pastpapers, Syllabus and reference books",0,2024-04-09 00:54:25+00:00,2025-02-25 06:35:46+00:00,2024-08-22 01:52:35+00:00,,689729,22,22,HTML,1,1,1,1,0,0,9,0,0,0,,1,0,0,public,9,0,22,main,1,"['saw1993', 'Pooraka']",,"# Bachelor in Information Technology (BIT) Study Material Repository

Welcome to the **Notes and Past Papers** repository! This repository is dedicated to collecting and sharing educational materials, including notes, syllabus, past papers, reference books, etc., to assist students in their studies.

## Degree Information

- **Degree:** Bachelor in Information Technology (BIT)
- **Institution:** University of Colombo School of Computing

## Contents

- **Notes:** Browse through our collection of notes covering various subjects and topics.
- **Syllabus:** Access the syllabus documents to understand course outlines and requirements.
- **Past Papers:** Find past exam papers to help you prepare for upcoming assessments.
- **Reference Books:** Explore recommended reference books for additional learning resources.

## How to Contribute

We welcome contributions from the community to help expand our collection and make it more comprehensive. If you have notes, syllabus documents, past papers, reference books, or any other educational materials that you'd like to share, feel free to contribute to this repository. Here's how you can contribute:

1. **Fork the Repository:** Click the ""Fork"" button at the top-right corner of this repository to create a copy in your GitHub account.

2. **Clone the Repository:** Clone your forked repository to your local machine using the following command:

   git clone https://github.com/saw1993/UCSC-BIT-Notes.git

4. **Add Your Materials:** Add your materials to the appropriate folders within the repository.

5. **Commit Changes:** Commit your changes with descriptive commit messages.

   git push origin master

7. **Push Changes:** Push your changes to your forked repository on GitHub:

6. **Submit a Pull Request:** Go to your forked repository on GitHub and click the ""New Pull Request"" button to submit your changes for review.

## Community Guidelines

- Please ensure that all contributed materials are accurate, relevant, and appropriate for educational purposes.
- Respect copyright and intellectual property rights. Only contribute materials that you have the right to share.
- All credit goes to the respective owners of the documents. Please provide appropriate attribution when contributing materials.

Thank you for contributing to our repository and helping to make educational resources more accessible to everyone!



",0,0.51,0.51,,,,,,0,1,,,
842842870,R_kgDOMjzC9g,UCSC,gayanmadusanka123/UCSC,0,gayanmadusanka123,https://github.com/gayanmadusanka123/UCSC,,0,2024-08-15 07:59:40+00:00,2024-08-15 07:59:40+00:00,2024-08-15 07:59:40+00:00,,0,0,0,,1,1,1,1,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,[],,,-1,0.7,0.7,,,,,,0,1,,,
664800694,R_kgDOJ6ANtg,ReturnToTheTomb,chrshffmn/ReturnToTheTomb,0,chrshffmn,https://github.com/chrshffmn/ReturnToTheTomb,"This repository contains the compiled VR experience, Return to the Tomb, completed in 2020 by a team from UC Berkeley and UC Santa Cruz.",0,2023-07-10 19:24:24+00:00,2025-02-10 22:12:06+00:00,2025-02-10 22:11:59+00:00,,163673,0,0,ASP.NET,1,1,1,0,0,0,0,0,0,0,,1,0,0,public,0,0,0,main,1,['chrshffmn'],,"# Return To The Tomb

***This repository contains very large files! A downloaded zip file will require approximately 170MB of disk space. When unzipped, the contents will require 670MB of disk space!***

## Description
“Return to the Tomb” is a prototype virtual reality (VR) experience that combines a 3D reconstruction model of the ancient Egyptian necropolis of Saqqara with a photogrammetric model of the Late Period basalt inner sarcophagus of Psamtek, “chief physician” and “overseer of the Temehu (Libyan mercenaries)”. 

Directed by Dr. Rita Lucarelli (UC Berkeley) and Dr. Elaine Sullivan (UC Santa Cruz), the project team included staff and student technologists at UC Berkeley and UC Santa Cruz. Development of this version of the experience began in 2019 and ended in 2020 and was funded by a seed grant from the CITRIS and the Banatao Institute of the University of California.

The experience was built using Unity version 2018.3.14f1 for the HTC Vive headset. It has been demonstrated to run on HTC Vive, HTC Cosmos, and Meta Quest 2 headsets. See System Requirements below. The contents of this repository include only the compiled application from August 2020. If you are interested in the actual code and assets, please contact the authors.

Next Steps: Dr. Lucarelli and Dr. Sullivan are designing a new version of Return to the Tomb which will include more details about the tomb of Psamtek as well as other tombs from Saqqara. For that effort, they are pleased to welcome Dr. Matthias Lang (Universität Bonn) and Dr. Eiman Elgewely (Virginia Tech) to the team.

## How to contact us
For questions about this version of the Return to the Tomb VR experience, please contact Dr. Rita Lucarelli (rita.lucarelli at berkeley.edu).

## License
The Return to the Tomb experience is made available using the Attribution-NonCommercial-NoDerivatives 4.0 International license: [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)

## System Requirements
This version of the Return to the Tomb VR experience was developed using Unity version 2018.3.14f1 for the HTC Vive headset using Steam VR. It has also run successfully on the HTC Cosmos headset. These HTC configurations require a tethered connection to a VR-ready Windows PC. The experience also runs on the Meta Quest 2 headset using the Quest Link system to connect the headset to a VR-ready Windows PC (see this [Meta document](https://www.meta.com/help/quest/140991407990979/) for cable and PC requirements including the NVIDIA and AMD GPU cards that are compatible with Quest Link).

### Download repository from GitHub
Download this GitHub repository. In most cases, this will involve downloading a zip file of the entire repository using the ""Code"" button. See instructions [here](https://docs.github.com/en/repositories/working-with-files/using-files/downloading-source-code-archives). However, because some of the content in this repository is large, you may need to download subsets and then reassemble the contents to match the file structure in the repository. Note that the downloaded zip file will require nearly 170MB of disk space, and when unzipped, the contents will require 670MB of disk space.

To run this experience you will need the following:
### For HTC Vive and HTC Cosmos headsets
* VR-ready Windows PC with the assets in this repository stored locally
* Steam VR application and account
* With the HTC headset connected to the VR-ready Windows PC, open the folder and double-click the executable file “The Doctor.exe”. 

### For Meta Quest 2 headset (Note these instructions work for the original Quest headset as well)
* VR-ready Windows PC with the assets in this repository stored locally
* Steam VR application and account
* Quest Link cable with successful connection to the VR-ready Windows PC ([cable and PC requirements](https://www.meta.com/help/quest/140991407990979/); [setup instructions](https://www.meta.com/help/quest/509273027107091/)). This requires a Meta account and the Meta Quest Link software.
* With the Meta Quest 2 headset connected to the VR-ready Windows PC and Quest Link working correctly, open the folder and double-click the executable file “The Doctor.exe”. 

## References
Sullivan, Elaine A. and Rita Lucarelli. 2021. “From the museum back to the tomb: The virtual rejoining of a 26th Dynasty sarcophagus and its burial at Saqqara”. Abusir and Saqqara in the Year 2020, edited by Miroslav Bárta, Filip Coppens, and Jaromír Krejčí. Faculty of Arts, Charles University.

## The Team
Note that affiliations were current at the time of project completion in 2020.

### Project Direction
* Rita Lucarelli (UC Berkeley)
* Elaine Sullivan (UC Santa Cruz)
* Chris Hoffman (UC Berkeley)

### Lead Developer
* Reed Scriven (undergraduate student UC Santa Cruz)

### Staff Contributors
* Kristy Golubiewski-Davis (Librarian, UC Santa Cruz)
* Daniel Story (Librarian, UC Santa Cruz)
* Chris Cain (Technical Director of the Film and Digital Media Department, UC Santa Cruz)

### Student Contributors
* Jessica Johnson (graduate student, UC Berkeley)
* Kea Johnston (graduate student, UC Berkeley)
* Savannah Dawson (undergraduate student, UC Santa Cruz)
* Mike Lee, Nik Yerasi, Olivia Kim, Xiaoyan Kang (undergraduate students, UC Berkeley)
",1,0.82,0.82,,,,,,0,1,,,
46860810,MDEwOlJlcG9zaXRvcnk0Njg2MDgxMA==,ISOP,nghiavtr/ISOP,0,nghiavtr,https://github.com/nghiavtr/ISOP,Isoform-level expression patterns in single-cell RNA-sequencing data,0,2015-11-25 12:39:45+00:00,2023-09-22 09:32:33+00:00,2023-04-20 11:16:57+00:00,,291,11,11,R,1,1,1,1,0,0,6,0,0,1,,1,0,0,public,6,1,11,master,1,['nghiavtr'],,"# ISOP
**ISO**form-level expression **P**atterns in single-cell RNA-sequencing data

# How to install ""ISOP""
### Latest release
[Version 0.99.1](https://github.com/nghiavtr/ISOP/releases/download/v0.99.1/ISOP_0.99.1.tar.gz)
##### Install from command line:
```R
R CMD INSTALL ISOP_x.y.z.tar.gz 
```
where ISOP_x.y.z.tar.gz is one version of ISOP
##### ISOP package requires the some packages installed before using:
```R
AnnotationDbi, TxDb.Hsapiens.UCSC.hg19.knownGene, doParallel
```
### Development version from Github using R:
```R
install.packages(""devtools"")
library(""devtools"")
install_github(""nghiavtr/ISOP"")
library(""ISOP"")
```
# View vignette for user guide
```R
vignette(""ISOP"")
```
# Summary
RNA-sequencing of single-cells enables characterization of transcriptional heterogeneity in seemingly homogenous cell populations. Single-cell gene-level expression variability has been characterized by RNA-sequencing in multitudes of biological context to date, but few studies have focused on heterogeneity at isoform-level expression. 
We propose a novel method ISOform-Patterns (ISOP) [1], based on mixture modeling, to characterize the expression patterns of pairs of isoform from the same genes and determine if isoform-level expression patterns are random or signify biological effects. The method allows to investigate single-cell isoform preference and commitment, and assess heterogeneity on the level of isoform expression. It also provides a way to assess biological effects in single-cell RNA-seq data through the isoform patterns, then discover differential-pattern genes (DP genes).
# Tutorial
We introduce practical uses of the ISOP method for analyzing isoform patterns and discovering differential-pattern genes.

### Detect isoform patterns
In this section, we use a small example dataset to show a practical use of ISOP for
- Modelling expression differences of pairs of isoforms by the Gaussian mixture model
- Detecting principal isoform patterns from the isoform pairs

##### Load reference annotation (txdb) and load data for the experiment
```R
#Load libaries
library(ISOP)
library(TxDb.Hsapiens.UCSC.hg19.knownGene)
library(AnnotationDbi)

# reference annotation
txdb=TxDb.Hsapiens.UCSC.hg19.knownGene
#Load sample data
data(isoformDataSample)
```
The reference annotation (txdb) in this example is loaded from UCSC hg19. However, the txdb also can be imported from a gtf file using the R package GenomicFeatures
##### Preprocessing step
```R
#Only read counts no less than 3 are consider as expressed
isoformDataSample=ifelse(isoformDataSample <= 3,0,isoformDataSample)
isoformDataSample=isoformDataSample[which(rowSums(isoformDataSample)>0),] 

#Tranform the read counts to log scale
isoformDataSample=ifelse(isoformDataSample==0,0,log2(isoformDataSample))
```
Now, the data and reference annotation are ready. We start to detect isoform patterns.

```R
#Define the number of breaks
tbreak=round(sqrt(ncol(isoformDataSample)))

#Do mixture modelling
model.res=doMixtureModelMatrix(isoformLevel.data=isoformDataSample,txdb=txdb,tbreak=tbreak)

#Detect patterns
pattern.res=detectPatternType(dmix.list=model.res$dmix.list, nq.list=model.res$nq.list,isoformLevel.data=isoformDataSample)

#Display in a pie chart
patternTable=table(pattern.res$pattern.type)
pie(patternTable,col=colors()[c(12,11,132,131,137,136)], labels=paste(names(patternTable),""("",round(patternTable/sum(patternTable)*100,2),"" %)"", sep=""""))
```

#### Validate the non-randomness of the patterns
In order to test if isoform pair patterns are significant (non-random) we applied chi-squared goodness-of-fit test combined with a permutation-based approach. It should use 10000 permutations for the test, however due to time limit,  we use only 100 permuations in this example.
```R
#Register the number of cores for parallel computing if we apply useParallel=TRUE
#library(doParallel)
#registerDoParallel(cores=4)
set.seed(2015)

iso.pair.names=names(model.res$nq.list)
res=validateIsoformPair(iso.pair.names=iso.pair.names, isoformLevel.data=isoformDataSample,per.num=100,tbreak=tbreak,useParallel=FALSE)

#Extract p-values
PVAL.X2.list=unlist(res[names(res)==""pval""])
fdr.val=p.adjust(PVAL.X2.list,method=""BH"")

#Number of significant isoform patterns
length(which(fdr.val <= 0.05))
hist(PVAL.X2.list, breaks=10)
```

#### Discover differential-pattern (DP) genes
The sample dataset contains cells in treated group and control group that are indicated in column names of the data matrix. Next, we do differential pattern analysis through two functions:
- assignCellComp(): assigning cells to components of the mixture models to create cluster labels using maximum a posteriori probability (MAP) estimation
- getDPIP():computing the association of the cluster labels and the true group labels via Chi-squared test


In this example we also run permutation test for the DP analysis but limit only 100 permutations to get it fast.

```R
#Register the number of cores for parallel computing if we apply useParallel=TRUE
#library(doParallel)
#registerDoParallel(cores=4)
set.seed(2015)

#Extract group labels of cells
group.label=unlist(lapply(colnames(isoformDataSample), function(x) unlist(strsplit(x,""_""))[1]))

#Do DP analysis
iso.pair.names=names(model.res$nq.list)
map.res=assignCellComp(iso.pair.names,model.res$dmix.list, model.res$nq.list, isoformLevel.data=isoformDataSample)
res=getDPIP(map.res$cellCompMat.prob,group.label,usePermutation=TRUE, per.num=100,useParallel=FALSE)

#Extract DP isoform pairs from permutation-based p-values
FDR=p.adjust(res$e.PVAL, method=""BH"")
dp.pattern.id=which(FDR <= 0.05)

#Plot model-based p-values (res$t.PVAL) and permutation-based p-values (res$e.PVAL)
par(mfrow=c(1,2),mar=c(1,2,2,1)+0.2,oma=c(1,1,1,1));
hist(res$t.PVAL,breaks=10, main = ""Model-based p-values"")
hist(res$e.PVAL,breaks=10, main = ""Permutation-based p-values"")
```
We select a DP isoform pair and draw the mixture model plot and pair-line plot of the pattern.
```R
#Select the first DP pattern: dp.pattern.id[1]
pattern.name=unlist(strsplit(names(dp.pattern.id[1]),""\\^""))
#The name of pattern includes: gene name, isoform a and isoform b
cat(pattern.name)

#Get the expression difference of isoform a an isoform b
iso_a=isoformDataSample[which(rownames(isoformDataSample)==pattern.name[2]),]
iso_b=isoformDataSample[which(rownames(isoformDataSample)==pattern.name[3]),]
deltaVal=iso_a-iso_b

#Choose the best model of this isoform pair indicated by nq.list in the model.res
compNum=model.res$nq.list[[names(dp.pattern.id)[1]]]

#Extract the mixture model
mydmix=model.res$dmix.list[[names(dp.pattern.id)[1]]][[compNum]]

#Display the model and pair-line plot
par(mfrow=c(1,2),mar=c(1,2,2,1)+0.2,oma=c(1,1,1,1));
plotHistModels(deltaVal,mydmix,plot.title="""",fit.line=FALSE,lwd=4)
plotPairFeatures(iso_a,iso_b,group.label=group.label)
```
# Reference
Vu T.N, et al., (2018), ""Isoform-level gene expression patterns in single-cell RNA-sequencing data"" Bioinformatics, bty100. https://doi.org/10.1093/bioinformatics/bty100
",0,0.55,0.55,,,,,,73,4,,,
161284042,MDEwOlJlcG9zaXRvcnkxNjEyODQwNDI=,JuLI,sgilab/JuLI,0,sgilab,https://github.com/sgilab/JuLI,Junction Location Identifier (JuLI): DNA fusion detection tool for clinical sequencing.,0,2018-12-11 05:53:12+00:00,2023-10-19 03:19:04+00:00,2021-11-11 06:07:49+00:00,,46597,8,8,,1,1,1,1,0,0,5,0,0,10,gpl-3.0,1,0,0,public,5,10,8,master,1,['sgilab'],,"<p align=""center"">
  <a href=""https://pubmed.ncbi.nlm.nih.gov/31881333/"">
    <img height=""150"" src=""https://github.com/sgilab/JuLI/blob/master/JuLI_logo.png"">
  </a>
  <h1 align=""center"">Junction Location Identifier (JuLI)</h1>
  <h5 align=""center"">J Mol Diagn. 2020 Mar;22(3):304-318</h5>
</p>


Update JuLI
----------------
2019.12 V0.1.6: changed to also handle with NCBI style chromosome names (1,2..X,Y).

2020.10 V0.1.6.1: changed the errors from compatibility of data.table library.

2021.11 V0.1.6.2: fixed the bugs at merging bed files.

Installing JuLI
----------------
download https://github.com/sgilab/JuLI/raw/master/JuLI-v0.1.6.2.zip

unzip JuLI-vX.X.X.zip (type the downloaded version)

Open R (>= 3.3.1)

    install.packages('devtools') # install library for installation
    
    library(devtools) # load library
    
    setwd('/path/JuLI-vX.X.X') # set download path
    
    install('julivX.X.X') # install JuLI
  
  
Input of JuLI
----------------
Sorted BAM


Running JuLI
----------------
Open R (>= 3.3.1)

    library(julivX.X.X)

 *Function for call fusions
    
    callfusion(CaseBam='/InputPath/Input.bam',             
               TestID='TestID',           
               OutputPath='/OutputPath',     
               Thread=integer,         
               Refgene='/path/JuLI-vX.X.X/references/refGene_hg19.txt',          
               Gap='/path/JuLI-vX.X.X/references/gap_hg19.txt',
               Reference='/path/hg19.fa')

[Options]  
CaseBam: Path of case bam. If you want joint call from multiple bams, insert path of bams separated by comma (ex:CaseBam='/path/input1.bam,/path/input2.bam ...').  
ControlBam: Path of control bam.    
TestID: Name of sample. Default is a CaseBam name. If you want joint call, insert names separated by comma (ex:TestID='name1,name2 ...').  
OutputPath: Path of output. Default is a path of case bam.  
Thread: Thread number for parallel processing. Default is one thread.  
ControlPanel: Path of control panel. The panel is generated by JuLI's 'controlpanel' function.  
TargetBed: Path of target region file with BED format.  
Refgene: Path of gene information file from UCSC database. It is uploaded to JuLI's github.  
Gap: Path of gap information file from UCSC database. It is uploaded to JuLI's github.  
Reference: Path of reference fasta file.  
AnalysisType: Alignment type for analysis. Default is 'DP'. DP; discordant and proper pair, D; discordant pair only, P; proper pair only.  
AnalysisUnit: Analysis unit of each processing. Default is 1000.  
MinMappingQuality: Minimum mapping quality of analysis. Default is 0.  
SplitCutoff: Cutoff of split reads. Default is two.  
DiscordantCutoff: Cutoff of discordant reads. Default is three.  
NucDiv: Use of nucleotide diversity filter. Default is TRUE.  
SplitRatio: Split ratio cutoff of pairwise alignment. Default is 0.7.  
MatchBase: Sequence length cutoff of pairwise alignment. Default is 10bp.  
Log: Log file genereation. Default is FALSE.  


*Function for annotation fusions

    annofusion(Output='/OutputPath/TestID.txt',         
               Refgene='/path/JuLI-vX.X.X/references/Refgene_hg19.txt',            
               Cosmic='/path/JuLI-vX.X.X/references/CosmicFusionExport_V76.tsv',             
               Pfam='/path/JuLI-vX.X.X/references/Pfam-A.full.human',             
               Uniprot='/path/JuLI-vX.X.X/references/HGNC_GeneName_UniProtID_160524.txt')

[Options]  
Output: Path of callfusion output.  
Refgene: Path of gene information file from UCSC database. It is uploaded to JuLI's github.  
Cosmic: Path of cosmic data file. It is uploaded to JuLI's github.  
Pfam: Path of Pfam data file. It is uploaded to JuLI's github.  
Uniprot: Path of Uniprot data file. It is uploaded to JuLI's github.  


*Function for generation of control panel

    controlpanel(ControlBams=c('/path/control1.bam,/path/control2.bam ...'),
                 ID='ID',
                 OutputPath='/OutputPath',
                 Thread=integer)

[Options]  
ControlBams: Path of control bams separated by comma.  
ID: Name of control panel.  
OutputPath: Path of output.  
Thread: Thread number for parallel processing. Default is one thread.  



Output of JuLI
----------------
-	TestID.txt: raw output

-	TestID.annotated.txt: annotated output (strand specific event, flanking gene, frame, and cosmic)

-	TestID.annotated.gene.pdf: visualized gene-gene events with domain information

-	TestID.BamStat.txt: statistics of case bam

-	TestID.log: calling log (option)


Interpretation of JuLI
----------------
-	chrA,B: reference sequence names of each breaks

-	OriA,B: fusion orientation of breaks of each breaks

    0: 5 prime end of positive reference strand

    1: 3 prime end of positive reference strand

-	DisA,B: discordant reads count supporting each breaks

-	SplitA,B: split reads count supporting each breaks

-	Event: rearrangement type (e.g. deletion, inversion, tandem, interchromosomal_translocation)

-	GeneA,B: gene names of each breaks

-	StrGeneA,B: strand of each genes

-	Direction: fusion direction

-	Frame: prediction of reading frame status of chimeric transcripts

    Inframe: no frame change

    Outframe: frame chage

    Possible Outframe/inframe: frame prediction when breaks in exonic region

-	Cosmic: frequency of fusion events according cancer types


Test JuLI
----------------
Open R (>= 3.3.1)

    library(julivX.X.X)

    callfusion(CaseBam='/path/JuLI-master/test/JuLI_test.bam',         
               TestID='JuLI_test',           
               OutputPath='/path/JuLI-master/test',           
               Thread=1,         
               Refgene='/path/JuLI-master/references/refGene_hg19.txt',          
               Gap='/path/JuLI-master/references/gap_hg19.txt',
               Reference='/path/hg19.fa')

    annofusion(Output='/path/JuLI-master/test/JuLI_test.txt',        
               Refgene='/path/JuLI-master/references/refGene_hg19.txt',            
               Cosmic='/path/JuLI-master/references/CosmicFusionExport_V76.tsv',           
               Pfam='/path/JuLI-master/references/Pfam-A.full.human',          
               Uniprot='/path/JuLI-master/references/HGNC_GeneName_UniProtID_160524.txt')

",0,0.59,0.59,,,,,,0,1,,,